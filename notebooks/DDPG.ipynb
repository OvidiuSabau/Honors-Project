{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "import torch.autograd\n",
    "import os\n",
    "import time\n",
    "import imageio\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import morphsim as m\n",
    "from graphenvs import HalfCheetahGraphEnv\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet as p \n",
    "import pybullet \n",
    "import pybullet_envs.gym_pendulum_envs \n",
    "import pybullet_envs.gym_locomotion_envs\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "import time\n",
    "import networkx as nx\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, np.array([reward]), next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValue(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_state,\n",
    "        input_size_action,\n",
    "        hidden_sizes\n",
    "    ):\n",
    "        super(QValue, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size_action + input_size_state\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], 1))\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "class Policy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_state,\n",
    "        hidden_sizes,\n",
    "        output_size\n",
    "    ):\n",
    "        super(Policy, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size_state\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], output_size))\n",
    "        self.layers.append(nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGagent:\n",
    "    def __init__(self, env, q_hidden_sizes=[32, 64, 128, 64, 32], p_hidden_sizes=[32, 64, 128, 64, 32], actor_learning_rate=1e-3, critic_learning_rate=1e-3, gamma=0.99, tau=1e-2, max_memory_size=int(1e6)):\n",
    "        # Params\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.shape[0]\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        # Networks\n",
    "\n",
    "        self.actor = Policy(self.num_states, p_hidden_sizes, self.num_actions).to(device)\n",
    "        self.actor_target = Policy(self.num_states, p_hidden_sizes, self.num_actions).to(device)\n",
    "        self.critic = QValue(self.num_states, self.num_actions, q_hidden_sizes).to(device)\n",
    "        self.critic_target = QValue(self.num_states, self.num_actions, q_hidden_sizes).to(device)\n",
    "        \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        # Training\n",
    "        self.memory = Memory(max_memory_size)\n",
    "        self.critic_criterion  = nn.MSELoss()\n",
    "        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=actor_learning_rate)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_learning_rate)\n",
    "        lmbda = lambda epoch: 0.8\n",
    "        self.actor_lr_scheduler = optim.lr_scheduler.MultiplicativeLR(self.actor_optimizer, lr_lambda=lmbda)\n",
    "        self.critic_lr_scheduler = optim.lr_scheduler.MultiplicativeLR(self.actor_optimizer, lr_lambda=lmbda)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action = self.actor.forward(state)\n",
    "        return action.cpu().detach().numpy()\n",
    "    \n",
    "    def get_latest_lr(self):\n",
    "        return self.critic_lr_scheduler.get_last_lr()\n",
    "    \n",
    "    def update_lr(self):\n",
    "        self.critic_lr_scheduler.step()\n",
    "        self.actor_lr_scheduler.step()\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, _ = self.memory.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "    \n",
    "        # Critic loss        \n",
    "        Qvals = self.critic.forward(states, actions)\n",
    "        next_actions = self.actor_target.forward(next_states)\n",
    "        next_Q = self.critic_target.forward(next_states, next_actions.detach())\n",
    "        Qprime = rewards + self.gamma * next_Q\n",
    "        critic_loss = self.critic_criterion(Qvals, Qprime)\n",
    "\n",
    "        # Actor loss\n",
    "        policy_loss = -self.critic.forward(states, self.actor.forward(states)).mean()\n",
    "        \n",
    "        # update networks\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # update target networks \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "            \n",
    "    def save_agent_networks(self, prefix):\n",
    "        torch.save(self.actor, prefix + '-actor.pt')\n",
    "        torch.save(self.critic, prefix + '-critic.pt')\n",
    "        \n",
    "    def load_agent_networks(self, prefix):\n",
    "        self.actor = torch.load(prefix + '-actor.pt').to(device)\n",
    "        self.critic = torch.load(prefix + '-critic.pt').to(device)\n",
    "        self.actor_target = self.actor\n",
    "        self.critic_target = self.critic\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ovi/anaconda3/envs/honors-project/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.03777093e-03  5.56999840e-04 -2.44483827e-03 -1.87259568e-01\n",
      "  1.00000000e+00 -1.01141480e-03 -1.35534559e-03 -9.27744783e-04\n",
      "  6.63089668e-05 -1.91760424e-03 -3.97040334e-04 -3.12577497e-04\n",
      " -9.94791389e-02 -8.66170898e-02 -1.48267895e-01 -7.03494474e-02\n",
      " -1.36180922e-01 -7.16667175e-02 -7.55913779e-02]\n"
     ]
    }
   ],
   "source": [
    "env = HalfCheetahGraphEnv(None)\n",
    "env.set_morphology(5)\n",
    "state = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08971457  0.          0.63248918]\n"
     ]
    }
   ],
   "source": [
    "print(env._current_env._pb_env.robot.robot_body.pose().xyz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6dc0cdc12f97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5daae16e02ed>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    env._current_env.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "env = HalfCheetahGraphEnv(None)\n",
    "allAgents = dict()\n",
    "allRewards = dict()\n",
    "allAvgRewards = dict()\n",
    "for idx in [5]:\n",
    "    env.set_morphology(idx)\n",
    "    state = env.reset()\n",
    "\n",
    "    agent = DDPGagent(env, gamma=0.99, actor_learning_rate=1e-2, critic_learning_rate=1e-2)\n",
    "    batch_size = 128\n",
    "    rewards = []\n",
    "    avg_rewards = []\n",
    "    learningRates = []\n",
    "    episodeLengths = []\n",
    "    for episode in range(800):\n",
    "        t0 = time.time()\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        for i in range(1000):\n",
    "            \n",
    "            env._current_env._pb_env.camera_adjust()\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            # Only add noise in every second episode\n",
    "            if episode % 2 == 0:\n",
    "                action = np.clip(action + np.random.normal(0, 0.1, env.action_space.shape), -1, 1)\n",
    "                \n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            agent.memory.push(state, action, reward, new_state, done)\n",
    "\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.update(batch_size)\n",
    "\n",
    "            state = new_state\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            agent.update_lr()\n",
    "\n",
    "        print(\"episode {} in {}s: reward for episode: {} || average reward: {} || episode length: {}\\n\".format(episode, np.round(time.time() - t0, decimals=1), np.round(episode_reward, decimals=2), \n",
    "                                                                                                                  np.round(np.mean(rewards[-25:]), decimals=2), step))\n",
    "\n",
    "        episodeLengths.append(step)\n",
    "        rewards.append(episode_reward)\n",
    "        avg_rewards.append(np.mean(rewards[-25:]))\n",
    "        learningRates.append(agent.get_latest_lr())\n",
    "    \n",
    "#     allAgents[idx] = agent\n",
    "#     allRewards[idx] = rewards\n",
    "#     allAvgRewards[idx] = avg_rewards\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(len(rewards)), rewards)\n",
    "    ax.plot(range(len(avg_rewards)), avg_rewards)\n",
    "    ax.set(ylabel='Reward')\n",
    "    ax.legend([\"Episode\", \"Last 25 Avg\"])\n",
    "    plt.xlabel('Episode')\n",
    "    plt.suptitle('Half-Cheetah {}'.format(idx))\n",
    "    plt.show()\n",
    "    fig.savefig('{}-new-halfCheetah-Training.jpg'.format(idx))\n",
    "    \n",
    "    agent.save_agent_networks('{}-new-halfCheetah'.format(idx))\n",
    "    \n",
    "    env._current_env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "10000\n",
      "Average Rewards 240.26221017763854\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "20000\n",
      "Average Rewards 210.92435154213283\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "30000\n",
      "Average Rewards 258.60719913876693\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "40000\n",
      "Average Rewards 239.1132104626899\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "50000\n",
      "Average Rewards 243.38109145038663\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "60000\n",
      "Average Rewards 212.10179258076104\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "70000\n",
      "Average Rewards 184.1651066220892\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "80000\n",
      "Average Rewards 208.51275609783303\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "90000\n",
      "Average Rewards 195.0824582840781\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "100000\n",
      "Average Rewards 244.10145127556123\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "110000\n",
      "Average Rewards 225.14149991765635\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "120000\n",
      "Average Rewards 217.6348895183247\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "130000\n",
      "Average Rewards 219.03513042039287\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "140000\n",
      "Average Rewards 259.73810738112206\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "150000\n",
      "Average Rewards 242.48596298465554\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "160000\n",
      "Average Rewards 217.36335387292075\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "170000\n",
      "Average Rewards 230.8551450428397\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "180000\n",
      "Average Rewards 207.1706233591624\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "190000\n",
      "Average Rewards 245.9544166330372\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "200000\n",
      "Average Rewards 206.25285086618504\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "210000\n",
      "Average Rewards 236.33221651636555\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "220000\n",
      "Average Rewards 217.20698251019016\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "230000\n",
      "Average Rewards 230.00739834087395\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "240000\n",
      "Average Rewards 213.78682702323394\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "250000\n",
      "Average Rewards 220.9798103243502\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "260000\n",
      "Average Rewards 243.0361959105479\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "270000\n",
      "Average Rewards 200.01289033351497\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "280000\n",
      "Average Rewards 256.1984419313943\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "290000\n",
      "Average Rewards 228.7924843263313\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "300000\n",
      "Average Rewards 219.99258395815468\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "310000\n",
      "Average Rewards 233.6479283365153\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "320000\n",
      "Average Rewards 247.27498350542186\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "330000\n",
      "Average Rewards 224.86584862258147\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "340000\n",
      "Average Rewards 214.7464511960414\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "350000\n",
      "Average Rewards 231.6013614838208\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "360000\n",
      "Average Rewards 254.73783587331314\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "370000\n",
      "Average Rewards 244.09666690159256\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "380000\n",
      "Average Rewards 194.51668334780337\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "390000\n",
      "Average Rewards 218.31031474975882\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "400000\n",
      "Average Rewards 227.2232141185899\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "410000\n",
      "Average Rewards 253.8185425846694\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "420000\n",
      "Average Rewards 257.7825495981099\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "430000\n",
      "Average Rewards 207.2871735752509\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "440000\n",
      "Average Rewards 236.7264893432932\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "450000\n",
      "Average Rewards 204.62323069659354\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "460000\n",
      "Average Rewards 218.97592635171566\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "470000\n",
      "Average Rewards 185.1254652453655\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "480000\n",
      "Average Rewards 216.08585100446845\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "490000\n",
      "Average Rewards 235.2460362757547\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "500000\n",
      "Average Rewards 77.75088583212822\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "510000\n",
      "Average Rewards 205.57631139948515\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "520000\n",
      "Average Rewards 213.19089189763798\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "530000\n",
      "Average Rewards 215.2007785765655\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "540000\n",
      "Average Rewards 243.99544442509833\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "550000\n",
      "Average Rewards 178.75864887340026\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "560000\n",
      "Average Rewards 237.33299575078556\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "570000\n",
      "Average Rewards 217.26261606948052\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "580000\n",
      "Average Rewards 220.37611324159334\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "590000\n",
      "Average Rewards 120.94185938671936\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "600000\n",
      "Average Rewards 236.39254300567063\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "610000\n",
      "Average Rewards 235.60393201712634\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "620000\n",
      "Average Rewards 192.69023524123472\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "630000\n",
      "Average Rewards 214.59523878185726\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "640000\n",
      "Average Rewards 11.9127365916636\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "650000\n",
      "Average Rewards 221.56608056119066\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "660000\n",
      "Average Rewards 257.72043442232274\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "670000\n",
      "Average Rewards 239.32760136877025\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "680000\n",
      "Average Rewards 209.97286705771484\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "690000\n",
      "Average Rewards 218.78638059241214\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "700000\n",
      "Average Rewards 240.1620922737476\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "710000\n",
      "Average Rewards 216.59064557080944\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "720000\n",
      "Average Rewards 222.33525373410393\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "730000\n",
      "Average Rewards 239.19224392643713\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "740000\n",
      "Average Rewards 203.2344190676957\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "750000\n",
      "Average Rewards 206.74794197556747\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "760000\n",
      "Average Rewards 211.62611333888194\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "770000\n",
      "Average Rewards 222.6661772237094\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "780000\n",
      "Average Rewards 244.10581487717656\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "790000\n",
      "Average Rewards 223.51372048519326\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "800000\n",
      "Average Rewards 190.03151514220716\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "810000\n",
      "Average Rewards 225.07263362749362\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "820000\n",
      "Average Rewards 232.24702881686963\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "830000\n",
      "Average Rewards 158.3045169597713\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "840000\n",
      "Average Rewards 238.7648838469657\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "850000\n",
      "Average Rewards 243.88899888630147\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "860000\n",
      "Average Rewards 208.62966716874186\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "870000\n",
      "Average Rewards 233.64390850409757\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "880000\n",
      "Average Rewards 246.3992985524426\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "890000\n",
      "Average Rewards 231.10007474414505\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "900000\n",
      "Average Rewards 241.73046353732443\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "910000\n",
      "Average Rewards 223.29771600718362\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "920000\n",
      "Average Rewards 229.25007976324167\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "930000\n",
      "Average Rewards 200.73501461256\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "940000\n",
      "Average Rewards 248.17840585525002\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "950000\n",
      "Average Rewards 228.5078304075306\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "960000\n",
      "Average Rewards 223.14314763766512\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "970000\n",
      "Average Rewards 178.19792509860443\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "980000\n",
      "Average Rewards 242.4311457344274\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "990000\n",
      "Average Rewards 207.69951780343428\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "1000000\n",
      "Average Rewards 214.989298755692\n"
     ]
    }
   ],
   "source": [
    "for idx in [4]:\n",
    "    \n",
    "    try:\n",
    "        env._current_env.close()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    env = HalfCheetahGraphEnv(None)\n",
    "    env.set_morphology(idx)\n",
    "    \n",
    "    agent = DDPGagent(env)\n",
    "    agent.load_agent_networks('../old-run/{}-halfCheetah'.format(idx))\n",
    "    \n",
    "    episodeRewards = []\n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "    im_array, speeds = [], []\n",
    "    collectedSamples = 0\n",
    "    samplesToPrint = 10000\n",
    "    episode = 0\n",
    "    while collectedSamples < 1e6:\n",
    "        episode += 1\n",
    "        state = env.reset()\n",
    "\n",
    "        step = 0\n",
    "        episodeReward = 0\n",
    "        for i in range(1000):\n",
    "            \n",
    "#             env._current_env._pb_env.camera_adjust()\n",
    "#             im_array.append(env._current_env._pb_env.render_camera_image())\n",
    "            action = agent.get_action(state)\n",
    "            action = np.clip(action + np.random.normal(0, 0.25, env.action_space.shape), -1, 1)\n",
    "                \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episodeReward += reward\n",
    "            states.append(state.astype(np.float32))\n",
    "            actions.append(action.astype(np.float32))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state.astype(np.float32))\n",
    "            dones.append(done)\n",
    "            \n",
    "            speeds.append(state[0])\n",
    "            \n",
    "            state = next_state\n",
    "            step += 1\n",
    "            collectedSamples += 1\n",
    "            samplesToPrint -= 1\n",
    "        \n",
    "        print('hi')\n",
    "        episodeRewards.append(episodeReward)\n",
    "        \n",
    "        if samplesToPrint <= 0:\n",
    "            print(collectedSamples)\n",
    "            print('Average Rewards', np.mean(episodeRewards[-1:]))\n",
    "            samplesToPrint = 10000\n",
    "            \n",
    "    np.save(\"../datasets-old-run/{}/actions_array\".format(idx), np.array(actions))\n",
    "    np.save(\"../datasets-old-run/{}/states_array\".format(idx), np.array(states))\n",
    "    np.save(\"../datasets-old-run/{}/next_states_array\".format(idx), np.array(next_states))\n",
    "    np.save(\"../datasets-old-run/{}/rewards_array\".format(idx), np.array(rewards))\n",
    "    np.save(\"../datasets-old-run/{}/dones_array\".format(idx), np.array(dones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeds = np.array(speeds)\n",
    "im_array = (np.array(im_array) * 256).astype(np.uint8)\n",
    "rewards = np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089bcb930b1e4946b83af6287fddc6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (320, 200) to (320, 208) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, sharex=True)\n",
    "ax[0].set_title('Speeds')\n",
    "ax[1].set_title('Cumulative Rewards')\n",
    "ax[0].plot(uniform_filter1d(speeds, size=10))\n",
    "ax[1].plot(np.cumsum(rewards))\n",
    "ax[1].set_xlabel('Steps')\n",
    "ax[1].set_ylabel('Reward')\n",
    "ax[1].set_ylabel('Speed')\n",
    "fig.savefig('trained-agent-speeds.png')\n",
    "video = imageio.get_writer('trained-agent.mp4', mode='I', fps=24, codec='libx264', bitrate='16M')\n",
    "for img in im_array:\n",
    "    video.append_data(img)\n",
    "video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../datasets-old-run//{}/actions_array\".format(idx), np.array(actions))\n",
    "np.save(\"../datasets-old-run/{}/states_array\".format(idx), np.array(states))\n",
    "np.save(\"../datasets-old-run/{}/next_states_array\".format(idx), np.array(next_states))\n",
    "np.save(\"../datasets-old-run/{}/rewards_array\".format(idx), np.array(rewards))\n",
    "np.save(\"../datasets-old-run/{}/dones_array\".format(idx), np.array(dones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_array = np.array(actions)\n",
    "states_array = np.array(states)\n",
    "next_states_array = np.array(next_states)\n",
    "rewards_array = np.array(rewards)\n",
    "dones_array = np.array(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"doubleInvertedPendulumDataset/actions_array\", actions_array)\n",
    "np.save(\"doubleInvertedPendulumDataset/states_array\", states_array)\n",
    "np.save(\"doubleInvertedPendulumDataset/next_states_array\", next_states_array)\n",
    "np.save(\"doubleInvertedPendulumDataset/rewards_array\", rewards_array)\n",
    "np.save(\"doubleInvertedPendulumDataset/dones_array\", dones_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.load('../datasets/4/rewards_array.npy').reshape((1000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([184.9556964 , 181.8523796 , 193.94408341, 183.44129337,\n",
       "       180.64109176, 180.19431868, 195.05416098, 191.66075613,\n",
       "       185.23178686, 184.83808625, 189.29054343, 187.26766719,\n",
       "       172.66705911, 183.35678013, 191.58278757, 191.19228284,\n",
       "       188.60357521, 179.35838471, 185.06772218, 180.38621829,\n",
       "       198.65760625, 186.80150511, 186.49208369, 186.33933581,\n",
       "       179.75608735, 189.31464979, 180.91363922, 187.0118959 ,\n",
       "       182.96585696, 186.74660712, 174.89349945, 197.73106056,\n",
       "       185.23813966, 187.38368109, 202.80282222, 186.39710432,\n",
       "       194.81292199, 182.14079405, 188.37803969, 184.80948302,\n",
       "       182.89129084, 185.54310638, 183.39655991, 181.38017131,\n",
       "       186.82772071, 177.02382323, 181.4150698 , 191.67631171,\n",
       "       189.39870314, 177.17671724, 190.19296676, 187.40461689,\n",
       "       181.24970671, 181.92511812, 188.27552174, 172.58033548,\n",
       "       195.6184433 , 184.56487202, 181.26803971, 188.24231399,\n",
       "       186.09620526, 195.51260621, 183.98206852, 189.23722103,\n",
       "       190.36989626, 196.47518906,   1.97478073, 185.55768931,\n",
       "       171.07122289, 186.65032531, 185.11078921, 187.91223903,\n",
       "       174.34346221, 190.71554191, 190.44666156, 177.27110999,\n",
       "       175.60331125, 184.59674701, 191.80921635, 189.42134019,\n",
       "       185.24684923, 186.96297511, 183.40674819, 194.17318789,\n",
       "       185.71111564, 184.05008071, 188.93915212, 184.9727966 ,\n",
       "       187.58127284, 190.77175243, 188.26382078, 185.62630175,\n",
       "       198.8102909 , 193.11635656, 178.37580755, 185.45656282,\n",
       "       187.96947807, 187.66306143, 174.11037755, 182.47779696,\n",
       "       190.62381495, 183.84226923, 180.66114   , 186.77525582,\n",
       "       177.46131098, 183.02857697, 191.83844032, 184.4194566 ,\n",
       "       183.09573425, 179.38332917, 183.77170322, 190.02816376,\n",
       "       191.44094552, 189.90593755, 197.86683121, 191.78254027,\n",
       "       181.20868991, 191.40695719, 173.66384816, 192.06286026,\n",
       "       190.60111029, 180.87922871, 175.68317568, 191.98284813,\n",
       "       189.63184703, 183.21021437, 188.02727195, 191.76286322,\n",
       "       183.68565669, 178.46971556, 190.96443141, 182.7587713 ,\n",
       "       193.16049275, 180.81331665, 195.1649451 , 185.25978771,\n",
       "       178.55018625, 194.57192371, 188.3306315 , 190.3950806 ,\n",
       "       183.2574004 , 186.32350313, 185.11169106, 172.61806665,\n",
       "       186.03029592, 186.3332282 , 189.55312788, 193.32227102,\n",
       "       184.17868917, 191.69409714, 187.58076557, 193.43631256,\n",
       "       192.32521491, 186.84010207, 177.52788796, 190.59016456,\n",
       "       190.60872246, 189.73931631, 190.29935499, 175.88424767,\n",
       "       195.59899132, 189.33740358, 174.58254727, 182.29317029,\n",
       "       193.49772354, 192.68319089, 184.284997  , 181.73093596,\n",
       "       177.77858214, 196.28205336, 180.20311346, 186.6716344 ,\n",
       "       194.38834999, 199.80372319, 181.7159679 , 184.90027287,\n",
       "       179.99805354, 184.90623233, 194.67797023, 191.39749125,\n",
       "       183.80663216, 186.08853428, 184.45554103, 188.36384444,\n",
       "       194.93305211, 185.55344203, 192.18436302, 179.79014435,\n",
       "       191.0149058 , 198.14128779, 183.76136412, 189.03338403,\n",
       "       185.04776741, 177.69243868, 190.06882077, 184.72953599,\n",
       "       188.82601139, 186.80195503, 185.05978662, 194.84194149,\n",
       "       186.60793646, 169.65057345, 183.6344162 , 192.33752263,\n",
       "       179.11774339, 184.07893201, 180.19784037, 179.67272692,\n",
       "       182.03281243, 186.99995201, 192.86489662, 185.12170343,\n",
       "       181.94479477, 176.63933542, 184.93306807, 197.19892246,\n",
       "       195.24035842, 187.35707656, 188.03338568, 171.36083284,\n",
       "       190.63786979, 178.9177012 , 182.38623484, 189.78079811,\n",
       "       192.54305176, 197.15615819, 190.9177935 , 183.17314913,\n",
       "       185.54318685, 176.92012803, 192.09364447, 177.33661195,\n",
       "       181.00632579, 185.91283328, 184.35944988, 185.76146765,\n",
       "       174.483751  , 174.81147281, 192.87003807, 181.23367914,\n",
       "       177.01379933, 184.52033732, 192.56391291, 202.22593458,\n",
       "       188.08561999, 186.42676057, 189.95378465, 188.14157994,\n",
       "       184.1134365 , 186.85879673, 187.52206615, 182.28859944,\n",
       "       190.50062503, 183.09021406, 179.05784297, 190.6139584 ,\n",
       "       185.62353836, 186.52437051, 190.91712927, 189.11000107,\n",
       "       188.86514801, 185.78838469, 188.22426461, 188.59781297,\n",
       "       181.72667443, 182.53995278, 186.36157425, 186.08770907,\n",
       "       186.78096305, 195.15273496, 194.23052913, 171.67547843,\n",
       "       185.67585047, 180.49495139, 180.13378201, 187.57675359,\n",
       "       186.7663745 , 179.92433824, 192.23326041, 187.18505203,\n",
       "       182.42560599, 174.26677582, 180.13938642, 182.44962961,\n",
       "       189.79951238, 185.93562256, 185.3335912 , 193.06108106,\n",
       "       182.57941916, 192.82613434, 184.81987498, 184.53529267,\n",
       "       178.54720779, 193.04476037, 184.82255603, 177.31570378,\n",
       "       186.02515608, 192.43821295, 180.48198514, 185.19458387,\n",
       "       184.94120038, 191.82396891, 191.77555641, 194.02904438,\n",
       "       186.35268436, 187.37355154, 192.20414519, 192.6910072 ,\n",
       "       177.86536149, 188.04333593, 171.37311016, 181.94683316,\n",
       "       185.17558829, 175.9622567 , 184.82975801, 189.74099709,\n",
       "       188.48367689, 185.73988261, 185.90050511, 181.94345844,\n",
       "       195.90333482, 188.55806459, 185.26571018, 195.44983254,\n",
       "       187.1419035 , 191.32338018, 190.79236045, 179.26280987,\n",
       "       184.67652925, 192.99752017, 184.92788474, 183.74667703,\n",
       "       192.86651089, 183.76260868, 181.95470617, 176.3334703 ,\n",
       "       183.74791576, 190.8179693 , 189.35036658, 195.79675702,\n",
       "       193.6241753 , 191.13900727, 183.54535305, 185.8327123 ,\n",
       "       186.92239255, 180.7380263 , 193.32525777, 190.24445998,\n",
       "       178.60666857, 191.69970982, 192.2846102 , 194.52901538,\n",
       "       196.77566733, 179.9655436 , 187.96945296, 199.20803238,\n",
       "       185.54031706, 192.17271038, 185.81558659, 187.15324218,\n",
       "       183.07934478, 184.08944002, 181.51861102, 196.58953597,\n",
       "       177.47633465, 191.22314051, 188.16678124, 190.82742028,\n",
       "       176.99466301, 190.27461895, 188.78433831, 191.21539129,\n",
       "       184.20183805, 175.74257332, 186.98344008, 188.08827209,\n",
       "       181.82702988, 186.68730445, 194.72998333, 187.1787173 ,\n",
       "       189.00245886, 186.2974396 , 193.85348344, 186.69675137,\n",
       "       190.56762297, 182.93461632, 192.51488395, 191.04278871,\n",
       "       163.64617794, 186.54044388, 184.97767597, 158.21628233,\n",
       "       188.69444164, 188.54149521, 183.66743697, 190.46095306,\n",
       "       188.25839553, 187.63481534, 197.72200963, 185.91091564,\n",
       "       187.43145083, 197.71111687, 195.75279763, 183.02889629,\n",
       "       190.52194116, 182.23364625, 184.70076013, 186.82644847,\n",
       "       184.21997368, 182.66879834, 188.92550072, 200.11821382,\n",
       "       182.43339794, 194.03478317, 189.13978084, 191.35722674,\n",
       "       187.13438207, 196.61172705, 185.4636555 , 188.85871234,\n",
       "       176.17360061, 194.20721545, 190.10464448, 184.55555044,\n",
       "       195.98645741, 180.63225771, 176.26762889, 179.03818708,\n",
       "       190.51503825, 186.79454707, 185.81728331, 180.24746391,\n",
       "       187.84834904, 189.82832033, 182.47962185, 191.12282567,\n",
       "       185.66296163, 178.65271174, 187.38107638, 189.74541444,\n",
       "       188.08134119, 190.84970115, 192.9779132 , 198.4142941 ,\n",
       "       190.76724571, 181.52717081, 181.89499685, 190.65528968,\n",
       "       174.72057372, 188.64850447, 184.17581309, 185.67741239,\n",
       "       191.13428227, 181.14510047, 174.4476601 , 191.16381794,\n",
       "       183.6450922 , 179.30106253, 184.57763954, 184.84414384,\n",
       "       189.12335585, 180.7304961 , 185.29744456, 175.49521435,\n",
       "       188.53702185, 178.41619226, 186.52664981, 194.86687064,\n",
       "       190.7970946 , 181.47993423, 190.22292389, 189.11938129,\n",
       "       191.12647313, 176.78081021, 185.99768055, 197.15371474,\n",
       "       187.35601572, 182.01910654, 180.66041907, 186.66012635,\n",
       "       195.99493699, 190.53158379, 183.64311964, 186.88318039,\n",
       "       186.64090772, 190.65241051, 184.24886098, 181.28322663,\n",
       "       190.9617437 , 174.69565752, 190.34045665, 193.13290926,\n",
       "       180.148879  , 182.96193297, 185.66568933, 187.25664259,\n",
       "       182.9341772 , 182.12292023, 178.13137881, 187.5825425 ,\n",
       "       185.27897923, 180.69753296, 183.49461809, 185.4797745 ,\n",
       "       180.82246678, 193.23448066, 175.13303643, 181.22463056,\n",
       "       183.60949587, 180.99338982, 187.44942634, 194.53248719,\n",
       "       195.95127344, 188.30825883, 188.02248684, 189.12625592,\n",
       "       177.16162744, 184.33223642, 184.65616636, 197.29086815,\n",
       "       176.6547457 , 195.08833423, 178.69594896, 185.56769736,\n",
       "       188.01629785, 183.97758227, 184.71805331, 193.85887594,\n",
       "       184.18801199, 184.08696505, 180.16135964, 189.40037024,\n",
       "       191.24128182, 180.84573178, 183.93168195, 184.75081236,\n",
       "       190.76972533, 172.35170116, 186.05558264, 184.96169239,\n",
       "       171.25997668, 188.92754815, 191.95517841, 182.90603011,\n",
       "       183.66949733, 185.0928889 , 203.91484378, 190.75538247,\n",
       "       194.35661323, 182.90256628, 188.45778633, 188.25987376,\n",
       "       185.02211362, 185.47716133, 191.91835175, 189.06828622,\n",
       "       178.60180369, 186.46736167, 184.58389326, 183.85469579,\n",
       "       189.78686753, 189.96566734, 182.74732409, 201.15147679,\n",
       "       180.28900509, 199.00000398, 180.58156475, 187.77542678,\n",
       "       190.05299344, 191.76264505, 190.73180643, 187.59313279,\n",
       "       191.38752533, 184.97616154, 184.15725166, 190.42456247,\n",
       "       184.17179111, 187.02910913, 185.09560875, 190.85996027,\n",
       "       177.21244123, 185.28484959, 191.32872324, 192.02641223,\n",
       "       184.18778465, 179.80337114, 187.04976961, 195.1670353 ,\n",
       "       187.65446378, 180.67353249, 183.42674169, 184.62217191,\n",
       "       189.23311407, 183.12111977, 181.27620501, 191.06347679,\n",
       "       183.01453149, 184.91725143, 192.33313416, 187.48243434,\n",
       "       186.26851942, 175.29285745, 186.28968595, 181.39385003,\n",
       "       175.36250778, 192.61641577, 193.55060786, 175.11012627,\n",
       "       184.6501565 , 186.54071727, 181.63535818, 183.03875013,\n",
       "       185.11092805, 187.04329523, 183.45643987, 195.89101976,\n",
       "       185.2552588 , 182.04303331, 197.29230929, 193.6504327 ,\n",
       "       190.64778681, 183.55562967, 187.19519532, 179.1287086 ,\n",
       "       183.00327247, 183.27358483, 190.96247499, 186.53999929,\n",
       "       190.06861351, 190.18175111, 181.34450523, 188.52439857,\n",
       "       186.49310909, 182.27281125, 187.69617798, 189.8178277 ,\n",
       "       195.74031835, 185.03950499, 189.33932393, 187.92321851,\n",
       "       194.1183897 , 196.21230965, 182.9523617 , 182.13045186,\n",
       "       195.19225129, 179.50279982, 184.84938353, 194.92115051,\n",
       "       187.25392251, 186.9025668 , 181.8672044 , 193.12557055,\n",
       "       189.54416218, 186.40916503, 187.22419773, 192.43432276,\n",
       "       180.00508845, 186.61427048, 195.00155457, 178.01197605,\n",
       "       181.00134489, 184.75876256, 189.63407696, 180.32725602,\n",
       "       182.18031945, 190.82158114, 179.49428023, 177.60668524,\n",
       "       190.17131345, 178.80546555, 189.62976212, 179.32864662,\n",
       "       186.28781548, 185.6049396 , 189.30681582, 187.79065431,\n",
       "       179.15654477, 185.82673625, 201.32578345, 186.44490525,\n",
       "       197.12291841, 184.64592282, 185.35815476, 186.75045198,\n",
       "       172.98669944, 194.0789538 , 189.11884937, 178.64950975,\n",
       "       186.56646183, 191.30821566, 186.29909831, 192.76469981,\n",
       "       180.9471591 , 181.70654006, 190.51160955, 189.48321979,\n",
       "       176.51807566, 187.59285826, 186.73057368, 179.12768604,\n",
       "       178.89412761, 183.16051136, 185.89730699, 188.95080775,\n",
       "       185.4363499 , 176.16522467, 181.38379745, 183.90612152,\n",
       "       181.1345339 , 176.0520224 , 187.94550547, 181.8949321 ,\n",
       "       182.10919601, 183.15236066, 181.8852602 , 174.83681054,\n",
       "       187.52946688, 187.23207978, 179.59559668, 202.49267706,\n",
       "       182.56141034, 170.03717442, 194.85446966, 197.42162221,\n",
       "       177.44752268, 186.13476034, 190.42785402, 188.83834238,\n",
       "       182.92797612, 181.34885051, 182.69778174, 182.8499339 ,\n",
       "       180.14227806, 180.73272668, 189.72524636, 183.25030831,\n",
       "       180.90392097, 181.42566923, 193.58670576, 197.86773603,\n",
       "       188.40046479, 179.04126367, 173.07311004, 190.15391002,\n",
       "       185.92237734, 185.85168192, 186.23599719, 177.50441987,\n",
       "       183.12225358, 190.51045798, 184.3906478 , 185.56698531,\n",
       "       184.61072235, 189.97609047, 185.86332757, 186.8327952 ,\n",
       "       188.68344076, 187.45637247, 184.78001283, 192.59449326,\n",
       "       186.70795799, 185.77248168, 192.45372388, 195.61297771,\n",
       "       189.61487048, 189.02127932, 184.85371044, 182.49624945,\n",
       "       186.48878509, 187.52775806, 174.38482126, 176.39800041,\n",
       "       181.58407349, 186.23998748, 179.14173745, 191.62370705,\n",
       "       180.60495163, 186.95771078, 192.51286307, 186.1763639 ,\n",
       "       192.30848575, 181.21367927, 177.68384889, 187.71233788,\n",
       "       179.19535728, 181.88285504, 189.56074462, 187.4504853 ,\n",
       "       180.71281663, 183.39663852, 184.90809682, 176.38850302,\n",
       "       194.30681337, 183.00457337, 180.88179635, 187.67495439,\n",
       "       181.82355475, 190.32656664, 184.83663534, 186.4772577 ,\n",
       "       189.40648847, 190.81317094, 189.4785769 , 185.91436019,\n",
       "       180.74539488, 196.58405716, 197.39826747, 184.30776042,\n",
       "       187.87923796, 188.31932823, 184.00952329, 190.62872184,\n",
       "       190.7875265 , 182.3167137 , 184.69235913, 185.69494993,\n",
       "       192.63319892, 189.44969382, 180.39729658, 184.0920259 ,\n",
       "       188.64798235, 175.05360861, 189.74478608, 176.60933485,\n",
       "       177.58027208, 182.54156975, 183.44925377, 189.18508157,\n",
       "       194.3471343 , 194.52531282, 179.43190626, 187.52927693,\n",
       "       192.38502127, 184.49120196, 193.99795955, 189.50747835,\n",
       "       189.08585689, 186.67796183, 189.82903702, 185.8758835 ,\n",
       "       185.64864506, 190.20376897, 188.24615187, 183.70961226,\n",
       "       183.64168612, 189.52664992, 186.24886403, 190.90369635,\n",
       "       195.81748012, 176.42829852, 186.77716306, 176.49166058,\n",
       "       186.75021985, 184.17122877, 185.92008785, 183.06049172,\n",
       "       181.60040638, 189.77472495, 189.38316193, 186.47887152,\n",
       "       179.84124603, 189.54645386, 179.13257817, 188.94693637,\n",
       "       202.13722652, 181.12289415, 182.68787627, 194.33662047,\n",
       "       189.19083733, 188.87809657, 185.79882322, 185.72168539,\n",
       "       177.22851223, 191.02976399, 179.97231975, 180.46470951,\n",
       "       185.67895021, 194.85713959, 181.83455026, 184.70364684,\n",
       "       184.18737796, 183.12667326, 183.67747439, 181.82996865,\n",
       "       186.04201173, 186.66288515, 184.07223814, 186.09765093,\n",
       "       188.98626027, 181.40160756, 189.62699792, 190.60697699,\n",
       "       186.00389699, 181.71821255, 183.96265987, 192.805728  ,\n",
       "       184.15915615, 172.56712145, 190.68430225, 182.36169969,\n",
       "       196.11051456, 192.06038285, 189.45903687, 187.58090717,\n",
       "       177.96714628, 182.15274311, 178.67979302, 191.06347142,\n",
       "       189.24835456, 189.1796685 , 183.7870878 , 188.61742599,\n",
       "       178.26177714, 194.16017822, 195.42003695, 185.51173128,\n",
       "       184.18298545, 186.08399248, 177.69285208, 193.23318361,\n",
       "       183.96162502, 183.40037429, 180.99214445, 196.60231999,\n",
       "       179.17217247, 189.55451444, 188.61119594, 182.6420593 ,\n",
       "       183.33189453, 191.15377499, 181.41550909, 193.74345972,\n",
       "       190.86924745, 186.01250244, 171.83037356, 191.81682831,\n",
       "       179.28320032, 191.74399126, 181.17571323, 181.57041708,\n",
       "       176.59860802, 190.48344732, 186.84026748, 180.21337072,\n",
       "        77.19163518, 188.65205442, 183.24406577, 185.52521752,\n",
       "       193.08727449, 186.43077205, 198.93584543, 188.51588825,\n",
       "       188.83150756, 192.41165303, 194.57068613, 201.79911092,\n",
       "       187.45486642, 180.75210149, 178.72249901, 189.21161261,\n",
       "       191.79723004, 199.16091114, 184.45882974, 188.96268062,\n",
       "       183.78837324, 186.72863784, 183.61763462, 188.37464734,\n",
       "       183.90033117, 180.65886924, 185.75469909, 179.62675995,\n",
       "       178.67934256, 187.95418753, 199.14342056, 183.32577128,\n",
       "       174.78969697, 190.8736694 , 192.18324304, 184.50289346,\n",
       "       184.27262588, 193.4656619 , 187.61653848, 179.93921096,\n",
       "       187.36927312, 183.37168435, 190.39709024, 181.23064183,\n",
       "       189.41503309, 174.22771396, 186.66273618, 185.39698507,\n",
       "       193.9521023 , 192.73310042, 188.41284819, 176.73754762,\n",
       "       179.77682136, 194.0899285 , 199.30674863, 184.11955471])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards.sum(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
