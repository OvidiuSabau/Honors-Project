{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.autograd\n",
    "import os\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet as p \n",
    "import pybullet \n",
    "import pybullet_envs.gym_pendulum_envs \n",
    "import pybullet_envs.gym_locomotion_envs\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ornstein-Ulhenbeck Process\n",
    "# Taken from #https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py\n",
    "class OUNoise(object):\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[0]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_action(self, action, t=0):\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, self.low, self.high)\n",
    "\n",
    "\n",
    "# https://github.com/openai/gym/blob/master/gym/core.py\n",
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def _action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)\n",
    "        \n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, np.array([reward]), next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValue(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_state,\n",
    "        input_size_action,\n",
    "        hidden_sizes\n",
    "    ):\n",
    "        super(QValue, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size_action + input_size_state\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], 1))\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "class Policy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_state,\n",
    "        hidden_sizes,\n",
    "        output_size\n",
    "    ):\n",
    "        super(Policy, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size_state\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], output_size))\n",
    "        self.layers.append(nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGagent:\n",
    "    def __init__(self, env, q_hidden_sizes=[32, 64, 128, 64, 32], p_hidden_sizes=[32, 64, 128, 64, 32], actor_learning_rate=1e-4, critic_learning_rate=1e-3, gamma=0.99, tau=1e-2, max_memory_size=int(1e5)):\n",
    "        # Params\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.shape[0]\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        # Networks\n",
    "\n",
    "        self.actor = Policy(self.num_states, p_hidden_sizes, self.num_actions).to(device)\n",
    "        self.actor_target = Policy(self.num_states, p_hidden_sizes, self.num_actions).to(device)\n",
    "        self.critic = QValue(self.num_states, self.num_actions, q_hidden_sizes).to(device)\n",
    "        self.critic_target = QValue(self.num_states, self.num_actions, q_hidden_sizes).to(device)\n",
    "        \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        # Training\n",
    "        self.memory = Memory(max_memory_size)\n",
    "        self.critic_criterion  = nn.MSELoss()\n",
    "        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=actor_learning_rate)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_learning_rate)\n",
    "        self.actor_lr_scheduler = optim.lr_scheduler.StepLR(self.actor_optimizer, step_size=10, gamma=0.8)\n",
    "        self.critic_lr_scheduler = optim.lr_scheduler.StepLR(self.actor_optimizer, step_size=10, gamma=0.8)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action = self.actor.forward(state)\n",
    "        return action.cpu().detach().numpy()\n",
    "    \n",
    "    def get_latest_lr(self):\n",
    "        return self.critic_lr_scheduler.get_last_lr()\n",
    "    \n",
    "    def update_lr(self):\n",
    "        self.critic_lr_scheduler.step()\n",
    "        self.actor_lr_scheduler.step()\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, _ = self.memory.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "    \n",
    "        # Critic loss        \n",
    "        Qvals = self.critic.forward(states, actions)\n",
    "        next_actions = self.actor_target.forward(next_states)\n",
    "        next_Q = self.critic_target.forward(next_states, next_actions.detach())\n",
    "        Qprime = rewards + self.gamma * next_Q\n",
    "        critic_loss = self.critic_criterion(Qvals, Qprime)\n",
    "\n",
    "        # Actor loss\n",
    "        policy_loss = -self.critic.forward(states, self.actor.forward(states)).mean()\n",
    "        \n",
    "        # update networks\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # update target networks \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "            \n",
    "    def save_agent_networks(self, prefix):\n",
    "        torch.save(self.actor, prefix + '-actor.pt')\n",
    "        torch.save(self.critic, prefix + '-critic.pt')\n",
    "        \n",
    "    def load_agent_networks(self, prefix):\n",
    "        self.actor = torch.load(prefix + '-actor.pt').to(device)\n",
    "        self.critic = torch.load(prefix + '-critic.pt').to(device)\n",
    "        self.actor_target = self.actor\n",
    "        self.critic_target = self.critic\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 || reward for episode: -369.98 || average reward: nan || episode length: 300\n",
      "\n",
      "episode: 1 || reward for episode: -421.75 || average reward: -369.97779971624885 || episode length: 300\n",
      "\n",
      "episode: 2 || reward for episode: -263.49 || average reward: -395.8649411113156 || episode length: 300\n",
      "\n",
      "episode: 3 || reward for episode: -152.01 || average reward: -351.73867340926364 || episode length: 300\n",
      "\n",
      "episode: 4 || reward for episode: -401.4 || average reward: -301.8054987290412 || episode length: 300\n",
      "\n",
      "episode: 5 || reward for episode: -317.95 || average reward: -321.72375164210104 || episode length: 300\n",
      "\n",
      "episode: 6 || reward for episode: -135.4 || average reward: -321.09461911074226 || episode length: 300\n",
      "\n",
      "episode: 7 || reward for episode: -274.02 || average reward: -294.5662105529726 || episode length: 300\n",
      "\n",
      "episode: 8 || reward for episode: -290.83 || average reward: -291.9975526751362 || episode length: 300\n",
      "\n",
      "episode: 9 || reward for episode: -286.39 || average reward: -291.86743500855295 || episode length: 300\n",
      "\n",
      "episode: 10 || reward for episode: -373.31 || average reward: -291.319772030788 || episode length: 300\n",
      "\n",
      "episode: 11 || reward for episode: -338.52 || average reward: -298.77313587898027 || episode length: 300\n",
      "\n",
      "episode: 12 || reward for episode: -259.79 || average reward: -302.085089723483 || episode length: 300\n",
      "\n",
      "episode: 13 || reward for episode: -292.27 || average reward: -298.83161833669294 || episode length: 300\n",
      "\n",
      "episode: 14 || reward for episode: -257.52 || average reward: -298.3630985844287 || episode length: 300\n",
      "\n",
      "episode: 15 || reward for episode: -131.73 || average reward: -295.64035971009713 || episode length: 300\n",
      "\n",
      "episode: 16 || reward for episode: -285.47 || average reward: -285.39580401987655 || episode length: 300\n",
      "\n",
      "episode: 17 || reward for episode: -300.83 || average reward: -285.4004081084083 || episode length: 300\n",
      "\n",
      "episode: 18 || reward for episode: -412.78 || average reward: -286.25774163474483 || episode length: 300\n",
      "\n",
      "episode: 19 || reward for episode: -300.6 || average reward: -292.916765264832 || episode length: 300\n",
      "\n",
      "episode: 20 || reward for episode: -385.05 || average reward: -293.30076281391536 || episode length: 300\n",
      "\n",
      "episode: 21 || reward for episode: -249.53 || average reward: -297.66998647817525 || episode length: 300\n",
      "\n",
      "episode: 22 || reward for episode: -175.15 || average reward: -295.481765291709 || episode length: 300\n",
      "\n",
      "episode: 23 || reward for episode: -147.35 || average reward: -290.24985236558945 || episode length: 300\n",
      "\n",
      "episode: 24 || reward for episode: -259.81 || average reward: -284.2955081295879 || episode length: 300\n",
      "\n",
      "episode: 25 || reward for episode: -76.79 || average reward: -283.31627782388364 || episode length: 300\n",
      "\n",
      "episode: 26 || reward for episode: -77.01 || average reward: -271.588858264973 || episode length: 300\n",
      "\n",
      "episode: 27 || reward for episode: -317.46 || average reward: -257.79919519718925 || episode length: 300\n",
      "\n",
      "episode: 28 || reward for episode: -325.42 || average reward: -259.9582639809107 || episode length: 300\n",
      "\n",
      "episode: 29 || reward for episode: -152.01 || average reward: -266.89477355163655 || episode length: 300\n",
      "\n",
      "episode: 30 || reward for episode: -174.17 || average reward: -256.9194143462985 || episode length: 300\n",
      "\n",
      "episode: 31 || reward for episode: -130.92 || average reward: -251.16812993229763 || episode length: 300\n",
      "\n",
      "episode: 32 || reward for episode: -65.73 || average reward: -250.9890613088568 || episode length: 300\n",
      "\n",
      "episode: 33 || reward for episode: -198.88 || average reward: -242.6577556757638 || episode length: 300\n",
      "\n",
      "episode: 34 || reward for episode: -236.13 || average reward: -238.97986723863332 || episode length: 300\n",
      "\n",
      "episode: 35 || reward for episode: -208.05 || average reward: -236.96928035914462 || episode length: 300\n",
      "\n",
      "episode: 36 || reward for episode: 39.56 || average reward: -230.35882122683452 || episode length: 300\n",
      "\n",
      "episode: 37 || reward for episode: -351.28 || average reward: -215.23568767559408 || episode length: 300\n",
      "\n",
      "episode: 38 || reward for episode: -76.9 || average reward: -218.89533016853085 || episode length: 300\n",
      "\n",
      "episode: 39 || reward for episode: 95.82 || average reward: -210.2806189132759 || episode length: 300\n",
      "\n",
      "episode: 40 || reward for episode: 4.11 || average reward: -196.14695546272335 || episode length: 300\n",
      "\n",
      "episode: 41 || reward for episode: -180.94 || average reward: -190.71335558336602 || episode length: 300\n",
      "\n",
      "episode: 42 || reward for episode: -232.89 || average reward: -186.53183800057647 || episode length: 300\n",
      "\n",
      "episode: 43 || reward for episode: -229.96 || average reward: -183.81413865823714 || episode length: 300\n",
      "\n",
      "episode: 44 || reward for episode: -340.09 || average reward: -176.501565766122 || episode length: 300\n",
      "\n",
      "episode: 45 || reward for episode: -379.91 || average reward: -178.08136350448316 || episode length: 300\n",
      "\n",
      "episode: 46 || reward for episode: -455.16 || average reward: -177.87551164323196 || episode length: 300\n",
      "\n",
      "episode: 47 || reward for episode: -498.94 || average reward: -186.10073352703938 || episode length: 300\n",
      "\n",
      "episode: 48 || reward for episode: -478.42 || average reward: -199.05251991694863 || episode length: 300\n",
      "\n",
      "episode: 49 || reward for episode: -461.33 || average reward: -212.2956604011383 || episode length: 300\n",
      "\n",
      "episode: 50 || reward for episode: -475.6 || average reward: -220.35614009461307 || episode length: 300\n",
      "\n",
      "episode: 51 || reward for episode: -477.79 || average reward: -236.30863925715732 || episode length: 300\n",
      "\n",
      "episode: 52 || reward for episode: -434.84 || average reward: -252.34001675691823 || episode length: 300\n",
      "\n",
      "episode: 53 || reward for episode: -460.03 || average reward: -257.03504460839446 || episode length: 300\n",
      "\n",
      "episode: 54 || reward for episode: -437.13 || average reward: -262.4195113241026 || episode length: 300\n",
      "\n",
      "episode: 55 || reward for episode: -474.09 || average reward: -273.82412649970377 || episode length: 300\n",
      "\n",
      "episode: 56 || reward for episode: -462.41 || average reward: -285.82120981011053 || episode length: 300\n",
      "\n",
      "episode: 57 || reward for episode: -451.94 || average reward: -299.0808050458579 || episode length: 300\n",
      "\n",
      "episode: 58 || reward for episode: -447.55 || average reward: -314.5289946628386 || episode length: 300\n",
      "\n",
      "episode: 59 || reward for episode: -453.37 || average reward: -324.4758010539728 || episode length: 300\n",
      "\n",
      "episode: 60 || reward for episode: -137.66 || average reward: -333.1654382999067 || episode length: 300\n",
      "\n",
      "episode: 61 || reward for episode: 4.11 || average reward: -330.3500115550367 || episode length: 300\n",
      "\n",
      "episode: 62 || reward for episode: 70.63 || average reward: -331.76802261119553 || episode length: 300\n",
      "\n",
      "episode: 63 || reward for episode: -85.98 || average reward: -314.8916780099317 || episode length: 300\n",
      "\n",
      "episode: 64 || reward for episode: -90.65 || average reward: -315.25467990292236 || episode length: 300\n",
      "\n",
      "episode: 65 || reward for episode: -381.39 || average reward: -322.71365904286336 || episode length: 300\n",
      "\n",
      "episode: 66 || reward for episode: -445.67 || average reward: -338.133886794682 || episode length: 300\n",
      "\n",
      "episode: 67 || reward for episode: -431.67 || average reward: -348.7230520382285 || episode length: 300\n",
      "\n",
      "episode: 68 || reward for episode: -454.1 || average reward: -356.6740757854999 || episode length: 300\n",
      "\n",
      "episode: 69 || reward for episode: -149.82 || average reward: -365.6395475090703 || episode length: 300\n",
      "\n",
      "episode: 70 || reward for episode: -226.5 || average reward: -358.0288473953419 || episode length: 300\n",
      "\n",
      "episode: 71 || reward for episode: -334.9 || average reward: -351.8923297537832 || episode length: 300\n",
      "\n",
      "episode: 72 || reward for episode: -461.91 || average reward: -347.0820017307072 || episode length: 300\n",
      "\n",
      "episode: 73 || reward for episode: -398.72 || average reward: -345.60089595584117 || episode length: 300\n",
      "\n",
      "episode: 74 || reward for episode: -437.96 || average reward: -342.4126998867566 || episode length: 300\n",
      "\n",
      "episode: 75 || reward for episode: -432.12 || average reward: -341.47800803149175 || episode length: 300\n",
      "\n",
      "episode: 76 || reward for episode: -391.24 || average reward: -339.738439402297 || episode length: 300\n",
      "\n",
      "episode: 77 || reward for episode: -322.5 || average reward: -336.276132333209 || episode length: 300\n",
      "\n",
      "episode: 78 || reward for episode: -433.38 || average reward: -331.78269379378236 || episode length: 300\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 79 || reward for episode: -447.31 || average reward: -330.71671432116904 || episode length: 300\n",
      "\n",
      "episode: 80 || reward for episode: -435.87 || average reward: -331.1240189699578 || episode length: 300\n",
      "\n",
      "episode: 81 || reward for episode: -448.82 || average reward: -329.59504629049593 || episode length: 300\n",
      "\n",
      "episode: 82 || reward for episode: -441.17 || average reward: -329.0516299674213 || episode length: 300\n",
      "\n",
      "episode: 83 || reward for episode: -460.38 || average reward: -328.62082544283277 || episode length: 300\n",
      "\n",
      "episode: 84 || reward for episode: -443.74 || average reward: -329.1339986552921 || episode length: 300\n",
      "\n",
      "episode: 85 || reward for episode: -188.3 || average reward: -328.7488050241988 || episode length: 300\n",
      "\n",
      "episode: 86 || reward for episode: -161.51 || average reward: -330.77451432813956 || episode length: 300\n",
      "\n",
      "episode: 87 || reward for episode: -253.5 || average reward: -337.39955368027864 || episode length: 300\n",
      "\n",
      "episode: 88 || reward for episode: -329.89 || average reward: -350.36466269303855 || episode length: 300\n",
      "\n",
      "episode: 89 || reward for episode: -368.52 || average reward: -360.12124723040864 || episode length: 300\n",
      "\n",
      "episode: 90 || reward for episode: -314.04 || average reward: -371.2357832087109 || episode length: 300\n",
      "\n",
      "episode: 91 || reward for episode: -282.91 || average reward: -368.5415237359773 || episode length: 300\n",
      "\n",
      "episode: 92 || reward for episode: -408.0 || average reward: -362.0313802547083 || episode length: 300\n",
      "\n",
      "episode: 93 || reward for episode: -418.52 || average reward: -361.08456663359647 || episode length: 300\n",
      "\n",
      "episode: 94 || reward for episode: -20.28 || average reward: -359.66144445534417 || episode length: 300\n",
      "\n",
      "episode: 95 || reward for episode: -122.71 || average reward: -354.4795088776161 || episode length: 300\n",
      "\n",
      "episode: 96 || reward for episode: -243.85 || average reward: -350.32825073422936 || episode length: 300\n",
      "\n",
      "episode: 97 || reward for episode: -295.51 || average reward: -346.68635143889094 || episode length: 300\n",
      "\n",
      "episode: 98 || reward for episode: 85.26 || average reward: -340.0301381077482 || episode length: 300\n",
      "\n",
      "episode: 99 || reward for episode: -5.85 || average reward: -320.6707867729643 || episode length: 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('HalfCheetahBulletEnv-v0')\n",
    "agent = DDPGagent(env, gamma=0.9999)\n",
    "noise = OUNoise(env.action_space)\n",
    "batch_size = 4096\n",
    "rewards = []\n",
    "avg_rewards = []\n",
    "learningRates = []\n",
    "episodeLengths = []\n",
    "for episode in range(100):\n",
    "    state = env.reset()\n",
    "    noise.reset()\n",
    "    episode_reward = 0\n",
    "    step = 0\n",
    "    done = False\n",
    "    \n",
    "    for i in range(300):\n",
    "        action = agent.get_action(state)\n",
    "        action = noise.get_action(action, step)\n",
    "        new_state, reward, done, _ = env.step(action) \n",
    "        agent.memory.push(state, action, reward, new_state, done)\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.update(batch_size)\n",
    "        \n",
    "        \n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "        step += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    print(\"episode: {} || reward for episode: {} || average reward: {} || episode length: {}\\n\".format(episode, np.round(episode_reward, decimals=2), np.mean(rewards[-25:]), step))\n",
    "    \n",
    "    episodeLengths.append(step)\n",
    "    rewards.append(episode_reward)\n",
    "    avg_rewards.append(np.mean(rewards[-25:]))\n",
    "    learningRates.append(agent.get_latest_lr())\n",
    "    agent.update_lr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(3, sharex=True)\n",
    "ax[0].plot(range(len(rewards)), rewards)\n",
    "ax[0].plot(range(len(avg_rewards)), avg_rewards)\n",
    "ax[0].set(ylabel='Reward')\n",
    "ax[1].plot(range(len(episodeLengths)), episodeLengths)\n",
    "ax[2].plot(range(len(learningRates)), np.log10(learningRates))\n",
    "ax[1].set(ylabel='Episode Length')\n",
    "ax[2].set(ylabel='Log Learning Rate')\n",
    "ax[0].legend([\"Episode\", \"Last 25 Avg\"])\n",
    "plt.xlabel('Episode')\n",
    "plt.show()\n",
    "plt.savefig('training-InvertedDoublePendulum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_agent_networks('doublePendulum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"InvertedDoublePendulumBulletEnv-v0\")\n",
    "agent = DDPGagent(env)\n",
    "agent.load_agent_networks('doublePendulum')\n",
    "# env.render()\n",
    "states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "collectedSamples = 0\n",
    "samplesToPrint = 10000\n",
    "while collectedSamples < 1e6:\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    step = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(np.array([action]))\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        next_states.append(next_state)\n",
    "        dones.append(done)\n",
    "        \n",
    "        state = next_state\n",
    "        step += 1\n",
    "        collectedSamples += 1\n",
    "        samplesToPrint -= 1\n",
    "    \n",
    "    if step != 1000:\n",
    "        print(step)\n",
    "        \n",
    "    if samplesToPrint <= 0:\n",
    "        print(collectedSamples)\n",
    "        samplesToPrint = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_array = np.array(actions)\n",
    "states_array = np.array(states)\n",
    "next_states_array = np.array(next_states)\n",
    "rewards_array = np.array(rewards)\n",
    "dones_array = np.array(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"doubleInvertedPendulumDataset/actions_array\", actions_array)\n",
    "np.save(\"doubleInvertedPendulumDataset/states_array\", states_array)\n",
    "np.save(\"doubleInvertedPendulumDataset/next_states_array\", next_states_array)\n",
    "np.save(\"doubleInvertedPendulumDataset/rewards_array\", rewards_array)\n",
    "np.save(\"doubleInvertedPendulumDataset/dones_array\", dones_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(actions_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
