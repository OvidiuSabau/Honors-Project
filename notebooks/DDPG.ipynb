{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.autograd\n",
    "import os\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import morphsim as m\n",
    "from graphenvs import HalfCheetahGraphEnv\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet as p \n",
    "import pybullet \n",
    "import pybullet_envs.gym_pendulum_envs \n",
    "import pybullet_envs.gym_locomotion_envs\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "import time\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, np.array([reward]), next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValue(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_state,\n",
    "        input_size_action,\n",
    "        hidden_sizes\n",
    "    ):\n",
    "        super(QValue, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size_action + input_size_state\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], 1))\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "class Policy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_state,\n",
    "        hidden_sizes,\n",
    "        output_size\n",
    "    ):\n",
    "        super(Policy, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size_state\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], output_size))\n",
    "        self.layers.append(nn.Tanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGagent:\n",
    "    def __init__(self, env, q_hidden_sizes=[32, 64, 128, 64, 32], p_hidden_sizes=[32, 64, 128, 64, 32], actor_learning_rate=1e-3, critic_learning_rate=1e-3, gamma=0.99, tau=1e-2, max_memory_size=int(1e6)):\n",
    "        # Params\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.shape[0]\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        # Networks\n",
    "\n",
    "        self.actor = Policy(self.num_states, p_hidden_sizes, self.num_actions).to(device)\n",
    "        self.actor_target = Policy(self.num_states, p_hidden_sizes, self.num_actions).to(device)\n",
    "        self.critic = QValue(self.num_states, self.num_actions, q_hidden_sizes).to(device)\n",
    "        self.critic_target = QValue(self.num_states, self.num_actions, q_hidden_sizes).to(device)\n",
    "        \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        # Training\n",
    "        self.memory = Memory(max_memory_size)\n",
    "        self.critic_criterion  = nn.MSELoss()\n",
    "        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=actor_learning_rate)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_learning_rate)\n",
    "        lmbda = lambda epoch: 0.8\n",
    "        self.actor_lr_scheduler = optim.lr_scheduler.MultiplicativeLR(self.actor_optimizer, lr_lambda=lmbda)\n",
    "        self.critic_lr_scheduler = optim.lr_scheduler.MultiplicativeLR(self.actor_optimizer, lr_lambda=lmbda)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action = self.actor.forward(state)\n",
    "        return action.cpu().detach().numpy()\n",
    "    \n",
    "    def get_latest_lr(self):\n",
    "        return self.critic_lr_scheduler.get_last_lr()\n",
    "    \n",
    "    def update_lr(self):\n",
    "        self.critic_lr_scheduler.step()\n",
    "        self.actor_lr_scheduler.step()\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, _ = self.memory.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "    \n",
    "        # Critic loss        \n",
    "        Qvals = self.critic.forward(states, actions)\n",
    "        next_actions = self.actor_target.forward(next_states)\n",
    "        next_Q = self.critic_target.forward(next_states, next_actions.detach())\n",
    "        Qprime = rewards + self.gamma * next_Q\n",
    "        critic_loss = self.critic_criterion(Qvals, Qprime)\n",
    "\n",
    "        # Actor loss\n",
    "        policy_loss = -self.critic.forward(states, self.actor.forward(states)).mean()\n",
    "        \n",
    "        # update networks\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # update target networks \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "            \n",
    "    def save_agent_networks(self, prefix):\n",
    "        torch.save(self.actor, prefix + '-actor.pt')\n",
    "        torch.save(self.critic, prefix + '-critic.pt')\n",
    "        \n",
    "    def load_agent_networks(self, prefix):\n",
    "        self.actor = torch.load(prefix + '-actor.pt').to(device)\n",
    "        self.critic = torch.load(prefix + '-critic.pt').to(device)\n",
    "        self.actor_target = self.actor\n",
    "        self.critic_target = self.critic\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ovi/anaconda3/envs/honors-project/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ovi/anaconda3/envs/honors-project/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/ovi/anaconda3/envs/honors-project/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 in 10.2s: reward for episode: 1.07 || average reward: nan || episode length: 1000\n",
      "\n",
      "episode 1 in 12.0s: reward for episode: 0.45 || average reward: 1.07 || episode length: 1000\n",
      "\n",
      "episode 2 in 12.4s: reward for episode: 1.74 || average reward: 0.76 || episode length: 1000\n",
      "\n",
      "episode 3 in 11.6s: reward for episode: 2.56 || average reward: 1.09 || episode length: 1000\n",
      "\n",
      "episode 4 in 13.0s: reward for episode: 4.57 || average reward: 1.46 || episode length: 1000\n",
      "\n",
      "episode 5 in 13.5s: reward for episode: 5.5 || average reward: 2.08 || episode length: 1000\n",
      "\n",
      "episode 6 in 12.0s: reward for episode: 6.53 || average reward: 2.65 || episode length: 1000\n",
      "\n",
      "episode 7 in 11.5s: reward for episode: 9.9 || average reward: 3.2 || episode length: 1000\n",
      "\n",
      "episode 8 in 11.3s: reward for episode: 12.51 || average reward: 4.04 || episode length: 1000\n",
      "\n",
      "episode 9 in 11.2s: reward for episode: 18.42 || average reward: 4.98 || episode length: 1000\n",
      "\n",
      "episode 10 in 11.3s: reward for episode: 25.87 || average reward: 6.33 || episode length: 1000\n",
      "\n",
      "episode 11 in 11.3s: reward for episode: 25.49 || average reward: 8.1 || episode length: 1000\n",
      "\n",
      "episode 12 in 11.3s: reward for episode: 19.25 || average reward: 9.55 || episode length: 1000\n",
      "\n",
      "episode 13 in 11.2s: reward for episode: 12.19 || average reward: 10.3 || episode length: 1000\n",
      "\n",
      "episode 14 in 11.3s: reward for episode: 5.85 || average reward: 10.43 || episode length: 1000\n",
      "\n",
      "episode 15 in 12.9s: reward for episode: 14.72 || average reward: 10.13 || episode length: 1000\n",
      "\n",
      "episode 16 in 13.0s: reward for episode: 32.48 || average reward: 10.41 || episode length: 1000\n",
      "\n",
      "episode 17 in 11.5s: reward for episode: 49.91 || average reward: 11.71 || episode length: 1000\n",
      "\n",
      "episode 18 in 11.9s: reward for episode: -12.82 || average reward: 13.83 || episode length: 1000\n",
      "\n",
      "episode 19 in 11.6s: reward for episode: 35.14 || average reward: 12.43 || episode length: 1000\n",
      "\n",
      "episode 20 in 11.7s: reward for episode: 28.16 || average reward: 13.57 || episode length: 1000\n",
      "\n",
      "episode 21 in 13.3s: reward for episode: 31.48 || average reward: 14.26 || episode length: 1000\n",
      "\n",
      "episode 22 in 12.7s: reward for episode: 20.72 || average reward: 15.04 || episode length: 1000\n",
      "\n",
      "episode 23 in 14.9s: reward for episode: 34.1 || average reward: 15.29 || episode length: 1000\n",
      "\n",
      "episode 24 in 14.7s: reward for episode: 38.03 || average reward: 16.07 || episode length: 1000\n",
      "\n",
      "episode 25 in 12.4s: reward for episode: 51.61 || average reward: 16.95 || episode length: 1000\n",
      "\n",
      "episode 26 in 11.4s: reward for episode: 50.11 || average reward: 18.97 || episode length: 1000\n",
      "\n",
      "episode 27 in 11.4s: reward for episode: 55.73 || average reward: 20.96 || episode length: 1000\n",
      "\n",
      "episode 28 in 11.5s: reward for episode: 57.71 || average reward: 23.12 || episode length: 1000\n",
      "\n",
      "episode 29 in 11.4s: reward for episode: 54.4 || average reward: 25.33 || episode length: 1000\n",
      "\n",
      "episode 30 in 11.5s: reward for episode: 59.54 || average reward: 27.32 || episode length: 1000\n",
      "\n",
      "episode 31 in 13.0s: reward for episode: 51.26 || average reward: 29.48 || episode length: 1000\n",
      "\n",
      "episode 32 in 14.3s: reward for episode: 60.95 || average reward: 31.27 || episode length: 1000\n",
      "\n",
      "episode 33 in 11.6s: reward for episode: 26.42 || average reward: 33.31 || episode length: 1000\n",
      "\n",
      "episode 34 in 10.9s: reward for episode: 31.79 || average reward: 33.87 || episode length: 1000\n",
      "\n",
      "episode 35 in 12.7s: reward for episode: 12.64 || average reward: 34.4 || episode length: 1000\n",
      "\n",
      "episode 36 in 15.5s: reward for episode: 11.6 || average reward: 33.87 || episode length: 1000\n",
      "\n",
      "episode 37 in 14.6s: reward for episode: -27.97 || average reward: 33.32 || episode length: 1000\n",
      "\n",
      "episode 38 in 13.3s: reward for episode: 84.96 || average reward: 31.43 || episode length: 1000\n",
      "\n",
      "episode 39 in 12.6s: reward for episode: 58.36 || average reward: 34.34 || episode length: 1000\n",
      "\n",
      "episode 40 in 11.9s: reward for episode: 68.9 || average reward: 36.44 || episode length: 1000\n",
      "\n",
      "episode 41 in 12.6s: reward for episode: 7.17 || average reward: 38.61 || episode length: 1000\n",
      "\n",
      "episode 42 in 13.2s: reward for episode: 39.85 || average reward: 37.6 || episode length: 1000\n",
      "\n",
      "episode 43 in 13.1s: reward for episode: 10.9 || average reward: 37.19 || episode length: 1000\n",
      "\n",
      "episode 44 in 13.3s: reward for episode: 24.25 || average reward: 38.14 || episode length: 1000\n",
      "\n",
      "episode 45 in 13.2s: reward for episode: 13.71 || average reward: 37.71 || episode length: 1000\n",
      "\n",
      "episode 46 in 14.2s: reward for episode: 57.5 || average reward: 37.13 || episode length: 1000\n",
      "\n",
      "episode 47 in 14.3s: reward for episode: 58.09 || average reward: 38.17 || episode length: 1000\n",
      "\n",
      "episode 48 in 14.8s: reward for episode: 15.72 || average reward: 39.66 || episode length: 1000\n",
      "\n",
      "episode 49 in 14.7s: reward for episode: 74.44 || average reward: 38.93 || episode length: 1000\n",
      "\n",
      "episode 50 in 14.0s: reward for episode: 66.6 || average reward: 40.39 || episode length: 1000\n",
      "\n",
      "episode 51 in 14.0s: reward for episode: -78.3 || average reward: 40.99 || episode length: 1000\n",
      "\n",
      "episode 52 in 14.0s: reward for episode: 100.27 || average reward: 35.85 || episode length: 1000\n",
      "\n",
      "episode 53 in 14.1s: reward for episode: 97.4 || average reward: 37.63 || episode length: 1000\n",
      "\n",
      "episode 54 in 14.3s: reward for episode: 69.85 || average reward: 39.22 || episode length: 1000\n",
      "\n",
      "episode 55 in 14.0s: reward for episode: 17.79 || average reward: 39.84 || episode length: 1000\n",
      "\n",
      "episode 56 in 14.6s: reward for episode: 92.02 || average reward: 38.17 || episode length: 1000\n",
      "\n",
      "episode 57 in 13.0s: reward for episode: 89.24 || average reward: 39.8 || episode length: 1000\n",
      "\n",
      "episode 58 in 12.2s: reward for episode: 59.29 || average reward: 40.93 || episode length: 1000\n",
      "\n",
      "episode 59 in 12.5s: reward for episode: 43.35 || average reward: 42.24 || episode length: 1000\n",
      "\n",
      "episode 60 in 12.8s: reward for episode: 2.11 || average reward: 42.71 || episode length: 1000\n",
      "\n",
      "episode 61 in 12.3s: reward for episode: 122.45 || average reward: 42.28 || episode length: 1000\n",
      "\n",
      "episode 62 in 11.9s: reward for episode: 153.05 || average reward: 46.72 || episode length: 1000\n",
      "\n",
      "episode 63 in 11.3s: reward for episode: 106.01 || average reward: 53.96 || episode length: 1000\n",
      "\n",
      "episode 64 in 11.9s: reward for episode: 103.64 || average reward: 54.8 || episode length: 1000\n",
      "\n",
      "episode 65 in 11.6s: reward for episode: 21.96 || average reward: 56.61 || episode length: 1000\n",
      "\n",
      "episode 66 in 11.9s: reward for episode: 73.52 || average reward: 54.73 || episode length: 1000\n",
      "\n",
      "episode 67 in 11.4s: reward for episode: 144.41 || average reward: 57.39 || episode length: 1000\n",
      "\n",
      "episode 68 in 11.5s: reward for episode: 130.96 || average reward: 61.57 || episode length: 1000\n",
      "\n",
      "episode 69 in 11.4s: reward for episode: 157.93 || average reward: 66.37 || episode length: 1000\n",
      "\n",
      "episode 70 in 11.8s: reward for episode: 114.8 || average reward: 71.72 || episode length: 1000\n",
      "\n",
      "episode 71 in 11.5s: reward for episode: 124.62 || average reward: 75.76 || episode length: 1000\n",
      "\n",
      "episode 72 in 11.5s: reward for episode: 113.28 || average reward: 78.45 || episode length: 1000\n",
      "\n",
      "episode 73 in 11.2s: reward for episode: 85.59 || average reward: 80.66 || episode length: 1000\n",
      "\n",
      "episode 74 in 11.2s: reward for episode: 72.65 || average reward: 83.45 || episode length: 1000\n",
      "\n",
      "episode 75 in 11.2s: reward for episode: 39.42 || average reward: 83.38 || episode length: 1000\n",
      "\n",
      "episode 76 in 11.2s: reward for episode: 84.76 || average reward: 82.29 || episode length: 1000\n",
      "\n",
      "episode 77 in 11.1s: reward for episode: 111.23 || average reward: 88.81 || episode length: 1000\n",
      "\n",
      "episode 78 in 11.3s: reward for episode: 157.2 || average reward: 89.25 || episode length: 1000\n",
      "\n",
      "episode 79 in 11.2s: reward for episode: 138.72 || average reward: 91.64 || episode length: 1000\n",
      "\n",
      "episode 80 in 11.2s: reward for episode: 122.52 || average reward: 94.4 || episode length: 1000\n",
      "\n",
      "episode 81 in 11.2s: reward for episode: -43.2 || average reward: 98.59 || episode length: 1000\n",
      "\n",
      "episode 82 in 11.1s: reward for episode: 110.9 || average reward: 93.18 || episode length: 1000\n",
      "\n",
      "episode 83 in 11.3s: reward for episode: 151.61 || average reward: 94.05 || episode length: 1000\n",
      "\n",
      "episode 84 in 11.1s: reward for episode: 127.28 || average reward: 97.74 || episode length: 1000\n",
      "\n",
      "episode 85 in 11.0s: reward for episode: 161.49 || average reward: 101.1 || episode length: 1000\n",
      "\n",
      "episode 86 in 11.2s: reward for episode: 139.67 || average reward: 107.47 || episode length: 1000\n",
      "\n",
      "episode 87 in 11.2s: reward for episode: 154.01 || average reward: 108.16 || episode length: 1000\n",
      "\n",
      "episode 88 in 11.2s: reward for episode: 99.87 || average reward: 108.2 || episode length: 1000\n",
      "\n",
      "episode 89 in 11.0s: reward for episode: 126.62 || average reward: 107.95 || episode length: 1000\n",
      "\n",
      "episode 90 in 11.2s: reward for episode: 77.52 || average reward: 108.87 || episode length: 1000\n",
      "\n",
      "episode 91 in 11.1s: reward for episode: 119.07 || average reward: 111.1 || episode length: 1000\n",
      "\n",
      "episode 92 in 11.2s: reward for episode: 118.73 || average reward: 112.92 || episode length: 1000\n",
      "\n",
      "episode 93 in 11.3s: reward for episode: 117.24 || average reward: 111.89 || episode length: 1000\n",
      "\n",
      "episode 94 in 11.3s: reward for episode: 144.8 || average reward: 111.34 || episode length: 1000\n",
      "\n",
      "episode 95 in 11.1s: reward for episode: 96.97 || average reward: 110.82 || episode length: 1000\n",
      "\n",
      "episode 96 in 11.3s: reward for episode: 90.73 || average reward: 110.1 || episode length: 1000\n",
      "\n",
      "episode 97 in 11.2s: reward for episode: 171.86 || average reward: 108.75 || episode length: 1000\n",
      "\n",
      "episode 98 in 11.2s: reward for episode: 153.73 || average reward: 111.09 || episode length: 1000\n",
      "\n",
      "episode 99 in 11.3s: reward for episode: 141.33 || average reward: 113.82 || episode length: 1000\n",
      "\n",
      "episode 100 in 13.1s: reward for episode: 181.85 || average reward: 116.56 || episode length: 1000\n",
      "\n",
      "episode 101 in 13.7s: reward for episode: 116.75 || average reward: 122.26 || episode length: 1000\n",
      "\n",
      "episode 102 in 13.1s: reward for episode: 203.0 || average reward: 123.54 || episode length: 1000\n",
      "\n",
      "episode 103 in 13.0s: reward for episode: 210.29 || average reward: 127.21 || episode length: 1000\n",
      "\n",
      "episode 104 in 13.1s: reward for episode: 175.39 || average reward: 129.34 || episode length: 1000\n",
      "\n",
      "episode 105 in 13.2s: reward for episode: 138.21 || average reward: 130.8 || episode length: 1000\n",
      "\n",
      "episode 106 in 13.7s: reward for episode: 150.51 || average reward: 131.43 || episode length: 1000\n",
      "\n",
      "episode 107 in 13.7s: reward for episode: 163.89 || average reward: 139.18 || episode length: 1000\n",
      "\n",
      "episode 108 in 13.9s: reward for episode: 72.22 || average reward: 141.3 || episode length: 1000\n",
      "\n",
      "episode 109 in 13.9s: reward for episode: 183.11 || average reward: 138.12 || episode length: 1000\n",
      "\n",
      "episode 110 in 13.9s: reward for episode: 100.57 || average reward: 140.35 || episode length: 1000\n",
      "\n",
      "episode 111 in 13.8s: reward for episode: 204.66 || average reward: 137.92 || episode length: 1000\n",
      "\n",
      "episode 112 in 14.5s: reward for episode: 75.2 || average reward: 140.52 || episode length: 1000\n",
      "\n",
      "episode 113 in 14.5s: reward for episode: 24.96 || average reward: 137.36 || episode length: 1000\n",
      "\n",
      "episode 114 in 14.2s: reward for episode: 47.97 || average reward: 134.37 || episode length: 1000\n",
      "\n",
      "episode 115 in 14.5s: reward for episode: 19.86 || average reward: 131.22 || episode length: 1000\n",
      "\n",
      "episode 116 in 14.4s: reward for episode: 68.29 || average reward: 128.92 || episode length: 1000\n",
      "\n",
      "episode 117 in 14.3s: reward for episode: 191.59 || average reward: 126.88 || episode length: 1000\n",
      "\n",
      "episode 118 in 14.3s: reward for episode: 122.34 || average reward: 129.8 || episode length: 1000\n",
      "\n",
      "episode 119 in 14.4s: reward for episode: 144.27 || average reward: 130.0 || episode length: 1000\n",
      "\n",
      "episode 120 in 14.4s: reward for episode: 148.86 || average reward: 129.98 || episode length: 1000\n",
      "\n",
      "episode 121 in 14.4s: reward for episode: 147.44 || average reward: 132.06 || episode length: 1000\n",
      "\n",
      "episode 122 in 14.5s: reward for episode: 151.64 || average reward: 134.33 || episode length: 1000\n",
      "\n",
      "episode 123 in 14.5s: reward for episode: 210.75 || average reward: 133.52 || episode length: 1000\n",
      "\n",
      "episode 124 in 14.5s: reward for episode: 155.31 || average reward: 135.8 || episode length: 1000\n",
      "\n",
      "episode 125 in 13.4s: reward for episode: 152.1 || average reward: 136.36 || episode length: 1000\n",
      "\n",
      "episode 126 in 11.4s: reward for episode: 200.54 || average reward: 135.17 || episode length: 1000\n",
      "\n",
      "episode 127 in 11.4s: reward for episode: 195.69 || average reward: 138.52 || episode length: 1000\n",
      "\n",
      "episode 128 in 14.1s: reward for episode: 175.79 || average reward: 138.23 || episode length: 1000\n",
      "\n",
      "episode 129 in 14.4s: reward for episode: 215.15 || average reward: 136.85 || episode length: 1000\n",
      "\n",
      "episode 130 in 14.4s: reward for episode: 160.85 || average reward: 138.44 || episode length: 1000\n",
      "\n",
      "episode 131 in 14.4s: reward for episode: 174.19 || average reward: 139.34 || episode length: 1000\n",
      "\n",
      "episode 132 in 14.9s: reward for episode: 12.19 || average reward: 140.29 || episode length: 1000\n",
      "\n",
      "episode 133 in 14.6s: reward for episode: 4.56 || average reward: 134.22 || episode length: 1000\n",
      "\n",
      "episode 134 in 14.6s: reward for episode: 2.09 || average reward: 131.52 || episode length: 1000\n",
      "\n",
      "episode 135 in 14.5s: reward for episode: -23.9 || average reward: 124.27 || episode length: 1000\n",
      "\n",
      "episode 136 in 14.6s: reward for episode: 4.78 || average reward: 119.3 || episode length: 1000\n",
      "\n",
      "episode 137 in 14.5s: reward for episode: 8.86 || average reward: 111.3 || episode length: 1000\n",
      "\n",
      "episode 138 in 14.8s: reward for episode: 2.56 || average reward: 108.65 || episode length: 1000\n",
      "\n",
      "episode 139 in 14.6s: reward for episode: 0.58 || average reward: 107.75 || episode length: 1000\n",
      "\n",
      "episode 140 in 14.6s: reward for episode: 4.2 || average reward: 105.86 || episode length: 1000\n",
      "\n",
      "episode 141 in 14.6s: reward for episode: 33.2 || average reward: 105.23 || episode length: 1000\n",
      "\n",
      "episode 142 in 14.8s: reward for episode: 19.42 || average reward: 103.82 || episode length: 1000\n",
      "\n",
      "episode 143 in 14.9s: reward for episode: 8.6 || average reward: 96.94 || episode length: 1000\n",
      "\n",
      "episode 144 in 14.7s: reward for episode: 47.12 || average reward: 92.39 || episode length: 1000\n",
      "\n",
      "episode 145 in 14.6s: reward for episode: 68.78 || average reward: 88.5 || episode length: 1000\n",
      "\n",
      "episode 146 in 14.7s: reward for episode: 60.96 || average reward: 85.3 || episode length: 1000\n",
      "\n",
      "episode 147 in 14.5s: reward for episode: 94.99 || average reward: 81.84 || episode length: 1000\n",
      "\n",
      "episode 148 in 14.6s: reward for episode: 101.8 || average reward: 79.57 || episode length: 1000\n",
      "\n",
      "episode 149 in 14.6s: reward for episode: 90.8 || average reward: 75.22 || episode length: 1000\n",
      "\n",
      "episode 150 in 14.7s: reward for episode: 148.34 || average reward: 72.64 || episode length: 1000\n",
      "\n",
      "episode 151 in 14.7s: reward for episode: 134.35 || average reward: 72.49 || episode length: 1000\n",
      "\n",
      "episode 152 in 15.1s: reward for episode: 134.95 || average reward: 69.84 || episode length: 1000\n",
      "\n",
      "episode 153 in 14.8s: reward for episode: 185.86 || average reward: 67.41 || episode length: 1000\n",
      "\n",
      "episode 154 in 14.9s: reward for episode: 162.95 || average reward: 67.81 || episode length: 1000\n",
      "\n",
      "episode 155 in 14.8s: reward for episode: 69.45 || average reward: 65.72 || episode length: 1000\n",
      "\n",
      "episode 156 in 14.8s: reward for episode: 177.56 || average reward: 62.07 || episode length: 1000\n",
      "\n",
      "episode 157 in 14.9s: reward for episode: 195.98 || average reward: 62.2 || episode length: 1000\n",
      "\n",
      "episode 158 in 15.0s: reward for episode: 166.31 || average reward: 69.55 || episode length: 1000\n",
      "\n",
      "episode 159 in 14.9s: reward for episode: 206.33 || average reward: 76.02 || episode length: 1000\n",
      "\n",
      "episode 160 in 14.8s: reward for episode: 179.23 || average reward: 84.19 || episode length: 1000\n",
      "\n",
      "episode 161 in 14.8s: reward for episode: 231.5 || average reward: 92.32 || episode length: 1000\n",
      "\n",
      "episode 162 in 14.9s: reward for episode: 172.05 || average reward: 101.39 || episode length: 1000\n",
      "\n",
      "episode 163 in 15.0s: reward for episode: 83.76 || average reward: 107.91 || episode length: 1000\n",
      "\n",
      "episode 164 in 15.1s: reward for episode: 183.88 || average reward: 111.16 || episode length: 1000\n",
      "\n",
      "episode 165 in 15.0s: reward for episode: 142.38 || average reward: 118.5 || episode length: 1000\n",
      "\n",
      "episode 166 in 15.2s: reward for episode: 222.76 || average reward: 124.02 || episode length: 1000\n",
      "\n",
      "episode 167 in 14.9s: reward for episode: 207.68 || average reward: 131.6 || episode length: 1000\n",
      "\n",
      "episode 168 in 15.5s: reward for episode: 143.84 || average reward: 139.14 || episode length: 1000\n",
      "\n",
      "episode 169 in 15.3s: reward for episode: 224.43 || average reward: 144.55 || episode length: 1000\n",
      "\n",
      "episode 170 in 15.4s: reward for episode: 206.48 || average reward: 151.64 || episode length: 1000\n",
      "\n",
      "episode 171 in 15.3s: reward for episode: 232.67 || average reward: 157.15 || episode length: 1000\n",
      "\n",
      "episode 172 in 15.5s: reward for episode: 188.06 || average reward: 164.01 || episode length: 1000\n",
      "\n",
      "episode 173 in 15.4s: reward for episode: 240.58 || average reward: 167.74 || episode length: 1000\n",
      "\n",
      "episode 174 in 15.5s: reward for episode: 229.94 || average reward: 173.29 || episode length: 1000\n",
      "\n",
      "episode 175 in 15.5s: reward for episode: 242.99 || average reward: 178.85 || episode length: 1000\n",
      "\n",
      "episode 176 in 15.7s: reward for episode: 201.62 || average reward: 182.64 || episode length: 1000\n",
      "\n",
      "episode 177 in 15.5s: reward for episode: 232.46 || average reward: 185.33 || episode length: 1000\n",
      "\n",
      "episode 178 in 15.7s: reward for episode: 204.98 || average reward: 189.23 || episode length: 1000\n",
      "\n",
      "episode 179 in 15.5s: reward for episode: 220.96 || average reward: 190.0 || episode length: 1000\n",
      "\n",
      "episode 180 in 15.7s: reward for episode: 205.83 || average reward: 192.32 || episode length: 1000\n",
      "\n",
      "episode 181 in 15.7s: reward for episode: 220.6 || average reward: 197.77 || episode length: 1000\n",
      "\n",
      "episode 182 in 15.7s: reward for episode: 221.43 || average reward: 199.49 || episode length: 1000\n",
      "\n",
      "episode 183 in 15.7s: reward for episode: 172.43 || average reward: 200.51 || episode length: 1000\n",
      "\n",
      "episode 184 in 15.7s: reward for episode: 176.99 || average reward: 200.75 || episode length: 1000\n",
      "\n",
      "episode 185 in 15.7s: reward for episode: 212.9 || average reward: 199.58 || episode length: 1000\n",
      "\n",
      "episode 186 in 15.7s: reward for episode: 128.64 || average reward: 200.93 || episode length: 1000\n",
      "\n",
      "episode 187 in 15.6s: reward for episode: 200.71 || average reward: 196.81 || episode length: 1000\n",
      "\n",
      "episode 188 in 15.9s: reward for episode: 159.43 || average reward: 197.96 || episode length: 1000\n",
      "\n",
      "episode 189 in 15.8s: reward for episode: 5.11 || average reward: 200.99 || episode length: 1000\n",
      "\n",
      "episode 190 in 15.8s: reward for episode: 218.11 || average reward: 193.84 || episode length: 1000\n",
      "\n",
      "episode 191 in 15.8s: reward for episode: 228.76 || average reward: 196.87 || episode length: 1000\n",
      "\n",
      "episode 192 in 15.9s: reward for episode: 71.47 || average reward: 197.11 || episode length: 1000\n",
      "\n",
      "episode 193 in 16.4s: reward for episode: 13.42 || average reward: 191.66 || episode length: 1000\n",
      "\n",
      "episode 194 in 16.2s: reward for episode: 65.89 || average reward: 186.44 || episode length: 1000\n",
      "\n",
      "episode 195 in 15.8s: reward for episode: 58.57 || average reward: 180.1 || episode length: 1000\n",
      "\n",
      "episode 196 in 16.1s: reward for episode: 29.15 || average reward: 174.18 || episode length: 1000\n",
      "\n",
      "episode 197 in 15.9s: reward for episode: 128.53 || average reward: 166.04 || episode length: 1000\n",
      "\n",
      "episode 198 in 16.1s: reward for episode: 124.14 || average reward: 163.66 || episode length: 1000\n",
      "\n",
      "episode 199 in 15.8s: reward for episode: 210.8 || average reward: 159.0 || episode length: 1000\n",
      "\n",
      "episode 200 in 16.0s: reward for episode: 162.83 || average reward: 158.24 || episode length: 1000\n",
      "\n",
      "episode 201 in 16.3s: reward for episode: 87.43 || average reward: 155.03 || episode length: 1000\n",
      "\n",
      "episode 202 in 15.7s: reward for episode: 203.77 || average reward: 150.46 || episode length: 1000\n",
      "\n",
      "episode 203 in 15.9s: reward for episode: 233.06 || average reward: 149.31 || episode length: 1000\n",
      "\n",
      "episode 204 in 16.2s: reward for episode: 46.57 || average reward: 150.44 || episode length: 1000\n",
      "\n",
      "episode 205 in 15.9s: reward for episode: 224.5 || average reward: 143.46 || episode length: 1000\n",
      "\n",
      "episode 206 in 16.0s: reward for episode: 225.11 || average reward: 144.21 || episode length: 1000\n",
      "\n",
      "episode 207 in 16.0s: reward for episode: 232.43 || average reward: 144.39 || episode length: 1000\n",
      "\n",
      "episode 208 in 16.0s: reward for episode: 71.31 || average reward: 144.83 || episode length: 1000\n",
      "\n",
      "episode 209 in 15.9s: reward for episode: -18.64 || average reward: 140.78 || episode length: 1000\n",
      "\n",
      "episode 210 in 16.0s: reward for episode: 236.0 || average reward: 132.96 || episode length: 1000\n",
      "\n",
      "episode 211 in 16.0s: reward for episode: 204.49 || average reward: 133.88 || episode length: 1000\n",
      "\n",
      "episode 212 in 16.1s: reward for episode: 182.79 || average reward: 136.92 || episode length: 1000\n",
      "\n",
      "episode 213 in 16.0s: reward for episode: 245.92 || average reward: 136.2 || episode length: 1000\n",
      "\n",
      "episode 214 in 16.0s: reward for episode: 236.21 || average reward: 139.66 || episode length: 1000\n",
      "\n",
      "episode 215 in 16.0s: reward for episode: 174.12 || average reward: 148.9 || episode length: 1000\n",
      "\n",
      "episode 216 in 16.2s: reward for episode: 223.68 || average reward: 147.14 || episode length: 1000\n",
      "\n",
      "episode 217 in 15.9s: reward for episode: 240.46 || average reward: 146.94 || episode length: 1000\n",
      "\n",
      "episode 218 in 16.2s: reward for episode: 198.73 || average reward: 153.7 || episode length: 1000\n",
      "\n",
      "episode 219 in 16.3s: reward for episode: 237.19 || average reward: 161.11 || episode length: 1000\n",
      "\n",
      "episode 220 in 16.2s: reward for episode: 198.87 || average reward: 167.97 || episode length: 1000\n",
      "\n",
      "episode 221 in 16.3s: reward for episode: 172.97 || average reward: 173.58 || episode length: 1000\n",
      "\n",
      "episode 222 in 16.3s: reward for episode: 176.69 || average reward: 179.33 || episode length: 1000\n",
      "\n",
      "episode 223 in 16.3s: reward for episode: 24.23 || average reward: 181.26 || episode length: 1000\n",
      "\n",
      "episode 224 in 16.1s: reward for episode: 220.31 || average reward: 177.26 || episode length: 1000\n",
      "\n",
      "episode 225 in 16.2s: reward for episode: 205.93 || average reward: 177.64 || episode length: 1000\n",
      "\n",
      "episode 226 in 16.3s: reward for episode: 221.13 || average reward: 179.37 || episode length: 1000\n",
      "\n",
      "episode 227 in 16.3s: reward for episode: 229.08 || average reward: 184.71 || episode length: 1000\n",
      "\n",
      "episode 228 in 16.4s: reward for episode: 214.41 || average reward: 185.73 || episode length: 1000\n",
      "\n",
      "episode 229 in 16.3s: reward for episode: 228.9 || average reward: 184.98 || episode length: 1000\n",
      "\n",
      "episode 230 in 16.5s: reward for episode: 228.92 || average reward: 192.27 || episode length: 1000\n",
      "\n",
      "episode 231 in 16.3s: reward for episode: 242.57 || average reward: 192.45 || episode length: 1000\n",
      "\n",
      "episode 232 in 16.5s: reward for episode: 223.5 || average reward: 193.15 || episode length: 1000\n",
      "\n",
      "episode 233 in 16.4s: reward for episode: 221.98 || average reward: 192.79 || episode length: 1000\n",
      "\n",
      "episode 234 in 16.6s: reward for episode: 229.7 || average reward: 198.82 || episode length: 1000\n",
      "\n",
      "episode 235 in 16.5s: reward for episode: 180.28 || average reward: 208.75 || episode length: 1000\n",
      "\n",
      "episode 236 in 16.5s: reward for episode: 219.93 || average reward: 206.52 || episode length: 1000\n",
      "\n",
      "episode 237 in 14.8s: reward for episode: 219.06 || average reward: 207.14 || episode length: 1000\n",
      "\n",
      "episode 238 in 13.4s: reward for episode: 247.97 || average reward: 208.59 || episode length: 1000\n",
      "\n",
      "episode 239 in 13.5s: reward for episode: 267.03 || average reward: 208.67 || episode length: 1000\n",
      "\n",
      "episode 240 in 13.4s: reward for episode: 245.25 || average reward: 209.91 || episode length: 1000\n",
      "\n",
      "episode 241 in 13.3s: reward for episode: 263.02 || average reward: 212.75 || episode length: 1000\n",
      "\n",
      "episode 242 in 13.4s: reward for episode: 221.26 || average reward: 214.32 || episode length: 1000\n",
      "\n",
      "episode 243 in 13.3s: reward for episode: 245.21 || average reward: 213.56 || episode length: 1000\n",
      "\n",
      "episode 244 in 13.4s: reward for episode: 257.06 || average reward: 215.42 || episode length: 1000\n",
      "\n",
      "episode 245 in 13.3s: reward for episode: 221.1 || average reward: 216.21 || episode length: 1000\n",
      "\n",
      "episode 246 in 14.6s: reward for episode: 256.01 || average reward: 217.1 || episode length: 1000\n",
      "\n",
      "episode 247 in 15.2s: reward for episode: 285.74 || average reward: 220.42 || episode length: 1000\n",
      "\n",
      "episode 248 in 15.2s: reward for episode: 266.49 || average reward: 224.78 || episode length: 1000\n",
      "\n",
      "episode 249 in 15.2s: reward for episode: 260.71 || average reward: 234.47 || episode length: 1000\n",
      "\n",
      "episode 250 in 15.4s: reward for episode: 255.87 || average reward: 236.09 || episode length: 1000\n",
      "\n",
      "episode 251 in 15.5s: reward for episode: 246.6 || average reward: 238.09 || episode length: 1000\n",
      "\n",
      "episode 252 in 15.6s: reward for episode: 269.19 || average reward: 239.11 || episode length: 1000\n",
      "\n",
      "episode 253 in 15.5s: reward for episode: 246.01 || average reward: 240.71 || episode length: 1000\n",
      "\n",
      "episode 254 in 16.0s: reward for episode: 244.05 || average reward: 241.97 || episode length: 1000\n",
      "\n",
      "episode 255 in 15.9s: reward for episode: 279.95 || average reward: 242.58 || episode length: 1000\n",
      "\n",
      "episode 256 in 16.8s: reward for episode: 261.52 || average reward: 244.62 || episode length: 1000\n",
      "\n",
      "episode 257 in 16.7s: reward for episode: 266.52 || average reward: 245.38 || episode length: 1000\n",
      "\n",
      "episode 258 in 16.3s: reward for episode: 210.02 || average reward: 247.1 || episode length: 1000\n",
      "\n",
      "episode 259 in 16.9s: reward for episode: 268.0 || average reward: 246.62 || episode length: 1000\n",
      "\n",
      "episode 260 in 17.0s: reward for episode: 253.9 || average reward: 248.15 || episode length: 1000\n",
      "\n",
      "episode 261 in 16.0s: reward for episode: 308.04 || average reward: 251.1 || episode length: 1000\n",
      "\n",
      "episode 262 in 16.9s: reward for episode: 238.0 || average reward: 254.62 || episode length: 1000\n",
      "\n",
      "episode 263 in 16.9s: reward for episode: 245.51 || average reward: 255.38 || episode length: 1000\n",
      "\n",
      "episode 264 in 17.1s: reward for episode: 245.61 || average reward: 255.28 || episode length: 1000\n",
      "\n",
      "episode 265 in 17.2s: reward for episode: 262.25 || average reward: 254.43 || episode length: 1000\n",
      "\n",
      "episode 266 in 17.2s: reward for episode: 238.9 || average reward: 255.11 || episode length: 1000\n",
      "\n",
      "episode 267 in 18.0s: reward for episode: 213.64 || average reward: 254.14 || episode length: 1000\n",
      "\n",
      "episode 268 in 16.7s: reward for episode: 242.64 || average reward: 253.84 || episode length: 1000\n",
      "\n",
      "episode 269 in 13.8s: reward for episode: 234.72 || average reward: 253.73 || episode length: 1000\n",
      "\n",
      "episode 270 in 16.6s: reward for episode: 219.46 || average reward: 252.84 || episode length: 1000\n",
      "\n",
      "episode 271 in 17.3s: reward for episode: 254.58 || average reward: 252.77 || episode length: 1000\n",
      "\n",
      "episode 272 in 17.3s: reward for episode: 207.2 || average reward: 252.72 || episode length: 1000\n",
      "\n",
      "episode 273 in 15.4s: reward for episode: 276.9 || average reward: 249.58 || episode length: 1000\n",
      "\n",
      "episode 274 in 15.9s: reward for episode: 262.4 || average reward: 249.99 || episode length: 1000\n",
      "\n",
      "episode 275 in 16.0s: reward for episode: 290.77 || average reward: 250.06 || episode length: 1000\n",
      "\n",
      "episode 276 in 16.5s: reward for episode: 215.92 || average reward: 251.46 || episode length: 1000\n",
      "\n",
      "episode 277 in 16.3s: reward for episode: 260.57 || average reward: 250.23 || episode length: 1000\n",
      "\n",
      "episode 278 in 16.5s: reward for episode: 245.28 || average reward: 249.88 || episode length: 1000\n",
      "\n",
      "episode 279 in 16.3s: reward for episode: 229.15 || average reward: 249.85 || episode length: 1000\n",
      "\n",
      "episode 280 in 16.5s: reward for episode: 232.75 || average reward: 249.26 || episode length: 1000\n",
      "\n",
      "episode 281 in 16.4s: reward for episode: 265.2 || average reward: 247.37 || episode length: 1000\n",
      "\n",
      "episode 282 in 16.4s: reward for episode: 231.91 || average reward: 247.52 || episode length: 1000\n",
      "\n",
      "episode 283 in 16.3s: reward for episode: 257.28 || average reward: 246.13 || episode length: 1000\n",
      "\n",
      "episode 284 in 16.5s: reward for episode: 267.85 || average reward: 248.02 || episode length: 1000\n",
      "\n",
      "episode 285 in 16.5s: reward for episode: 245.02 || average reward: 248.02 || episode length: 1000\n",
      "\n",
      "episode 286 in 16.5s: reward for episode: 261.75 || average reward: 247.66 || episode length: 1000\n",
      "\n",
      "episode 287 in 16.7s: reward for episode: 282.33 || average reward: 245.81 || episode length: 1000\n",
      "\n",
      "episode 288 in 16.9s: reward for episode: 245.65 || average reward: 247.58 || episode length: 1000\n",
      "\n",
      "episode 289 in 16.8s: reward for episode: 299.54 || average reward: 247.59 || episode length: 1000\n",
      "\n",
      "episode 290 in 16.8s: reward for episode: 244.21 || average reward: 249.75 || episode length: 1000\n",
      "\n",
      "episode 291 in 14.3s: reward for episode: 289.35 || average reward: 249.03 || episode length: 1000\n",
      "\n",
      "episode 292 in 15.8s: reward for episode: 267.9 || average reward: 251.04 || episode length: 1000\n",
      "\n",
      "episode 293 in 17.5s: reward for episode: 274.02 || average reward: 253.21 || episode length: 1000\n",
      "\n",
      "episode 294 in 17.7s: reward for episode: 248.96 || average reward: 254.47 || episode length: 1000\n",
      "\n",
      "episode 295 in 17.8s: reward for episode: 234.36 || average reward: 255.04 || episode length: 1000\n",
      "\n",
      "episode 296 in 17.8s: reward for episode: 261.58 || average reward: 255.63 || episode length: 1000\n",
      "\n",
      "episode 297 in 16.1s: reward for episode: 311.82 || average reward: 255.91 || episode length: 1000\n",
      "\n",
      "episode 298 in 16.4s: reward for episode: 260.65 || average reward: 260.1 || episode length: 1000\n",
      "\n",
      "episode 299 in 16.5s: reward for episode: 219.36 || average reward: 259.45 || episode length: 1000\n",
      "\n",
      "episode 300 in 16.7s: reward for episode: 201.19 || average reward: 257.73 || episode length: 1000\n",
      "\n",
      "episode 301 in 16.6s: reward for episode: 254.97 || average reward: 254.15 || episode length: 1000\n",
      "\n",
      "episode 302 in 17.0s: reward for episode: 261.48 || average reward: 255.71 || episode length: 1000\n",
      "\n",
      "episode 303 in 17.0s: reward for episode: 273.67 || average reward: 255.74 || episode length: 1000\n",
      "\n",
      "episode 304 in 17.0s: reward for episode: 275.11 || average reward: 256.88 || episode length: 1000\n",
      "\n",
      "episode 305 in 17.1s: reward for episode: 288.22 || average reward: 258.72 || episode length: 1000\n",
      "\n",
      "episode 306 in 17.2s: reward for episode: 274.77 || average reward: 260.94 || episode length: 1000\n",
      "\n",
      "episode 307 in 17.0s: reward for episode: 277.02 || average reward: 261.32 || episode length: 1000\n",
      "\n",
      "episode 308 in 17.2s: reward for episode: 238.78 || average reward: 263.12 || episode length: 1000\n",
      "\n",
      "episode 309 in 17.2s: reward for episode: 275.02 || average reward: 262.38 || episode length: 1000\n",
      "\n",
      "episode 310 in 17.4s: reward for episode: 245.25 || average reward: 262.67 || episode length: 1000\n",
      "\n",
      "episode 311 in 17.2s: reward for episode: 290.55 || average reward: 262.68 || episode length: 1000\n",
      "\n",
      "episode 312 in 17.3s: reward for episode: 279.05 || average reward: 263.83 || episode length: 1000\n",
      "\n",
      "episode 313 in 17.2s: reward for episode: 291.2 || average reward: 263.7 || episode length: 1000\n",
      "\n",
      "episode 314 in 17.3s: reward for episode: 260.25 || average reward: 265.52 || episode length: 1000\n",
      "\n",
      "episode 315 in 17.3s: reward for episode: 282.47 || average reward: 263.95 || episode length: 1000\n",
      "\n",
      "episode 316 in 17.3s: reward for episode: 262.88 || average reward: 265.48 || episode length: 1000\n",
      "\n",
      "episode 317 in 17.3s: reward for episode: 262.56 || average reward: 264.42 || episode length: 1000\n",
      "\n",
      "episode 318 in 17.3s: reward for episode: 248.32 || average reward: 264.21 || episode length: 1000\n",
      "\n",
      "episode 319 in 17.2s: reward for episode: 300.47 || average reward: 263.18 || episode length: 1000\n",
      "\n",
      "episode 320 in 17.3s: reward for episode: 237.62 || average reward: 265.24 || episode length: 1000\n",
      "\n",
      "episode 321 in 17.4s: reward for episode: 286.98 || average reward: 265.37 || episode length: 1000\n",
      "\n",
      "episode 322 in 17.4s: reward for episode: 263.79 || average reward: 266.39 || episode length: 1000\n",
      "\n",
      "episode 323 in 17.4s: reward for episode: 274.1 || average reward: 264.47 || episode length: 1000\n",
      "\n",
      "episode 324 in 17.4s: reward for episode: 253.51 || average reward: 265.0 || episode length: 1000\n",
      "\n",
      "episode 325 in 17.3s: reward for episode: 263.3 || average reward: 266.37 || episode length: 1000\n",
      "\n",
      "episode 326 in 17.4s: reward for episode: 262.42 || average reward: 268.85 || episode length: 1000\n",
      "\n",
      "episode 327 in 17.4s: reward for episode: 303.56 || average reward: 269.15 || episode length: 1000\n",
      "\n",
      "episode 328 in 17.5s: reward for episode: 247.82 || average reward: 270.83 || episode length: 1000\n",
      "\n",
      "episode 329 in 17.5s: reward for episode: 300.28 || average reward: 269.8 || episode length: 1000\n",
      "\n",
      "episode 330 in 17.6s: reward for episode: 257.08 || average reward: 270.81 || episode length: 1000\n",
      "\n",
      "episode 331 in 17.5s: reward for episode: 272.12 || average reward: 269.56 || episode length: 1000\n",
      "\n",
      "episode 332 in 17.7s: reward for episode: 255.15 || average reward: 269.46 || episode length: 1000\n",
      "\n",
      "episode 333 in 17.4s: reward for episode: 301.46 || average reward: 268.58 || episode length: 1000\n",
      "\n",
      "episode 334 in 17.6s: reward for episode: 286.64 || average reward: 271.09 || episode length: 1000\n",
      "\n",
      "episode 335 in 17.6s: reward for episode: 306.7 || average reward: 271.55 || episode length: 1000\n",
      "\n",
      "episode 336 in 17.6s: reward for episode: 262.3 || average reward: 274.01 || episode length: 1000\n",
      "\n",
      "episode 337 in 18.1s: reward for episode: 288.63 || average reward: 272.88 || episode length: 1000\n",
      "\n",
      "episode 338 in 18.8s: reward for episode: 272.29 || average reward: 273.26 || episode length: 1000\n",
      "\n",
      "episode 339 in 18.6s: reward for episode: 282.05 || average reward: 272.51 || episode length: 1000\n",
      "\n",
      "episode 340 in 18.4s: reward for episode: 269.21 || average reward: 273.38 || episode length: 1000\n",
      "\n",
      "episode 341 in 18.2s: reward for episode: 273.29 || average reward: 272.85 || episode length: 1000\n",
      "\n",
      "episode 342 in 18.8s: reward for episode: 264.64 || average reward: 273.27 || episode length: 1000\n",
      "\n",
      "episode 343 in 18.4s: reward for episode: 276.32 || average reward: 273.35 || episode length: 1000\n",
      "\n",
      "episode 344 in 18.3s: reward for episode: 291.09 || average reward: 274.47 || episode length: 1000\n",
      "\n",
      "episode 345 in 18.3s: reward for episode: 308.96 || average reward: 274.09 || episode length: 1000\n",
      "\n",
      "episode 346 in 18.4s: reward for episode: 271.78 || average reward: 276.95 || episode length: 1000\n",
      "\n",
      "episode 347 in 18.2s: reward for episode: 299.74 || average reward: 276.34 || episode length: 1000\n",
      "\n",
      "episode 348 in 18.3s: reward for episode: 289.9 || average reward: 277.78 || episode length: 1000\n",
      "\n",
      "episode 349 in 18.3s: reward for episode: 285.37 || average reward: 278.41 || episode length: 1000\n",
      "\n",
      "episode 350 in 18.3s: reward for episode: 258.9 || average reward: 279.68 || episode length: 1000\n",
      "\n",
      "episode 351 in 18.2s: reward for episode: 309.87 || average reward: 279.51 || episode length: 1000\n",
      "\n",
      "episode 352 in 18.4s: reward for episode: 283.97 || average reward: 281.41 || episode length: 1000\n",
      "\n",
      "episode 353 in 17.7s: reward for episode: 299.63 || average reward: 280.62 || episode length: 1000\n",
      "\n",
      "episode 354 in 18.1s: reward for episode: 276.67 || average reward: 282.69 || episode length: 1000\n",
      "\n",
      "episode 355 in 18.0s: reward for episode: 254.66 || average reward: 281.75 || episode length: 1000\n",
      "\n",
      "episode 356 in 18.1s: reward for episode: 254.27 || average reward: 281.65 || episode length: 1000\n",
      "\n",
      "episode 357 in 17.8s: reward for episode: 293.71 || average reward: 280.94 || episode length: 1000\n",
      "\n",
      "episode 358 in 18.3s: reward for episode: 278.25 || average reward: 282.48 || episode length: 1000\n",
      "\n",
      "episode 359 in 18.2s: reward for episode: 301.49 || average reward: 281.55 || episode length: 1000\n",
      "\n",
      "episode 360 in 18.1s: reward for episode: 289.96 || average reward: 282.15 || episode length: 1000\n",
      "\n",
      "episode 361 in 18.2s: reward for episode: 304.25 || average reward: 281.48 || episode length: 1000\n",
      "\n",
      "episode 362 in 18.3s: reward for episode: 256.89 || average reward: 283.16 || episode length: 1000\n",
      "\n",
      "episode 363 in 18.3s: reward for episode: 310.24 || average reward: 281.89 || episode length: 1000\n",
      "\n",
      "episode 364 in 18.2s: reward for episode: 292.42 || average reward: 283.4 || episode length: 1000\n",
      "\n",
      "episode 365 in 18.4s: reward for episode: 293.96 || average reward: 283.82 || episode length: 1000\n",
      "\n",
      "episode 366 in 18.2s: reward for episode: 214.08 || average reward: 284.81 || episode length: 1000\n",
      "\n",
      "episode 367 in 18.1s: reward for episode: 319.5 || average reward: 282.44 || episode length: 1000\n",
      "\n",
      "episode 368 in 18.1s: reward for episode: 283.14 || average reward: 284.64 || episode length: 1000\n",
      "\n",
      "episode 369 in 18.2s: reward for episode: 298.77 || average reward: 284.91 || episode length: 1000\n",
      "\n",
      "episode 370 in 18.5s: reward for episode: 276.58 || average reward: 285.21 || episode length: 1000\n",
      "\n",
      "episode 371 in 18.2s: reward for episode: 299.1 || average reward: 283.92 || episode length: 1000\n",
      "\n",
      "episode 372 in 18.3s: reward for episode: 274.12 || average reward: 285.01 || episode length: 1000\n",
      "\n",
      "episode 373 in 18.2s: reward for episode: 303.97 || average reward: 283.99 || episode length: 1000\n",
      "\n",
      "episode 374 in 18.5s: reward for episode: 279.42 || average reward: 284.55 || episode length: 1000\n",
      "\n",
      "episode 375 in 18.4s: reward for episode: 301.73 || average reward: 284.31 || episode length: 1000\n",
      "\n",
      "episode 376 in 18.6s: reward for episode: 270.67 || average reward: 286.03 || episode length: 1000\n",
      "\n",
      "episode 377 in 18.5s: reward for episode: 324.64 || average reward: 284.46 || episode length: 1000\n",
      "\n",
      "episode 378 in 18.5s: reward for episode: 281.51 || average reward: 286.08 || episode length: 1000\n",
      "\n",
      "episode 379 in 18.4s: reward for episode: 298.12 || average reward: 285.36 || episode length: 1000\n",
      "\n",
      "episode 380 in 18.4s: reward for episode: 257.78 || average reward: 286.22 || episode length: 1000\n",
      "\n",
      "episode 381 in 18.4s: reward for episode: 306.3 || average reward: 286.34 || episode length: 1000\n",
      "\n",
      "episode 382 in 18.6s: reward for episode: 281.8 || average reward: 288.42 || episode length: 1000\n",
      "\n",
      "episode 383 in 18.5s: reward for episode: 308.25 || average reward: 287.95 || episode length: 1000\n",
      "\n",
      "episode 384 in 18.5s: reward for episode: 274.01 || average reward: 289.15 || episode length: 1000\n",
      "\n",
      "episode 385 in 18.3s: reward for episode: 286.99 || average reward: 288.05 || episode length: 1000\n",
      "\n",
      "episode 386 in 18.4s: reward for episode: 261.0 || average reward: 287.93 || episode length: 1000\n",
      "\n",
      "episode 387 in 18.5s: reward for episode: 304.77 || average reward: 286.2 || episode length: 1000\n",
      "\n",
      "episode 388 in 18.6s: reward for episode: 268.28 || average reward: 288.12 || episode length: 1000\n",
      "\n",
      "episode 389 in 18.5s: reward for episode: 307.47 || average reward: 286.44 || episode length: 1000\n",
      "\n",
      "episode 390 in 17.7s: reward for episode: 289.19 || average reward: 287.04 || episode length: 1000\n",
      "\n",
      "episode 391 in 16.2s: reward for episode: 313.69 || average reward: 286.85 || episode length: 1000\n",
      "\n",
      "episode 392 in 17.3s: reward for episode: 295.5 || average reward: 290.83 || episode length: 1000\n",
      "\n",
      "episode 393 in 17.5s: reward for episode: 316.26 || average reward: 289.87 || episode length: 1000\n",
      "\n",
      "episode 394 in 17.7s: reward for episode: 291.83 || average reward: 291.2 || episode length: 1000\n",
      "\n",
      "episode 395 in 18.0s: reward for episode: 305.46 || average reward: 290.92 || episode length: 1000\n",
      "\n",
      "episode 396 in 17.8s: reward for episode: 282.83 || average reward: 292.08 || episode length: 1000\n",
      "\n",
      "episode 397 in 15.4s: reward for episode: 277.88 || average reward: 291.42 || episode length: 1000\n",
      "\n",
      "episode 398 in 15.6s: reward for episode: 285.64 || average reward: 291.57 || episode length: 1000\n",
      "\n",
      "episode 399 in 15.6s: reward for episode: 292.09 || average reward: 290.84 || episode length: 1000\n",
      "\n",
      "episode 400 in 15.6s: reward for episode: 265.46 || average reward: 291.35 || episode length: 1000\n",
      "\n",
      "episode 401 in 16.0s: reward for episode: 291.52 || average reward: 289.9 || episode length: 1000\n",
      "\n",
      "episode 402 in 17.4s: reward for episode: 254.11 || average reward: 290.73 || episode length: 1000\n",
      "\n",
      "episode 403 in 17.2s: reward for episode: 271.36 || average reward: 287.91 || episode length: 1000\n",
      "\n",
      "episode 404 in 17.7s: reward for episode: 274.33 || average reward: 287.5 || episode length: 1000\n",
      "\n",
      "episode 405 in 17.9s: reward for episode: 258.92 || average reward: 286.55 || episode length: 1000\n",
      "\n",
      "episode 406 in 18.0s: reward for episode: 292.71 || average reward: 286.6 || episode length: 1000\n",
      "\n",
      "episode 407 in 18.0s: reward for episode: 290.89 || average reward: 286.05 || episode length: 1000\n",
      "\n",
      "episode 408 in 18.2s: reward for episode: 291.75 || average reward: 286.42 || episode length: 1000\n",
      "\n",
      "episode 409 in 18.4s: reward for episode: 295.11 || average reward: 285.76 || episode length: 1000\n",
      "\n",
      "episode 410 in 18.8s: reward for episode: 285.53 || average reward: 286.6 || episode length: 1000\n",
      "\n",
      "episode 411 in 18.4s: reward for episode: 291.89 || average reward: 286.54 || episode length: 1000\n",
      "\n",
      "episode 412 in 19.2s: reward for episode: 273.43 || average reward: 287.78 || episode length: 1000\n",
      "\n",
      "episode 413 in 18.4s: reward for episode: 300.72 || average reward: 286.53 || episode length: 1000\n",
      "\n",
      "episode 414 in 19.6s: reward for episode: 287.3 || average reward: 287.82 || episode length: 1000\n",
      "\n",
      "episode 415 in 18.5s: reward for episode: 303.69 || average reward: 287.02 || episode length: 1000\n",
      "\n",
      "episode 416 in 18.6s: reward for episode: 253.38 || average reward: 287.6 || episode length: 1000\n",
      "\n",
      "episode 417 in 18.6s: reward for episode: 287.07 || average reward: 285.18 || episode length: 1000\n",
      "\n",
      "episode 418 in 18.6s: reward for episode: 304.06 || average reward: 284.85 || episode length: 1000\n",
      "\n",
      "episode 419 in 18.6s: reward for episode: 314.0 || average reward: 284.36 || episode length: 1000\n",
      "\n",
      "episode 420 in 18.8s: reward for episode: 301.19 || average reward: 285.25 || episode length: 1000\n",
      "\n",
      "episode 421 in 18.6s: reward for episode: 311.76 || average reward: 285.07 || episode length: 1000\n",
      "\n",
      "episode 422 in 18.8s: reward for episode: 299.04 || average reward: 286.23 || episode length: 1000\n",
      "\n",
      "episode 423 in 18.8s: reward for episode: 311.75 || average reward: 287.08 || episode length: 1000\n",
      "\n",
      "episode 424 in 19.1s: reward for episode: 258.21 || average reward: 288.12 || episode length: 1000\n",
      "\n",
      "episode 425 in 19.1s: reward for episode: 325.14 || average reward: 286.77 || episode length: 1000\n",
      "\n",
      "episode 426 in 21.3s: reward for episode: 289.29 || average reward: 289.15 || episode length: 1000\n",
      "\n",
      "episode 427 in 18.7s: reward for episode: 124.18 || average reward: 289.07 || episode length: 1000\n",
      "\n",
      "episode 428 in 19.2s: reward for episode: 291.52 || average reward: 283.87 || episode length: 1000\n",
      "\n",
      "episode 429 in 19.2s: reward for episode: 300.3 || average reward: 284.67 || episode length: 1000\n",
      "\n",
      "episode 430 in 19.4s: reward for episode: 288.19 || average reward: 285.71 || episode length: 1000\n",
      "\n",
      "episode 431 in 19.3s: reward for episode: 330.91 || average reward: 286.88 || episode length: 1000\n",
      "\n",
      "episode 432 in 19.6s: reward for episode: 300.3 || average reward: 288.41 || episode length: 1000\n",
      "\n",
      "episode 433 in 19.4s: reward for episode: 308.75 || average reward: 288.79 || episode length: 1000\n",
      "\n",
      "episode 434 in 19.4s: reward for episode: 225.41 || average reward: 289.47 || episode length: 1000\n",
      "\n",
      "episode 435 in 19.3s: reward for episode: 279.71 || average reward: 286.68 || episode length: 1000\n",
      "\n",
      "episode 436 in 19.4s: reward for episode: 272.39 || average reward: 286.45 || episode length: 1000\n",
      "\n",
      "episode 437 in 19.2s: reward for episode: 301.9 || average reward: 285.67 || episode length: 1000\n",
      "\n",
      "episode 438 in 19.3s: reward for episode: 275.14 || average reward: 286.81 || episode length: 1000\n",
      "\n",
      "episode 439 in 19.4s: reward for episode: 343.99 || average reward: 285.78 || episode length: 1000\n",
      "\n",
      "episode 440 in 20.1s: reward for episode: 274.79 || average reward: 288.05 || episode length: 1000\n",
      "\n",
      "episode 441 in 19.8s: reward for episode: 334.01 || average reward: 286.89 || episode length: 1000\n",
      "\n",
      "episode 442 in 19.4s: reward for episode: 282.14 || average reward: 290.12 || episode length: 1000\n",
      "\n",
      "episode 443 in 19.1s: reward for episode: 330.4 || average reward: 289.92 || episode length: 1000\n",
      "\n",
      "episode 444 in 17.4s: reward for episode: 298.27 || average reward: 290.98 || episode length: 1000\n",
      "\n",
      "episode 445 in 16.2s: reward for episode: 299.09 || average reward: 290.35 || episode length: 1000\n",
      "\n",
      "episode 446 in 16.4s: reward for episode: 288.25 || average reward: 290.26 || episode length: 1000\n",
      "\n",
      "episode 447 in 17.9s: reward for episode: 217.7 || average reward: 289.32 || episode length: 1000\n",
      "\n",
      "episode 448 in 18.2s: reward for episode: 271.4 || average reward: 286.07 || episode length: 1000\n",
      "\n",
      "episode 449 in 18.2s: reward for episode: 328.32 || average reward: 284.46 || episode length: 1000\n",
      "\n",
      "episode 450 in 18.3s: reward for episode: 303.57 || average reward: 287.26 || episode length: 1000\n",
      "\n",
      "episode 451 in 18.7s: reward for episode: 317.57 || average reward: 286.4 || episode length: 1000\n",
      "\n",
      "episode 452 in 18.8s: reward for episode: 298.2 || average reward: 287.53 || episode length: 1000\n",
      "\n",
      "episode 453 in 18.8s: reward for episode: 311.55 || average reward: 294.49 || episode length: 1000\n",
      "\n",
      "episode 454 in 18.7s: reward for episode: 281.27 || average reward: 295.29 || episode length: 1000\n",
      "\n",
      "episode 455 in 18.8s: reward for episode: 302.86 || average reward: 294.53 || episode length: 1000\n",
      "\n",
      "episode 456 in 19.6s: reward for episode: 283.55 || average reward: 295.12 || episode length: 1000\n",
      "\n",
      "episode 457 in 15.3s: reward for episode: 321.67 || average reward: 293.22 || episode length: 1000\n",
      "\n",
      "episode 458 in 15.9s: reward for episode: 294.55 || average reward: 294.08 || episode length: 1000\n",
      "\n",
      "episode 459 in 15.9s: reward for episode: 297.75 || average reward: 293.51 || episode length: 1000\n",
      "\n",
      "episode 460 in 14.6s: reward for episode: 297.82 || average reward: 296.4 || episode length: 1000\n",
      "\n",
      "episode 461 in 14.6s: reward for episode: 313.07 || average reward: 297.13 || episode length: 1000\n",
      "\n",
      "episode 462 in 14.7s: reward for episode: 257.2 || average reward: 298.75 || episode length: 1000\n",
      "\n",
      "episode 463 in 14.6s: reward for episode: 309.48 || average reward: 296.97 || episode length: 1000\n",
      "\n",
      "episode 464 in 14.5s: reward for episode: 283.79 || average reward: 298.34 || episode length: 1000\n",
      "\n",
      "episode 465 in 14.6s: reward for episode: 253.34 || average reward: 295.93 || episode length: 1000\n",
      "\n",
      "episode 466 in 14.7s: reward for episode: 261.62 || average reward: 295.07 || episode length: 1000\n",
      "\n",
      "episode 467 in 14.6s: reward for episode: 317.74 || average reward: 292.18 || episode length: 1000\n",
      "\n",
      "episode 468 in 14.7s: reward for episode: 280.87 || average reward: 293.6 || episode length: 1000\n",
      "\n",
      "episode 469 in 14.6s: reward for episode: 323.87 || average reward: 291.62 || episode length: 1000\n",
      "\n",
      "episode 470 in 14.8s: reward for episode: 272.08 || average reward: 292.65 || episode length: 1000\n",
      "\n",
      "episode 471 in 14.7s: reward for episode: 307.52 || average reward: 291.56 || episode length: 1000\n",
      "\n",
      "episode 472 in 14.7s: reward for episode: 285.36 || average reward: 292.34 || episode length: 1000\n",
      "\n",
      "episode 473 in 14.7s: reward for episode: 306.12 || average reward: 295.04 || episode length: 1000\n",
      "\n",
      "episode 474 in 14.7s: reward for episode: 296.3 || average reward: 296.43 || episode length: 1000\n",
      "\n",
      "episode 475 in 14.8s: reward for episode: 333.37 || average reward: 295.15 || episode length: 1000\n",
      "\n",
      "episode 476 in 14.8s: reward for episode: 294.14 || average reward: 296.34 || episode length: 1000\n",
      "\n",
      "episode 477 in 15.1s: reward for episode: 313.4 || average reward: 295.4 || episode length: 1000\n",
      "\n",
      "episode 478 in 14.6s: reward for episode: 296.36 || average reward: 296.01 || episode length: 1000\n",
      "\n",
      "episode 479 in 15.2s: reward for episode: 335.12 || average reward: 295.4 || episode length: 1000\n",
      "\n",
      "episode 480 in 15.0s: reward for episode: 280.89 || average reward: 297.56 || episode length: 1000\n",
      "\n",
      "episode 481 in 15.1s: reward for episode: 298.67 || average reward: 296.68 || episode length: 1000\n",
      "\n",
      "episode 482 in 15.0s: reward for episode: 290.13 || average reward: 297.28 || episode length: 1000\n",
      "\n",
      "episode 483 in 14.9s: reward for episode: 298.17 || average reward: 296.02 || episode length: 1000\n",
      "\n",
      "episode 484 in 15.0s: reward for episode: 295.19 || average reward: 296.17 || episode length: 1000\n",
      "\n",
      "episode 485 in 15.1s: reward for episode: 325.98 || average reward: 296.06 || episode length: 1000\n",
      "\n",
      "episode 486 in 15.1s: reward for episode: 303.18 || average reward: 297.19 || episode length: 1000\n",
      "\n",
      "episode 487 in 14.8s: reward for episode: 307.01 || average reward: 296.8 || episode length: 1000\n",
      "\n",
      "episode 488 in 15.1s: reward for episode: 293.84 || average reward: 298.79 || episode length: 1000\n",
      "\n",
      "episode 489 in 14.7s: reward for episode: 327.2 || average reward: 298.16 || episode length: 1000\n",
      "\n",
      "episode 490 in 15.0s: reward for episode: 300.82 || average reward: 299.9 || episode length: 1000\n",
      "\n",
      "episode 491 in 15.1s: reward for episode: 327.67 || average reward: 301.8 || episode length: 1000\n",
      "\n",
      "episode 492 in 15.0s: reward for episode: 288.94 || average reward: 304.44 || episode length: 1000\n",
      "\n",
      "episode 493 in 14.9s: reward for episode: 323.53 || average reward: 303.29 || episode length: 1000\n",
      "\n",
      "episode 494 in 15.1s: reward for episode: 278.81 || average reward: 304.99 || episode length: 1000\n",
      "\n",
      "episode 495 in 14.9s: reward for episode: 319.96 || average reward: 303.19 || episode length: 1000\n",
      "\n",
      "episode 496 in 15.1s: reward for episode: 302.07 || average reward: 305.11 || episode length: 1000\n",
      "\n",
      "episode 497 in 15.1s: reward for episode: 318.94 || average reward: 304.89 || episode length: 1000\n",
      "\n",
      "episode 498 in 15.1s: reward for episode: 296.51 || average reward: 306.23 || episode length: 1000\n",
      "\n",
      "episode 499 in 14.9s: reward for episode: 315.09 || average reward: 305.85 || episode length: 1000\n",
      "\n",
      "episode 500 in 15.0s: reward for episode: 281.84 || average reward: 306.6 || episode length: 1000\n",
      "\n",
      "episode 501 in 14.9s: reward for episode: 304.65 || average reward: 304.54 || episode length: 1000\n",
      "\n",
      "episode 502 in 15.1s: reward for episode: 303.23 || average reward: 304.96 || episode length: 1000\n",
      "\n",
      "episode 503 in 15.0s: reward for episode: 284.53 || average reward: 304.55 || episode length: 1000\n",
      "\n",
      "episode 504 in 15.0s: reward for episode: 304.22 || average reward: 304.08 || episode length: 1000\n",
      "\n",
      "episode 505 in 14.9s: reward for episode: 294.61 || average reward: 302.84 || episode length: 1000\n",
      "\n",
      "episode 506 in 15.1s: reward for episode: 300.71 || average reward: 303.39 || episode length: 1000\n",
      "\n",
      "episode 507 in 15.0s: reward for episode: 309.86 || average reward: 303.47 || episode length: 1000\n",
      "\n",
      "episode 508 in 15.0s: reward for episode: 308.14 || average reward: 304.26 || episode length: 1000\n",
      "\n",
      "episode 509 in 15.0s: reward for episode: 330.12 || average reward: 304.66 || episode length: 1000\n",
      "\n",
      "episode 510 in 15.1s: reward for episode: 297.19 || average reward: 306.06 || episode length: 1000\n",
      "\n",
      "episode 511 in 14.9s: reward for episode: 331.54 || average reward: 304.91 || episode length: 1000\n",
      "\n",
      "episode 512 in 15.0s: reward for episode: 300.06 || average reward: 306.04 || episode length: 1000\n",
      "\n",
      "episode 513 in 15.0s: reward for episode: 334.6 || average reward: 305.76 || episode length: 1000\n",
      "\n",
      "episode 514 in 15.1s: reward for episode: 309.99 || average reward: 307.39 || episode length: 1000\n",
      "\n",
      "episode 515 in 15.0s: reward for episode: 326.75 || average reward: 306.7 || episode length: 1000\n",
      "\n",
      "episode 516 in 15.1s: reward for episode: 275.69 || average reward: 307.74 || episode length: 1000\n",
      "\n",
      "episode 517 in 14.9s: reward for episode: 333.33 || average reward: 305.66 || episode length: 1000\n",
      "\n",
      "episode 518 in 15.1s: reward for episode: 275.4 || average reward: 307.44 || episode length: 1000\n",
      "\n",
      "episode 519 in 15.0s: reward for episode: 327.76 || average reward: 305.51 || episode length: 1000\n",
      "\n",
      "episode 520 in 15.9s: reward for episode: 292.74 || average reward: 307.47 || episode length: 1000\n",
      "\n",
      "episode 521 in 16.8s: reward for episode: 320.97 || average reward: 306.38 || episode length: 1000\n",
      "\n",
      "episode 522 in 18.1s: reward for episode: 297.61 || average reward: 307.14 || episode length: 1000\n",
      "\n",
      "episode 523 in 18.7s: reward for episode: 314.82 || average reward: 306.29 || episode length: 1000\n",
      "\n",
      "episode 524 in 18.1s: reward for episode: 299.05 || average reward: 307.02 || episode length: 1000\n",
      "\n",
      "episode 525 in 18.2s: reward for episode: 303.7 || average reward: 306.38 || episode length: 1000\n",
      "\n",
      "episode 526 in 18.0s: reward for episode: 289.5 || average reward: 307.25 || episode length: 1000\n",
      "\n",
      "episode 527 in 17.5s: reward for episode: 295.02 || average reward: 306.64 || episode length: 1000\n",
      "\n",
      "episode 528 in 17.7s: reward for episode: 297.87 || average reward: 306.32 || episode length: 1000\n",
      "\n",
      "episode 529 in 17.4s: reward for episode: 304.47 || average reward: 306.85 || episode length: 1000\n",
      "\n",
      "episode 530 in 17.6s: reward for episode: 287.01 || average reward: 306.86 || episode length: 1000\n",
      "\n",
      "episode 531 in 17.6s: reward for episode: 317.39 || average reward: 306.56 || episode length: 1000\n",
      "\n",
      "episode 532 in 17.7s: reward for episode: 289.26 || average reward: 307.22 || episode length: 1000\n",
      "\n",
      "episode 533 in 17.6s: reward for episode: 309.26 || average reward: 306.4 || episode length: 1000\n",
      "\n",
      "episode 534 in 17.5s: reward for episode: 279.03 || average reward: 306.44 || episode length: 1000\n",
      "\n",
      "episode 535 in 17.5s: reward for episode: 327.22 || average reward: 304.4 || episode length: 1000\n",
      "\n",
      "episode 536 in 17.6s: reward for episode: 284.97 || average reward: 305.6 || episode length: 1000\n",
      "\n",
      "episode 537 in 18.1s: reward for episode: 329.14 || average reward: 303.74 || episode length: 1000\n",
      "\n",
      "episode 538 in 16.8s: reward for episode: 291.1 || average reward: 304.9 || episode length: 1000\n",
      "\n",
      "episode 539 in 15.5s: reward for episode: 328.41 || average reward: 303.16 || episode length: 1000\n",
      "\n",
      "episode 540 in 15.7s: reward for episode: 296.64 || average reward: 303.9 || episode length: 1000\n",
      "\n",
      "episode 541 in 15.8s: reward for episode: 311.04 || average reward: 302.7 || episode length: 1000\n",
      "\n",
      "episode 542 in 18.8s: reward for episode: 296.07 || average reward: 304.11 || episode length: 1000\n",
      "\n",
      "episode 543 in 18.3s: reward for episode: 324.97 || average reward: 302.62 || episode length: 1000\n",
      "\n",
      "episode 544 in 18.4s: reward for episode: 289.29 || average reward: 304.6 || episode length: 1000\n",
      "\n",
      "episode 545 in 18.3s: reward for episode: 290.46 || average reward: 303.06 || episode length: 1000\n",
      "\n",
      "episode 546 in 18.5s: reward for episode: 282.21 || average reward: 302.97 || episode length: 1000\n",
      "\n",
      "episode 547 in 18.4s: reward for episode: 301.8 || average reward: 301.42 || episode length: 1000\n",
      "\n",
      "episode 548 in 18.5s: reward for episode: 253.67 || average reward: 301.59 || episode length: 1000\n",
      "\n",
      "episode 549 in 18.3s: reward for episode: 309.91 || average reward: 299.14 || episode length: 1000\n",
      "\n",
      "episode 550 in 18.4s: reward for episode: 287.06 || average reward: 299.58 || episode length: 1000\n",
      "\n",
      "episode 551 in 18.4s: reward for episode: 312.72 || average reward: 298.91 || episode length: 1000\n",
      "\n",
      "episode 552 in 18.4s: reward for episode: 301.58 || average reward: 299.84 || episode length: 1000\n",
      "\n",
      "episode 553 in 18.4s: reward for episode: 332.96 || average reward: 300.1 || episode length: 1000\n",
      "\n",
      "episode 554 in 18.4s: reward for episode: 277.71 || average reward: 301.51 || episode length: 1000\n",
      "\n",
      "episode 555 in 18.6s: reward for episode: 331.87 || average reward: 300.44 || episode length: 1000\n",
      "\n",
      "episode 556 in 18.5s: reward for episode: 281.32 || average reward: 302.23 || episode length: 1000\n",
      "\n",
      "episode 557 in 18.5s: reward for episode: 307.6 || average reward: 300.79 || episode length: 1000\n",
      "\n",
      "episode 558 in 18.6s: reward for episode: 279.31 || average reward: 301.52 || episode length: 1000\n",
      "\n",
      "episode 559 in 18.4s: reward for episode: 304.83 || average reward: 300.32 || episode length: 1000\n",
      "\n",
      "episode 560 in 18.5s: reward for episode: 297.44 || average reward: 301.35 || episode length: 1000\n",
      "\n",
      "episode 561 in 18.5s: reward for episode: 327.69 || average reward: 300.16 || episode length: 1000\n",
      "\n",
      "episode 562 in 18.5s: reward for episode: 289.8 || average reward: 301.87 || episode length: 1000\n",
      "\n",
      "episode 563 in 18.5s: reward for episode: 330.27 || average reward: 300.3 || episode length: 1000\n",
      "\n",
      "episode 564 in 18.5s: reward for episode: 297.29 || average reward: 301.87 || episode length: 1000\n",
      "\n",
      "episode 565 in 18.6s: reward for episode: 310.75 || average reward: 300.62 || episode length: 1000\n",
      "\n",
      "episode 566 in 18.7s: reward for episode: 295.38 || average reward: 301.19 || episode length: 1000\n",
      "\n",
      "episode 567 in 18.5s: reward for episode: 319.09 || average reward: 300.56 || episode length: 1000\n",
      "\n",
      "episode 568 in 18.7s: reward for episode: 305.52 || average reward: 301.48 || episode length: 1000\n",
      "\n",
      "episode 569 in 18.6s: reward for episode: 316.0 || average reward: 300.7 || episode length: 1000\n",
      "\n",
      "episode 570 in 18.7s: reward for episode: 288.08 || average reward: 301.77 || episode length: 1000\n",
      "\n",
      "episode 571 in 18.7s: reward for episode: 320.72 || average reward: 301.67 || episode length: 1000\n",
      "\n",
      "episode 572 in 18.9s: reward for episode: 297.18 || average reward: 303.21 || episode length: 1000\n",
      "\n",
      "episode 573 in 18.8s: reward for episode: 318.01 || average reward: 303.03 || episode length: 1000\n",
      "\n",
      "episode 574 in 18.8s: reward for episode: 284.73 || average reward: 305.6 || episode length: 1000\n",
      "\n",
      "episode 575 in 18.8s: reward for episode: 315.82 || average reward: 304.6 || episode length: 1000\n",
      "\n",
      "episode 576 in 18.7s: reward for episode: 298.78 || average reward: 305.75 || episode length: 1000\n",
      "\n",
      "episode 577 in 19.2s: reward for episode: 332.34 || average reward: 305.19 || episode length: 1000\n",
      "\n",
      "episode 578 in 18.7s: reward for episode: 305.49 || average reward: 306.42 || episode length: 1000\n",
      "\n",
      "episode 579 in 18.8s: reward for episode: 329.44 || average reward: 305.32 || episode length: 1000\n",
      "\n",
      "episode 580 in 18.7s: reward for episode: 293.99 || average reward: 307.39 || episode length: 1000\n",
      "\n",
      "episode 581 in 18.8s: reward for episode: 324.9 || average reward: 305.87 || episode length: 1000\n",
      "\n",
      "episode 582 in 18.9s: reward for episode: 292.9 || average reward: 307.62 || episode length: 1000\n",
      "\n",
      "episode 583 in 18.8s: reward for episode: 319.53 || average reward: 307.03 || episode length: 1000\n",
      "\n",
      "episode 584 in 19.0s: reward for episode: 309.03 || average reward: 308.64 || episode length: 1000\n",
      "\n",
      "episode 585 in 16.2s: reward for episode: 332.46 || average reward: 308.81 || episode length: 1000\n",
      "\n",
      "episode 586 in 17.8s: reward for episode: 307.16 || average reward: 310.21 || episode length: 1000\n",
      "\n",
      "episode 587 in 16.4s: reward for episode: 334.9 || average reward: 309.39 || episode length: 1000\n",
      "\n",
      "episode 588 in 16.1s: reward for episode: 278.16 || average reward: 311.19 || episode length: 1000\n",
      "\n",
      "episode 589 in 16.1s: reward for episode: 332.93 || average reward: 309.11 || episode length: 1000\n",
      "\n",
      "episode 590 in 17.5s: reward for episode: 304.56 || average reward: 310.53 || episode length: 1000\n",
      "\n",
      "episode 591 in 17.0s: reward for episode: 232.74 || average reward: 310.28 || episode length: 1000\n",
      "\n",
      "episode 592 in 16.4s: reward for episode: 287.04 || average reward: 307.78 || episode length: 1000\n",
      "\n",
      "episode 593 in 16.3s: reward for episode: 319.65 || average reward: 306.5 || episode length: 1000\n",
      "\n",
      "episode 594 in 16.5s: reward for episode: 286.59 || average reward: 307.06 || episode length: 1000\n",
      "\n",
      "episode 595 in 16.4s: reward for episode: 305.83 || average reward: 305.88 || episode length: 1000\n",
      "\n",
      "episode 596 in 17.5s: reward for episode: 299.74 || average reward: 306.59 || episode length: 1000\n",
      "\n",
      "episode 597 in 17.2s: reward for episode: 321.17 || average reward: 305.76 || episode length: 1000\n",
      "\n",
      "episode 598 in 17.8s: reward for episode: 250.99 || average reward: 306.72 || episode length: 1000\n",
      "\n",
      "episode 599 in 18.3s: reward for episode: 323.09 || average reward: 304.03 || episode length: 1000\n",
      "\n",
      "episode 600 in 19.2s: reward for episode: 301.04 || average reward: 305.57 || episode length: 1000\n",
      "\n",
      "episode 601 in 19.3s: reward for episode: 351.64 || average reward: 304.98 || episode length: 1000\n",
      "\n",
      "episode 602 in 19.3s: reward for episode: 299.76 || average reward: 307.09 || episode length: 1000\n",
      "\n",
      "episode 603 in 19.2s: reward for episode: 335.27 || average reward: 305.79 || episode length: 1000\n",
      "\n",
      "episode 604 in 19.3s: reward for episode: 300.6 || average reward: 306.98 || episode length: 1000\n",
      "\n",
      "episode 605 in 19.2s: reward for episode: 332.03 || average reward: 305.83 || episode length: 1000\n",
      "\n",
      "episode 606 in 19.3s: reward for episode: 291.84 || average reward: 307.35 || episode length: 1000\n",
      "\n",
      "episode 607 in 19.2s: reward for episode: 336.67 || average reward: 306.03 || episode length: 1000\n",
      "\n",
      "episode 608 in 19.3s: reward for episode: 311.09 || average reward: 307.78 || episode length: 1000\n",
      "\n",
      "episode 609 in 19.3s: reward for episode: 339.38 || average reward: 307.44 || episode length: 1000\n",
      "\n",
      "episode 610 in 19.4s: reward for episode: 298.73 || average reward: 308.65 || episode length: 1000\n",
      "\n",
      "episode 611 in 19.1s: reward for episode: 327.05 || average reward: 307.3 || episode length: 1000\n",
      "\n",
      "episode 612 in 19.3s: reward for episode: 304.92 || average reward: 308.1 || episode length: 1000\n",
      "\n",
      "episode 613 in 19.3s: reward for episode: 340.04 || average reward: 306.9 || episode length: 1000\n",
      "\n",
      "episode 614 in 19.4s: reward for episode: 302.76 || average reward: 309.38 || episode length: 1000\n",
      "\n",
      "episode 615 in 19.5s: reward for episode: 321.88 || average reward: 308.17 || episode length: 1000\n",
      "\n",
      "episode 616 in 19.5s: reward for episode: 307.97 || average reward: 308.86 || episode length: 1000\n",
      "\n",
      "episode 617 in 19.7s: reward for episode: 338.76 || average reward: 311.87 || episode length: 1000\n",
      "\n",
      "episode 618 in 20.2s: reward for episode: 265.21 || average reward: 313.94 || episode length: 1000\n",
      "\n",
      "episode 619 in 19.5s: reward for episode: 332.52 || average reward: 311.76 || episode length: 1000\n",
      "\n",
      "episode 620 in 19.5s: reward for episode: 308.18 || average reward: 313.6 || episode length: 1000\n",
      "\n",
      "episode 621 in 19.4s: reward for episode: 323.5 || average reward: 313.69 || episode length: 1000\n",
      "\n",
      "episode 622 in 19.6s: reward for episode: 307.33 || average reward: 314.64 || episode length: 1000\n",
      "\n",
      "episode 623 in 19.5s: reward for episode: 336.35 || average reward: 314.09 || episode length: 1000\n",
      "\n",
      "episode 624 in 19.7s: reward for episode: 300.55 || average reward: 317.5 || episode length: 1000\n",
      "\n",
      "episode 625 in 19.4s: reward for episode: 327.02 || average reward: 316.6 || episode length: 1000\n",
      "\n",
      "episode 626 in 19.7s: reward for episode: 290.23 || average reward: 317.64 || episode length: 1000\n",
      "\n",
      "episode 627 in 19.5s: reward for episode: 342.6 || average reward: 315.19 || episode length: 1000\n",
      "\n",
      "episode 628 in 19.7s: reward for episode: 299.52 || average reward: 316.9 || episode length: 1000\n",
      "\n",
      "episode 629 in 19.6s: reward for episode: 342.3 || average reward: 315.47 || episode length: 1000\n",
      "\n",
      "episode 630 in 19.7s: reward for episode: 296.49 || average reward: 317.14 || episode length: 1000\n",
      "\n",
      "episode 631 in 19.5s: reward for episode: 335.87 || average reward: 315.72 || episode length: 1000\n",
      "\n",
      "episode 632 in 19.6s: reward for episode: 305.77 || average reward: 317.48 || episode length: 1000\n",
      "\n",
      "episode 633 in 19.6s: reward for episode: 333.48 || average reward: 316.24 || episode length: 1000\n",
      "\n",
      "episode 634 in 19.8s: reward for episode: 304.15 || average reward: 317.14 || episode length: 1000\n",
      "\n",
      "episode 635 in 19.9s: reward for episode: 325.79 || average reward: 315.73 || episode length: 1000\n",
      "\n",
      "episode 636 in 18.8s: reward for episode: 302.71 || average reward: 316.81 || episode length: 1000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8b8ee264276a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5daae16e02ed>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-94280dba921f>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdone_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mexperience\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/honors-project/lib/python3.8/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0mselected_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = HalfCheetahGraphEnv(None)\n",
    "allAgents = dict()\n",
    "allRewards = dict()\n",
    "allAvgRewards = dict()\n",
    "for idx in range(7):\n",
    "    env.set_morphology(idx)\n",
    "    state = env.reset()\n",
    "\n",
    "    agent = DDPGagent(env, gamma=0.99)\n",
    "    batch_size = 128\n",
    "    rewards = []\n",
    "    avg_rewards = []\n",
    "    learningRates = []\n",
    "    episodeLengths = []\n",
    "    for episode in range(800):\n",
    "        t0 = time.time()\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        for i in range(1000):\n",
    "            \n",
    "            env._current_env._pb_env.camera_adjust()\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            # Only add noise in every second episode\n",
    "            if episode % 2 == 0:\n",
    "                action = np.clip(action + np.random.normal(0, 0.25, env.action_space.shape), -1, 1)\n",
    "                \n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            agent.memory.push(state, action, reward, new_state, done)\n",
    "\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.update(batch_size)\n",
    "\n",
    "\n",
    "            state = new_state\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            agent.update_lr()\n",
    "\n",
    "        print(\"episode {} in {}s: reward for episode: {} || average reward: {} || episode length: {}\\n\".format(episode, np.round(time.time() - t0, decimals=1), np.round(episode_reward, decimals=2), \n",
    "                                                                                                                  np.round(np.mean(rewards[-25:]), decimals=2), step))\n",
    "\n",
    "        episodeLengths.append(step)\n",
    "        rewards.append(episode_reward)\n",
    "        avg_rewards.append(np.mean(rewards[-25:]))\n",
    "        learningRates.append(agent.get_latest_lr())\n",
    "    \n",
    "#     allAgents[idx] = agent\n",
    "#     allRewards[idx] = rewards\n",
    "#     allAvgRewards[idx] = avg_rewards\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(len(rewards)), rewards)\n",
    "    ax.plot(range(len(avg_rewards)), avg_rewards)\n",
    "    ax.set(ylabel='Reward')\n",
    "    ax.legend([\"Episode\", \"Last 25 Avg\"])\n",
    "    plt.xlabel('Episode')\n",
    "    plt.suptitle('Half-Cheetah {}'.format(idx))\n",
    "    plt.show()\n",
    "    fig.savefig('{}-new-halfCheetah-Training.jpg'.format(idx))\n",
    "    \n",
    "    agent.save_agent_networks('{}-new-halfCheetah'.format(idx))\n",
    "    \n",
    "    env._current_env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HalfCheetahGraphEnv(None)\n",
    "env.set_morphology(0)\n",
    "g = env.get_graph()._get_dgl_graph()\n",
    "nx_G = g.cpu().to_networkx().to_undirected()\n",
    "# Kamada-Kawaii layout usually looks pretty for arbitrary graphs\n",
    "pos = nx.kamada_kawai_layout(nx_G)\n",
    "nx.draw(nx_G, pos, with_labels=True, node_color=[[.7, .7, .7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in [1]:\n",
    "    avg_rewards = allAvgRewards[idx]\n",
    "    rewards = allRewards[idx]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(len(rewards)), rewards)\n",
    "    ax.plot(range(len(avg_rewards)), avg_rewards)\n",
    "    ax.set(ylabel='Reward')\n",
    "    ax.legend([\"Episode\", \"Last 25 Avg\"])\n",
    "    plt.xlabel('Episode')\n",
    "    plt.suptitle('Half-Cheetah {}'.format(idx))\n",
    "    plt.show()\n",
    "    fig.savefig('{}-new-halfCheetah-Training.jpg'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in [1]:\n",
    "    allAgents[idx].save_agent_networks('{}-new-halfCheetah'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(7):\n",
    "    env = HalfCheetahGraphEnv(None)\n",
    "    env.set_morphology(idx)\n",
    "    \n",
    "    agent = DDPGagent(env)\n",
    "    agent.load_agent_networks('new/{}-halfCheetah'.format(idx))\n",
    "    \n",
    "    episodeRewards = []\n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "    collectedSamples = 0\n",
    "    samplesToPrint = 10000\n",
    "    while collectedSamples < 1e6:\n",
    "        state = env.reset()\n",
    "\n",
    "        step = 0\n",
    "        episodeReward = 0\n",
    "        for i in range(500):\n",
    "            action = agent.get_action(state)\n",
    "            action = np.clip(action + np.random.normal(0, 0.1, env.action_space.shape), -1, 1)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episodeReward += reward\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "            step += 1\n",
    "            collectedSamples += 1\n",
    "            samplesToPrint -= 1\n",
    "        \n",
    "        episodeRewards.append(episodeReward)\n",
    "        if samplesToPrint <= 0:\n",
    "            print(collectedSamples)\n",
    "            print('Average Rewards', np.mean(episodeRewards[-20]))\n",
    "            samplesToPrint = 10000\n",
    "        \n",
    "    np.save(\"../datasets/{}/actions_array\".format(idx), np.array(actions))\n",
    "    np.save(\"../datasets/{}/states_array\".format(idx), np.array(states))\n",
    "    np.save(\"../datasets/{}/next_states_array\".format(idx), np.array(next_states))\n",
    "    np.save(\"../datasets/{}/rewards_array\".format(idx), np.array(rewards))\n",
    "    np.save(\"../datasets/{}/dones_array\".format(idx), np.array(dones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_array = np.array(actions)\n",
    "states_array = np.array(states)\n",
    "next_states_array = np.array(next_states)\n",
    "rewards_array = np.array(rewards)\n",
    "dones_array = np.array(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"doubleInvertedPendulumDataset/actions_array\", actions_array)\n",
    "np.save(\"doubleInvertedPendulumDataset/states_array\", states_array)\n",
    "np.save(\"doubleInvertedPendulumDataset/next_states_array\", next_states_array)\n",
    "np.save(\"doubleInvertedPendulumDataset/rewards_array\", rewards_array)\n",
    "np.save(\"doubleInvertedPendulumDataset/dones_array\", dones_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
