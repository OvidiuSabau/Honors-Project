{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.autograd\n",
    "import os\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet as p \n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import pybullet \n",
    "import pybullet_envs.gym_pendulum_envs \n",
    "import pybullet_envs.gym_locomotion_envs\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import dgl\n",
    "import morphsim as m\n",
    "from graphenvs import HalfCheetahGraphEnv\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        hidden_sizes,\n",
    "        batch_size,\n",
    "        with_batch_norm=True,\n",
    "        activation=None\n",
    "    ):\n",
    "        super(Network, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        if with_batch_norm:\n",
    "            self.layers.append(nn.BatchNorm1d(batch_size))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            if with_batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(batch_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], self.output_size))\n",
    "        \n",
    "        if activation is not None:\n",
    "            self.layers.append(activation())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputNetwork,\n",
    "        messageNetwork,\n",
    "        updateNetwork,\n",
    "        outputNetwork,\n",
    "        numMessagePassingIterations,\n",
    "        withInputNetwork = True\n",
    "    ):\n",
    "        \n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "                \n",
    "        self.inputNetwork = inputNetwork\n",
    "        self.messageNetwork = messageNetwork\n",
    "        self.updateNetwork = updateNetwork\n",
    "        self.outputNetwork = outputNetwork\n",
    "        \n",
    "        self.numMessagePassingIterations = numMessagePassingIterations\n",
    "        self.withInputNetwork = withInputNetwork\n",
    "        \n",
    "    def inputFunction(self, nodes):\n",
    "        return {'state' : self.inputNetwork(nodes.data['input'])}\n",
    "    \n",
    "    def messageFunction(self, edges):\n",
    "        \n",
    "        edgeData = edges.data['feature'].repeat(edges.src['state'].shape[1], 1).T.unsqueeze(-1)\n",
    "        \n",
    "        return {'m' : self.messageNetwork(torch.cat((edges.src['state'], edgeData), -1))}\n",
    "    \n",
    "    def updateFunction(self, nodes):\n",
    "        return {'state': self.updateNetwork(torch.cat((nodes.data['m_hat'], nodes.data['state']), -1))}\n",
    "    \n",
    "    def outputFunction(self, nodes):\n",
    "        \n",
    "#         numNodes, batchSize, stateSize = graph.ndata['state'].shape\n",
    "#         return self.outputNetwork.forward(graph.ndata['state'])\n",
    "        return {'output': self.outputNetwork(nodes.data['state'])}\n",
    "\n",
    "\n",
    "    def forward(self, graph, state):\n",
    "        \n",
    "        self.update_states_in_graph(graph, state)\n",
    "        \n",
    "        if self.withInputNetwork:\n",
    "            graph.apply_nodes(self.inputFunction)\n",
    "        \n",
    "        for messagePassingIteration in range(self.numMessagePassingIterations):\n",
    "            graph.update_all(self.messageFunction, dgl.function.max('m', 'm_hat'), self.updateFunction)\n",
    "        \n",
    "        graph.apply_nodes(self.outputFunction)\n",
    "        \n",
    "        output = graph.ndata['output']\n",
    "        output = torch.transpose(output, dim0=0, dim1=1)\n",
    "        output = torch.squeeze(output, dim=-1)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def update_states_in_graph(self, graph, state):\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        numGraphFeature = 6\n",
    "        numGlobalStateInformation = 6\n",
    "        numLocalStateInformation = 2\n",
    "        numStateVar = state.shape[1] - 1 \n",
    "        globalInformation = torch.cat((state[:, 0:5], state[:, -1].unsqueeze(1)), -1)\n",
    "        \n",
    "        numNodes = (numStateVar - 5) // 2\n",
    "\n",
    "        nodeData = torch.empty((numNodes, state.shape[0], numGraphFeature + numGlobalStateInformation + numLocalStateInformation)).to(device)\n",
    "        \n",
    "        nodeData[:, :, 0:numGlobalStateInformation] = globalInformation\n",
    "        \n",
    "        for nodeIdx in range(numNodes):\n",
    "            # Assign global features from graph\n",
    "            nodeData[nodeIdx, :, numGlobalStateInformation:numGlobalStateInformation + numGraphFeature] = graph.ndata['feature'][nodeIdx]\n",
    "            # Assign local state information\n",
    "            nodeData[nodeIdx, :, numGlobalStateInformation + numGraphFeature] = state[:, 5 + nodeIdx]\n",
    "            nodeData[nodeIdx, :, numGlobalStateInformation + numGraphFeature + 1] = state[:, 5 + numNodes + nodeIdx]\n",
    "        \n",
    "        if self.withInputNetwork:\n",
    "            graph.ndata['input'] = nodeData        \n",
    "        \n",
    "        else:\n",
    "            graph.ndata['state'] = nodeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ovi/anaconda3/envs/honors-project/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    }
   ],
   "source": [
    "states = {}\n",
    "actions = {}\n",
    "rewards = {}\n",
    "next_states = {}\n",
    "dones = {}\n",
    "env = {}\n",
    "\n",
    "for morphIdx in range(7):\n",
    "\n",
    "    prefix = '../datasets/{}/'.format(morphIdx)\n",
    "    \n",
    "    states[morphIdx] = np.load(prefix + 'states_array.npy')\n",
    "    actions[morphIdx] = np.load(prefix + 'actions_array.npy')\n",
    "    rewards[morphIdx] = np.load(prefix + 'rewards_array.npy')\n",
    "    next_states[morphIdx] = np.load(prefix + 'next_states_array.npy')\n",
    "    dones[morphIdx] = np.load(prefix + 'dones_array.npy')\n",
    "    \n",
    "    env[morphIdx] = HalfCheetahGraphEnv(None)\n",
    "    env[morphIdx].set_morphology(morphIdx)\n",
    "    env[morphIdx].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = {}\n",
    "X_train = {}\n",
    "Y_test = {}\n",
    "Y_train = {}\n",
    "\n",
    "for morphIdx in range(7):\n",
    "    X = np.concatenate((states[morphIdx], np.expand_dims(next_states[morphIdx][:, 0], axis=1)), -1)\n",
    "    Y = actions[morphIdx]\n",
    "    permutation = np.random.permutation(X.shape[0])\n",
    "    X = X[permutation]\n",
    "    X_test[morphIdx] = torch.from_numpy(X[:100000]).float()\n",
    "    X_train[morphIdx] = torch.from_numpy(X[100000:]).float()\n",
    "    Y = Y[permutation]\n",
    "    Y_test[morphIdx] = torch.from_numpy(Y[:100000]).float()\n",
    "    Y_train[morphIdx] = torch.from_numpy(Y[100000:]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [64, 64]\n",
    "\n",
    "inputSize = 14\n",
    "stateSize = 32\n",
    "messageSize = 32\n",
    "outputSize = 1\n",
    "numMessagePassingIterations = 4\n",
    "batch_size = 1024\n",
    "with_batch_norm=False\n",
    "numBatchesPerTrainingStep = 4\n",
    "\n",
    "inputNetwork = Network(inputSize, stateSize, hidden_sizes, batch_size, with_batch_norm)\n",
    "messageNetwork = Network(stateSize + 1, messageSize, hidden_sizes, batch_size, with_batch_norm, nn.Tanh)\n",
    "updateNetwork = Network(stateSize + messageSize, stateSize, hidden_sizes, batch_size, with_batch_norm)\n",
    "outputNetwork = Network(stateSize, outputSize, hidden_sizes, batch_size, with_batch_norm, nn.Tanh)\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "numTrainingBatches = int(np.ceil(X_train[0].shape[0] / batch_size))\n",
    "numTestingBatches = int(np.ceil(X_test[0].shape[0] / batch_size))\n",
    "\n",
    "gnn = GraphNeuralNetwork(inputNetwork, messageNetwork, updateNetwork, outputNetwork, numMessagePassingIterations).to(device)\n",
    "\n",
    "optimizer = optim.Adam(itertools.chain(inputNetwork.parameters(), messageNetwork.parameters(), updateNetwork.parameters(), outputNetwork.parameters())\n",
    "                       , lr, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "lmbda = lambda epoch: 0.8\n",
    "lr_scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lmbda)\n",
    "criterion  = nn.SmoothL1Loss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 0 in 3.5784902572631836 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4223, 0.4334, 0.3907, 0.4336]) \n",
      "Test Loss tensor([0.4226, 0.4309, 0.3921, 0.4327])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4131, 0.4549, 0.4709, 0.4407, 0.4409]) \n",
      "Test Loss tensor([0.4159, 0.4518, 0.4691, 0.4430, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4489, 0.4535, 0.4272, 0.4359, 0.3657, 0.4599, 0.4711]) \n",
      "Test Loss tensor([0.4481, 0.4553, 0.4315, 0.4355, 0.3694, 0.4612, 0.4689])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4528, 0.4214, 0.4528, 0.4511, 0.4269, 0.4422, 0.4708]) \n",
      "Test Loss tensor([0.4544, 0.4217, 0.4542, 0.4526, 0.4225, 0.4402, 0.4671])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4711, 0.4628, 0.4495, 0.4514, 0.4522, 0.4717, 0.3899, 0.4724]) \n",
      "Test Loss tensor([0.4686, 0.4633, 0.4524, 0.4537, 0.4546, 0.4705, 0.3906, 0.4699])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4517, 0.4682, 0.4613, 0.4520, 0.4368, 0.4483])\n",
      "Valid Idx 3 | Loss tensor([0.4535, 0.4514, 0.4452, 0.4445, 0.3495])\n",
      "Gradients: Input 1.5247699138853932e-06 | Message 0.0024115333799272776 | Update 0.027063369750976562 | Output 1.332198977470398\n",
      "\n",
      "************** Batch 4 in 3.738367795944214 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4229, 0.4290, 0.3959, 0.4326]) \n",
      "Test Loss tensor([0.4241, 0.4295, 0.3935, 0.4323])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4174, 0.4518, 0.4696, 0.4420, 0.4446]) \n",
      "Test Loss tensor([0.4148, 0.4527, 0.4675, 0.4434, 0.4423])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4470, 0.4546, 0.4314, 0.4379, 0.3676, 0.4609, 0.4683]) \n",
      "Test Loss tensor([0.4458, 0.4572, 0.4326, 0.4359, 0.3697, 0.4608, 0.4674])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4553, 0.4223, 0.4552, 0.4529, 0.4221, 0.4399, 0.4687]) \n",
      "Test Loss tensor([0.4564, 0.4204, 0.4562, 0.4539, 0.4212, 0.4386, 0.4656])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4671, 0.4635, 0.4508, 0.4548, 0.4540, 0.4695, 0.3912, 0.4689]) \n",
      "Test Loss tensor([0.4671, 0.4613, 0.4541, 0.4555, 0.4566, 0.4683, 0.3908, 0.4686])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4512, 0.4662, 0.4590, 0.4532, 0.4358, 0.4499])\n",
      "Valid Idx 3 | Loss tensor([0.4552, 0.4539, 0.4466, 0.4455, 0.3483])\n",
      "\n",
      "************** Batch 8 in 3.6734604835510254 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4231, 0.4319, 0.3968, 0.4337]) \n",
      "Test Loss tensor([0.4233, 0.4296, 0.3946, 0.4318])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4163, 0.4520, 0.4669, 0.4439, 0.4427]) \n",
      "Test Loss tensor([0.4149, 0.4522, 0.4654, 0.4444, 0.4422])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4460, 0.4562, 0.4325, 0.4344, 0.3672, 0.4587, 0.4688]) \n",
      "Test Loss tensor([0.4451, 0.4583, 0.4331, 0.4345, 0.3690, 0.4607, 0.4650])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4577, 0.4199, 0.4555, 0.4524, 0.4235, 0.4392, 0.4664]) \n",
      "Test Loss tensor([0.4591, 0.4179, 0.4581, 0.4561, 0.4218, 0.4380, 0.4637])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4674, 0.4596, 0.4543, 0.4566, 0.4569, 0.4663, 0.3930, 0.4688]) \n",
      "Test Loss tensor([0.4649, 0.4594, 0.4559, 0.4573, 0.4580, 0.4664, 0.3906, 0.4664])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4519, 0.4644, 0.4578, 0.4550, 0.4343, 0.4506])\n",
      "Valid Idx 3 | Loss tensor([0.4568, 0.4552, 0.4474, 0.4463, 0.3514])\n",
      "\n",
      "************** Batch 12 in 3.8993637561798096 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4235, 0.4285, 0.3914, 0.4279]) \n",
      "Test Loss tensor([0.4243, 0.4287, 0.3963, 0.4303])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4173, 0.4532, 0.4646, 0.4433, 0.4436]) \n",
      "Test Loss tensor([0.4145, 0.4525, 0.4633, 0.4455, 0.4418])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4446, 0.4576, 0.4343, 0.4340, 0.3659, 0.4609, 0.4652]) \n",
      "Test Loss tensor([0.4429, 0.4601, 0.4345, 0.4349, 0.3679, 0.4608, 0.4635])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4590, 0.4197, 0.4600, 0.4539, 0.4230, 0.4364, 0.4633]) \n",
      "Test Loss tensor([0.4605, 0.4172, 0.4600, 0.4574, 0.4213, 0.4365, 0.4624])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4647, 0.4591, 0.4544, 0.4572, 0.4577, 0.4673, 0.3908, 0.4665]) \n",
      "Test Loss tensor([0.4635, 0.4575, 0.4581, 0.4585, 0.4602, 0.4643, 0.3900, 0.4643])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4517, 0.4626, 0.4561, 0.4565, 0.4326, 0.4518])\n",
      "Valid Idx 3 | Loss tensor([0.4590, 0.4576, 0.4500, 0.4473, 0.3507])\n",
      "\n",
      "************** Batch 16 in 3.6484873294830322 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4239, 0.4313, 0.3976, 0.4320]) \n",
      "Test Loss tensor([0.4238, 0.4291, 0.3970, 0.4301])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4110, 0.4525, 0.4623, 0.4456, 0.4437]) \n",
      "Test Loss tensor([0.4129, 0.4523, 0.4616, 0.4456, 0.4420])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4439, 0.4607, 0.4337, 0.4317, 0.3712, 0.4606, 0.4658]) \n",
      "Test Loss tensor([0.4419, 0.4630, 0.4354, 0.4342, 0.3683, 0.4606, 0.4616])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4620, 0.4165, 0.4604, 0.4570, 0.4235, 0.4365, 0.4628]) \n",
      "Test Loss tensor([0.4623, 0.4159, 0.4621, 0.4593, 0.4206, 0.4355, 0.4613])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4631, 0.4572, 0.4566, 0.4604, 0.4597, 0.4654, 0.3930, 0.4652]) \n",
      "Test Loss tensor([0.4614, 0.4560, 0.4593, 0.4606, 0.4615, 0.4624, 0.3910, 0.4626])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4516, 0.4606, 0.4542, 0.4585, 0.4317, 0.4529])\n",
      "Valid Idx 3 | Loss tensor([0.4607, 0.4593, 0.4515, 0.4474, 0.3532])\n",
      "\n",
      "************** Batch 20 in 3.72902774810791 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4225, 0.4288, 0.3983, 0.4301]) \n",
      "Test Loss tensor([0.4254, 0.4278, 0.3983, 0.4296])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4108, 0.4538, 0.4633, 0.4435, 0.4411]) \n",
      "Test Loss tensor([0.4121, 0.4531, 0.4595, 0.4464, 0.4418])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4395, 0.4623, 0.4396, 0.4335, 0.3655, 0.4613, 0.4610]) \n",
      "Test Loss tensor([0.4397, 0.4650, 0.4363, 0.4333, 0.3679, 0.4603, 0.4597])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4618, 0.4222, 0.4611, 0.4584, 0.4199, 0.4358, 0.4607]) \n",
      "Test Loss tensor([0.4642, 0.4163, 0.4637, 0.4607, 0.4200, 0.4336, 0.4591])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4614, 0.4567, 0.4601, 0.4610, 0.4628, 0.4633, 0.3923, 0.4633]) \n",
      "Test Loss tensor([0.4598, 0.4548, 0.4615, 0.4618, 0.4644, 0.4608, 0.3908, 0.4612])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4522, 0.4588, 0.4529, 0.4599, 0.4306, 0.4548])\n",
      "Valid Idx 3 | Loss tensor([0.4624, 0.4611, 0.4534, 0.4488, 0.3533])\n",
      "Gradients: Input 1.4535378340951866e-06 | Message 0.0016693791840225458 | Update 0.02354367822408676 | Output 1.1795867681503296\n",
      "\n",
      "************** Batch 24 in 3.6703526973724365 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4256, 0.4296, 0.3991, 0.4281]) \n",
      "Test Loss tensor([0.4246, 0.4280, 0.4000, 0.4286])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4129, 0.4534, 0.4603, 0.4458, 0.4427]) \n",
      "Test Loss tensor([0.4123, 0.4523, 0.4584, 0.4465, 0.4420])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4372, 0.4635, 0.4353, 0.4320, 0.3658, 0.4600, 0.4592]) \n",
      "Test Loss tensor([0.4384, 0.4661, 0.4377, 0.4331, 0.3671, 0.4600, 0.4578])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4634, 0.4151, 0.4649, 0.4616, 0.4196, 0.4327, 0.4612]) \n",
      "Test Loss tensor([0.4662, 0.4143, 0.4654, 0.4625, 0.4205, 0.4325, 0.4574])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4595, 0.4533, 0.4608, 0.4629, 0.4647, 0.4588, 0.3896, 0.4612]) \n",
      "Test Loss tensor([0.4578, 0.4538, 0.4635, 0.4640, 0.4663, 0.4588, 0.3887, 0.4591])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4522, 0.4576, 0.4509, 0.4618, 0.4298, 0.4550])\n",
      "Valid Idx 3 | Loss tensor([0.4646, 0.4627, 0.4546, 0.4488, 0.3543])\n",
      "\n",
      "************** Batch 28 in 3.675523042678833 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4244, 0.4259, 0.3993, 0.4319]) \n",
      "Test Loss tensor([0.4253, 0.4271, 0.4011, 0.4277])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4109, 0.4526, 0.4573, 0.4455, 0.4431]) \n",
      "Test Loss tensor([0.4122, 0.4536, 0.4561, 0.4485, 0.4418])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4389, 0.4673, 0.4382, 0.4307, 0.3661, 0.4594, 0.4566]) \n",
      "Test Loss tensor([0.4367, 0.4680, 0.4384, 0.4318, 0.3655, 0.4601, 0.4563])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4666, 0.4119, 0.4648, 0.4628, 0.4194, 0.4343, 0.4574]) \n",
      "Test Loss tensor([0.4678, 0.4135, 0.4677, 0.4641, 0.4194, 0.4318, 0.4556])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4589, 0.4521, 0.4645, 0.4649, 0.4654, 0.4594, 0.3907, 0.4580]) \n",
      "Test Loss tensor([0.4561, 0.4514, 0.4653, 0.4660, 0.4681, 0.4565, 0.3895, 0.4570])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4522, 0.4554, 0.4500, 0.4634, 0.4286, 0.4556])\n",
      "Valid Idx 3 | Loss tensor([0.4663, 0.4649, 0.4559, 0.4501, 0.3536])\n",
      "\n",
      "************** Batch 32 in 3.7168567180633545 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4282, 0.4255, 0.4026, 0.4295]) \n",
      "Test Loss tensor([0.4262, 0.4267, 0.4033, 0.4279])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4113, 0.4547, 0.4565, 0.4485, 0.4419]) \n",
      "Test Loss tensor([0.4121, 0.4531, 0.4543, 0.4482, 0.4424])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4390, 0.4667, 0.4369, 0.4278, 0.3634, 0.4595, 0.4562]) \n",
      "Test Loss tensor([0.4354, 0.4701, 0.4401, 0.4306, 0.3672, 0.4600, 0.4537])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4664, 0.4107, 0.4672, 0.4639, 0.4199, 0.4311, 0.4551]) \n",
      "Test Loss tensor([0.4699, 0.4119, 0.4694, 0.4658, 0.4194, 0.4306, 0.4541])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4567, 0.4514, 0.4655, 0.4640, 0.4677, 0.4555, 0.3886, 0.4574]) \n",
      "Test Loss tensor([0.4539, 0.4497, 0.4670, 0.4677, 0.4696, 0.4552, 0.3899, 0.4548])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4519, 0.4538, 0.4482, 0.4650, 0.4272, 0.4575])\n",
      "Valid Idx 3 | Loss tensor([0.4685, 0.4665, 0.4577, 0.4506, 0.3555])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 36 in 3.6422650814056396 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4276, 0.4290, 0.4011, 0.4267]) \n",
      "Test Loss tensor([0.4263, 0.4263, 0.4032, 0.4272])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4131, 0.4515, 0.4539, 0.4487, 0.4412]) \n",
      "Test Loss tensor([0.4109, 0.4538, 0.4520, 0.4495, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4342, 0.4693, 0.4396, 0.4295, 0.3643, 0.4605, 0.4535]) \n",
      "Test Loss tensor([0.4332, 0.4719, 0.4407, 0.4311, 0.3667, 0.4597, 0.4521])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4697, 0.4106, 0.4687, 0.4637, 0.4185, 0.4320, 0.4538]) \n",
      "Test Loss tensor([0.4720, 0.4105, 0.4712, 0.4676, 0.4188, 0.4302, 0.4527])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4539, 0.4486, 0.4684, 0.4677, 0.4695, 0.4537, 0.3893, 0.4555]) \n",
      "Test Loss tensor([0.4528, 0.4483, 0.4690, 0.4689, 0.4714, 0.4533, 0.3893, 0.4530])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4529, 0.4522, 0.4464, 0.4665, 0.4259, 0.4584])\n",
      "Valid Idx 3 | Loss tensor([0.4697, 0.4690, 0.4587, 0.4511, 0.3567])\n",
      "\n",
      "************** Batch 40 in 3.6496942043304443 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4268, 0.4264, 0.4015, 0.4287]) \n",
      "Test Loss tensor([0.4259, 0.4250, 0.4049, 0.4259])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4083, 0.4553, 0.4516, 0.4477, 0.4427]) \n",
      "Test Loss tensor([0.4117, 0.4530, 0.4502, 0.4497, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4345, 0.4730, 0.4420, 0.4296, 0.3670, 0.4597, 0.4521]) \n",
      "Test Loss tensor([0.4324, 0.4740, 0.4417, 0.4294, 0.3657, 0.4596, 0.4500])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4717, 0.4105, 0.4711, 0.4677, 0.4181, 0.4278, 0.4518]) \n",
      "Test Loss tensor([0.4738, 0.4094, 0.4727, 0.4694, 0.4181, 0.4285, 0.4508])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4534, 0.4487, 0.4692, 0.4697, 0.4705, 0.4525, 0.3921, 0.4511]) \n",
      "Test Loss tensor([0.4503, 0.4461, 0.4705, 0.4707, 0.4735, 0.4514, 0.3881, 0.4515])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4521, 0.4508, 0.4452, 0.4679, 0.4247, 0.4594])\n",
      "Valid Idx 3 | Loss tensor([0.4717, 0.4705, 0.4613, 0.4515, 0.3579])\n",
      "Gradients: Input 1.5229618384182686e-06 | Message 0.001553402398712933 | Update 0.019528929144144058 | Output 0.9609634280204773\n",
      "\n",
      "************** Batch 44 in 3.6877071857452393 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4263, 0.4267, 0.4052, 0.4258]) \n",
      "Test Loss tensor([0.4274, 0.4252, 0.4058, 0.4263])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4091, 0.4544, 0.4511, 0.4498, 0.4423]) \n",
      "Test Loss tensor([0.4104, 0.4536, 0.4492, 0.4502, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4307, 0.4739, 0.4416, 0.4301, 0.3660, 0.4603, 0.4503]) \n",
      "Test Loss tensor([0.4303, 0.4759, 0.4428, 0.4291, 0.3665, 0.4595, 0.4483])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4738, 0.4103, 0.4736, 0.4681, 0.4178, 0.4301, 0.4504]) \n",
      "Test Loss tensor([0.4753, 0.4088, 0.4748, 0.4710, 0.4183, 0.4279, 0.4490])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4513, 0.4483, 0.4716, 0.4694, 0.4721, 0.4517, 0.3879, 0.4509]) \n",
      "Test Loss tensor([0.4487, 0.4452, 0.4723, 0.4729, 0.4756, 0.4498, 0.3892, 0.4493])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4525, 0.4485, 0.4432, 0.4697, 0.4238, 0.4607])\n",
      "Valid Idx 3 | Loss tensor([0.4734, 0.4727, 0.4622, 0.4526, 0.3571])\n",
      "\n",
      "************** Batch 48 in 3.8711705207824707 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4293, 0.4232, 0.4036, 0.4251]) \n",
      "Test Loss tensor([0.4280, 0.4252, 0.4068, 0.4262])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4125, 0.4542, 0.4483, 0.4467, 0.4424]) \n",
      "Test Loss tensor([0.4104, 0.4537, 0.4467, 0.4508, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4317, 0.4763, 0.4462, 0.4302, 0.3605, 0.4589, 0.4469]) \n",
      "Test Loss tensor([0.4288, 0.4782, 0.4440, 0.4280, 0.3650, 0.4592, 0.4466])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4763, 0.4116, 0.4743, 0.4707, 0.4195, 0.4300, 0.4483]) \n",
      "Test Loss tensor([0.4774, 0.4068, 0.4765, 0.4725, 0.4176, 0.4267, 0.4477])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4483, 0.4451, 0.4735, 0.4724, 0.4746, 0.4490, 0.3884, 0.4485]) \n",
      "Test Loss tensor([0.4471, 0.4436, 0.4740, 0.4744, 0.4774, 0.4477, 0.3877, 0.4474])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4525, 0.4468, 0.4416, 0.4709, 0.4232, 0.4619])\n",
      "Valid Idx 3 | Loss tensor([0.4754, 0.4744, 0.4633, 0.4534, 0.3587])\n",
      "\n",
      "************** Batch 52 in 3.9052090644836426 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4310, 0.4242, 0.4072, 0.4251]) \n",
      "Test Loss tensor([0.4276, 0.4235, 0.4070, 0.4245])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4143, 0.4510, 0.4446, 0.4511, 0.4432]) \n",
      "Test Loss tensor([0.4086, 0.4534, 0.4449, 0.4512, 0.4409])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4286, 0.4775, 0.4430, 0.4268, 0.3622, 0.4603, 0.4461]) \n",
      "Test Loss tensor([0.4274, 0.4801, 0.4448, 0.4280, 0.3656, 0.4593, 0.4451])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4771, 0.4053, 0.4769, 0.4729, 0.4190, 0.4265, 0.4480]) \n",
      "Test Loss tensor([0.4793, 0.4063, 0.4785, 0.4742, 0.4169, 0.4250, 0.4457])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4471, 0.4427, 0.4748, 0.4755, 0.4777, 0.4487, 0.3903, 0.4482]) \n",
      "Test Loss tensor([0.4454, 0.4418, 0.4765, 0.4754, 0.4792, 0.4456, 0.3894, 0.4458])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4522, 0.4451, 0.4407, 0.4721, 0.4211, 0.4628])\n",
      "Valid Idx 3 | Loss tensor([0.4772, 0.4762, 0.4656, 0.4537, 0.3613])\n",
      "\n",
      "************** Batch 56 in 3.6382851600646973 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4303, 0.4240, 0.4068, 0.4280]) \n",
      "Test Loss tensor([0.4286, 0.4248, 0.4088, 0.4243])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4117, 0.4537, 0.4448, 0.4524, 0.4407]) \n",
      "Test Loss tensor([0.4095, 0.4535, 0.4435, 0.4527, 0.4416])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4263, 0.4785, 0.4428, 0.4269, 0.3642, 0.4577, 0.4452]) \n",
      "Test Loss tensor([0.4261, 0.4817, 0.4470, 0.4266, 0.3658, 0.4594, 0.4426])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4784, 0.4065, 0.4782, 0.4726, 0.4192, 0.4263, 0.4466]) \n",
      "Test Loss tensor([0.4815, 0.4051, 0.4801, 0.4761, 0.4168, 0.4243, 0.4441])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4451, 0.4407, 0.4759, 0.4775, 0.4799, 0.4452, 0.3899, 0.4467]) \n",
      "Test Loss tensor([0.4437, 0.4399, 0.4782, 0.4773, 0.4812, 0.4446, 0.3895, 0.4435])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4523, 0.4434, 0.4391, 0.4750, 0.4209, 0.4641])\n",
      "Valid Idx 3 | Loss tensor([0.4791, 0.4778, 0.4670, 0.4546, 0.3604])\n",
      "\n",
      "************** Batch 60 in 3.654155969619751 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4304, 0.4256, 0.4096, 0.4237]) \n",
      "Test Loss tensor([0.4289, 0.4231, 0.4100, 0.4232])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4058, 0.4538, 0.4435, 0.4527, 0.4398]) \n",
      "Test Loss tensor([0.4097, 0.4537, 0.4416, 0.4531, 0.4421])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4247, 0.4801, 0.4460, 0.4266, 0.3619, 0.4589, 0.4423]) \n",
      "Test Loss tensor([0.4249, 0.4829, 0.4476, 0.4257, 0.3643, 0.4592, 0.4412])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4821, 0.4058, 0.4806, 0.4773, 0.4139, 0.4230, 0.4439]) \n",
      "Test Loss tensor([0.4831, 0.4034, 0.4814, 0.4784, 0.4164, 0.4239, 0.4430])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4422, 0.4407, 0.4782, 0.4780, 0.4798, 0.4440, 0.3915, 0.4435]) \n",
      "Test Loss tensor([0.4416, 0.4388, 0.4794, 0.4795, 0.4832, 0.4419, 0.3888, 0.4423])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4526, 0.4415, 0.4372, 0.4760, 0.4189, 0.4655])\n",
      "Valid Idx 3 | Loss tensor([0.4809, 0.4799, 0.4688, 0.4557, 0.3618])\n",
      "Gradients: Input 1.6655460512993159e-06 | Message 0.0014665985945612192 | Update 0.01612786203622818 | Output 0.7901448011398315\n",
      "\n",
      "************** Batch 64 in 3.7420666217803955 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4280, 0.4247, 0.4092, 0.4216]) \n",
      "Test Loss tensor([0.4306, 0.4227, 0.4121, 0.4232])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4081, 0.4559, 0.4409, 0.4534, 0.4401]) \n",
      "Test Loss tensor([0.4093, 0.4536, 0.4399, 0.4540, 0.4421])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4247, 0.4832, 0.4481, 0.4294, 0.3623, 0.4589, 0.4411]) \n",
      "Test Loss tensor([0.4234, 0.4856, 0.4486, 0.4253, 0.3639, 0.4589, 0.4398])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4819, 0.4038, 0.4821, 0.4763, 0.4153, 0.4253, 0.4418]) \n",
      "Test Loss tensor([0.4852, 0.4031, 0.4836, 0.4797, 0.4164, 0.4224, 0.4412])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4432, 0.4391, 0.4793, 0.4805, 0.4821, 0.4416, 0.3887, 0.4426]) \n",
      "Test Loss tensor([0.4403, 0.4370, 0.4819, 0.4810, 0.4845, 0.4407, 0.3883, 0.4397])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4525, 0.4402, 0.4361, 0.4779, 0.4177, 0.4656])\n",
      "Valid Idx 3 | Loss tensor([0.4825, 0.4819, 0.4700, 0.4555, 0.3611])\n",
      "\n",
      "************** Batch 68 in 3.683894395828247 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4288, 0.4226, 0.4142, 0.4244]) \n",
      "Test Loss tensor([0.4300, 0.4217, 0.4137, 0.4219])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4090, 0.4534, 0.4393, 0.4521, 0.4420]) \n",
      "Test Loss tensor([0.4081, 0.4538, 0.4382, 0.4546, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4234, 0.4857, 0.4502, 0.4216, 0.3603, 0.4572, 0.4393]) \n",
      "Test Loss tensor([0.4215, 0.4869, 0.4494, 0.4248, 0.3642, 0.4589, 0.4377])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4843, 0.4029, 0.4847, 0.4794, 0.4170, 0.4200, 0.4414]) \n",
      "Test Loss tensor([0.4870, 0.4015, 0.4853, 0.4810, 0.4163, 0.4209, 0.4397])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4384, 0.4360, 0.4799, 0.4797, 0.4845, 0.4396, 0.3906, 0.4411]) \n",
      "Test Loss tensor([0.4384, 0.4355, 0.4831, 0.4825, 0.4866, 0.4388, 0.3891, 0.4386])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4528, 0.4383, 0.4350, 0.4791, 0.4167, 0.4668])\n",
      "Valid Idx 3 | Loss tensor([0.4837, 0.4830, 0.4715, 0.4569, 0.3632])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 72 in 3.6681034564971924 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4275, 0.4212, 0.4131, 0.4191]) \n",
      "Test Loss tensor([0.4299, 0.4211, 0.4141, 0.4217])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4090, 0.4558, 0.4368, 0.4520, 0.4426]) \n",
      "Test Loss tensor([0.4069, 0.4543, 0.4358, 0.4551, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4234, 0.4865, 0.4536, 0.4229, 0.3655, 0.4584, 0.4380]) \n",
      "Test Loss tensor([0.4207, 0.4887, 0.4515, 0.4255, 0.3635, 0.4587, 0.4356])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4870, 0.4003, 0.4857, 0.4824, 0.4143, 0.4220, 0.4401]) \n",
      "Test Loss tensor([0.4885, 0.4007, 0.4872, 0.4826, 0.4158, 0.4202, 0.4382])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4377, 0.4366, 0.4829, 0.4822, 0.4852, 0.4382, 0.3952, 0.4386]) \n",
      "Test Loss tensor([0.4366, 0.4347, 0.4854, 0.4842, 0.4882, 0.4366, 0.3888, 0.4367])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4530, 0.4368, 0.4327, 0.4809, 0.4169, 0.4682])\n",
      "Valid Idx 3 | Loss tensor([0.4854, 0.4849, 0.4728, 0.4574, 0.3636])\n",
      "\n",
      "************** Batch 76 in 3.7241311073303223 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4303, 0.4249, 0.4136, 0.4218]) \n",
      "Test Loss tensor([0.4320, 0.4215, 0.4170, 0.4217])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4076, 0.4560, 0.4363, 0.4571, 0.4431]) \n",
      "Test Loss tensor([0.4073, 0.4544, 0.4347, 0.4561, 0.4406])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4211, 0.4877, 0.4493, 0.4264, 0.3607, 0.4580, 0.4352]) \n",
      "Test Loss tensor([0.4193, 0.4904, 0.4511, 0.4231, 0.3625, 0.4590, 0.4345])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4871, 0.3981, 0.4871, 0.4823, 0.4182, 0.4209, 0.4390]) \n",
      "Test Loss tensor([0.4909, 0.4003, 0.4889, 0.4845, 0.4154, 0.4193, 0.4365])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4367, 0.4333, 0.4845, 0.4835, 0.4878, 0.4360, 0.3879, 0.4375]) \n",
      "Test Loss tensor([0.4353, 0.4326, 0.4871, 0.4858, 0.4899, 0.4352, 0.3881, 0.4349])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4532, 0.4353, 0.4320, 0.4820, 0.4143, 0.4695])\n",
      "Valid Idx 3 | Loss tensor([0.4880, 0.4870, 0.4749, 0.4574, 0.3666])\n",
      "\n",
      "************** Batch 80 in 3.679615020751953 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4331, 0.4236, 0.4128, 0.4151]) \n",
      "Test Loss tensor([0.4328, 0.4206, 0.4165, 0.4206])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4061, 0.4535, 0.4346, 0.4568, 0.4411]) \n",
      "Test Loss tensor([0.4063, 0.4547, 0.4331, 0.4566, 0.4416])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4192, 0.4903, 0.4534, 0.4274, 0.3610, 0.4588, 0.4342]) \n",
      "Test Loss tensor([0.4176, 0.4924, 0.4520, 0.4239, 0.3635, 0.4589, 0.4323])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4903, 0.4010, 0.4874, 0.4861, 0.4137, 0.4191, 0.4352]) \n",
      "Test Loss tensor([0.4920, 0.3981, 0.4904, 0.4860, 0.4153, 0.4182, 0.4350])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4344, 0.4337, 0.4862, 0.4852, 0.4906, 0.4357, 0.3882, 0.4353]) \n",
      "Test Loss tensor([0.4333, 0.4320, 0.4887, 0.4872, 0.4916, 0.4333, 0.3865, 0.4339])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4522, 0.4338, 0.4306, 0.4835, 0.4130, 0.4696])\n",
      "Valid Idx 3 | Loss tensor([0.4891, 0.4890, 0.4760, 0.4588, 0.3665])\n",
      "Gradients: Input 2.0056879748153733e-06 | Message 0.001382483635097742 | Update 0.016474861651659012 | Output 0.7366145849227905\n",
      "\n",
      "************** Batch 84 in 3.6721630096435547 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4342, 0.4228, 0.4174, 0.4206]) \n",
      "Test Loss tensor([0.4325, 0.4201, 0.4200, 0.4193])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4084, 0.4554, 0.4320, 0.4551, 0.4430]) \n",
      "Test Loss tensor([0.4060, 0.4550, 0.4307, 0.4575, 0.4414])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4175, 0.4915, 0.4506, 0.4242, 0.3615, 0.4590, 0.4340]) \n",
      "Test Loss tensor([0.4168, 0.4941, 0.4533, 0.4219, 0.3626, 0.4582, 0.4306])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4927, 0.3936, 0.4915, 0.4857, 0.4155, 0.4158, 0.4367]) \n",
      "Test Loss tensor([0.4935, 0.3982, 0.4916, 0.4868, 0.4148, 0.4173, 0.4339])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4330, 0.4316, 0.4880, 0.4893, 0.4918, 0.4330, 0.3869, 0.4340]) \n",
      "Test Loss tensor([0.4320, 0.4306, 0.4903, 0.4892, 0.4932, 0.4315, 0.3891, 0.4313])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4528, 0.4324, 0.4289, 0.4850, 0.4129, 0.4720])\n",
      "Valid Idx 3 | Loss tensor([0.4910, 0.4909, 0.4772, 0.4595, 0.3671])\n",
      "\n",
      "************** Batch 88 in 3.7000558376312256 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4324, 0.4210, 0.4203, 0.4156]) \n",
      "Test Loss tensor([0.4331, 0.4201, 0.4194, 0.4186])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4054, 0.4542, 0.4315, 0.4588, 0.4430]) \n",
      "Test Loss tensor([0.4063, 0.4546, 0.4294, 0.4565, 0.4408])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4159, 0.4951, 0.4567, 0.4217, 0.3621, 0.4575, 0.4301]) \n",
      "Test Loss tensor([0.4153, 0.4956, 0.4552, 0.4231, 0.3615, 0.4588, 0.4294])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4956, 0.3981, 0.4931, 0.4876, 0.4116, 0.4181, 0.4343]) \n",
      "Test Loss tensor([0.4955, 0.3969, 0.4936, 0.4889, 0.4146, 0.4172, 0.4320])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4327, 0.4286, 0.4914, 0.4901, 0.4922, 0.4312, 0.3861, 0.4315]) \n",
      "Test Loss tensor([0.4301, 0.4295, 0.4917, 0.4908, 0.4951, 0.4302, 0.3880, 0.4298])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4530, 0.4307, 0.4276, 0.4864, 0.4104, 0.4724])\n",
      "Valid Idx 3 | Loss tensor([0.4926, 0.4925, 0.4785, 0.4598, 0.3673])\n",
      "\n",
      "************** Batch 92 in 3.7045750617980957 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4312, 0.4189, 0.4160, 0.4199]) \n",
      "Test Loss tensor([0.4333, 0.4196, 0.4208, 0.4182])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4060, 0.4544, 0.4304, 0.4577, 0.4436]) \n",
      "Test Loss tensor([0.4053, 0.4548, 0.4277, 0.4576, 0.4407])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4118, 0.4961, 0.4563, 0.4246, 0.3598, 0.4597, 0.4294]) \n",
      "Test Loss tensor([0.4130, 0.4975, 0.4562, 0.4218, 0.3623, 0.4590, 0.4274])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4950, 0.3991, 0.4927, 0.4885, 0.4136, 0.4190, 0.4319]) \n",
      "Test Loss tensor([0.4972, 0.3958, 0.4953, 0.4906, 0.4135, 0.4162, 0.4307])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4304, 0.4304, 0.4923, 0.4906, 0.4964, 0.4304, 0.3893, 0.4288]) \n",
      "Test Loss tensor([0.4287, 0.4274, 0.4937, 0.4921, 0.4973, 0.4283, 0.3876, 0.4281])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4530, 0.4295, 0.4264, 0.4879, 0.4107, 0.4732])\n",
      "Valid Idx 3 | Loss tensor([0.4945, 0.4941, 0.4798, 0.4604, 0.3680])\n",
      "\n",
      "************** Batch 96 in 3.7261009216308594 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4316, 0.4210, 0.4228, 0.4153]) \n",
      "Test Loss tensor([0.4339, 0.4189, 0.4213, 0.4188])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4076, 0.4551, 0.4262, 0.4597, 0.4402]) \n",
      "Test Loss tensor([0.4049, 0.4549, 0.4261, 0.4591, 0.4411])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4097, 0.4990, 0.4536, 0.4221, 0.3620, 0.4580, 0.4283]) \n",
      "Test Loss tensor([0.4118, 0.4991, 0.4570, 0.4203, 0.3622, 0.4592, 0.4256])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4984, 0.3989, 0.4947, 0.4896, 0.4154, 0.4131, 0.4284]) \n",
      "Test Loss tensor([0.4990, 0.3941, 0.4974, 0.4919, 0.4142, 0.4154, 0.4293])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4299, 0.4280, 0.4934, 0.4912, 0.4971, 0.4289, 0.3853, 0.4307]) \n",
      "Test Loss tensor([0.4271, 0.4259, 0.4955, 0.4940, 0.4990, 0.4270, 0.3891, 0.4265])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4531, 0.4277, 0.4251, 0.4897, 0.4101, 0.4749])\n",
      "Valid Idx 3 | Loss tensor([0.4955, 0.4957, 0.4824, 0.4612, 0.3687])\n",
      "\n",
      "************** Batch 100 in 3.651728630065918 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4338, 0.4198, 0.4179, 0.4172]) \n",
      "Test Loss tensor([0.4350, 0.4192, 0.4230, 0.4182])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4025, 0.4527, 0.4265, 0.4600, 0.4398]) \n",
      "Test Loss tensor([0.4046, 0.4549, 0.4243, 0.4601, 0.4418])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4134, 0.4969, 0.4569, 0.4214, 0.3612, 0.4582, 0.4249]) \n",
      "Test Loss tensor([0.4109, 0.5005, 0.4582, 0.4207, 0.3616, 0.4588, 0.4243])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4995, 0.3931, 0.4985, 0.4926, 0.4131, 0.4170, 0.4298]) \n",
      "Test Loss tensor([0.5006, 0.3942, 0.4986, 0.4938, 0.4134, 0.4133, 0.4272])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4287, 0.4268, 0.4961, 0.4947, 0.4972, 0.4270, 0.3904, 0.4261]) \n",
      "Test Loss tensor([0.4258, 0.4243, 0.4972, 0.4951, 0.5005, 0.4255, 0.3889, 0.4248])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4532, 0.4266, 0.4237, 0.4912, 0.4083, 0.4761])\n",
      "Valid Idx 3 | Loss tensor([0.4973, 0.4973, 0.4830, 0.4622, 0.3701])\n",
      "Gradients: Input 1.8372509202890797e-06 | Message 0.0012040783185511827 | Update 0.011270608752965927 | Output 0.4863385558128357\n",
      "\n",
      "************** Batch 104 in 3.7096333503723145 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4320, 0.4192, 0.4199, 0.4162]) \n",
      "Test Loss tensor([0.4343, 0.4181, 0.4249, 0.4173])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4039, 0.4553, 0.4242, 0.4591, 0.4400]) \n",
      "Test Loss tensor([0.4047, 0.4551, 0.4231, 0.4596, 0.4407])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4109, 0.5009, 0.4561, 0.4203, 0.3622, 0.4582, 0.4252]) \n",
      "Test Loss tensor([0.4103, 0.5027, 0.4585, 0.4190, 0.3608, 0.4583, 0.4227])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5010, 0.3984, 0.4997, 0.4919, 0.4116, 0.4124, 0.4260]) \n",
      "Test Loss tensor([0.5023, 0.3935, 0.5002, 0.4950, 0.4129, 0.4130, 0.4267])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4276, 0.4242, 0.4972, 0.4958, 0.4987, 0.4253, 0.3870, 0.4251]) \n",
      "Test Loss tensor([0.4244, 0.4230, 0.4987, 0.4966, 0.5021, 0.4235, 0.3869, 0.4236])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4532, 0.4254, 0.4222, 0.4926, 0.4077, 0.4767])\n",
      "Valid Idx 3 | Loss tensor([0.4994, 0.4994, 0.4843, 0.4623, 0.3705])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 108 in 3.7382805347442627 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4364, 0.4195, 0.4201, 0.4179]) \n",
      "Test Loss tensor([0.4354, 0.4182, 0.4248, 0.4170])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4020, 0.4562, 0.4225, 0.4595, 0.4418]) \n",
      "Test Loss tensor([0.4051, 0.4555, 0.4211, 0.4616, 0.4414])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4110, 0.5019, 0.4613, 0.4200, 0.3609, 0.4598, 0.4222]) \n",
      "Test Loss tensor([0.4084, 0.5043, 0.4599, 0.4202, 0.3610, 0.4585, 0.4211])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5034, 0.3900, 0.5015, 0.4966, 0.4123, 0.4124, 0.4256]) \n",
      "Test Loss tensor([0.5039, 0.3926, 0.5020, 0.4970, 0.4130, 0.4117, 0.4249])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4235, 0.4227, 0.4960, 0.4972, 0.5029, 0.4241, 0.3927, 0.4232]) \n",
      "Test Loss tensor([0.4231, 0.4225, 0.5000, 0.4984, 0.5035, 0.4220, 0.3875, 0.4219])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4535, 0.4235, 0.4213, 0.4937, 0.4066, 0.4772])\n",
      "Valid Idx 3 | Loss tensor([0.5008, 0.5008, 0.4857, 0.4630, 0.3710])\n",
      "\n",
      "************** Batch 112 in 3.699754476547241 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4344, 0.4189, 0.4194, 0.4187]) \n",
      "Test Loss tensor([0.4365, 0.4175, 0.4262, 0.4169])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4035, 0.4544, 0.4196, 0.4617, 0.4430]) \n",
      "Test Loss tensor([0.4045, 0.4556, 0.4195, 0.4603, 0.4413])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4083, 0.5044, 0.4591, 0.4166, 0.3591, 0.4580, 0.4221]) \n",
      "Test Loss tensor([0.4072, 0.5058, 0.4609, 0.4181, 0.3604, 0.4590, 0.4197])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5031, 0.3874, 0.5021, 0.4971, 0.4152, 0.4097, 0.4259]) \n",
      "Test Loss tensor([0.5058, 0.3906, 0.5034, 0.4983, 0.4126, 0.4109, 0.4239])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4223, 0.4208, 0.4980, 0.4979, 0.5038, 0.4224, 0.3891, 0.4221]) \n",
      "Test Loss tensor([0.4215, 0.4208, 0.5016, 0.4995, 0.5056, 0.4207, 0.3896, 0.4199])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4537, 0.4215, 0.4189, 0.4953, 0.4049, 0.4791])\n",
      "Valid Idx 3 | Loss tensor([0.5024, 0.5018, 0.4874, 0.4624, 0.3732])\n",
      "\n",
      "************** Batch 116 in 3.6599464416503906 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4347, 0.4192, 0.4315, 0.4149]) \n",
      "Test Loss tensor([0.4357, 0.4168, 0.4280, 0.4155])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4022, 0.4566, 0.4199, 0.4606, 0.4418]) \n",
      "Test Loss tensor([0.4032, 0.4556, 0.4182, 0.4625, 0.4409])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4091, 0.5058, 0.4625, 0.4162, 0.3596, 0.4565, 0.4207]) \n",
      "Test Loss tensor([0.4063, 0.5070, 0.4615, 0.4183, 0.3602, 0.4585, 0.4185])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5069, 0.3904, 0.5019, 0.4975, 0.4111, 0.4126, 0.4230]) \n",
      "Test Loss tensor([0.5077, 0.3899, 0.5047, 0.4986, 0.4129, 0.4105, 0.4226])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4204, 0.4207, 0.5017, 0.5002, 0.5056, 0.4200, 0.3885, 0.4194]) \n",
      "Test Loss tensor([0.4204, 0.4199, 0.5035, 0.5018, 0.5066, 0.4189, 0.3867, 0.4194])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4546, 0.4209, 0.4183, 0.4963, 0.4047, 0.4806])\n",
      "Valid Idx 3 | Loss tensor([0.5036, 0.5037, 0.4887, 0.4643, 0.3727])\n",
      "\n",
      "************** Batch 120 in 3.7073171138763428 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4359, 0.4214, 0.4228, 0.4172]) \n",
      "Test Loss tensor([0.4363, 0.4167, 0.4278, 0.4154])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4031, 0.4576, 0.4177, 0.4624, 0.4400]) \n",
      "Test Loss tensor([0.4043, 0.4553, 0.4166, 0.4624, 0.4408])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4054, 0.5079, 0.4619, 0.4171, 0.3548, 0.4566, 0.4168]) \n",
      "Test Loss tensor([0.4051, 0.5093, 0.4625, 0.4178, 0.3610, 0.4586, 0.4167])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5077, 0.3886, 0.5043, 0.4977, 0.4087, 0.4081, 0.4224]) \n",
      "Test Loss tensor([0.5086, 0.3891, 0.5065, 0.5010, 0.4123, 0.4092, 0.4211])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4190, 0.4197, 0.5030, 0.5037, 0.5066, 0.4201, 0.3888, 0.4189]) \n",
      "Test Loss tensor([0.4184, 0.4181, 0.5048, 0.5026, 0.5087, 0.4176, 0.3873, 0.4177])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4537, 0.4195, 0.4176, 0.4977, 0.4039, 0.4809])\n",
      "Valid Idx 3 | Loss tensor([0.5052, 0.5049, 0.4898, 0.4645, 0.3772])\n",
      "Gradients: Input 1.9203428109904053e-06 | Message 0.0011314812581986189 | Update 0.010124213993549347 | Output 0.4198869466781616\n",
      "\n",
      "************** Batch 124 in 3.69385027885437 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4366, 0.4183, 0.4235, 0.4169]) \n",
      "Test Loss tensor([0.4368, 0.4166, 0.4288, 0.4155])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4027, 0.4560, 0.4172, 0.4661, 0.4385]) \n",
      "Test Loss tensor([0.4040, 0.4561, 0.4154, 0.4632, 0.4422])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4041, 0.5077, 0.4634, 0.4204, 0.3623, 0.4555, 0.4145]) \n",
      "Test Loss tensor([0.4037, 0.5107, 0.4641, 0.4178, 0.3607, 0.4578, 0.4151])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5090, 0.3911, 0.5070, 0.5013, 0.4110, 0.4114, 0.4199]) \n",
      "Test Loss tensor([0.5103, 0.3891, 0.5082, 0.5022, 0.4123, 0.4087, 0.4195])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4153, 0.4164, 0.5029, 0.5023, 0.5105, 0.4188, 0.3884, 0.4192]) \n",
      "Test Loss tensor([0.4166, 0.4171, 0.5063, 0.5038, 0.5099, 0.4164, 0.3887, 0.4158])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4542, 0.4181, 0.4155, 0.4996, 0.4029, 0.4821])\n",
      "Valid Idx 3 | Loss tensor([0.5068, 0.5073, 0.4910, 0.4649, 0.3749])\n",
      "\n",
      "************** Batch 128 in 3.6796000003814697 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4357, 0.4168, 0.4291, 0.4158]) \n",
      "Test Loss tensor([0.4377, 0.4162, 0.4299, 0.4151])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4042, 0.4547, 0.4166, 0.4667, 0.4415]) \n",
      "Test Loss tensor([0.4027, 0.4556, 0.4142, 0.4636, 0.4409])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4036, 0.5087, 0.4636, 0.4166, 0.3580, 0.4553, 0.4140]) \n",
      "Test Loss tensor([0.4026, 0.5120, 0.4640, 0.4159, 0.3592, 0.4570, 0.4139])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5097, 0.3887, 0.5083, 0.5040, 0.4077, 0.4116, 0.4193]) \n",
      "Test Loss tensor([0.5118, 0.3883, 0.5093, 0.5036, 0.4114, 0.4079, 0.4187])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4164, 0.4182, 0.5064, 0.5048, 0.5083, 0.4184, 0.3854, 0.4163]) \n",
      "Test Loss tensor([0.4161, 0.4158, 0.5077, 0.5053, 0.5115, 0.4152, 0.3884, 0.4142])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4537, 0.4163, 0.4151, 0.5000, 0.4016, 0.4825])\n",
      "Valid Idx 3 | Loss tensor([0.5080, 0.5088, 0.4931, 0.4662, 0.3756])\n",
      "\n",
      "************** Batch 132 in 3.6501691341400146 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4357, 0.4153, 0.4288, 0.4183]) \n",
      "Test Loss tensor([0.4374, 0.4163, 0.4310, 0.4140])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4043, 0.4570, 0.4138, 0.4623, 0.4411]) \n",
      "Test Loss tensor([0.4032, 0.4557, 0.4123, 0.4647, 0.4409])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4026, 0.5132, 0.4631, 0.4176, 0.3571, 0.4571, 0.4124]) \n",
      "Test Loss tensor([0.4013, 0.5137, 0.4653, 0.4158, 0.3588, 0.4579, 0.4126])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5122, 0.3871, 0.5098, 0.5050, 0.4104, 0.4082, 0.4189]) \n",
      "Test Loss tensor([0.5134, 0.3871, 0.5108, 0.5048, 0.4120, 0.4079, 0.4173])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4155, 0.4154, 0.5069, 0.5062, 0.5118, 0.4152, 0.3911, 0.4144]) \n",
      "Test Loss tensor([0.4142, 0.4147, 0.5090, 0.5069, 0.5132, 0.4135, 0.3890, 0.4133])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4543, 0.4162, 0.4144, 0.5013, 0.4015, 0.4834])\n",
      "Valid Idx 3 | Loss tensor([0.5094, 0.5097, 0.4941, 0.4671, 0.3748])\n",
      "\n",
      "************** Batch 136 in 3.6904029846191406 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4366, 0.4154, 0.4302, 0.4131]) \n",
      "Test Loss tensor([0.4381, 0.4155, 0.4319, 0.4134])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4072, 0.4540, 0.4131, 0.4650, 0.4429]) \n",
      "Test Loss tensor([0.4029, 0.4569, 0.4112, 0.4656, 0.4411])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.4002, 0.5139, 0.4642, 0.4144, 0.3599, 0.4566, 0.4107]) \n",
      "Test Loss tensor([0.4001, 0.5149, 0.4649, 0.4164, 0.3602, 0.4587, 0.4113])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5128, 0.3894, 0.5119, 0.5038, 0.4107, 0.4048, 0.4187]) \n",
      "Test Loss tensor([0.5148, 0.3869, 0.5124, 0.5055, 0.4106, 0.4057, 0.4164])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4145, 0.4154, 0.5106, 0.5072, 0.5139, 0.4134, 0.3890, 0.4119]) \n",
      "Test Loss tensor([0.4137, 0.4134, 0.5104, 0.5081, 0.5145, 0.4125, 0.3874, 0.4123])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4539, 0.4144, 0.4126, 0.5033, 0.4002, 0.4847])\n",
      "Valid Idx 3 | Loss tensor([0.5111, 0.5108, 0.4954, 0.4681, 0.3768])\n",
      "\n",
      "************** Batch 140 in 3.68664813041687 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4373, 0.4167, 0.4317, 0.4135]) \n",
      "Test Loss tensor([0.4386, 0.4145, 0.4331, 0.4123])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4027, 0.4570, 0.4120, 0.4648, 0.4407]) \n",
      "Test Loss tensor([0.4026, 0.4567, 0.4099, 0.4656, 0.4420])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3986, 0.5164, 0.4653, 0.4143, 0.3568, 0.4569, 0.4118]) \n",
      "Test Loss tensor([0.3992, 0.5162, 0.4665, 0.4153, 0.3582, 0.4575, 0.4101])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5153, 0.3852, 0.5126, 0.5079, 0.4115, 0.4101, 0.4176]) \n",
      "Test Loss tensor([0.5159, 0.3861, 0.5139, 0.5073, 0.4111, 0.4056, 0.4150])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4134, 0.4144, 0.5087, 0.5071, 0.5136, 0.4125, 0.3844, 0.4131]) \n",
      "Test Loss tensor([0.4121, 0.4127, 0.5118, 0.5092, 0.5158, 0.4111, 0.3868, 0.4107])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4543, 0.4131, 0.4113, 0.5040, 0.3989, 0.4853])\n",
      "Valid Idx 3 | Loss tensor([0.5122, 0.5127, 0.4965, 0.4671, 0.3780])\n",
      "Gradients: Input 2.0930287973897066e-06 | Message 0.0014227819629013538 | Update 0.00835767388343811 | Output 0.29581719636917114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 144 in 3.6670353412628174 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4356, 0.4166, 0.4342, 0.4155]) \n",
      "Test Loss tensor([0.4384, 0.4154, 0.4344, 0.4133])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3985, 0.4575, 0.4110, 0.4662, 0.4443]) \n",
      "Test Loss tensor([0.4016, 0.4570, 0.4088, 0.4669, 0.4409])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3970, 0.5185, 0.4677, 0.4139, 0.3603, 0.4557, 0.4098]) \n",
      "Test Loss tensor([0.3989, 0.5178, 0.4683, 0.4146, 0.3598, 0.4583, 0.4085])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5158, 0.3865, 0.5144, 0.5070, 0.4101, 0.4073, 0.4161]) \n",
      "Test Loss tensor([0.5178, 0.3853, 0.5149, 0.5085, 0.4114, 0.4051, 0.4142])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4104, 0.4147, 0.5120, 0.5103, 0.5158, 0.4096, 0.3888, 0.4107]) \n",
      "Test Loss tensor([0.4112, 0.4115, 0.5127, 0.5105, 0.5172, 0.4102, 0.3889, 0.4093])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4547, 0.4118, 0.4104, 0.5051, 0.3988, 0.4862])\n",
      "Valid Idx 3 | Loss tensor([0.5141, 0.5138, 0.4978, 0.4676, 0.3773])\n",
      "\n",
      "************** Batch 148 in 3.6960902214050293 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4357, 0.4176, 0.4302, 0.4162]) \n",
      "Test Loss tensor([0.4388, 0.4145, 0.4347, 0.4130])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4018, 0.4572, 0.4105, 0.4665, 0.4385]) \n",
      "Test Loss tensor([0.4025, 0.4569, 0.4080, 0.4657, 0.4414])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3970, 0.5165, 0.4649, 0.4140, 0.3579, 0.4574, 0.4097]) \n",
      "Test Loss tensor([0.3968, 0.5190, 0.4678, 0.4150, 0.3588, 0.4583, 0.4074])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5164, 0.3863, 0.5159, 0.5076, 0.4095, 0.4069, 0.4151]) \n",
      "Test Loss tensor([0.5191, 0.3841, 0.5169, 0.5099, 0.4102, 0.4043, 0.4127])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4119, 0.4110, 0.5126, 0.5098, 0.5183, 0.4089, 0.3834, 0.4094]) \n",
      "Test Loss tensor([0.4094, 0.4109, 0.5147, 0.5116, 0.5181, 0.4089, 0.3871, 0.4084])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4546, 0.4110, 0.4092, 0.5061, 0.3981, 0.4871])\n",
      "Valid Idx 3 | Loss tensor([0.5147, 0.5154, 0.4983, 0.4691, 0.3785])\n",
      "\n",
      "************** Batch 152 in 3.712725877761841 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4410, 0.4164, 0.4349, 0.4137]) \n",
      "Test Loss tensor([0.4387, 0.4146, 0.4348, 0.4119])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4036, 0.4562, 0.4075, 0.4652, 0.4421]) \n",
      "Test Loss tensor([0.4024, 0.4565, 0.4069, 0.4668, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3986, 0.5185, 0.4721, 0.4116, 0.3601, 0.4541, 0.4076]) \n",
      "Test Loss tensor([0.3958, 0.5199, 0.4681, 0.4137, 0.3578, 0.4579, 0.4064])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5184, 0.3874, 0.5154, 0.5105, 0.4072, 0.4024, 0.4130]) \n",
      "Test Loss tensor([0.5192, 0.3839, 0.5175, 0.5113, 0.4105, 0.4036, 0.4118])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4092, 0.4102, 0.5144, 0.5134, 0.5177, 0.4089, 0.3861, 0.4094]) \n",
      "Test Loss tensor([0.4086, 0.4099, 0.5161, 0.5123, 0.5193, 0.4074, 0.3870, 0.4067])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4547, 0.4098, 0.4092, 0.5075, 0.3973, 0.4880])\n",
      "Valid Idx 3 | Loss tensor([0.5155, 0.5160, 0.5000, 0.4694, 0.3784])\n",
      "\n",
      "************** Batch 156 in 3.721573829650879 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4400, 0.4165, 0.4323, 0.4131]) \n",
      "Test Loss tensor([0.4400, 0.4135, 0.4359, 0.4113])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4028, 0.4534, 0.4066, 0.4641, 0.4407]) \n",
      "Test Loss tensor([0.4013, 0.4565, 0.4057, 0.4671, 0.4408])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3970, 0.5191, 0.4699, 0.4116, 0.3557, 0.4569, 0.4070]) \n",
      "Test Loss tensor([0.3957, 0.5213, 0.4699, 0.4131, 0.3584, 0.4572, 0.4053])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5194, 0.3814, 0.5175, 0.5109, 0.4119, 0.4028, 0.4107]) \n",
      "Test Loss tensor([0.5212, 0.3831, 0.5186, 0.5116, 0.4095, 0.4035, 0.4105])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4096, 0.4092, 0.5167, 0.5137, 0.5204, 0.4075, 0.3851, 0.4061]) \n",
      "Test Loss tensor([0.4081, 0.4086, 0.5163, 0.5139, 0.5203, 0.4067, 0.3886, 0.4060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4543, 0.4089, 0.4082, 0.5083, 0.3968, 0.4881])\n",
      "Valid Idx 3 | Loss tensor([0.5171, 0.5177, 0.4998, 0.4695, 0.3799])\n",
      "\n",
      "************** Batch 160 in 3.7476871013641357 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4399, 0.4158, 0.4385, 0.4143]) \n",
      "Test Loss tensor([0.4395, 0.4133, 0.4372, 0.4107])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4008, 0.4572, 0.4064, 0.4693, 0.4450]) \n",
      "Test Loss tensor([0.4012, 0.4571, 0.4041, 0.4679, 0.4418])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3954, 0.5213, 0.4711, 0.4106, 0.3592, 0.4582, 0.4041]) \n",
      "Test Loss tensor([0.3951, 0.5219, 0.4718, 0.4135, 0.3583, 0.4582, 0.4044])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5198, 0.3825, 0.5194, 0.5118, 0.4082, 0.4030, 0.4118]) \n",
      "Test Loss tensor([0.5218, 0.3818, 0.5191, 0.5123, 0.4104, 0.4026, 0.4100])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4085, 0.4072, 0.5166, 0.5149, 0.5210, 0.4069, 0.3848, 0.4073]) \n",
      "Test Loss tensor([0.4072, 0.4078, 0.5176, 0.5154, 0.5215, 0.4056, 0.3879, 0.4048])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4548, 0.4078, 0.4067, 0.5090, 0.3960, 0.4883])\n",
      "Valid Idx 3 | Loss tensor([0.5175, 0.5180, 0.5008, 0.4706, 0.3797])\n",
      "Gradients: Input 1.8435042647979571e-06 | Message 0.0013536198530346155 | Update 0.014417724683880806 | Output 0.1336374282836914\n",
      "\n",
      "************** Batch 164 in 4.072894334793091 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4394, 0.4134, 0.4404, 0.4140]) \n",
      "Test Loss tensor([0.4396, 0.4130, 0.4383, 0.4104])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4002, 0.4586, 0.4043, 0.4690, 0.4440]) \n",
      "Test Loss tensor([0.4003, 0.4574, 0.4043, 0.4691, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3946, 0.5227, 0.4749, 0.4148, 0.3569, 0.4591, 0.4051]) \n",
      "Test Loss tensor([0.3938, 0.5234, 0.4698, 0.4139, 0.3580, 0.4568, 0.4036])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5219, 0.3815, 0.5184, 0.5136, 0.4100, 0.4027, 0.4093]) \n",
      "Test Loss tensor([0.5230, 0.3825, 0.5208, 0.5135, 0.4096, 0.4018, 0.4092])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4070, 0.4088, 0.5159, 0.5166, 0.5206, 0.4055, 0.3901, 0.4047]) \n",
      "Test Loss tensor([0.4059, 0.4080, 0.5187, 0.5157, 0.5220, 0.4047, 0.3876, 0.4038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4537, 0.4075, 0.4061, 0.5098, 0.3949, 0.4883])\n",
      "Valid Idx 3 | Loss tensor([0.5187, 0.5198, 0.5021, 0.4702, 0.3795])\n",
      "\n",
      "************** Batch 168 in 3.850942850112915 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4443, 0.4138, 0.4386, 0.4092]) \n",
      "Test Loss tensor([0.4403, 0.4138, 0.4386, 0.4118])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4033, 0.4584, 0.4011, 0.4677, 0.4423]) \n",
      "Test Loss tensor([0.4006, 0.4571, 0.4030, 0.4683, 0.4421])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3913, 0.5208, 0.4722, 0.4099, 0.3597, 0.4575, 0.4028]) \n",
      "Test Loss tensor([0.3938, 0.5236, 0.4710, 0.4123, 0.3584, 0.4580, 0.4021])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5215, 0.3811, 0.5192, 0.5125, 0.4104, 0.4005, 0.4098]) \n",
      "Test Loss tensor([0.5237, 0.3816, 0.5220, 0.5140, 0.4093, 0.4016, 0.4087])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4063, 0.4089, 0.5177, 0.5152, 0.5223, 0.4047, 0.3904, 0.4041]) \n",
      "Test Loss tensor([0.4058, 0.4063, 0.5193, 0.5163, 0.5232, 0.4044, 0.3866, 0.4030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4556, 0.4067, 0.4053, 0.5105, 0.3947, 0.4906])\n",
      "Valid Idx 3 | Loss tensor([0.5195, 0.5207, 0.5025, 0.4703, 0.3801])\n",
      "\n",
      "************** Batch 172 in 3.6837165355682373 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4420, 0.4104, 0.4360, 0.4109]) \n",
      "Test Loss tensor([0.4413, 0.4135, 0.4388, 0.4115])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4028, 0.4565, 0.4039, 0.4686, 0.4410]) \n",
      "Test Loss tensor([0.4004, 0.4579, 0.4019, 0.4697, 0.4420])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3945, 0.5245, 0.4709, 0.4119, 0.3567, 0.4573, 0.4021]) \n",
      "Test Loss tensor([0.3932, 0.5248, 0.4720, 0.4126, 0.3584, 0.4584, 0.4018])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5248, 0.3801, 0.5222, 0.5154, 0.4100, 0.3992, 0.4083]) \n",
      "Test Loss tensor([0.5249, 0.3807, 0.5222, 0.5148, 0.4101, 0.4008, 0.4085])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4055, 0.4069, 0.5183, 0.5169, 0.5236, 0.4046, 0.3918, 0.4043]) \n",
      "Test Loss tensor([0.4047, 0.4063, 0.5205, 0.5176, 0.5240, 0.4035, 0.3879, 0.4021])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4550, 0.4065, 0.4053, 0.5111, 0.3955, 0.4907])\n",
      "Valid Idx 3 | Loss tensor([0.5204, 0.5213, 0.5036, 0.4717, 0.3819])\n",
      "\n",
      "************** Batch 176 in 3.717949390411377 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4431, 0.4165, 0.4357, 0.4143]) \n",
      "Test Loss tensor([0.4404, 0.4133, 0.4402, 0.4104])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3982, 0.4567, 0.4030, 0.4675, 0.4420]) \n",
      "Test Loss tensor([0.4000, 0.4570, 0.4015, 0.4686, 0.4416])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3932, 0.5247, 0.4710, 0.4108, 0.3591, 0.4585, 0.4020]) \n",
      "Test Loss tensor([0.3930, 0.5247, 0.4723, 0.4130, 0.3580, 0.4579, 0.4008])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5245, 0.3775, 0.5231, 0.5171, 0.4112, 0.4005, 0.4089]) \n",
      "Test Loss tensor([0.5251, 0.3822, 0.5230, 0.5155, 0.4094, 0.4016, 0.4073])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4043, 0.4039, 0.5205, 0.5172, 0.5250, 0.4039, 0.3868, 0.4031]) \n",
      "Test Loss tensor([0.4041, 0.4060, 0.5208, 0.5182, 0.5247, 0.4025, 0.3875, 0.4015])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4555, 0.4053, 0.4043, 0.5122, 0.3954, 0.4911])\n",
      "Valid Idx 3 | Loss tensor([0.5211, 0.5223, 0.5038, 0.4710, 0.3845])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 180 in 3.7127628326416016 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4411, 0.4142, 0.4381, 0.4089]) \n",
      "Test Loss tensor([0.4416, 0.4125, 0.4390, 0.4115])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3963, 0.4594, 0.4012, 0.4658, 0.4408]) \n",
      "Test Loss tensor([0.4003, 0.4571, 0.4004, 0.4684, 0.4407])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3929, 0.5263, 0.4758, 0.4126, 0.3598, 0.4579, 0.4019]) \n",
      "Test Loss tensor([0.3919, 0.5267, 0.4723, 0.4115, 0.3576, 0.4581, 0.4004])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5269, 0.3830, 0.5216, 0.5160, 0.4103, 0.4018, 0.4087]) \n",
      "Test Loss tensor([0.5257, 0.3802, 0.5239, 0.5167, 0.4092, 0.4005, 0.4069])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4042, 0.4054, 0.5201, 0.5181, 0.5250, 0.4041, 0.3882, 0.4020]) \n",
      "Test Loss tensor([0.4033, 0.4053, 0.5211, 0.5192, 0.5255, 0.4022, 0.3871, 0.4007])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4555, 0.4045, 0.4040, 0.5126, 0.3943, 0.4918])\n",
      "Valid Idx 3 | Loss tensor([0.5220, 0.5227, 0.5051, 0.4719, 0.3838])\n",
      "Gradients: Input 2.2548874767380767e-06 | Message 0.0017908129375427961 | Update 0.002526070922613144 | Output 0.07291791588068008\n",
      "\n",
      "************** Batch 184 in 3.6987996101379395 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4375, 0.4173, 0.4396, 0.4109]) \n",
      "Test Loss tensor([0.4417, 0.4119, 0.4403, 0.4103])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3997, 0.4561, 0.4015, 0.4676, 0.4416]) \n",
      "Test Loss tensor([0.4004, 0.4579, 0.3996, 0.4695, 0.4414])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3917, 0.5252, 0.4733, 0.4121, 0.3570, 0.4572, 0.4008]) \n",
      "Test Loss tensor([0.3913, 0.5266, 0.4743, 0.4117, 0.3581, 0.4582, 0.3997])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5264, 0.3785, 0.5228, 0.5175, 0.4070, 0.3990, 0.4080]) \n",
      "Test Loss tensor([0.5266, 0.3803, 0.5247, 0.5173, 0.4098, 0.3995, 0.4061])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4029, 0.4056, 0.5205, 0.5190, 0.5258, 0.4019, 0.3830, 0.4007]) \n",
      "Test Loss tensor([0.4027, 0.4046, 0.5217, 0.5189, 0.5266, 0.4018, 0.3874, 0.4004])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4553, 0.4042, 0.4031, 0.5132, 0.3941, 0.4918])\n",
      "Valid Idx 3 | Loss tensor([0.5221, 0.5237, 0.5053, 0.4720, 0.3817])\n",
      "\n",
      "************** Batch 188 in 3.720736026763916 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4383, 0.4121, 0.4390, 0.4094]) \n",
      "Test Loss tensor([0.4421, 0.4129, 0.4420, 0.4101])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3985, 0.4578, 0.4012, 0.4740, 0.4435]) \n",
      "Test Loss tensor([0.3995, 0.4575, 0.3992, 0.4698, 0.4425])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3882, 0.5262, 0.4755, 0.4089, 0.3558, 0.4569, 0.3989]) \n",
      "Test Loss tensor([0.3904, 0.5272, 0.4713, 0.4105, 0.3570, 0.4577, 0.3987])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5281, 0.3761, 0.5249, 0.5172, 0.4083, 0.3990, 0.4053]) \n",
      "Test Loss tensor([0.5276, 0.3801, 0.5252, 0.5179, 0.4087, 0.4000, 0.4054])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4023, 0.4042, 0.5258, 0.5196, 0.5259, 0.4013, 0.3869, 0.4002]) \n",
      "Test Loss tensor([0.4025, 0.4045, 0.5226, 0.5206, 0.5266, 0.4010, 0.3883, 0.3996])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.4035, 0.4021, 0.5136, 0.3926, 0.4927])\n",
      "Valid Idx 3 | Loss tensor([0.5234, 0.5240, 0.5052, 0.4727, 0.3824])\n",
      "\n",
      "************** Batch 192 in 3.683579444885254 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4418, 0.4139, 0.4365, 0.4086]) \n",
      "Test Loss tensor([0.4415, 0.4128, 0.4403, 0.4099])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4010, 0.4579, 0.3991, 0.4688, 0.4431]) \n",
      "Test Loss tensor([0.4005, 0.4578, 0.3984, 0.4704, 0.4412])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3895, 0.5272, 0.4694, 0.4129, 0.3568, 0.4578, 0.3983]) \n",
      "Test Loss tensor([0.3903, 0.5280, 0.4740, 0.4111, 0.3571, 0.4582, 0.3984])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5273, 0.3765, 0.5251, 0.5181, 0.4077, 0.4011, 0.4046]) \n",
      "Test Loss tensor([0.5283, 0.3780, 0.5263, 0.5176, 0.4098, 0.3986, 0.4049])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4019, 0.4030, 0.5231, 0.5225, 0.5250, 0.4010, 0.3870, 0.3994]) \n",
      "Test Loss tensor([0.4019, 0.4038, 0.5233, 0.5208, 0.5276, 0.4000, 0.3879, 0.3988])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4553, 0.4038, 0.4023, 0.5146, 0.3931, 0.4933])\n",
      "Valid Idx 3 | Loss tensor([0.5236, 0.5257, 0.5063, 0.4722, 0.3821])\n",
      "\n",
      "************** Batch 196 in 3.693284749984741 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4416, 0.4109, 0.4461, 0.4067]) \n",
      "Test Loss tensor([0.4415, 0.4127, 0.4410, 0.4102])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3989, 0.4588, 0.3993, 0.4685, 0.4405]) \n",
      "Test Loss tensor([0.3999, 0.4576, 0.3983, 0.4698, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3902, 0.5274, 0.4754, 0.4111, 0.3558, 0.4577, 0.3981]) \n",
      "Test Loss tensor([0.3895, 0.5288, 0.4736, 0.4117, 0.3561, 0.4579, 0.3977])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5284, 0.3788, 0.5272, 0.5192, 0.4059, 0.3990, 0.4039]) \n",
      "Test Loss tensor([0.5284, 0.3794, 0.5260, 0.5186, 0.4101, 0.4002, 0.4044])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4011, 0.4049, 0.5218, 0.5211, 0.5282, 0.4009, 0.3861, 0.3976]) \n",
      "Test Loss tensor([0.4016, 0.4034, 0.5241, 0.5208, 0.5279, 0.3998, 0.3876, 0.3979])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4549, 0.4029, 0.4012, 0.5146, 0.3935, 0.4931])\n",
      "Valid Idx 3 | Loss tensor([0.5236, 0.5261, 0.5066, 0.4725, 0.3846])\n",
      "\n",
      "************** Batch 200 in 3.6829824447631836 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4456, 0.4132, 0.4383, 0.4089]) \n",
      "Test Loss tensor([0.4418, 0.4111, 0.4418, 0.4096])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3966, 0.4599, 0.3981, 0.4683, 0.4433]) \n",
      "Test Loss tensor([0.3994, 0.4574, 0.3974, 0.4693, 0.4405])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3902, 0.5273, 0.4752, 0.4126, 0.3571, 0.4567, 0.3966]) \n",
      "Test Loss tensor([0.3892, 0.5294, 0.4738, 0.4109, 0.3571, 0.4577, 0.3971])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5296, 0.3816, 0.5258, 0.5185, 0.4100, 0.4007, 0.4039]) \n",
      "Test Loss tensor([0.5291, 0.3794, 0.5276, 0.5193, 0.4095, 0.3989, 0.4041])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4013, 0.4018, 0.5242, 0.5232, 0.5286, 0.4001, 0.3845, 0.3981]) \n",
      "Test Loss tensor([0.4010, 0.4034, 0.5242, 0.5213, 0.5284, 0.3992, 0.3879, 0.3978])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4558, 0.4027, 0.4020, 0.5152, 0.3927, 0.4927])\n",
      "Valid Idx 3 | Loss tensor([0.5243, 0.5257, 0.5065, 0.4728, 0.3841])\n",
      "Gradients: Input 2.2437175175582524e-06 | Message 0.001992966514080763 | Update 0.002094750991091132 | Output 0.010760600678622723\n",
      "\n",
      "************** Batch 204 in 3.683936357498169 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4455, 0.4106, 0.4369, 0.4102]) \n",
      "Test Loss tensor([0.4419, 0.4129, 0.4427, 0.4097])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3963, 0.4586, 0.3979, 0.4688, 0.4423]) \n",
      "Test Loss tensor([0.3991, 0.4573, 0.3971, 0.4708, 0.4420])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3864, 0.5307, 0.4724, 0.4099, 0.3555, 0.4560, 0.3962]) \n",
      "Test Loss tensor([0.3896, 0.5298, 0.4740, 0.4098, 0.3570, 0.4581, 0.3971])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5283, 0.3821, 0.5264, 0.5184, 0.4076, 0.3995, 0.4030]) \n",
      "Test Loss tensor([0.5297, 0.3781, 0.5280, 0.5198, 0.4086, 0.3996, 0.4035])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3998, 0.4028, 0.5252, 0.5203, 0.5292, 0.3993, 0.3888, 0.3985]) \n",
      "Test Loss tensor([0.4003, 0.4025, 0.5252, 0.5223, 0.5290, 0.3991, 0.3885, 0.3972])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4558, 0.4021, 0.4010, 0.5157, 0.3912, 0.4940])\n",
      "Valid Idx 3 | Loss tensor([0.5249, 0.5269, 0.5072, 0.4734, 0.3836])\n",
      "\n",
      "************** Batch 208 in 4.08322286605835 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4412, 0.4123, 0.4394, 0.4088]) \n",
      "Test Loss tensor([0.4432, 0.4119, 0.4434, 0.4097])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4018, 0.4545, 0.3967, 0.4695, 0.4398]) \n",
      "Test Loss tensor([0.3999, 0.4573, 0.3971, 0.4704, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3905, 0.5297, 0.4739, 0.4062, 0.3565, 0.4585, 0.3960]) \n",
      "Test Loss tensor([0.3881, 0.5301, 0.4750, 0.4103, 0.3565, 0.4574, 0.3961])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5318, 0.3763, 0.5285, 0.5204, 0.4130, 0.3979, 0.4043]) \n",
      "Test Loss tensor([0.5304, 0.3778, 0.5279, 0.5201, 0.4079, 0.3997, 0.4030])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4015, 0.4013, 0.5235, 0.5233, 0.5289, 0.3989, 0.3896, 0.3975]) \n",
      "Test Loss tensor([0.4000, 0.4023, 0.5247, 0.5231, 0.5295, 0.3985, 0.3863, 0.3971])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4548, 0.4020, 0.4007, 0.5159, 0.3918, 0.4947])\n",
      "Valid Idx 3 | Loss tensor([0.5257, 0.5274, 0.5084, 0.4735, 0.3838])\n",
      "\n",
      "************** Batch 212 in 3.9949450492858887 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4417, 0.4145, 0.4437, 0.4082]) \n",
      "Test Loss tensor([0.4424, 0.4118, 0.4429, 0.4089])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4016, 0.4542, 0.3966, 0.4685, 0.4386]) \n",
      "Test Loss tensor([0.3995, 0.4581, 0.3960, 0.4709, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3903, 0.5300, 0.4713, 0.4082, 0.3558, 0.4568, 0.3948]) \n",
      "Test Loss tensor([0.3882, 0.5305, 0.4750, 0.4107, 0.3564, 0.4573, 0.3960])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5291, 0.3809, 0.5294, 0.5201, 0.4064, 0.4010, 0.4036]) \n",
      "Test Loss tensor([0.5300, 0.3789, 0.5285, 0.5204, 0.4082, 0.3987, 0.4029])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3992, 0.4024, 0.5260, 0.5250, 0.5292, 0.3987, 0.3899, 0.3961]) \n",
      "Test Loss tensor([0.3994, 0.4028, 0.5259, 0.5232, 0.5299, 0.3983, 0.3876, 0.3965])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4563, 0.4016, 0.3996, 0.5165, 0.3924, 0.4945])\n",
      "Valid Idx 3 | Loss tensor([0.5263, 0.5282, 0.5078, 0.4738, 0.3857])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 216 in 4.170600414276123 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4428, 0.4129, 0.4433, 0.4085]) \n",
      "Test Loss tensor([0.4425, 0.4118, 0.4421, 0.4086])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3980, 0.4590, 0.3981, 0.4723, 0.4411]) \n",
      "Test Loss tensor([0.3983, 0.4581, 0.3952, 0.4713, 0.4413])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3891, 0.5284, 0.4782, 0.4116, 0.3548, 0.4568, 0.3954]) \n",
      "Test Loss tensor([0.3885, 0.5308, 0.4766, 0.4109, 0.3569, 0.4582, 0.3951])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5318, 0.3729, 0.5297, 0.5208, 0.4098, 0.4009, 0.4021]) \n",
      "Test Loss tensor([0.5309, 0.3778, 0.5292, 0.5209, 0.4081, 0.3976, 0.4024])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4007, 0.4042, 0.5253, 0.5245, 0.5317, 0.3989, 0.3890, 0.3952]) \n",
      "Test Loss tensor([0.3993, 0.4019, 0.5260, 0.5235, 0.5304, 0.3977, 0.3889, 0.3959])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4555, 0.4004, 0.3995, 0.5172, 0.3914, 0.4945])\n",
      "Valid Idx 3 | Loss tensor([0.5264, 0.5278, 0.5091, 0.4737, 0.3859])\n",
      "\n",
      "************** Batch 220 in 4.290215969085693 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4406, 0.4151, 0.4436, 0.4086]) \n",
      "Test Loss tensor([0.4421, 0.4122, 0.4423, 0.4090])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4012, 0.4576, 0.3950, 0.4713, 0.4410]) \n",
      "Test Loss tensor([0.3987, 0.4576, 0.3955, 0.4709, 0.4403])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3870, 0.5301, 0.4731, 0.4140, 0.3508, 0.4562, 0.3956]) \n",
      "Test Loss tensor([0.3875, 0.5316, 0.4746, 0.4104, 0.3571, 0.4575, 0.3950])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5302, 0.3779, 0.5304, 0.5210, 0.4029, 0.3992, 0.4016]) \n",
      "Test Loss tensor([0.5310, 0.3780, 0.5294, 0.5210, 0.4080, 0.3986, 0.4021])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3991, 0.4011, 0.5263, 0.5258, 0.5299, 0.3995, 0.3907, 0.3952]) \n",
      "Test Loss tensor([0.3987, 0.4016, 0.5263, 0.5241, 0.5307, 0.3975, 0.3883, 0.3953])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4551, 0.4007, 0.3996, 0.5164, 0.3916, 0.4952])\n",
      "Valid Idx 3 | Loss tensor([0.5264, 0.5291, 0.5092, 0.4747, 0.3839])\n",
      "Gradients: Input 2.08903429665952e-06 | Message 0.0011788952397182584 | Update 0.0011036950163543224 | Output 0.025096958503127098\n",
      "\n",
      "************** Batch 224 in 4.1988115310668945 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4416, 0.4100, 0.4422, 0.4063]) \n",
      "Test Loss tensor([0.4431, 0.4124, 0.4440, 0.4089])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3961, 0.4586, 0.3945, 0.4737, 0.4438]) \n",
      "Test Loss tensor([0.3988, 0.4576, 0.3946, 0.4712, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3849, 0.5312, 0.4792, 0.4077, 0.3573, 0.4593, 0.3953]) \n",
      "Test Loss tensor([0.3881, 0.5319, 0.4765, 0.4098, 0.3578, 0.4577, 0.3946])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5330, 0.3766, 0.5308, 0.5205, 0.4018, 0.3988, 0.4013]) \n",
      "Test Loss tensor([0.5307, 0.3779, 0.5299, 0.5210, 0.4089, 0.3969, 0.4018])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3988, 0.3995, 0.5244, 0.5231, 0.5304, 0.3969, 0.3864, 0.3963]) \n",
      "Test Loss tensor([0.3989, 0.4010, 0.5269, 0.5243, 0.5312, 0.3971, 0.3884, 0.3954])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4562, 0.4005, 0.3993, 0.5171, 0.3907, 0.4952])\n",
      "Valid Idx 3 | Loss tensor([0.5273, 0.5296, 0.5091, 0.4736, 0.3850])\n",
      "\n",
      "************** Batch 228 in 4.283126354217529 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4398, 0.4137, 0.4432, 0.4045]) \n",
      "Test Loss tensor([0.4431, 0.4122, 0.4436, 0.4091])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3986, 0.4601, 0.3944, 0.4700, 0.4418]) \n",
      "Test Loss tensor([0.3995, 0.4574, 0.3946, 0.4715, 0.4413])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3867, 0.5308, 0.4787, 0.4100, 0.3530, 0.4570, 0.3949]) \n",
      "Test Loss tensor([0.3869, 0.5321, 0.4758, 0.4099, 0.3570, 0.4574, 0.3945])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5322, 0.3791, 0.5284, 0.5222, 0.4068, 0.4013, 0.4029]) \n",
      "Test Loss tensor([0.5315, 0.3775, 0.5304, 0.5216, 0.4086, 0.3978, 0.4016])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3988, 0.3999, 0.5263, 0.5252, 0.5322, 0.3976, 0.3900, 0.3948]) \n",
      "Test Loss tensor([0.3985, 0.4008, 0.5269, 0.5250, 0.5311, 0.3970, 0.3876, 0.3944])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4549, 0.3999, 0.3986, 0.5178, 0.3906, 0.4957])\n",
      "Valid Idx 3 | Loss tensor([0.5271, 0.5296, 0.5097, 0.4734, 0.3849])\n",
      "\n",
      "************** Batch 232 in 4.391924858093262 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4399, 0.4126, 0.4422, 0.4072]) \n",
      "Test Loss tensor([0.4418, 0.4115, 0.4439, 0.4087])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4010, 0.4570, 0.3947, 0.4687, 0.4409]) \n",
      "Test Loss tensor([0.3997, 0.4581, 0.3944, 0.4717, 0.4410])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3880, 0.5335, 0.4765, 0.4124, 0.3556, 0.4556, 0.3940]) \n",
      "Test Loss tensor([0.3874, 0.5322, 0.4771, 0.4084, 0.3564, 0.4575, 0.3942])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5331, 0.3789, 0.5307, 0.5180, 0.4077, 0.3987, 0.4023]) \n",
      "Test Loss tensor([0.5318, 0.3770, 0.5308, 0.5216, 0.4083, 0.3974, 0.4012])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3982, 0.4018, 0.5264, 0.5254, 0.5313, 0.3980, 0.3903, 0.3951]) \n",
      "Test Loss tensor([0.3983, 0.4013, 0.5275, 0.5253, 0.5310, 0.3969, 0.3874, 0.3946])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4566, 0.3998, 0.3988, 0.5175, 0.3905, 0.4969])\n",
      "Valid Idx 3 | Loss tensor([0.5275, 0.5302, 0.5095, 0.4741, 0.3855])\n",
      "\n",
      "************** Batch 236 in 3.9304697513580322 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4459, 0.4175, 0.4472, 0.4072]) \n",
      "Test Loss tensor([0.4436, 0.4107, 0.4433, 0.4080])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3959, 0.4594, 0.3945, 0.4716, 0.4428]) \n",
      "Test Loss tensor([0.4003, 0.4576, 0.3939, 0.4705, 0.4414])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3858, 0.5312, 0.4762, 0.4119, 0.3555, 0.4570, 0.3949]) \n",
      "Test Loss tensor([0.3874, 0.5322, 0.4761, 0.4096, 0.3575, 0.4575, 0.3935])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5320, 0.3777, 0.5308, 0.5219, 0.4109, 0.3997, 0.4016]) \n",
      "Test Loss tensor([0.5321, 0.3777, 0.5310, 0.5219, 0.4087, 0.3977, 0.4010])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3995, 0.3991, 0.5266, 0.5268, 0.5322, 0.3969, 0.3896, 0.3945]) \n",
      "Test Loss tensor([0.3979, 0.4002, 0.5273, 0.5253, 0.5316, 0.3962, 0.3888, 0.3937])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4550, 0.4000, 0.3986, 0.5176, 0.3907, 0.4954])\n",
      "Valid Idx 3 | Loss tensor([0.5278, 0.5306, 0.5096, 0.4747, 0.3873])\n",
      "\n",
      "************** Batch 240 in 3.8400142192840576 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4442, 0.4131, 0.4457, 0.4097]) \n",
      "Test Loss tensor([0.4419, 0.4118, 0.4439, 0.4082])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4001, 0.4563, 0.3926, 0.4727, 0.4436]) \n",
      "Test Loss tensor([0.3994, 0.4580, 0.3933, 0.4712, 0.4421])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3868, 0.5320, 0.4784, 0.4078, 0.3582, 0.4578, 0.3931]) \n",
      "Test Loss tensor([0.3870, 0.5327, 0.4767, 0.4092, 0.3559, 0.4575, 0.3936])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5321, 0.3750, 0.5297, 0.5214, 0.4108, 0.4011, 0.4009]) \n",
      "Test Loss tensor([0.5322, 0.3772, 0.5316, 0.5223, 0.4078, 0.3977, 0.4004])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3992, 0.4018, 0.5256, 0.5247, 0.5309, 0.3965, 0.3897, 0.3932]) \n",
      "Test Loss tensor([0.3979, 0.4003, 0.5275, 0.5257, 0.5313, 0.3963, 0.3877, 0.3938])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4560, 0.3995, 0.3980, 0.5181, 0.3909, 0.4961])\n",
      "Valid Idx 3 | Loss tensor([0.5283, 0.5302, 0.5096, 0.4739, 0.3870])\n",
      "Gradients: Input 2.3419520402967464e-06 | Message 0.0013861687621101737 | Update 0.0022693155333399773 | Output 0.07088326662778854\n",
      "\n",
      "************** Batch 244 in 3.8314526081085205 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4402, 0.4134, 0.4454, 0.4084]) \n",
      "Test Loss tensor([0.4423, 0.4114, 0.4444, 0.4084])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3996, 0.4587, 0.3937, 0.4706, 0.4411]) \n",
      "Test Loss tensor([0.4001, 0.4581, 0.3932, 0.4712, 0.4413])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3867, 0.5307, 0.4753, 0.4114, 0.3530, 0.4559, 0.3930]) \n",
      "Test Loss tensor([0.3870, 0.5324, 0.4774, 0.4088, 0.3564, 0.4577, 0.3934])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5328, 0.3806, 0.5299, 0.5216, 0.4030, 0.3981, 0.4015]) \n",
      "Test Loss tensor([0.5323, 0.3764, 0.5318, 0.5222, 0.4091, 0.3972, 0.4002])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3981, 0.4004, 0.5284, 0.5256, 0.5305, 0.3967, 0.3893, 0.3959]) \n",
      "Test Loss tensor([0.3976, 0.4000, 0.5283, 0.5259, 0.5321, 0.3966, 0.3873, 0.3932])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4550, 0.3994, 0.3979, 0.5179, 0.3905, 0.4967])\n",
      "Valid Idx 3 | Loss tensor([0.5283, 0.5309, 0.5099, 0.4741, 0.3859])\n",
      "\n",
      "************** Batch 248 in 3.819366931915283 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4427, 0.4112, 0.4428, 0.4115]) \n",
      "Test Loss tensor([0.4427, 0.4111, 0.4438, 0.4084])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3991, 0.4606, 0.3945, 0.4701, 0.4414]) \n",
      "Test Loss tensor([0.3997, 0.4577, 0.3936, 0.4723, 0.4418])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3858, 0.5324, 0.4739, 0.4073, 0.3564, 0.4579, 0.3945]) \n",
      "Test Loss tensor([0.3870, 0.5321, 0.4779, 0.4095, 0.3576, 0.4571, 0.3930])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5329, 0.3784, 0.5316, 0.5219, 0.4087, 0.3952, 0.4016]) \n",
      "Test Loss tensor([0.5326, 0.3768, 0.5317, 0.5225, 0.4091, 0.3978, 0.3999])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3979, 0.3994, 0.5262, 0.5251, 0.5329, 0.3949, 0.3865, 0.3932]) \n",
      "Test Loss tensor([0.3979, 0.4005, 0.5277, 0.5257, 0.5321, 0.3961, 0.3888, 0.3932])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4565, 0.3991, 0.3977, 0.5183, 0.3895, 0.4968])\n",
      "Valid Idx 3 | Loss tensor([0.5282, 0.5313, 0.5100, 0.4740, 0.3856])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 252 in 3.8306710720062256 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4485, 0.4125, 0.4449, 0.4100]) \n",
      "Test Loss tensor([0.4428, 0.4120, 0.4448, 0.4084])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3950, 0.4599, 0.3933, 0.4708, 0.4410]) \n",
      "Test Loss tensor([0.3985, 0.4576, 0.3927, 0.4718, 0.4414])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3884, 0.5322, 0.4761, 0.4089, 0.3540, 0.4547, 0.3926]) \n",
      "Test Loss tensor([0.3860, 0.5325, 0.4764, 0.4089, 0.3577, 0.4588, 0.3928])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5343, 0.3760, 0.5326, 0.5222, 0.4103, 0.3964, 0.3987]) \n",
      "Test Loss tensor([0.5328, 0.3765, 0.5319, 0.5225, 0.4072, 0.3971, 0.3999])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3978, 0.4009, 0.5261, 0.5267, 0.5332, 0.3960, 0.3830, 0.3925]) \n",
      "Test Loss tensor([0.3975, 0.4007, 0.5285, 0.5267, 0.5320, 0.3962, 0.3875, 0.3931])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4556, 0.4002, 0.3983, 0.5178, 0.3906, 0.4962])\n",
      "Valid Idx 3 | Loss tensor([0.5282, 0.5317, 0.5099, 0.4741, 0.3866])\n",
      "\n",
      "************** Batch 256 in 3.878901720046997 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4443, 0.4099, 0.4425, 0.4099]) \n",
      "Test Loss tensor([0.4441, 0.4114, 0.4449, 0.4084])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3987, 0.4560, 0.3944, 0.4703, 0.4395]) \n",
      "Test Loss tensor([0.3999, 0.4579, 0.3926, 0.4711, 0.4410])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3857, 0.5315, 0.4761, 0.4084, 0.3593, 0.4571, 0.3918]) \n",
      "Test Loss tensor([0.3862, 0.5327, 0.4763, 0.4092, 0.3562, 0.4576, 0.3926])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5331, 0.3788, 0.5328, 0.5229, 0.4077, 0.3992, 0.3992]) \n",
      "Test Loss tensor([0.5323, 0.3780, 0.5320, 0.5219, 0.4079, 0.3972, 0.4003])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3956, 0.4007, 0.5265, 0.5271, 0.5320, 0.3962, 0.3888, 0.3922]) \n",
      "Test Loss tensor([0.3978, 0.3997, 0.5281, 0.5260, 0.5322, 0.3962, 0.3883, 0.3930])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4568, 0.3996, 0.3972, 0.5180, 0.3906, 0.4976])\n",
      "Valid Idx 3 | Loss tensor([0.5288, 0.5318, 0.5100, 0.4750, 0.3864])\n",
      "\n",
      "************** Batch 260 in 4.206651210784912 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4410, 0.4136, 0.4392, 0.4056]) \n",
      "Test Loss tensor([0.4438, 0.4106, 0.4427, 0.4089])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4021, 0.4582, 0.3933, 0.4727, 0.4422]) \n",
      "Test Loss tensor([0.3978, 0.4577, 0.3926, 0.4715, 0.4418])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3865, 0.5321, 0.4761, 0.4101, 0.3574, 0.4590, 0.3920]) \n",
      "Test Loss tensor([0.3867, 0.5331, 0.4763, 0.4093, 0.3576, 0.4576, 0.3923])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5323, 0.3761, 0.5313, 0.5231, 0.4074, 0.3959, 0.4018]) \n",
      "Test Loss tensor([0.5330, 0.3765, 0.5327, 0.5221, 0.4076, 0.3972, 0.3993])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3978, 0.3995, 0.5292, 0.5278, 0.5338, 0.3955, 0.3901, 0.3923]) \n",
      "Test Loss tensor([0.3975, 0.4004, 0.5283, 0.5267, 0.5319, 0.3964, 0.3891, 0.3926])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4569, 0.3999, 0.3972, 0.5181, 0.3901, 0.4973])\n",
      "Valid Idx 3 | Loss tensor([0.5284, 0.5319, 0.5091, 0.4740, 0.3866])\n",
      "Gradients: Input 2.658151743162307e-06 | Message 0.0014675938291475177 | Update 0.002016237936913967 | Output 0.054883867502212524\n",
      "\n",
      "************** Batch 264 in 4.4172351360321045 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4437, 0.4091, 0.4385, 0.4070]) \n",
      "Test Loss tensor([0.4431, 0.4102, 0.4444, 0.4070])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3994, 0.4555, 0.3922, 0.4736, 0.4371]) \n",
      "Test Loss tensor([0.3995, 0.4581, 0.3927, 0.4719, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3856, 0.5313, 0.4751, 0.4084, 0.3519, 0.4579, 0.3927]) \n",
      "Test Loss tensor([0.3870, 0.5324, 0.4766, 0.4079, 0.3565, 0.4575, 0.3924])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5336, 0.3768, 0.5317, 0.5230, 0.4110, 0.3971, 0.4001]) \n",
      "Test Loss tensor([0.5327, 0.3765, 0.5325, 0.5224, 0.4075, 0.3971, 0.4000])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3963, 0.4006, 0.5261, 0.5254, 0.5335, 0.3959, 0.3891, 0.3915]) \n",
      "Test Loss tensor([0.3976, 0.4003, 0.5283, 0.5273, 0.5322, 0.3960, 0.3882, 0.3926])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4546, 0.3991, 0.3975, 0.5179, 0.3911, 0.4971])\n",
      "Valid Idx 3 | Loss tensor([0.5280, 0.5323, 0.5105, 0.4751, 0.3851])\n",
      "\n",
      "************** Batch 268 in 4.0262768268585205 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4421, 0.4114, 0.4408, 0.4094]) \n",
      "Test Loss tensor([0.4436, 0.4111, 0.4429, 0.4078])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4010, 0.4593, 0.3927, 0.4720, 0.4408]) \n",
      "Test Loss tensor([0.3984, 0.4578, 0.3924, 0.4719, 0.4409])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3860, 0.5329, 0.4772, 0.4078, 0.3554, 0.4567, 0.3923]) \n",
      "Test Loss tensor([0.3859, 0.5328, 0.4762, 0.4089, 0.3565, 0.4567, 0.3921])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5325, 0.3781, 0.5348, 0.5251, 0.4109, 0.3958, 0.4008]) \n",
      "Test Loss tensor([0.5327, 0.3772, 0.5325, 0.5223, 0.4083, 0.3965, 0.3993])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3985, 0.4013, 0.5270, 0.5259, 0.5325, 0.3961, 0.3897, 0.3932]) \n",
      "Test Loss tensor([0.3978, 0.4004, 0.5282, 0.5272, 0.5321, 0.3960, 0.3881, 0.3926])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4559, 0.3998, 0.3967, 0.5181, 0.3903, 0.4972])\n",
      "Valid Idx 3 | Loss tensor([0.5283, 0.5325, 0.5101, 0.4741, 0.3873])\n",
      "\n",
      "************** Batch 272 in 3.8185877799987793 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4389, 0.4076, 0.4410, 0.4094]) \n",
      "Test Loss tensor([0.4420, 0.4107, 0.4449, 0.4071])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3983, 0.4583, 0.3922, 0.4734, 0.4410]) \n",
      "Test Loss tensor([0.4004, 0.4578, 0.3923, 0.4715, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3886, 0.5328, 0.4752, 0.4078, 0.3619, 0.4567, 0.3912]) \n",
      "Test Loss tensor([0.3866, 0.5325, 0.4766, 0.4081, 0.3575, 0.4584, 0.3921])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5322, 0.3763, 0.5341, 0.5221, 0.4092, 0.3941, 0.3980]) \n",
      "Test Loss tensor([0.5328, 0.3767, 0.5330, 0.5222, 0.4095, 0.3972, 0.3995])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3981, 0.4000, 0.5276, 0.5275, 0.5312, 0.3955, 0.3853, 0.3925]) \n",
      "Test Loss tensor([0.3976, 0.4004, 0.5279, 0.5271, 0.5322, 0.3959, 0.3889, 0.3924])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4560, 0.3991, 0.3968, 0.5184, 0.3907, 0.4973])\n",
      "Valid Idx 3 | Loss tensor([0.5285, 0.5326, 0.5104, 0.4747, 0.3870])\n",
      "\n",
      "************** Batch 276 in 3.774549722671509 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4445, 0.4153, 0.4426, 0.4105]) \n",
      "Test Loss tensor([0.4430, 0.4115, 0.4446, 0.4077])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3982, 0.4566, 0.3921, 0.4722, 0.4433]) \n",
      "Test Loss tensor([0.4001, 0.4578, 0.3924, 0.4716, 0.4414])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3856, 0.5318, 0.4778, 0.4075, 0.3559, 0.4573, 0.3930]) \n",
      "Test Loss tensor([0.3863, 0.5326, 0.4773, 0.4088, 0.3573, 0.4570, 0.3918])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5333, 0.3759, 0.5333, 0.5221, 0.4064, 0.3949, 0.3985]) \n",
      "Test Loss tensor([0.5330, 0.3769, 0.5332, 0.5226, 0.4081, 0.3973, 0.3991])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3981, 0.3999, 0.5268, 0.5272, 0.5312, 0.3956, 0.3894, 0.3922]) \n",
      "Test Loss tensor([0.3975, 0.4001, 0.5280, 0.5276, 0.5325, 0.3959, 0.3878, 0.3922])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4557, 0.3997, 0.3970, 0.5180, 0.3911, 0.4980])\n",
      "Valid Idx 3 | Loss tensor([0.5285, 0.5323, 0.5096, 0.4742, 0.3889])\n",
      "\n",
      "************** Batch 280 in 3.7446889877319336 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4468, 0.4084, 0.4465, 0.4070]) \n",
      "Test Loss tensor([0.4437, 0.4114, 0.4453, 0.4087])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3988, 0.4586, 0.3920, 0.4690, 0.4411]) \n",
      "Test Loss tensor([0.3991, 0.4576, 0.3913, 0.4724, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3846, 0.5332, 0.4764, 0.4085, 0.3566, 0.4559, 0.3920]) \n",
      "Test Loss tensor([0.3865, 0.5334, 0.4772, 0.4089, 0.3574, 0.4579, 0.3918])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5316, 0.3800, 0.5339, 0.5205, 0.4087, 0.3980, 0.4003]) \n",
      "Test Loss tensor([0.5330, 0.3752, 0.5334, 0.5222, 0.4084, 0.3970, 0.3983])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3986, 0.3989, 0.5280, 0.5291, 0.5322, 0.3970, 0.3854, 0.3919]) \n",
      "Test Loss tensor([0.3972, 0.4005, 0.5277, 0.5279, 0.5322, 0.3958, 0.3870, 0.3916])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4562, 0.3994, 0.3965, 0.5185, 0.3902, 0.4978])\n",
      "Valid Idx 3 | Loss tensor([0.5281, 0.5325, 0.5107, 0.4744, 0.3875])\n",
      "Gradients: Input 3.0404082735913107e-06 | Message 0.0023651770316064358 | Update 0.0018607466481626034 | Output 0.01787656918168068\n",
      "\n",
      "************** Batch 284 in 3.7871556282043457 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4394, 0.4070, 0.4461, 0.4074]) \n",
      "Test Loss tensor([0.4445, 0.4113, 0.4440, 0.4087])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3986, 0.4592, 0.3911, 0.4720, 0.4439]) \n",
      "Test Loss tensor([0.3989, 0.4578, 0.3916, 0.4725, 0.4405])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3871, 0.5334, 0.4779, 0.4089, 0.3538, 0.4580, 0.3922]) \n",
      "Test Loss tensor([0.3869, 0.5331, 0.4762, 0.4087, 0.3565, 0.4575, 0.3914])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5329, 0.3744, 0.5325, 0.5230, 0.4095, 0.3988, 0.3996]) \n",
      "Test Loss tensor([0.5333, 0.3760, 0.5336, 0.5226, 0.4087, 0.3973, 0.3990])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3978, 0.3990, 0.5267, 0.5287, 0.5309, 0.3950, 0.3893, 0.3922]) \n",
      "Test Loss tensor([0.3974, 0.3998, 0.5281, 0.5282, 0.5327, 0.3961, 0.3867, 0.3920])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4555, 0.3995, 0.3964, 0.5179, 0.3905, 0.4981])\n",
      "Valid Idx 3 | Loss tensor([0.5284, 0.5330, 0.5104, 0.4746, 0.3860])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 288 in 3.824230909347534 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4412, 0.4112, 0.4435, 0.4086]) \n",
      "Test Loss tensor([0.4419, 0.4100, 0.4449, 0.4079])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4012, 0.4595, 0.3909, 0.4734, 0.4419]) \n",
      "Test Loss tensor([0.4000, 0.4581, 0.3917, 0.4717, 0.4425])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3870, 0.5339, 0.4768, 0.4082, 0.3596, 0.4576, 0.3911]) \n",
      "Test Loss tensor([0.3860, 0.5332, 0.4770, 0.4089, 0.3571, 0.4575, 0.3911])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5331, 0.3785, 0.5335, 0.5228, 0.4086, 0.3970, 0.3999]) \n",
      "Test Loss tensor([0.5331, 0.3770, 0.5338, 0.5223, 0.4079, 0.3976, 0.3991])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3967, 0.3999, 0.5268, 0.5273, 0.5324, 0.3968, 0.3857, 0.3912]) \n",
      "Test Loss tensor([0.3973, 0.4003, 0.5283, 0.5275, 0.5318, 0.3960, 0.3887, 0.3912])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4553, 0.3995, 0.3960, 0.5185, 0.3908, 0.4973])\n",
      "Valid Idx 3 | Loss tensor([0.5285, 0.5331, 0.5101, 0.4748, 0.3866])\n",
      "\n",
      "************** Batch 292 in 4.141778945922852 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4409, 0.4136, 0.4479, 0.4095]) \n",
      "Test Loss tensor([0.4423, 0.4109, 0.4452, 0.4069])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3997, 0.4573, 0.3899, 0.4745, 0.4422]) \n",
      "Test Loss tensor([0.4005, 0.4568, 0.3914, 0.4707, 0.4420])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3842, 0.5328, 0.4789, 0.4083, 0.3580, 0.4567, 0.3919]) \n",
      "Test Loss tensor([0.3865, 0.5326, 0.4767, 0.4088, 0.3573, 0.4571, 0.3912])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5338, 0.3798, 0.5342, 0.5228, 0.4076, 0.3959, 0.3985]) \n",
      "Test Loss tensor([0.5328, 0.3770, 0.5340, 0.5220, 0.4081, 0.3979, 0.3986])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3980, 0.4013, 0.5277, 0.5282, 0.5324, 0.3966, 0.3872, 0.3924]) \n",
      "Test Loss tensor([0.3969, 0.3999, 0.5282, 0.5287, 0.5317, 0.3956, 0.3875, 0.3915])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4551, 0.3990, 0.3962, 0.5180, 0.3903, 0.4978])\n",
      "Valid Idx 3 | Loss tensor([0.5285, 0.5331, 0.5103, 0.4737, 0.3874])\n",
      "\n",
      "************** Batch 296 in 4.149668216705322 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4461, 0.4100, 0.4442, 0.4071]) \n",
      "Test Loss tensor([0.4430, 0.4108, 0.4440, 0.4075])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3975, 0.4586, 0.3909, 0.4736, 0.4408]) \n",
      "Test Loss tensor([0.3988, 0.4587, 0.3910, 0.4724, 0.4422])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3870, 0.5323, 0.4743, 0.4084, 0.3573, 0.4563, 0.3902]) \n",
      "Test Loss tensor([0.3865, 0.5330, 0.4767, 0.4094, 0.3572, 0.4580, 0.3909])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5324, 0.3759, 0.5344, 0.5218, 0.4084, 0.3948, 0.3973]) \n",
      "Test Loss tensor([0.5329, 0.3761, 0.5347, 0.5226, 0.4084, 0.3964, 0.3989])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3975, 0.4005, 0.5279, 0.5277, 0.5309, 0.3960, 0.3925, 0.3909]) \n",
      "Test Loss tensor([0.3974, 0.4003, 0.5275, 0.5287, 0.5327, 0.3961, 0.3873, 0.3911])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4550, 0.3994, 0.3963, 0.5178, 0.3903, 0.4976])\n",
      "Valid Idx 3 | Loss tensor([0.5285, 0.5338, 0.5092, 0.4736, 0.3884])\n",
      "\n",
      "************** Batch 300 in 3.850240707397461 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4438, 0.4099, 0.4415, 0.4085]) \n",
      "Test Loss tensor([0.4427, 0.4117, 0.4434, 0.4084])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3952, 0.4591, 0.3928, 0.4700, 0.4428]) \n",
      "Test Loss tensor([0.3985, 0.4576, 0.3908, 0.4707, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3861, 0.5337, 0.4786, 0.4058, 0.3539, 0.4563, 0.3911]) \n",
      "Test Loss tensor([0.3860, 0.5327, 0.4764, 0.4085, 0.3559, 0.4580, 0.3906])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5327, 0.3774, 0.5340, 0.5235, 0.4050, 0.3982, 0.3993]) \n",
      "Test Loss tensor([0.5331, 0.3772, 0.5339, 0.5217, 0.4088, 0.3981, 0.3985])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3985, 0.3990, 0.5271, 0.5298, 0.5325, 0.3951, 0.3887, 0.3929]) \n",
      "Test Loss tensor([0.3973, 0.4000, 0.5282, 0.5285, 0.5320, 0.3963, 0.3889, 0.3909])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4557, 0.3998, 0.3963, 0.5179, 0.3898, 0.4973])\n",
      "Valid Idx 3 | Loss tensor([0.5281, 0.5334, 0.5095, 0.4736, 0.3884])\n",
      "Gradients: Input 3.394763098185649e-06 | Message 0.0019474391592666507 | Update 0.001827944885008037 | Output 0.01707650162279606\n",
      "\n",
      "************** Batch 304 in 3.814074754714966 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4403, 0.4118, 0.4426, 0.4085]) \n",
      "Test Loss tensor([0.4424, 0.4109, 0.4445, 0.4071])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3991, 0.4583, 0.3918, 0.4720, 0.4425]) \n",
      "Test Loss tensor([0.3989, 0.4580, 0.3909, 0.4723, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3876, 0.5337, 0.4783, 0.4105, 0.3531, 0.4537, 0.3907]) \n",
      "Test Loss tensor([0.3858, 0.5324, 0.4761, 0.4096, 0.3561, 0.4573, 0.3909])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5335, 0.3768, 0.5340, 0.5217, 0.4092, 0.3961, 0.3978]) \n",
      "Test Loss tensor([0.5329, 0.3763, 0.5341, 0.5223, 0.4094, 0.3974, 0.3983])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3979, 0.3993, 0.5287, 0.5310, 0.5323, 0.3978, 0.3873, 0.3896]) \n",
      "Test Loss tensor([0.3970, 0.3999, 0.5276, 0.5284, 0.5317, 0.3963, 0.3874, 0.3908])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.3996, 0.3963, 0.5184, 0.3902, 0.4981])\n",
      "Valid Idx 3 | Loss tensor([0.5278, 0.5338, 0.5102, 0.4740, 0.3892])\n",
      "\n",
      "************** Batch 308 in 3.7763302326202393 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4432, 0.4123, 0.4430, 0.4062]) \n",
      "Test Loss tensor([0.4444, 0.4103, 0.4435, 0.4077])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4015, 0.4577, 0.3912, 0.4735, 0.4418]) \n",
      "Test Loss tensor([0.3997, 0.4580, 0.3907, 0.4713, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3857, 0.5326, 0.4791, 0.4064, 0.3558, 0.4573, 0.3910]) \n",
      "Test Loss tensor([0.3871, 0.5320, 0.4774, 0.4077, 0.3573, 0.4575, 0.3905])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5336, 0.3765, 0.5331, 0.5231, 0.4077, 0.3983, 0.3980]) \n",
      "Test Loss tensor([0.5320, 0.3767, 0.5342, 0.5224, 0.4082, 0.3981, 0.3978])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3996, 0.4021, 0.5255, 0.5281, 0.5321, 0.3959, 0.3899, 0.3918]) \n",
      "Test Loss tensor([0.3975, 0.4006, 0.5278, 0.5292, 0.5320, 0.3965, 0.3869, 0.3909])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4554, 0.3999, 0.3957, 0.5176, 0.3912, 0.4977])\n",
      "Valid Idx 3 | Loss tensor([0.5281, 0.5338, 0.5097, 0.4745, 0.3870])\n",
      "\n",
      "************** Batch 312 in 4.1503260135650635 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4440, 0.4097, 0.4412, 0.4073]) \n",
      "Test Loss tensor([0.4430, 0.4102, 0.4429, 0.4070])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3977, 0.4579, 0.3909, 0.4713, 0.4395]) \n",
      "Test Loss tensor([0.3985, 0.4580, 0.3906, 0.4719, 0.4418])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3866, 0.5324, 0.4787, 0.4044, 0.3595, 0.4579, 0.3902]) \n",
      "Test Loss tensor([0.3867, 0.5323, 0.4762, 0.4080, 0.3568, 0.4580, 0.3901])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5304, 0.3784, 0.5347, 0.5207, 0.4073, 0.3958, 0.3974]) \n",
      "Test Loss tensor([0.5323, 0.3767, 0.5345, 0.5225, 0.4085, 0.3975, 0.3979])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3981, 0.4023, 0.5242, 0.5291, 0.5319, 0.3962, 0.3840, 0.3909]) \n",
      "Test Loss tensor([0.3975, 0.4005, 0.5274, 0.5285, 0.5316, 0.3967, 0.3878, 0.3906])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4564, 0.4002, 0.3957, 0.5179, 0.3909, 0.4993])\n",
      "Valid Idx 3 | Loss tensor([0.5282, 0.5342, 0.5100, 0.4743, 0.3885])\n",
      "\n",
      "************** Batch 316 in 4.305161952972412 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4452, 0.4103, 0.4397, 0.4062]) \n",
      "Test Loss tensor([0.4431, 0.4102, 0.4439, 0.4077])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3946, 0.4580, 0.3912, 0.4716, 0.4427]) \n",
      "Test Loss tensor([0.3992, 0.4575, 0.3903, 0.4724, 0.4416])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3877, 0.5316, 0.4763, 0.4072, 0.3562, 0.4567, 0.3902]) \n",
      "Test Loss tensor([0.3867, 0.5321, 0.4767, 0.4085, 0.3576, 0.4582, 0.3898])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5339, 0.3770, 0.5371, 0.5240, 0.4100, 0.3975, 0.3972]) \n",
      "Test Loss tensor([0.5326, 0.3771, 0.5349, 0.5222, 0.4078, 0.3975, 0.3978])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3981, 0.4007, 0.5268, 0.5304, 0.5315, 0.3970, 0.3907, 0.3905]) \n",
      "Test Loss tensor([0.3975, 0.4010, 0.5275, 0.5288, 0.5316, 0.3965, 0.3869, 0.3907])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4560, 0.4000, 0.3954, 0.5178, 0.3909, 0.4988])\n",
      "Valid Idx 3 | Loss tensor([0.5283, 0.5340, 0.5093, 0.4739, 0.3881])\n",
      "\n",
      "************** Batch 320 in 4.353636980056763 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4460, 0.4081, 0.4418, 0.4102]) \n",
      "Test Loss tensor([0.4425, 0.4100, 0.4449, 0.4059])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4005, 0.4583, 0.3898, 0.4735, 0.4431]) \n",
      "Test Loss tensor([0.4000, 0.4574, 0.3905, 0.4711, 0.4413])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3855, 0.5319, 0.4792, 0.4099, 0.3531, 0.4564, 0.3905]) \n",
      "Test Loss tensor([0.3863, 0.5321, 0.4762, 0.4089, 0.3564, 0.4579, 0.3899])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5330, 0.3763, 0.5359, 0.5216, 0.4067, 0.3971, 0.3992]) \n",
      "Test Loss tensor([0.5325, 0.3776, 0.5348, 0.5219, 0.4076, 0.3983, 0.3978])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3987, 0.4002, 0.5254, 0.5310, 0.5316, 0.3958, 0.3894, 0.3910]) \n",
      "Test Loss tensor([0.3979, 0.4007, 0.5278, 0.5294, 0.5318, 0.3967, 0.3885, 0.3900])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4550, 0.4002, 0.3955, 0.5181, 0.3904, 0.4983])\n",
      "Valid Idx 3 | Loss tensor([0.5280, 0.5343, 0.5098, 0.4739, 0.3886])\n",
      "Gradients: Input 4.532622824626742e-06 | Message 0.003064053598791361 | Update 0.0026256623677909374 | Output 0.036113616079092026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 324 in 4.100500106811523 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4401, 0.4078, 0.4428, 0.4099]) \n",
      "Test Loss tensor([0.4424, 0.4107, 0.4441, 0.4069])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4000, 0.4564, 0.3890, 0.4722, 0.4421]) \n",
      "Test Loss tensor([0.3990, 0.4578, 0.3896, 0.4707, 0.4414])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3882, 0.5303, 0.4776, 0.4074, 0.3607, 0.4608, 0.3897]) \n",
      "Test Loss tensor([0.3870, 0.5318, 0.4763, 0.4076, 0.3567, 0.4574, 0.3900])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5309, 0.3732, 0.5356, 0.5199, 0.4088, 0.3995, 0.3984]) \n",
      "Test Loss tensor([0.5324, 0.3777, 0.5352, 0.5215, 0.4082, 0.3981, 0.3976])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3973, 0.4000, 0.5263, 0.5307, 0.5320, 0.3976, 0.3868, 0.3886]) \n",
      "Test Loss tensor([0.3976, 0.4011, 0.5271, 0.5294, 0.5314, 0.3969, 0.3878, 0.3900])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4559, 0.3998, 0.3951, 0.5173, 0.3915, 0.4985])\n",
      "Valid Idx 3 | Loss tensor([0.5278, 0.5347, 0.5092, 0.4734, 0.3867])\n",
      "\n",
      "************** Batch 328 in 4.628107070922852 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4412, 0.4074, 0.4432, 0.4089]) \n",
      "Test Loss tensor([0.4431, 0.4105, 0.4437, 0.4084])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4018, 0.4586, 0.3889, 0.4748, 0.4430]) \n",
      "Test Loss tensor([0.3993, 0.4571, 0.3899, 0.4716, 0.4409])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3865, 0.5326, 0.4783, 0.4087, 0.3570, 0.4584, 0.3897]) \n",
      "Test Loss tensor([0.3878, 0.5317, 0.4758, 0.4086, 0.3574, 0.4571, 0.3900])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5329, 0.3768, 0.5342, 0.5194, 0.4079, 0.3972, 0.3978]) \n",
      "Test Loss tensor([0.5324, 0.3767, 0.5363, 0.5212, 0.4076, 0.3965, 0.3973])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3966, 0.4004, 0.5284, 0.5311, 0.5323, 0.3962, 0.3902, 0.3905]) \n",
      "Test Loss tensor([0.3979, 0.4010, 0.5267, 0.5295, 0.5311, 0.3971, 0.3881, 0.3898])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4564, 0.3998, 0.3954, 0.5175, 0.3912, 0.4990])\n",
      "Valid Idx 3 | Loss tensor([0.5281, 0.5345, 0.5097, 0.4748, 0.3875])\n",
      "\n",
      "************** Batch 332 in 4.543354272842407 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4433, 0.4074, 0.4423, 0.4043]) \n",
      "Test Loss tensor([0.4430, 0.4105, 0.4436, 0.4070])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3971, 0.4581, 0.3902, 0.4700, 0.4409]) \n",
      "Test Loss tensor([0.3989, 0.4581, 0.3896, 0.4716, 0.4418])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3849, 0.5325, 0.4760, 0.4090, 0.3582, 0.4572, 0.3901]) \n",
      "Test Loss tensor([0.3874, 0.5317, 0.4761, 0.4089, 0.3565, 0.4570, 0.3897])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5312, 0.3798, 0.5352, 0.5205, 0.4070, 0.3996, 0.3986]) \n",
      "Test Loss tensor([0.5324, 0.3770, 0.5358, 0.5214, 0.4077, 0.3979, 0.3972])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3984, 0.4027, 0.5276, 0.5287, 0.5324, 0.3977, 0.3856, 0.3900]) \n",
      "Test Loss tensor([0.3981, 0.4007, 0.5264, 0.5298, 0.5314, 0.3973, 0.3879, 0.3897])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4548, 0.4000, 0.3947, 0.5169, 0.3912, 0.4986])\n",
      "Valid Idx 3 | Loss tensor([0.5277, 0.5344, 0.5091, 0.4736, 0.3877])\n",
      "\n",
      "************** Batch 336 in 4.3857362270355225 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4436, 0.4128, 0.4459, 0.4112]) \n",
      "Test Loss tensor([0.4435, 0.4095, 0.4438, 0.4060])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4005, 0.4589, 0.3898, 0.4704, 0.4419]) \n",
      "Test Loss tensor([0.3987, 0.4576, 0.3892, 0.4724, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3854, 0.5321, 0.4793, 0.4095, 0.3584, 0.4579, 0.3907]) \n",
      "Test Loss tensor([0.3879, 0.5314, 0.4759, 0.4080, 0.3568, 0.4566, 0.3896])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5333, 0.3781, 0.5366, 0.5223, 0.4109, 0.4007, 0.3961]) \n",
      "Test Loss tensor([0.5323, 0.3767, 0.5356, 0.5212, 0.4090, 0.3978, 0.3969])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3970, 0.3992, 0.5266, 0.5305, 0.5311, 0.3968, 0.3881, 0.3902]) \n",
      "Test Loss tensor([0.3981, 0.4006, 0.5267, 0.5299, 0.5308, 0.3972, 0.3878, 0.3896])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4558, 0.4004, 0.3947, 0.5173, 0.3914, 0.4997])\n",
      "Valid Idx 3 | Loss tensor([0.5275, 0.5358, 0.5093, 0.4741, 0.3876])\n",
      "\n",
      "************** Batch 340 in 3.940169095993042 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4410, 0.4085, 0.4456, 0.4040]) \n",
      "Test Loss tensor([0.4431, 0.4100, 0.4439, 0.4065])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3987, 0.4575, 0.3899, 0.4690, 0.4384]) \n",
      "Test Loss tensor([0.3994, 0.4577, 0.3893, 0.4708, 0.4413])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3852, 0.5319, 0.4774, 0.4085, 0.3602, 0.4559, 0.3893]) \n",
      "Test Loss tensor([0.3868, 0.5314, 0.4747, 0.4085, 0.3573, 0.4573, 0.3889])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5334, 0.3754, 0.5365, 0.5227, 0.4077, 0.4013, 0.3979]) \n",
      "Test Loss tensor([0.5325, 0.3780, 0.5361, 0.5211, 0.4083, 0.3988, 0.3967])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3982, 0.3987, 0.5265, 0.5287, 0.5305, 0.3981, 0.3900, 0.3897]) \n",
      "Test Loss tensor([0.3983, 0.4011, 0.5264, 0.5299, 0.5309, 0.3971, 0.3884, 0.3893])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4564, 0.4008, 0.3946, 0.5171, 0.3907, 0.4996])\n",
      "Valid Idx 3 | Loss tensor([0.5276, 0.5356, 0.5098, 0.4743, 0.3886])\n",
      "Gradients: Input 5.403341219789581e-06 | Message 0.0027398266829550266 | Update 0.002430117689073086 | Output 0.040225666016340256\n",
      "\n",
      "************** Batch 344 in 3.863774538040161 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4422, 0.4108, 0.4383, 0.4062]) \n",
      "Test Loss tensor([0.4412, 0.4101, 0.4442, 0.4072])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3997, 0.4564, 0.3884, 0.4711, 0.4409]) \n",
      "Test Loss tensor([0.4003, 0.4575, 0.3893, 0.4720, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3874, 0.5309, 0.4771, 0.4074, 0.3572, 0.4562, 0.3887]) \n",
      "Test Loss tensor([0.3871, 0.5316, 0.4754, 0.4080, 0.3566, 0.4576, 0.3888])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5331, 0.3788, 0.5360, 0.5232, 0.4102, 0.3992, 0.3989]) \n",
      "Test Loss tensor([0.5314, 0.3786, 0.5362, 0.5211, 0.4088, 0.3986, 0.3968])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3980, 0.4030, 0.5246, 0.5290, 0.5316, 0.3981, 0.3903, 0.3890]) \n",
      "Test Loss tensor([0.3982, 0.4010, 0.5265, 0.5303, 0.5309, 0.3973, 0.3870, 0.3894])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4561, 0.4009, 0.3941, 0.5178, 0.3904, 0.4994])\n",
      "Valid Idx 3 | Loss tensor([0.5277, 0.5363, 0.5092, 0.4742, 0.3883])\n",
      "\n",
      "************** Batch 348 in 3.9100704193115234 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4448, 0.4109, 0.4452, 0.4062]) \n",
      "Test Loss tensor([0.4421, 0.4096, 0.4434, 0.4064])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4012, 0.4550, 0.3895, 0.4712, 0.4402]) \n",
      "Test Loss tensor([0.3983, 0.4568, 0.3887, 0.4719, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3864, 0.5308, 0.4759, 0.4059, 0.3557, 0.4586, 0.3898]) \n",
      "Test Loss tensor([0.3864, 0.5314, 0.4745, 0.4088, 0.3557, 0.4573, 0.3881])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5336, 0.3774, 0.5368, 0.5217, 0.4093, 0.3985, 0.3966]) \n",
      "Test Loss tensor([0.5321, 0.3780, 0.5369, 0.5208, 0.4084, 0.3973, 0.3964])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3981, 0.4009, 0.5256, 0.5291, 0.5299, 0.3970, 0.3873, 0.3885]) \n",
      "Test Loss tensor([0.3978, 0.4016, 0.5267, 0.5302, 0.5307, 0.3973, 0.3883, 0.3887])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4560, 0.4005, 0.3944, 0.5169, 0.3915, 0.5000])\n",
      "Valid Idx 3 | Loss tensor([0.5272, 0.5364, 0.5090, 0.4738, 0.3893])\n",
      "\n",
      "************** Batch 352 in 3.93516206741333 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4451, 0.4111, 0.4388, 0.4066]) \n",
      "Test Loss tensor([0.4419, 0.4099, 0.4430, 0.4063])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4007, 0.4577, 0.3886, 0.4710, 0.4436]) \n",
      "Test Loss tensor([0.3990, 0.4576, 0.3886, 0.4724, 0.4421])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3868, 0.5325, 0.4744, 0.4083, 0.3571, 0.4588, 0.3889]) \n",
      "Test Loss tensor([0.3873, 0.5312, 0.4752, 0.4078, 0.3574, 0.4576, 0.3881])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5306, 0.3748, 0.5374, 0.5226, 0.4132, 0.3982, 0.3953]) \n",
      "Test Loss tensor([0.5319, 0.3781, 0.5368, 0.5207, 0.4086, 0.3984, 0.3960])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3991, 0.4020, 0.5266, 0.5309, 0.5309, 0.3972, 0.3879, 0.3890]) \n",
      "Test Loss tensor([0.3986, 0.4017, 0.5261, 0.5308, 0.5306, 0.3972, 0.3876, 0.3888])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4553, 0.4013, 0.3943, 0.5164, 0.3917, 0.4998])\n",
      "Valid Idx 3 | Loss tensor([0.5269, 0.5356, 0.5081, 0.4740, 0.3886])\n",
      "\n",
      "************** Batch 356 in 3.7824313640594482 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4435, 0.4109, 0.4423, 0.4088]) \n",
      "Test Loss tensor([0.4414, 0.4100, 0.4424, 0.4073])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3986, 0.4588, 0.3887, 0.4722, 0.4437]) \n",
      "Test Loss tensor([0.3993, 0.4575, 0.3881, 0.4721, 0.4410])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3856, 0.5298, 0.4782, 0.4067, 0.3533, 0.4562, 0.3885]) \n",
      "Test Loss tensor([0.3876, 0.5308, 0.4757, 0.4078, 0.3578, 0.4575, 0.3883])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5321, 0.3780, 0.5366, 0.5193, 0.4075, 0.3980, 0.3969]) \n",
      "Test Loss tensor([0.5313, 0.3780, 0.5367, 0.5205, 0.4083, 0.3988, 0.3960])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3991, 0.4031, 0.5255, 0.5313, 0.5299, 0.3969, 0.3856, 0.3868]) \n",
      "Test Loss tensor([0.3986, 0.4015, 0.5263, 0.5313, 0.5308, 0.3975, 0.3881, 0.3883])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4555, 0.4008, 0.3934, 0.5172, 0.3906, 0.5002])\n",
      "Valid Idx 3 | Loss tensor([0.5269, 0.5370, 0.5087, 0.4743, 0.3866])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 360 in 3.9559988975524902 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4390, 0.4088, 0.4439, 0.4081]) \n",
      "Test Loss tensor([0.4413, 0.4101, 0.4429, 0.4059])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3986, 0.4595, 0.3883, 0.4679, 0.4405]) \n",
      "Test Loss tensor([0.3995, 0.4581, 0.3878, 0.4716, 0.4423])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3869, 0.5302, 0.4774, 0.4106, 0.3598, 0.4571, 0.3876]) \n",
      "Test Loss tensor([0.3876, 0.5307, 0.4766, 0.4073, 0.3574, 0.4573, 0.3875])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5315, 0.3753, 0.5378, 0.5221, 0.4085, 0.4006, 0.3960]) \n",
      "Test Loss tensor([0.5319, 0.3786, 0.5380, 0.5209, 0.4090, 0.3985, 0.3955])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3985, 0.4012, 0.5264, 0.5327, 0.5300, 0.3982, 0.3874, 0.3879]) \n",
      "Test Loss tensor([0.3983, 0.4019, 0.5265, 0.5318, 0.5300, 0.3973, 0.3870, 0.3879])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4553, 0.4014, 0.3936, 0.5166, 0.3913, 0.4997])\n",
      "Valid Idx 3 | Loss tensor([0.5280, 0.5376, 0.5083, 0.4728, 0.3895])\n",
      "Gradients: Input 7.171631295932457e-06 | Message 0.004198803566396236 | Update 0.004201984964311123 | Output 0.046197544783353806\n",
      "\n",
      "************** Batch 364 in 4.369916200637817 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4440, 0.4093, 0.4426, 0.4065]) \n",
      "Test Loss tensor([0.4422, 0.4102, 0.4426, 0.4075])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4011, 0.4562, 0.3883, 0.4697, 0.4397]) \n",
      "Test Loss tensor([0.3999, 0.4578, 0.3875, 0.4708, 0.4425])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3885, 0.5294, 0.4765, 0.4043, 0.3557, 0.4571, 0.3876]) \n",
      "Test Loss tensor([0.3879, 0.5306, 0.4754, 0.4071, 0.3575, 0.4577, 0.3871])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5318, 0.3745, 0.5369, 0.5206, 0.4090, 0.3984, 0.3949]) \n",
      "Test Loss tensor([0.5314, 0.3785, 0.5378, 0.5209, 0.4101, 0.3986, 0.3950])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3973, 0.3995, 0.5239, 0.5323, 0.5303, 0.3971, 0.3876, 0.3887]) \n",
      "Test Loss tensor([0.3987, 0.4019, 0.5256, 0.5317, 0.5301, 0.3982, 0.3879, 0.3875])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4559, 0.4008, 0.3930, 0.5165, 0.3918, 0.5005])\n",
      "Valid Idx 3 | Loss tensor([0.5270, 0.5373, 0.5085, 0.4733, 0.3884])\n",
      "\n",
      "************** Batch 368 in 4.395743131637573 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4435, 0.4085, 0.4426, 0.4060]) \n",
      "Test Loss tensor([0.4413, 0.4096, 0.4427, 0.4059])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3989, 0.4600, 0.3862, 0.4725, 0.4433]) \n",
      "Test Loss tensor([0.4003, 0.4578, 0.3874, 0.4707, 0.4423])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3861, 0.5317, 0.4767, 0.4090, 0.3566, 0.4578, 0.3867]) \n",
      "Test Loss tensor([0.3882, 0.5304, 0.4750, 0.4081, 0.3580, 0.4577, 0.3867])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5313, 0.3776, 0.5379, 0.5210, 0.4091, 0.3988, 0.3950]) \n",
      "Test Loss tensor([0.5312, 0.3784, 0.5381, 0.5199, 0.4090, 0.3986, 0.3953])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3982, 0.4024, 0.5257, 0.5346, 0.5297, 0.3986, 0.3879, 0.3885]) \n",
      "Test Loss tensor([0.3988, 0.4022, 0.5252, 0.5323, 0.5298, 0.3979, 0.3874, 0.3869])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4555, 0.4013, 0.3936, 0.5163, 0.3908, 0.4999])\n",
      "Valid Idx 3 | Loss tensor([0.5268, 0.5375, 0.5088, 0.4735, 0.3893])\n",
      "\n",
      "************** Batch 372 in 3.9108219146728516 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4419, 0.4106, 0.4430, 0.4067]) \n",
      "Test Loss tensor([0.4423, 0.4098, 0.4415, 0.4072])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3993, 0.4583, 0.3892, 0.4756, 0.4473]) \n",
      "Test Loss tensor([0.3999, 0.4581, 0.3869, 0.4705, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3881, 0.5294, 0.4790, 0.4073, 0.3557, 0.4564, 0.3863]) \n",
      "Test Loss tensor([0.3876, 0.5306, 0.4755, 0.4074, 0.3568, 0.4576, 0.3867])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5321, 0.3776, 0.5399, 0.5229, 0.4082, 0.3985, 0.3947]) \n",
      "Test Loss tensor([0.5314, 0.3779, 0.5386, 0.5202, 0.4089, 0.3992, 0.3945])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3984, 0.4016, 0.5261, 0.5331, 0.5297, 0.3984, 0.3901, 0.3879]) \n",
      "Test Loss tensor([0.3989, 0.4024, 0.5260, 0.5327, 0.5296, 0.3981, 0.3886, 0.3872])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4553, 0.4016, 0.3924, 0.5154, 0.3922, 0.5004])\n",
      "Valid Idx 3 | Loss tensor([0.5259, 0.5376, 0.5089, 0.4742, 0.3878])\n",
      "\n",
      "************** Batch 376 in 3.966644763946533 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4391, 0.4114, 0.4460, 0.4030]) \n",
      "Test Loss tensor([0.4427, 0.4096, 0.4415, 0.4061])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3972, 0.4581, 0.3883, 0.4717, 0.4421]) \n",
      "Test Loss tensor([0.3995, 0.4577, 0.3865, 0.4714, 0.4412])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3875, 0.5290, 0.4748, 0.4055, 0.3558, 0.4569, 0.3876]) \n",
      "Test Loss tensor([0.3876, 0.5299, 0.4743, 0.4079, 0.3569, 0.4570, 0.3863])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5312, 0.3759, 0.5385, 0.5214, 0.4048, 0.3989, 0.3950]) \n",
      "Test Loss tensor([0.5312, 0.3776, 0.5390, 0.5202, 0.4087, 0.3982, 0.3938])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4006, 0.4018, 0.5244, 0.5323, 0.5287, 0.3998, 0.3920, 0.3857]) \n",
      "Test Loss tensor([0.3993, 0.4026, 0.5252, 0.5325, 0.5294, 0.3984, 0.3875, 0.3868])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4559, 0.4011, 0.3926, 0.5162, 0.3922, 0.5011])\n",
      "Valid Idx 3 | Loss tensor([0.5264, 0.5389, 0.5079, 0.4740, 0.3879])\n",
      "\n",
      "************** Batch 380 in 3.855804681777954 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4443, 0.4119, 0.4413, 0.4093]) \n",
      "Test Loss tensor([0.4433, 0.4097, 0.4409, 0.4068])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3989, 0.4565, 0.3871, 0.4707, 0.4390]) \n",
      "Test Loss tensor([0.4007, 0.4580, 0.3865, 0.4709, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3856, 0.5300, 0.4717, 0.4078, 0.3572, 0.4572, 0.3868]) \n",
      "Test Loss tensor([0.3870, 0.5296, 0.4750, 0.4069, 0.3565, 0.4575, 0.3861])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5317, 0.3775, 0.5371, 0.5211, 0.4064, 0.4018, 0.3955]) \n",
      "Test Loss tensor([0.5310, 0.3770, 0.5393, 0.5196, 0.4090, 0.3987, 0.3938])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3994, 0.4014, 0.5237, 0.5325, 0.5290, 0.3969, 0.3883, 0.3863]) \n",
      "Test Loss tensor([0.3997, 0.4020, 0.5246, 0.5335, 0.5295, 0.3993, 0.3880, 0.3864])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.4021, 0.3921, 0.5160, 0.3915, 0.5016])\n",
      "Valid Idx 3 | Loss tensor([0.5256, 0.5385, 0.5081, 0.4734, 0.3901])\n",
      "Gradients: Input 9.92042896541534e-06 | Message 0.004169309977442026 | Update 0.008141383528709412 | Output 0.05723143368959427\n",
      "\n",
      "************** Batch 384 in 3.985426664352417 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4385, 0.4122, 0.4472, 0.4057]) \n",
      "Test Loss tensor([0.4421, 0.4099, 0.4415, 0.4065])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3992, 0.4571, 0.3860, 0.4734, 0.4435]) \n",
      "Test Loss tensor([0.3998, 0.4579, 0.3856, 0.4708, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3844, 0.5292, 0.4775, 0.4044, 0.3559, 0.4561, 0.3858]) \n",
      "Test Loss tensor([0.3877, 0.5294, 0.4742, 0.4076, 0.3575, 0.4575, 0.3856])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5319, 0.3785, 0.5398, 0.5199, 0.4106, 0.3961, 0.3945]) \n",
      "Test Loss tensor([0.5307, 0.3786, 0.5399, 0.5193, 0.4081, 0.3986, 0.3932])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4000, 0.4019, 0.5274, 0.5340, 0.5298, 0.3998, 0.3877, 0.3873]) \n",
      "Test Loss tensor([0.3990, 0.4031, 0.5242, 0.5340, 0.5288, 0.3987, 0.3872, 0.3857])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4554, 0.4015, 0.3916, 0.5155, 0.3926, 0.5020])\n",
      "Valid Idx 3 | Loss tensor([0.5263, 0.5393, 0.5078, 0.4724, 0.3907])\n",
      "\n",
      "************** Batch 388 in 3.844686985015869 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4419, 0.4093, 0.4402, 0.4077]) \n",
      "Test Loss tensor([0.4427, 0.4086, 0.4423, 0.4058])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3960, 0.4562, 0.3859, 0.4715, 0.4415]) \n",
      "Test Loss tensor([0.3994, 0.4578, 0.3857, 0.4701, 0.4422])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3879, 0.5289, 0.4748, 0.4073, 0.3629, 0.4589, 0.3843]) \n",
      "Test Loss tensor([0.3882, 0.5294, 0.4732, 0.4066, 0.3570, 0.4580, 0.3856])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5305, 0.3783, 0.5405, 0.5184, 0.4082, 0.3985, 0.3947]) \n",
      "Test Loss tensor([0.5311, 0.3784, 0.5400, 0.5190, 0.4086, 0.3998, 0.3929])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3986, 0.4037, 0.5252, 0.5333, 0.5285, 0.3983, 0.3900, 0.3861]) \n",
      "Test Loss tensor([0.3995, 0.4030, 0.5245, 0.5337, 0.5287, 0.3990, 0.3874, 0.3857])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4560, 0.4023, 0.3920, 0.5149, 0.3924, 0.5018])\n",
      "Valid Idx 3 | Loss tensor([0.5260, 0.5394, 0.5072, 0.4723, 0.3917])\n",
      "\n",
      "************** Batch 392 in 3.968660354614258 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4440, 0.4103, 0.4409, 0.4074]) \n",
      "Test Loss tensor([0.4427, 0.4087, 0.4410, 0.4061])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3985, 0.4594, 0.3856, 0.4726, 0.4417]) \n",
      "Test Loss tensor([0.3998, 0.4572, 0.3857, 0.4708, 0.4413])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3879, 0.5291, 0.4754, 0.4032, 0.3568, 0.4573, 0.3851]) \n",
      "Test Loss tensor([0.3879, 0.5288, 0.4744, 0.4060, 0.3573, 0.4578, 0.3849])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5300, 0.3800, 0.5403, 0.5200, 0.4050, 0.3983, 0.3938]) \n",
      "Test Loss tensor([0.5305, 0.3779, 0.5408, 0.5189, 0.4094, 0.3992, 0.3930])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4003, 0.4026, 0.5239, 0.5330, 0.5291, 0.3985, 0.3832, 0.3859]) \n",
      "Test Loss tensor([0.3995, 0.4037, 0.5238, 0.5341, 0.5286, 0.3994, 0.3877, 0.3852])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4547, 0.4024, 0.3909, 0.5145, 0.3919, 0.5012])\n",
      "Valid Idx 3 | Loss tensor([0.5258, 0.5403, 0.5068, 0.4728, 0.3902])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 396 in 3.9033148288726807 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4406, 0.4118, 0.4469, 0.4016]) \n",
      "Test Loss tensor([0.4408, 0.4088, 0.4402, 0.4048])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3985, 0.4601, 0.3847, 0.4733, 0.4408]) \n",
      "Test Loss tensor([0.3982, 0.4576, 0.3848, 0.4705, 0.4422])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3885, 0.5290, 0.4785, 0.4059, 0.3596, 0.4570, 0.3856]) \n",
      "Test Loss tensor([0.3889, 0.5283, 0.4739, 0.4060, 0.3573, 0.4576, 0.3842])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5301, 0.3771, 0.5395, 0.5202, 0.4046, 0.4004, 0.3950]) \n",
      "Test Loss tensor([0.5302, 0.3786, 0.5409, 0.5180, 0.4080, 0.4000, 0.3930])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3994, 0.4024, 0.5227, 0.5336, 0.5299, 0.3999, 0.3937, 0.3860]) \n",
      "Test Loss tensor([0.3997, 0.4034, 0.5241, 0.5351, 0.5283, 0.3999, 0.3867, 0.3851])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4553, 0.4030, 0.3913, 0.5152, 0.3925, 0.5021])\n",
      "Valid Idx 3 | Loss tensor([0.5254, 0.5400, 0.5060, 0.4726, 0.3900])\n",
      "\n",
      "************** Batch 400 in 3.75050950050354 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4444, 0.4081, 0.4401, 0.4077]) \n",
      "Test Loss tensor([0.4412, 0.4093, 0.4411, 0.4055])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3956, 0.4604, 0.3851, 0.4743, 0.4457]) \n",
      "Test Loss tensor([0.3990, 0.4578, 0.3845, 0.4703, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3909, 0.5295, 0.4733, 0.4063, 0.3554, 0.4560, 0.3842]) \n",
      "Test Loss tensor([0.3887, 0.5279, 0.4745, 0.4074, 0.3574, 0.4582, 0.3841])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5289, 0.3782, 0.5408, 0.5188, 0.4053, 0.4018, 0.3922]) \n",
      "Test Loss tensor([0.5304, 0.3795, 0.5417, 0.5183, 0.4096, 0.3993, 0.3920])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4011, 0.4019, 0.5238, 0.5358, 0.5269, 0.3989, 0.3896, 0.3839]) \n",
      "Test Loss tensor([0.4001, 0.4039, 0.5231, 0.5349, 0.5273, 0.4001, 0.3876, 0.3842])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4555, 0.4037, 0.3898, 0.5150, 0.3934, 0.5022])\n",
      "Valid Idx 3 | Loss tensor([0.5250, 0.5405, 0.5060, 0.4720, 0.3903])\n",
      "Gradients: Input 1.2788486856152304e-05 | Message 0.008078905753791332 | Update 0.012007689103484154 | Output 0.15960465371608734\n",
      "\n",
      "************** Batch 404 in 3.886044979095459 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4436, 0.4103, 0.4384, 0.4049]) \n",
      "Test Loss tensor([0.4406, 0.4089, 0.4409, 0.4058])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4031, 0.4573, 0.3842, 0.4705, 0.4418]) \n",
      "Test Loss tensor([0.3996, 0.4579, 0.3843, 0.4694, 0.4414])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3915, 0.5278, 0.4720, 0.4022, 0.3555, 0.4574, 0.3843]) \n",
      "Test Loss tensor([0.3892, 0.5277, 0.4736, 0.4061, 0.3577, 0.4577, 0.3838])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5300, 0.3778, 0.5420, 0.5204, 0.4110, 0.4004, 0.3913]) \n",
      "Test Loss tensor([0.5295, 0.3803, 0.5418, 0.5180, 0.4092, 0.4009, 0.3919])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4011, 0.4028, 0.5217, 0.5358, 0.5286, 0.4000, 0.3903, 0.3858]) \n",
      "Test Loss tensor([0.4003, 0.4040, 0.5232, 0.5358, 0.5276, 0.4005, 0.3880, 0.3840])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4549, 0.4030, 0.3903, 0.5145, 0.3929, 0.5024])\n",
      "Valid Idx 3 | Loss tensor([0.5246, 0.5418, 0.5067, 0.4728, 0.3913])\n",
      "\n",
      "************** Batch 408 in 3.69529128074646 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4432, 0.4111, 0.4412, 0.4052]) \n",
      "Test Loss tensor([0.4424, 0.4082, 0.4398, 0.4060])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3995, 0.4574, 0.3842, 0.4703, 0.4406]) \n",
      "Test Loss tensor([0.4003, 0.4573, 0.3833, 0.4700, 0.4423])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3881, 0.5286, 0.4745, 0.4053, 0.3576, 0.4596, 0.3827]) \n",
      "Test Loss tensor([0.3885, 0.5275, 0.4737, 0.4066, 0.3571, 0.4568, 0.3830])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5309, 0.3781, 0.5404, 0.5182, 0.4085, 0.4010, 0.3944]) \n",
      "Test Loss tensor([0.5291, 0.3797, 0.5421, 0.5169, 0.4105, 0.3999, 0.3915])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4002, 0.4053, 0.5216, 0.5350, 0.5279, 0.3990, 0.3891, 0.3837]) \n",
      "Test Loss tensor([0.4007, 0.4043, 0.5226, 0.5363, 0.5267, 0.4010, 0.3885, 0.3835])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.4039, 0.3901, 0.5133, 0.3938, 0.5030])\n",
      "Valid Idx 3 | Loss tensor([0.5248, 0.5418, 0.5049, 0.4713, 0.3918])\n",
      "\n",
      "************** Batch 412 in 3.8761305809020996 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4461, 0.4101, 0.4420, 0.4028]) \n",
      "Test Loss tensor([0.4407, 0.4093, 0.4408, 0.4059])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3979, 0.4582, 0.3830, 0.4683, 0.4453]) \n",
      "Test Loss tensor([0.3998, 0.4570, 0.3834, 0.4694, 0.4423])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3882, 0.5268, 0.4746, 0.4063, 0.3532, 0.4560, 0.3835]) \n",
      "Test Loss tensor([0.3894, 0.5273, 0.4727, 0.4049, 0.3583, 0.4586, 0.3827])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5309, 0.3825, 0.5419, 0.5178, 0.4086, 0.4006, 0.3929]) \n",
      "Test Loss tensor([0.5295, 0.3800, 0.5429, 0.5172, 0.4097, 0.4000, 0.3915])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4015, 0.4027, 0.5229, 0.5363, 0.5275, 0.4024, 0.3852, 0.3837]) \n",
      "Test Loss tensor([0.4009, 0.4045, 0.5225, 0.5366, 0.5269, 0.4014, 0.3871, 0.3830])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.4039, 0.3895, 0.5139, 0.3929, 0.5038])\n",
      "Valid Idx 3 | Loss tensor([0.5245, 0.5428, 0.5055, 0.4720, 0.3917])\n",
      "\n",
      "************** Batch 416 in 3.5642290115356445 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4410, 0.4069, 0.4431, 0.4059]) \n",
      "Test Loss tensor([0.4437, 0.4089, 0.4403, 0.4057])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3974, 0.4602, 0.3822, 0.4733, 0.4443]) \n",
      "Test Loss tensor([0.4002, 0.4573, 0.3824, 0.4690, 0.4406])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3861, 0.5258, 0.4737, 0.4072, 0.3586, 0.4573, 0.3830]) \n",
      "Test Loss tensor([0.3895, 0.5267, 0.4726, 0.4044, 0.3569, 0.4576, 0.3825])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5297, 0.3802, 0.5437, 0.5183, 0.4078, 0.3987, 0.3914]) \n",
      "Test Loss tensor([0.5288, 0.3807, 0.5436, 0.5172, 0.4089, 0.4005, 0.3909])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3998, 0.4033, 0.5214, 0.5363, 0.5265, 0.4015, 0.3877, 0.3830]) \n",
      "Test Loss tensor([0.4007, 0.4047, 0.5221, 0.5368, 0.5261, 0.4010, 0.3878, 0.3827])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.4045, 0.3897, 0.5131, 0.3935, 0.5035])\n",
      "Valid Idx 3 | Loss tensor([0.5241, 0.5429, 0.5042, 0.4713, 0.3922])\n",
      "\n",
      "************** Batch 420 in 3.8300118446350098 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4395, 0.4074, 0.4377, 0.4047]) \n",
      "Test Loss tensor([0.4413, 0.4076, 0.4399, 0.4050])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4033, 0.4583, 0.3829, 0.4721, 0.4400]) \n",
      "Test Loss tensor([0.3997, 0.4574, 0.3823, 0.4686, 0.4420])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3903, 0.5250, 0.4735, 0.4035, 0.3596, 0.4578, 0.3821]) \n",
      "Test Loss tensor([0.3896, 0.5267, 0.4720, 0.4061, 0.3568, 0.4576, 0.3820])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5283, 0.3794, 0.5416, 0.5160, 0.4106, 0.3966, 0.3888]) \n",
      "Test Loss tensor([0.5290, 0.3792, 0.5446, 0.5164, 0.4102, 0.4000, 0.3898])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4012, 0.4046, 0.5220, 0.5354, 0.5256, 0.4022, 0.3874, 0.3833]) \n",
      "Test Loss tensor([0.4012, 0.4049, 0.5217, 0.5370, 0.5258, 0.4016, 0.3881, 0.3814])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4561, 0.4051, 0.3878, 0.5125, 0.3938, 0.5042])\n",
      "Valid Idx 3 | Loss tensor([0.5244, 0.5433, 0.5051, 0.4717, 0.3930])\n",
      "Gradients: Input 2.1340485545806587e-05 | Message 0.008197538554668427 | Update 0.008925061672925949 | Output 0.039137281477451324\n",
      "\n",
      "************** Batch 424 in 4.041192293167114 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4427, 0.4089, 0.4390, 0.4060]) \n",
      "Test Loss tensor([0.4424, 0.4079, 0.4398, 0.4046])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3969, 0.4576, 0.3813, 0.4701, 0.4451]) \n",
      "Test Loss tensor([0.3998, 0.4577, 0.3811, 0.4698, 0.4421])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3898, 0.5261, 0.4741, 0.4042, 0.3572, 0.4570, 0.3815]) \n",
      "Test Loss tensor([0.3899, 0.5262, 0.4737, 0.4050, 0.3580, 0.4586, 0.3809])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5282, 0.3817, 0.5442, 0.5176, 0.4089, 0.4005, 0.3901]) \n",
      "Test Loss tensor([0.5288, 0.3801, 0.5451, 0.5167, 0.4094, 0.4000, 0.3895])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4018, 0.4027, 0.5211, 0.5396, 0.5268, 0.4020, 0.3895, 0.3833]) \n",
      "Test Loss tensor([0.4011, 0.4052, 0.5212, 0.5382, 0.5257, 0.4016, 0.3871, 0.3811])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4555, 0.4051, 0.3881, 0.5129, 0.3934, 0.5055])\n",
      "Valid Idx 3 | Loss tensor([0.5237, 0.5441, 0.5046, 0.4720, 0.3925])\n",
      "\n",
      "************** Batch 428 in 3.830650806427002 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4362, 0.4102, 0.4348, 0.4022]) \n",
      "Test Loss tensor([0.4409, 0.4076, 0.4389, 0.4048])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4007, 0.4585, 0.3815, 0.4686, 0.4436]) \n",
      "Test Loss tensor([0.4011, 0.4567, 0.3810, 0.4684, 0.4421])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3891, 0.5250, 0.4698, 0.4023, 0.3547, 0.4568, 0.3813]) \n",
      "Test Loss tensor([0.3894, 0.5257, 0.4727, 0.4058, 0.3581, 0.4578, 0.3804])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5289, 0.3815, 0.5463, 0.5165, 0.4110, 0.3980, 0.3909]) \n",
      "Test Loss tensor([0.5284, 0.3807, 0.5455, 0.5166, 0.4090, 0.4012, 0.3889])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4012, 0.4045, 0.5191, 0.5389, 0.5247, 0.4012, 0.3886, 0.3825]) \n",
      "Test Loss tensor([0.4014, 0.4050, 0.5213, 0.5393, 0.5252, 0.4022, 0.3872, 0.3803])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.4051, 0.3875, 0.5121, 0.3941, 0.5049])\n",
      "Valid Idx 3 | Loss tensor([0.5238, 0.5455, 0.5044, 0.4716, 0.3923])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 432 in 3.740424871444702 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4426, 0.4100, 0.4382, 0.4033]) \n",
      "Test Loss tensor([0.4428, 0.4081, 0.4396, 0.4043])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3999, 0.4583, 0.3817, 0.4679, 0.4423]) \n",
      "Test Loss tensor([0.4005, 0.4574, 0.3796, 0.4695, 0.4420])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3898, 0.5246, 0.4741, 0.4024, 0.3596, 0.4581, 0.3794]) \n",
      "Test Loss tensor([0.3899, 0.5255, 0.4722, 0.4048, 0.3574, 0.4571, 0.3795])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5285, 0.3818, 0.5463, 0.5172, 0.4132, 0.4013, 0.3898]) \n",
      "Test Loss tensor([0.5284, 0.3805, 0.5467, 0.5158, 0.4091, 0.4004, 0.3882])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4011, 0.4047, 0.5202, 0.5386, 0.5268, 0.4028, 0.3843, 0.3817]) \n",
      "Test Loss tensor([0.4013, 0.4057, 0.5205, 0.5397, 0.5256, 0.4024, 0.3867, 0.3801])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4556, 0.4054, 0.3866, 0.5123, 0.3940, 0.5062])\n",
      "Valid Idx 3 | Loss tensor([0.5238, 0.5463, 0.5045, 0.4714, 0.3945])\n",
      "\n",
      "************** Batch 436 in 3.7015573978424072 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4389, 0.4054, 0.4382, 0.3970]) \n",
      "Test Loss tensor([0.4417, 0.4081, 0.4399, 0.4044])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3992, 0.4599, 0.3798, 0.4712, 0.4419]) \n",
      "Test Loss tensor([0.3988, 0.4571, 0.3792, 0.4685, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3914, 0.5254, 0.4702, 0.4051, 0.3573, 0.4586, 0.3792]) \n",
      "Test Loss tensor([0.3895, 0.5254, 0.4708, 0.4045, 0.3581, 0.4580, 0.3787])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5285, 0.3802, 0.5472, 0.5170, 0.4156, 0.3988, 0.3890]) \n",
      "Test Loss tensor([0.5286, 0.3810, 0.5474, 0.5157, 0.4091, 0.4016, 0.3873])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4010, 0.4056, 0.5205, 0.5405, 0.5257, 0.4028, 0.3836, 0.3798]) \n",
      "Test Loss tensor([0.4016, 0.4062, 0.5201, 0.5408, 0.5246, 0.4031, 0.3880, 0.3785])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4560, 0.4059, 0.3858, 0.5121, 0.3949, 0.5064])\n",
      "Valid Idx 3 | Loss tensor([0.5233, 0.5468, 0.5038, 0.4712, 0.3938])\n",
      "\n",
      "************** Batch 440 in 3.733217239379883 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4411, 0.4067, 0.4377, 0.4079]) \n",
      "Test Loss tensor([0.4423, 0.4076, 0.4398, 0.4043])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4013, 0.4559, 0.3788, 0.4657, 0.4432]) \n",
      "Test Loss tensor([0.4000, 0.4576, 0.3780, 0.4688, 0.4420])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3906, 0.5263, 0.4718, 0.4047, 0.3592, 0.4573, 0.3786]) \n",
      "Test Loss tensor([0.3894, 0.5248, 0.4717, 0.4049, 0.3569, 0.4574, 0.3780])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5297, 0.3806, 0.5495, 0.5149, 0.4093, 0.3999, 0.3877]) \n",
      "Test Loss tensor([0.5281, 0.3813, 0.5481, 0.5150, 0.4101, 0.4014, 0.3866])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4013, 0.4059, 0.5183, 0.5436, 0.5258, 0.4017, 0.3928, 0.3793]) \n",
      "Test Loss tensor([0.4009, 0.4060, 0.5199, 0.5421, 0.5243, 0.4030, 0.3871, 0.3784])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4564, 0.4065, 0.3855, 0.5110, 0.3958, 0.5072])\n",
      "Valid Idx 3 | Loss tensor([0.5242, 0.5483, 0.5045, 0.4714, 0.3955])\n",
      "Gradients: Input 3.261759411543608e-05 | Message 0.008357442915439606 | Update 0.01736697182059288 | Output 0.08459015190601349\n",
      "\n",
      "************** Batch 444 in 3.761092185974121 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4447, 0.4086, 0.4348, 0.4035]) \n",
      "Test Loss tensor([0.4418, 0.4068, 0.4382, 0.4038])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3988, 0.4559, 0.3790, 0.4680, 0.4405]) \n",
      "Test Loss tensor([0.4004, 0.4577, 0.3767, 0.4689, 0.4422])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3903, 0.5262, 0.4712, 0.4040, 0.3558, 0.4556, 0.3781]) \n",
      "Test Loss tensor([0.3905, 0.5247, 0.4714, 0.4037, 0.3576, 0.4577, 0.3766])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5283, 0.3834, 0.5494, 0.5157, 0.4125, 0.4025, 0.3873]) \n",
      "Test Loss tensor([0.5282, 0.3805, 0.5498, 0.5156, 0.4098, 0.4004, 0.3859])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4010, 0.4067, 0.5170, 0.5420, 0.5250, 0.4023, 0.3871, 0.3778]) \n",
      "Test Loss tensor([0.4014, 0.4065, 0.5197, 0.5426, 0.5246, 0.4036, 0.3886, 0.3765])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4553, 0.4069, 0.3846, 0.5112, 0.3953, 0.5081])\n",
      "Valid Idx 3 | Loss tensor([0.5235, 0.5489, 0.5036, 0.4708, 0.3951])\n",
      "\n",
      "************** Batch 448 in 4.014979600906372 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4403, 0.4080, 0.4406, 0.4039]) \n",
      "Test Loss tensor([0.4422, 0.4074, 0.4379, 0.4029])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3992, 0.4552, 0.3756, 0.4677, 0.4432]) \n",
      "Test Loss tensor([0.3999, 0.4572, 0.3759, 0.4684, 0.4417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3887, 0.5250, 0.4723, 0.3991, 0.3615, 0.4597, 0.3766]) \n",
      "Test Loss tensor([0.3895, 0.5244, 0.4711, 0.4039, 0.3586, 0.4579, 0.3755])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5276, 0.3776, 0.5496, 0.5157, 0.4123, 0.4029, 0.3862]) \n",
      "Test Loss tensor([0.5280, 0.3818, 0.5509, 0.5148, 0.4089, 0.4011, 0.3851])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4022, 0.4060, 0.5209, 0.5455, 0.5248, 0.4032, 0.3878, 0.3775]) \n",
      "Test Loss tensor([0.4015, 0.4067, 0.5196, 0.5438, 0.5235, 0.4038, 0.3867, 0.3755])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4558, 0.4059, 0.3829, 0.5112, 0.3954, 0.5086])\n",
      "Valid Idx 3 | Loss tensor([0.5233, 0.5510, 0.5034, 0.4719, 0.3963])\n",
      "\n",
      "************** Batch 452 in 3.983610153198242 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4409, 0.4081, 0.4392, 0.4001]) \n",
      "Test Loss tensor([0.4410, 0.4063, 0.4382, 0.4023])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4014, 0.4566, 0.3760, 0.4706, 0.4400]) \n",
      "Test Loss tensor([0.4005, 0.4573, 0.3746, 0.4676, 0.4420])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3876, 0.5241, 0.4707, 0.4041, 0.3536, 0.4579, 0.3755]) \n",
      "Test Loss tensor([0.3905, 0.5239, 0.4718, 0.4028, 0.3583, 0.4577, 0.3747])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5269, 0.3818, 0.5519, 0.5160, 0.4102, 0.3986, 0.3849]) \n",
      "Test Loss tensor([0.5279, 0.3816, 0.5523, 0.5144, 0.4089, 0.4014, 0.3841])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4020, 0.4067, 0.5176, 0.5457, 0.5250, 0.4043, 0.3861, 0.3760]) \n",
      "Test Loss tensor([0.4018, 0.4065, 0.5197, 0.5451, 0.5237, 0.4034, 0.3884, 0.3747])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.4068, 0.3826, 0.5108, 0.3954, 0.5095])\n",
      "Valid Idx 3 | Loss tensor([0.5236, 0.5520, 0.5033, 0.4702, 0.3968])\n",
      "\n",
      "************** Batch 456 in 3.8252670764923096 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4437, 0.4083, 0.4429, 0.4032]) \n",
      "Test Loss tensor([0.4410, 0.4066, 0.4378, 0.4028])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3977, 0.4584, 0.3751, 0.4668, 0.4406]) \n",
      "Test Loss tensor([0.3993, 0.4577, 0.3729, 0.4690, 0.4423])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3906, 0.5242, 0.4704, 0.4028, 0.3567, 0.4574, 0.3740]) \n",
      "Test Loss tensor([0.3907, 0.5239, 0.4713, 0.4030, 0.3582, 0.4579, 0.3728])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5276, 0.3841, 0.5537, 0.5144, 0.4098, 0.4012, 0.3836]) \n",
      "Test Loss tensor([0.5280, 0.3815, 0.5539, 0.5142, 0.4099, 0.4018, 0.3834])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4015, 0.4055, 0.5202, 0.5448, 0.5247, 0.4039, 0.3846, 0.3741]) \n",
      "Test Loss tensor([0.4014, 0.4064, 0.5191, 0.5470, 0.5235, 0.4039, 0.3881, 0.3730])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4549, 0.4067, 0.3812, 0.5107, 0.3952, 0.5099])\n",
      "Valid Idx 3 | Loss tensor([0.5234, 0.5527, 0.5022, 0.4699, 0.3987])\n",
      "\n",
      "************** Batch 460 in 3.765052080154419 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4416, 0.4037, 0.4357, 0.4040]) \n",
      "Test Loss tensor([0.4422, 0.4062, 0.4401, 0.4022])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3991, 0.4585, 0.3729, 0.4689, 0.4436]) \n",
      "Test Loss tensor([0.3999, 0.4568, 0.3719, 0.4681, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3896, 0.5242, 0.4699, 0.4018, 0.3576, 0.4562, 0.3750]) \n",
      "Test Loss tensor([0.3906, 0.5237, 0.4710, 0.4023, 0.3586, 0.4583, 0.3716])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5269, 0.3814, 0.5541, 0.5158, 0.4071, 0.3993, 0.3836]) \n",
      "Test Loss tensor([0.5278, 0.3819, 0.5549, 0.5141, 0.4091, 0.4016, 0.3813])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4029, 0.4065, 0.5171, 0.5444, 0.5216, 0.4043, 0.3895, 0.3738]) \n",
      "Test Loss tensor([0.4019, 0.4076, 0.5190, 0.5479, 0.5231, 0.4042, 0.3878, 0.3720])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.4069, 0.3802, 0.5107, 0.3946, 0.5111])\n",
      "Valid Idx 3 | Loss tensor([0.5237, 0.5547, 0.5030, 0.4698, 0.3982])\n",
      "Gradients: Input 4.23011661041528e-05 | Message 0.008426688611507416 | Update 0.021144365891814232 | Output 0.10766305774450302\n",
      "\n",
      "************** Batch 464 in 3.990494728088379 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4421, 0.4063, 0.4364, 0.4049]) \n",
      "Test Loss tensor([0.4417, 0.4058, 0.4381, 0.4024])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3992, 0.4567, 0.3727, 0.4670, 0.4430]) \n",
      "Test Loss tensor([0.3994, 0.4565, 0.3704, 0.4686, 0.4427])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3908, 0.5233, 0.4713, 0.4036, 0.3558, 0.4577, 0.3716]) \n",
      "Test Loss tensor([0.3898, 0.5231, 0.4699, 0.4029, 0.3571, 0.4577, 0.3700])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5280, 0.3824, 0.5519, 0.5151, 0.4096, 0.3999, 0.3830]) \n",
      "Test Loss tensor([0.5282, 0.3823, 0.5564, 0.5139, 0.4095, 0.4031, 0.3803])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4016, 0.4051, 0.5195, 0.5473, 0.5225, 0.4049, 0.3837, 0.3711]) \n",
      "Test Loss tensor([0.4018, 0.4071, 0.5187, 0.5490, 0.5236, 0.4043, 0.3885, 0.3706])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4556, 0.4074, 0.3789, 0.5102, 0.3960, 0.5125])\n",
      "Valid Idx 3 | Loss tensor([0.5228, 0.5564, 0.5028, 0.4709, 0.3974])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 468 in 3.913914680480957 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4446, 0.4083, 0.4329, 0.4052]) \n",
      "Test Loss tensor([0.4419, 0.4060, 0.4372, 0.4025])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4010, 0.4559, 0.3721, 0.4663, 0.4388]) \n",
      "Test Loss tensor([0.3991, 0.4571, 0.3691, 0.4677, 0.4426])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3912, 0.5244, 0.4694, 0.3991, 0.3581, 0.4555, 0.3706]) \n",
      "Test Loss tensor([0.3894, 0.5232, 0.4712, 0.4015, 0.3580, 0.4576, 0.3688])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5286, 0.3827, 0.5566, 0.5168, 0.4125, 0.4041, 0.3798]) \n",
      "Test Loss tensor([0.5276, 0.3819, 0.5583, 0.5138, 0.4093, 0.4028, 0.3791])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4030, 0.4055, 0.5199, 0.5505, 0.5236, 0.4050, 0.3856, 0.3710]) \n",
      "Test Loss tensor([0.4015, 0.4075, 0.5180, 0.5504, 0.5228, 0.4045, 0.3883, 0.3689])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4553, 0.4082, 0.3784, 0.5105, 0.3954, 0.5131])\n",
      "Valid Idx 3 | Loss tensor([0.5229, 0.5577, 0.5022, 0.4712, 0.4000])\n",
      "\n",
      "************** Batch 472 in 3.85579776763916 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4420, 0.4086, 0.4371, 0.4002]) \n",
      "Test Loss tensor([0.4418, 0.4051, 0.4384, 0.4008])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4006, 0.4556, 0.3689, 0.4683, 0.4415]) \n",
      "Test Loss tensor([0.4001, 0.4561, 0.3675, 0.4673, 0.4407])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3908, 0.5211, 0.4714, 0.3989, 0.3583, 0.4574, 0.3691]) \n",
      "Test Loss tensor([0.3902, 0.5232, 0.4702, 0.4006, 0.3586, 0.4583, 0.3672])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5281, 0.3831, 0.5583, 0.5130, 0.4094, 0.4017, 0.3790]) \n",
      "Test Loss tensor([0.5283, 0.3826, 0.5598, 0.5133, 0.4098, 0.4025, 0.3778])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4016, 0.4086, 0.5184, 0.5530, 0.5243, 0.4039, 0.3872, 0.3695]) \n",
      "Test Loss tensor([0.4018, 0.4076, 0.5180, 0.5522, 0.5225, 0.4048, 0.3881, 0.3671])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4558, 0.4081, 0.3775, 0.5097, 0.3962, 0.5145])\n",
      "Valid Idx 3 | Loss tensor([0.5228, 0.5602, 0.5020, 0.4701, 0.4009])\n",
      "\n",
      "************** Batch 476 in 3.7958824634552 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4391, 0.4099, 0.4424, 0.4006]) \n",
      "Test Loss tensor([0.4417, 0.4053, 0.4367, 0.4009])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3978, 0.4563, 0.3680, 0.4679, 0.4422]) \n",
      "Test Loss tensor([0.3997, 0.4565, 0.3660, 0.4684, 0.4423])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3913, 0.5227, 0.4699, 0.4007, 0.3555, 0.4569, 0.3690]) \n",
      "Test Loss tensor([0.3906, 0.5233, 0.4710, 0.3997, 0.3586, 0.4582, 0.3660])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5308, 0.3813, 0.5597, 0.5138, 0.4072, 0.4020, 0.3785]) \n",
      "Test Loss tensor([0.5282, 0.3819, 0.5621, 0.5130, 0.4098, 0.4024, 0.3761])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4013, 0.4067, 0.5183, 0.5536, 0.5226, 0.4033, 0.3857, 0.3677]) \n",
      "Test Loss tensor([0.4018, 0.4085, 0.5173, 0.5540, 0.5220, 0.4047, 0.3889, 0.3660])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4551, 0.4082, 0.3760, 0.5098, 0.3958, 0.5153])\n",
      "Valid Idx 3 | Loss tensor([0.5229, 0.5622, 0.5010, 0.4703, 0.4012])\n",
      "\n",
      "************** Batch 480 in 3.742690086364746 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4380, 0.4064, 0.4339, 0.4037]) \n",
      "Test Loss tensor([0.4414, 0.4038, 0.4373, 0.3985])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3982, 0.4571, 0.3662, 0.4691, 0.4415]) \n",
      "Test Loss tensor([0.4009, 0.4569, 0.3637, 0.4668, 0.4426])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3900, 0.5233, 0.4685, 0.4033, 0.3612, 0.4589, 0.3663]) \n",
      "Test Loss tensor([0.3903, 0.5227, 0.4704, 0.3991, 0.3587, 0.4580, 0.3636])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5279, 0.3818, 0.5613, 0.5132, 0.4086, 0.4024, 0.3755]) \n",
      "Test Loss tensor([0.5283, 0.3829, 0.5638, 0.5127, 0.4106, 0.4038, 0.3750])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4020, 0.4072, 0.5178, 0.5531, 0.5226, 0.4054, 0.3878, 0.3663]) \n",
      "Test Loss tensor([0.4013, 0.4077, 0.5177, 0.5565, 0.5225, 0.4055, 0.3885, 0.3641])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4551, 0.4083, 0.3740, 0.5090, 0.3969, 0.5171])\n",
      "Valid Idx 3 | Loss tensor([0.5229, 0.5644, 0.5012, 0.4706, 0.4010])\n",
      "Gradients: Input 8.053844794631004e-05 | Message 0.010896751657128334 | Update 0.03465530276298523 | Output 0.10602305829524994\n",
      "\n",
      "************** Batch 484 in 4.0328850746154785 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4405, 0.4057, 0.4367, 0.3983]) \n",
      "Test Loss tensor([0.4416, 0.4043, 0.4368, 0.3994])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3955, 0.4563, 0.3635, 0.4682, 0.4415]) \n",
      "Test Loss tensor([0.4001, 0.4566, 0.3626, 0.4673, 0.4419])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3883, 0.5221, 0.4708, 0.3997, 0.3572, 0.4577, 0.3649]) \n",
      "Test Loss tensor([0.3894, 0.5220, 0.4700, 0.3989, 0.3576, 0.4580, 0.3617])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5291, 0.3822, 0.5640, 0.5142, 0.4089, 0.4043, 0.3741]) \n",
      "Test Loss tensor([0.5290, 0.3828, 0.5661, 0.5128, 0.4101, 0.4034, 0.3733])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4018, 0.4093, 0.5155, 0.5564, 0.5235, 0.4046, 0.3873, 0.3645]) \n",
      "Test Loss tensor([0.4012, 0.4084, 0.5162, 0.5583, 0.5217, 0.4055, 0.3867, 0.3618])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4544, 0.4086, 0.3720, 0.5091, 0.3973, 0.5185])\n",
      "Valid Idx 3 | Loss tensor([0.5231, 0.5662, 0.5017, 0.4697, 0.4047])\n",
      "\n",
      "************** Batch 488 in 3.995483875274658 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4414, 0.4066, 0.4377, 0.3984]) \n",
      "Test Loss tensor([0.4422, 0.4039, 0.4358, 0.3987])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3988, 0.4563, 0.3640, 0.4653, 0.4413]) \n",
      "Test Loss tensor([0.3994, 0.4571, 0.3607, 0.4657, 0.4426])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3907, 0.5231, 0.4685, 0.3981, 0.3628, 0.4566, 0.3619]) \n",
      "Test Loss tensor([0.3907, 0.5223, 0.4699, 0.3983, 0.3579, 0.4575, 0.3601])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5272, 0.3804, 0.5653, 0.5131, 0.4098, 0.4021, 0.3732]) \n",
      "Test Loss tensor([0.5291, 0.3827, 0.5678, 0.5129, 0.4103, 0.4028, 0.3713])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4008, 0.4080, 0.5164, 0.5579, 0.5219, 0.4065, 0.3884, 0.3628]) \n",
      "Test Loss tensor([0.4016, 0.4080, 0.5167, 0.5606, 0.5215, 0.4060, 0.3870, 0.3604])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4553, 0.4091, 0.3707, 0.5092, 0.3961, 0.5192])\n",
      "Valid Idx 3 | Loss tensor([0.5223, 0.5685, 0.5006, 0.4694, 0.4054])\n",
      "\n",
      "************** Batch 492 in 3.9508354663848877 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4452, 0.4025, 0.4333, 0.4012]) \n",
      "Test Loss tensor([0.4419, 0.4027, 0.4345, 0.3991])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3990, 0.4555, 0.3587, 0.4686, 0.4465]) \n",
      "Test Loss tensor([0.4003, 0.4573, 0.3584, 0.4677, 0.4422])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3896, 0.5215, 0.4717, 0.3969, 0.3570, 0.4569, 0.3615]) \n",
      "Test Loss tensor([0.3895, 0.5218, 0.4700, 0.3972, 0.3575, 0.4577, 0.3579])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5269, 0.3826, 0.5707, 0.5104, 0.4119, 0.4011, 0.3691]) \n",
      "Test Loss tensor([0.5285, 0.3830, 0.5706, 0.5126, 0.4094, 0.4027, 0.3698])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4006, 0.4065, 0.5154, 0.5596, 0.5210, 0.4056, 0.3895, 0.3596]) \n",
      "Test Loss tensor([0.4013, 0.4085, 0.5169, 0.5628, 0.5214, 0.4058, 0.3869, 0.3579])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.4092, 0.3699, 0.5084, 0.3965, 0.5206])\n",
      "Valid Idx 3 | Loss tensor([0.5225, 0.5708, 0.5010, 0.4705, 0.4039])\n",
      "\n",
      "************** Batch 496 in 3.8582136631011963 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4439, 0.4044, 0.4405, 0.4025]) \n",
      "Test Loss tensor([0.4413, 0.4021, 0.4355, 0.3984])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3984, 0.4557, 0.3582, 0.4665, 0.4432]) \n",
      "Test Loss tensor([0.3997, 0.4566, 0.3564, 0.4674, 0.4438])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3876, 0.5222, 0.4712, 0.3980, 0.3534, 0.4580, 0.3585]) \n",
      "Test Loss tensor([0.3894, 0.5213, 0.4685, 0.3978, 0.3587, 0.4578, 0.3558])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5291, 0.3840, 0.5721, 0.5118, 0.4059, 0.4011, 0.3712]) \n",
      "Test Loss tensor([0.5287, 0.3835, 0.5722, 0.5120, 0.4104, 0.4035, 0.3683])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4016, 0.4088, 0.5159, 0.5625, 0.5211, 0.4055, 0.3904, 0.3582]) \n",
      "Test Loss tensor([0.4009, 0.4089, 0.5155, 0.5644, 0.5205, 0.4059, 0.3884, 0.3564])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4551, 0.4099, 0.3670, 0.5084, 0.3969, 0.5234])\n",
      "Valid Idx 3 | Loss tensor([0.5226, 0.5733, 0.5004, 0.4693, 0.4069])\n",
      "\n",
      "************** Batch 500 in 3.7220513820648193 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4434, 0.3998, 0.4330, 0.3963]) \n",
      "Test Loss tensor([0.4415, 0.4032, 0.4366, 0.3976])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4013, 0.4579, 0.3556, 0.4660, 0.4442]) \n",
      "Test Loss tensor([0.3990, 0.4565, 0.3541, 0.4672, 0.4426])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3902, 0.5209, 0.4710, 0.3978, 0.3627, 0.4583, 0.3556]) \n",
      "Test Loss tensor([0.3901, 0.5211, 0.4688, 0.3966, 0.3591, 0.4581, 0.3539])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5286, 0.3857, 0.5733, 0.5132, 0.4096, 0.4035, 0.3687]) \n",
      "Test Loss tensor([0.5288, 0.3836, 0.5753, 0.5118, 0.4100, 0.4038, 0.3661])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4008, 0.4091, 0.5142, 0.5639, 0.5206, 0.4057, 0.3892, 0.3560]) \n",
      "Test Loss tensor([0.4012, 0.4092, 0.5152, 0.5661, 0.5206, 0.4068, 0.3888, 0.3539])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4556, 0.4102, 0.3660, 0.5077, 0.3984, 0.5244])\n",
      "Valid Idx 3 | Loss tensor([0.5221, 0.5751, 0.5001, 0.4698, 0.4076])\n",
      "Gradients: Input 0.00012147619418101385 | Message 0.013239594176411629 | Update 0.03505275398492813 | Output 0.23495671153068542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 504 in 3.990943431854248 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4404, 0.4022, 0.4370, 0.3968]) \n",
      "Test Loss tensor([0.4413, 0.4019, 0.4353, 0.3964])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4035, 0.4584, 0.3538, 0.4694, 0.4411]) \n",
      "Test Loss tensor([0.3995, 0.4562, 0.3523, 0.4665, 0.4427])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3921, 0.5224, 0.4721, 0.3990, 0.3552, 0.4584, 0.3536]) \n",
      "Test Loss tensor([0.3897, 0.5204, 0.4685, 0.3957, 0.3590, 0.4585, 0.3519])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5293, 0.3850, 0.5755, 0.5112, 0.4084, 0.4037, 0.3654]) \n",
      "Test Loss tensor([0.5285, 0.3841, 0.5776, 0.5110, 0.4101, 0.4041, 0.3644])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3995, 0.4115, 0.5171, 0.5679, 0.5206, 0.4069, 0.3828, 0.3557]) \n",
      "Test Loss tensor([0.4014, 0.4097, 0.5150, 0.5694, 0.5199, 0.4071, 0.3887, 0.3514])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4558, 0.4104, 0.3640, 0.5076, 0.3975, 0.5263])\n",
      "Valid Idx 3 | Loss tensor([0.5224, 0.5778, 0.4995, 0.4682, 0.4111])\n",
      "\n",
      "************** Batch 508 in 4.096289396286011 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4402, 0.4011, 0.4361, 0.3995]) \n",
      "Test Loss tensor([0.4408, 0.4009, 0.4350, 0.3954])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3994, 0.4574, 0.3524, 0.4658, 0.4454]) \n",
      "Test Loss tensor([0.4001, 0.4566, 0.3501, 0.4662, 0.4436])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3899, 0.5191, 0.4667, 0.3986, 0.3563, 0.4583, 0.3520]) \n",
      "Test Loss tensor([0.3898, 0.5198, 0.4682, 0.3961, 0.3590, 0.4580, 0.3494])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5279, 0.3842, 0.5795, 0.5102, 0.4126, 0.4072, 0.3632]) \n",
      "Test Loss tensor([0.5280, 0.3843, 0.5803, 0.5108, 0.4098, 0.4041, 0.3622])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4019, 0.4094, 0.5160, 0.5682, 0.5205, 0.4077, 0.3925, 0.3506]) \n",
      "Test Loss tensor([0.4016, 0.4102, 0.5141, 0.5714, 0.5196, 0.4076, 0.3891, 0.3491])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4555, 0.4112, 0.3620, 0.5076, 0.3975, 0.5270])\n",
      "Valid Idx 3 | Loss tensor([0.5217, 0.5804, 0.4995, 0.4684, 0.4118])\n",
      "\n",
      "************** Batch 512 in 4.061190366744995 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4395, 0.4028, 0.4373, 0.3958]) \n",
      "Test Loss tensor([0.4403, 0.4008, 0.4337, 0.3945])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4005, 0.4552, 0.3499, 0.4660, 0.4454]) \n",
      "Test Loss tensor([0.4002, 0.4570, 0.3477, 0.4659, 0.4441])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3898, 0.5193, 0.4668, 0.3987, 0.3603, 0.4555, 0.3508]) \n",
      "Test Loss tensor([0.3900, 0.5192, 0.4673, 0.3949, 0.3585, 0.4582, 0.3470])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5291, 0.3808, 0.5796, 0.5113, 0.4071, 0.4050, 0.3611]) \n",
      "Test Loss tensor([0.5284, 0.3851, 0.5831, 0.5102, 0.4105, 0.4049, 0.3598])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4023, 0.4095, 0.5155, 0.5712, 0.5189, 0.4075, 0.3887, 0.3479]) \n",
      "Test Loss tensor([0.4014, 0.4099, 0.5132, 0.5741, 0.5183, 0.4085, 0.3871, 0.3469])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4555, 0.4125, 0.3612, 0.5065, 0.3986, 0.5289])\n",
      "Valid Idx 3 | Loss tensor([0.5213, 0.5833, 0.4991, 0.4685, 0.4142])\n",
      "\n",
      "************** Batch 516 in 4.034180641174316 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4411, 0.3982, 0.4318, 0.3946]) \n",
      "Test Loss tensor([0.4415, 0.3992, 0.4330, 0.3954])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3995, 0.4562, 0.3472, 0.4659, 0.4430]) \n",
      "Test Loss tensor([0.4004, 0.4569, 0.3454, 0.4657, 0.4442])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3887, 0.5174, 0.4687, 0.3952, 0.3561, 0.4577, 0.3466]) \n",
      "Test Loss tensor([0.3908, 0.5187, 0.4677, 0.3947, 0.3589, 0.4574, 0.3448])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5297, 0.3833, 0.5808, 0.5091, 0.4109, 0.4029, 0.3615]) \n",
      "Test Loss tensor([0.5280, 0.3847, 0.5853, 0.5092, 0.4097, 0.4057, 0.3583])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4011, 0.4086, 0.5113, 0.5744, 0.5174, 0.4081, 0.3865, 0.3467]) \n",
      "Test Loss tensor([0.4019, 0.4112, 0.5122, 0.5761, 0.5174, 0.4089, 0.3882, 0.3448])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4551, 0.4130, 0.3575, 0.5058, 0.3992, 0.5306])\n",
      "Valid Idx 3 | Loss tensor([0.5208, 0.5862, 0.4985, 0.4677, 0.4147])\n",
      "\n",
      "************** Batch 520 in 4.003955841064453 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4406, 0.4044, 0.4348, 0.3942]) \n",
      "Test Loss tensor([0.4412, 0.4001, 0.4311, 0.3940])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3995, 0.4560, 0.3463, 0.4617, 0.4416]) \n",
      "Test Loss tensor([0.4005, 0.4567, 0.3426, 0.4657, 0.4446])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3896, 0.5174, 0.4696, 0.3896, 0.3580, 0.4574, 0.3459]) \n",
      "Test Loss tensor([0.3896, 0.5177, 0.4656, 0.3940, 0.3586, 0.4582, 0.3427])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5302, 0.3843, 0.5864, 0.5097, 0.4133, 0.4092, 0.3592]) \n",
      "Test Loss tensor([0.5278, 0.3856, 0.5883, 0.5090, 0.4109, 0.4061, 0.3559])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4022, 0.4095, 0.5106, 0.5758, 0.5194, 0.4080, 0.3859, 0.3451]) \n",
      "Test Loss tensor([0.4021, 0.4110, 0.5115, 0.5794, 0.5170, 0.4098, 0.3881, 0.3421])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4559, 0.4138, 0.3563, 0.5060, 0.3992, 0.5334])\n",
      "Valid Idx 3 | Loss tensor([0.5207, 0.5892, 0.4980, 0.4678, 0.4141])\n",
      "Gradients: Input 0.00020573174697346985 | Message 0.01435005385428667 | Update 0.047909192740917206 | Output 0.27937400341033936\n",
      "\n",
      "************** Batch 524 in 3.9631288051605225 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4405, 0.3977, 0.4306, 0.3943]) \n",
      "Test Loss tensor([0.4402, 0.3995, 0.4313, 0.3924])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3988, 0.4569, 0.3431, 0.4660, 0.4465]) \n",
      "Test Loss tensor([0.3994, 0.4567, 0.3403, 0.4656, 0.4435])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3895, 0.5175, 0.4685, 0.3913, 0.3608, 0.4605, 0.3426]) \n",
      "Test Loss tensor([0.3906, 0.5168, 0.4655, 0.3916, 0.3584, 0.4580, 0.3402])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5287, 0.3851, 0.5895, 0.5079, 0.4115, 0.4069, 0.3538]) \n",
      "Test Loss tensor([0.5274, 0.3863, 0.5912, 0.5075, 0.4102, 0.4058, 0.3535])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4020, 0.4127, 0.5101, 0.5811, 0.5174, 0.4104, 0.3903, 0.3413]) \n",
      "Test Loss tensor([0.4028, 0.4118, 0.5099, 0.5821, 0.5163, 0.4104, 0.3878, 0.3400])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.4148, 0.3543, 0.5046, 0.3999, 0.5353])\n",
      "Valid Idx 3 | Loss tensor([0.5194, 0.5927, 0.4969, 0.4679, 0.4162])\n",
      "\n",
      "************** Batch 528 in 4.048053026199341 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4412, 0.4001, 0.4276, 0.3960]) \n",
      "Test Loss tensor([0.4390, 0.3988, 0.4311, 0.3935])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3965, 0.4572, 0.3395, 0.4690, 0.4484]) \n",
      "Test Loss tensor([0.4005, 0.4563, 0.3375, 0.4651, 0.4436])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3947, 0.5171, 0.4647, 0.3916, 0.3596, 0.4593, 0.3395]) \n",
      "Test Loss tensor([0.3900, 0.5156, 0.4657, 0.3925, 0.3592, 0.4581, 0.3370])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5272, 0.3887, 0.5910, 0.5089, 0.4115, 0.4073, 0.3549]) \n",
      "Test Loss tensor([0.5272, 0.3866, 0.5944, 0.5069, 0.4107, 0.4076, 0.3506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4011, 0.4124, 0.5101, 0.5792, 0.5165, 0.4113, 0.3864, 0.3397]) \n",
      "Test Loss tensor([0.4026, 0.4130, 0.5089, 0.5842, 0.5151, 0.4112, 0.3880, 0.3373])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4553, 0.4157, 0.3530, 0.5028, 0.4018, 0.5369])\n",
      "Valid Idx 3 | Loss tensor([0.5190, 0.5954, 0.4957, 0.4660, 0.4199])\n",
      "\n",
      "************** Batch 532 in 3.9930243492126465 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4378, 0.3998, 0.4306, 0.3943]) \n",
      "Test Loss tensor([0.4403, 0.3985, 0.4302, 0.3910])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3987, 0.4565, 0.3371, 0.4626, 0.4462]) \n",
      "Test Loss tensor([0.3997, 0.4559, 0.3345, 0.4644, 0.4437])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3921, 0.5146, 0.4670, 0.3893, 0.3587, 0.4580, 0.3369]) \n",
      "Test Loss tensor([0.3912, 0.5144, 0.4641, 0.3902, 0.3596, 0.4583, 0.3348])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5265, 0.3901, 0.5945, 0.5078, 0.4121, 0.4058, 0.3506]) \n",
      "Test Loss tensor([0.5268, 0.3874, 0.5978, 0.5056, 0.4107, 0.4078, 0.3492])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4034, 0.4136, 0.5082, 0.5849, 0.5131, 0.4113, 0.3892, 0.3371]) \n",
      "Test Loss tensor([0.4034, 0.4135, 0.5080, 0.5884, 0.5140, 0.4122, 0.3879, 0.3345])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4551, 0.4165, 0.3510, 0.5023, 0.4024, 0.5377])\n",
      "Valid Idx 3 | Loss tensor([0.5186, 0.5987, 0.4951, 0.4663, 0.4210])\n",
      "\n",
      "************** Batch 536 in 3.861412525177002 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4431, 0.4042, 0.4313, 0.3923]) \n",
      "Test Loss tensor([0.4391, 0.3991, 0.4293, 0.3908])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4029, 0.4550, 0.3336, 0.4673, 0.4456]) \n",
      "Test Loss tensor([0.3994, 0.4558, 0.3322, 0.4634, 0.4442])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3911, 0.5147, 0.4625, 0.3932, 0.3564, 0.4572, 0.3351]) \n",
      "Test Loss tensor([0.3910, 0.5134, 0.4629, 0.3904, 0.3602, 0.4582, 0.3318])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5268, 0.3886, 0.5975, 0.5070, 0.4123, 0.4098, 0.3493]) \n",
      "Test Loss tensor([0.5259, 0.3884, 0.6006, 0.5055, 0.4117, 0.4085, 0.3468])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4027, 0.4123, 0.5068, 0.5892, 0.5147, 0.4121, 0.3892, 0.3338]) \n",
      "Test Loss tensor([0.4034, 0.4142, 0.5065, 0.5908, 0.5124, 0.4133, 0.3882, 0.3316])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4549, 0.4177, 0.3474, 0.5012, 0.4031, 0.5412])\n",
      "Valid Idx 3 | Loss tensor([0.5173, 0.6012, 0.4937, 0.4653, 0.4221])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 540 in 4.046449661254883 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4423, 0.3957, 0.4281, 0.3878]) \n",
      "Test Loss tensor([0.4389, 0.3984, 0.4276, 0.3920])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4026, 0.4554, 0.3325, 0.4622, 0.4446]) \n",
      "Test Loss tensor([0.3995, 0.4561, 0.3295, 0.4625, 0.4435])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3928, 0.5137, 0.4628, 0.3866, 0.3581, 0.4588, 0.3313]) \n",
      "Test Loss tensor([0.3916, 0.5116, 0.4624, 0.3888, 0.3588, 0.4583, 0.3293])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5254, 0.3887, 0.6017, 0.5038, 0.4129, 0.4075, 0.3452]) \n",
      "Test Loss tensor([0.5250, 0.3899, 0.6039, 0.5035, 0.4124, 0.4093, 0.3438])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4039, 0.4149, 0.5071, 0.5918, 0.5124, 0.4129, 0.3915, 0.3316]) \n",
      "Test Loss tensor([0.4042, 0.4159, 0.5046, 0.5935, 0.5114, 0.4145, 0.3874, 0.3289])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4546, 0.4196, 0.3461, 0.5010, 0.4040, 0.5424])\n",
      "Valid Idx 3 | Loss tensor([0.5161, 0.6057, 0.4921, 0.4646, 0.4237])\n",
      "Gradients: Input 0.0003321621334180236 | Message 0.018579835072159767 | Update 0.052417971193790436 | Output 0.3179592490196228\n",
      "\n",
      "************** Batch 544 in 3.8638947010040283 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4398, 0.4008, 0.4293, 0.3913]) \n",
      "Test Loss tensor([0.4398, 0.3973, 0.4274, 0.3886])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4004, 0.4548, 0.3305, 0.4619, 0.4460]) \n",
      "Test Loss tensor([0.4005, 0.4559, 0.3268, 0.4623, 0.4445])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3921, 0.5113, 0.4596, 0.3886, 0.3538, 0.4579, 0.3283]) \n",
      "Test Loss tensor([0.3922, 0.5107, 0.4611, 0.3895, 0.3595, 0.4584, 0.3255])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5251, 0.3911, 0.6034, 0.5025, 0.4101, 0.4113, 0.3456]) \n",
      "Test Loss tensor([0.5243, 0.3910, 0.6072, 0.5020, 0.4126, 0.4109, 0.3420])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4034, 0.4155, 0.5040, 0.5949, 0.5114, 0.4148, 0.3893, 0.3291]) \n",
      "Test Loss tensor([0.4045, 0.4167, 0.5034, 0.5967, 0.5100, 0.4158, 0.3877, 0.3252])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4554, 0.4211, 0.3437, 0.4992, 0.4049, 0.5441])\n",
      "Valid Idx 3 | Loss tensor([0.5154, 0.6094, 0.4916, 0.4637, 0.4261])\n",
      "\n",
      "************** Batch 548 in 3.760016918182373 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4397, 0.3985, 0.4233, 0.3882]) \n",
      "Test Loss tensor([0.4381, 0.3961, 0.4261, 0.3890])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3973, 0.4586, 0.3263, 0.4639, 0.4493]) \n",
      "Test Loss tensor([0.4012, 0.4553, 0.3230, 0.4616, 0.4463])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3924, 0.5111, 0.4644, 0.3842, 0.3616, 0.4593, 0.3257]) \n",
      "Test Loss tensor([0.3927, 0.5091, 0.4599, 0.3872, 0.3605, 0.4584, 0.3225])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5258, 0.3907, 0.6071, 0.5022, 0.4137, 0.4124, 0.3424]) \n",
      "Test Loss tensor([0.5238, 0.3915, 0.6112, 0.5010, 0.4114, 0.4110, 0.3392])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4037, 0.4167, 0.5038, 0.5985, 0.5101, 0.4170, 0.3888, 0.3259]) \n",
      "Test Loss tensor([0.4051, 0.4178, 0.5010, 0.6009, 0.5085, 0.4171, 0.3884, 0.3223])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4562, 0.4223, 0.3406, 0.4979, 0.4058, 0.5477])\n",
      "Valid Idx 3 | Loss tensor([0.5142, 0.6133, 0.4903, 0.4646, 0.4264])\n",
      "\n",
      "************** Batch 552 in 3.7472076416015625 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4385, 0.3933, 0.4261, 0.3896]) \n",
      "Test Loss tensor([0.4385, 0.3953, 0.4248, 0.3881])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4053, 0.4537, 0.3234, 0.4601, 0.4442]) \n",
      "Test Loss tensor([0.4006, 0.4558, 0.3197, 0.4606, 0.4456])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3934, 0.5071, 0.4606, 0.3869, 0.3582, 0.4587, 0.3233]) \n",
      "Test Loss tensor([0.3930, 0.5072, 0.4594, 0.3863, 0.3605, 0.4580, 0.3195])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5229, 0.3937, 0.6114, 0.5032, 0.4097, 0.4134, 0.3402]) \n",
      "Test Loss tensor([0.5228, 0.3929, 0.6153, 0.4998, 0.4124, 0.4120, 0.3361])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4046, 0.4170, 0.5018, 0.6033, 0.5088, 0.4171, 0.3898, 0.3234]) \n",
      "Test Loss tensor([0.4053, 0.4192, 0.4988, 0.6035, 0.5066, 0.4187, 0.3893, 0.3189])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4546, 0.4237, 0.3372, 0.4970, 0.4066, 0.5506])\n",
      "Valid Idx 3 | Loss tensor([0.5128, 0.6173, 0.4897, 0.4629, 0.4301])\n",
      "\n",
      "************** Batch 556 in 3.6402676105499268 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4372, 0.3960, 0.4257, 0.3872]) \n",
      "Test Loss tensor([0.4367, 0.3949, 0.4214, 0.3873])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4005, 0.4535, 0.3201, 0.4609, 0.4424]) \n",
      "Test Loss tensor([0.4007, 0.4557, 0.3167, 0.4603, 0.4467])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3929, 0.5062, 0.4591, 0.3823, 0.3564, 0.4587, 0.3189]) \n",
      "Test Loss tensor([0.3926, 0.5058, 0.4586, 0.3864, 0.3598, 0.4581, 0.3159])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5232, 0.3968, 0.6161, 0.4993, 0.4120, 0.4124, 0.3354]) \n",
      "Test Loss tensor([0.5224, 0.3941, 0.6193, 0.4974, 0.4127, 0.4136, 0.3331])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4053, 0.4194, 0.5008, 0.6053, 0.5060, 0.4184, 0.3855, 0.3178]) \n",
      "Test Loss tensor([0.4057, 0.4200, 0.4970, 0.6072, 0.5046, 0.4197, 0.3880, 0.3156])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4542, 0.4263, 0.3367, 0.4946, 0.4081, 0.5518])\n",
      "Valid Idx 3 | Loss tensor([0.5124, 0.6210, 0.4883, 0.4621, 0.4315])\n",
      "\n",
      "************** Batch 560 in 3.67516827583313 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4375, 0.3983, 0.4219, 0.3887]) \n",
      "Test Loss tensor([0.4385, 0.3949, 0.4224, 0.3862])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3996, 0.4548, 0.3171, 0.4595, 0.4492]) \n",
      "Test Loss tensor([0.4005, 0.4552, 0.3129, 0.4584, 0.4444])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3938, 0.5061, 0.4575, 0.3844, 0.3645, 0.4568, 0.3170]) \n",
      "Test Loss tensor([0.3936, 0.5039, 0.4566, 0.3825, 0.3608, 0.4586, 0.3123])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5222, 0.3949, 0.6194, 0.4973, 0.4108, 0.4135, 0.3321]) \n",
      "Test Loss tensor([0.5220, 0.3949, 0.6233, 0.4966, 0.4129, 0.4150, 0.3304])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4065, 0.4199, 0.4967, 0.6100, 0.5053, 0.4199, 0.3888, 0.3152]) \n",
      "Test Loss tensor([0.4065, 0.4210, 0.4959, 0.6127, 0.5034, 0.4215, 0.3877, 0.3121])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4543, 0.4273, 0.3327, 0.4940, 0.4087, 0.5562])\n",
      "Valid Idx 3 | Loss tensor([0.5113, 0.6258, 0.4862, 0.4618, 0.4333])\n",
      "Gradients: Input 0.000588903552852571 | Message 0.019864622503519058 | Update 0.05716189369559288 | Output 0.4403778314590454\n",
      "\n",
      "************** Batch 564 in 3.428532838821411 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4385, 0.3983, 0.4236, 0.3884]) \n",
      "Test Loss tensor([0.4366, 0.3955, 0.4204, 0.3858])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4022, 0.4563, 0.3137, 0.4566, 0.4464]) \n",
      "Test Loss tensor([0.4019, 0.4552, 0.3090, 0.4578, 0.4464])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3944, 0.5048, 0.4567, 0.3884, 0.3645, 0.4576, 0.3122]) \n",
      "Test Loss tensor([0.3947, 0.5021, 0.4560, 0.3835, 0.3615, 0.4590, 0.3090])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5226, 0.3962, 0.6262, 0.4980, 0.4100, 0.4140, 0.3279]) \n",
      "Test Loss tensor([0.5208, 0.3961, 0.6281, 0.4947, 0.4140, 0.4157, 0.3267])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4070, 0.4214, 0.4950, 0.6113, 0.5030, 0.4208, 0.3900, 0.3112]) \n",
      "Test Loss tensor([0.4065, 0.4220, 0.4938, 0.6154, 0.5011, 0.4232, 0.3884, 0.3083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4547, 0.4291, 0.3293, 0.4918, 0.4111, 0.5590])\n",
      "Valid Idx 3 | Loss tensor([0.5093, 0.6310, 0.4847, 0.4597, 0.4378])\n",
      "\n",
      "************** Batch 568 in 3.5296361446380615 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4388, 0.3933, 0.4216, 0.3807]) \n",
      "Test Loss tensor([0.4372, 0.3929, 0.4192, 0.3853])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4028, 0.4577, 0.3085, 0.4586, 0.4491]) \n",
      "Test Loss tensor([0.4015, 0.4548, 0.3050, 0.4571, 0.4461])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3950, 0.5017, 0.4541, 0.3848, 0.3599, 0.4606, 0.3077]) \n",
      "Test Loss tensor([0.3946, 0.5006, 0.4546, 0.3825, 0.3615, 0.4586, 0.3046])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5215, 0.3975, 0.6292, 0.4963, 0.4105, 0.4165, 0.3267]) \n",
      "Test Loss tensor([0.5212, 0.3968, 0.6322, 0.4932, 0.4135, 0.4167, 0.3237])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4064, 0.4244, 0.4932, 0.6140, 0.5011, 0.4234, 0.3885, 0.3084]) \n",
      "Test Loss tensor([0.4076, 0.4235, 0.4916, 0.6202, 0.4999, 0.4241, 0.3889, 0.3041])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4547, 0.4311, 0.3258, 0.4906, 0.4118, 0.5619])\n",
      "Valid Idx 3 | Loss tensor([0.5080, 0.6359, 0.4840, 0.4608, 0.4390])\n",
      "\n",
      "************** Batch 572 in 3.98897385597229 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4350, 0.3928, 0.4193, 0.3809]) \n",
      "Test Loss tensor([0.4360, 0.3936, 0.4172, 0.3838])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3997, 0.4552, 0.3049, 0.4555, 0.4517]) \n",
      "Test Loss tensor([0.4002, 0.4550, 0.3013, 0.4564, 0.4476])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3909, 0.5001, 0.4562, 0.3785, 0.3587, 0.4604, 0.3045]) \n",
      "Test Loss tensor([0.3946, 0.4989, 0.4528, 0.3802, 0.3624, 0.4585, 0.3004])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5201, 0.3963, 0.6318, 0.4928, 0.4137, 0.4157, 0.3231]) \n",
      "Test Loss tensor([0.5199, 0.3989, 0.6378, 0.4912, 0.4131, 0.4177, 0.3198])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4078, 0.4230, 0.4907, 0.6193, 0.4997, 0.4249, 0.3839, 0.3044]) \n",
      "Test Loss tensor([0.4078, 0.4241, 0.4895, 0.6248, 0.4977, 0.4263, 0.3896, 0.2998])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4550, 0.4339, 0.3229, 0.4886, 0.4140, 0.5654])\n",
      "Valid Idx 3 | Loss tensor([0.5070, 0.6403, 0.4825, 0.4593, 0.4418])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 576 in 3.8476803302764893 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4364, 0.3999, 0.4170, 0.3885]) \n",
      "Test Loss tensor([0.4356, 0.3923, 0.4156, 0.3824])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4019, 0.4550, 0.3009, 0.4561, 0.4454]) \n",
      "Test Loss tensor([0.4009, 0.4547, 0.2964, 0.4556, 0.4485])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3946, 0.4972, 0.4532, 0.3841, 0.3596, 0.4585, 0.3003]) \n",
      "Test Loss tensor([0.3948, 0.4969, 0.4516, 0.3795, 0.3613, 0.4593, 0.2964])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5201, 0.4000, 0.6374, 0.4931, 0.4162, 0.4184, 0.3213]) \n",
      "Test Loss tensor([0.5196, 0.3994, 0.6432, 0.4900, 0.4133, 0.4197, 0.3158])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4074, 0.4251, 0.4891, 0.6301, 0.4982, 0.4259, 0.3843, 0.2990]) \n",
      "Test Loss tensor([0.4078, 0.4258, 0.4873, 0.6291, 0.4966, 0.4275, 0.3886, 0.2958])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4535, 0.4354, 0.3191, 0.4879, 0.4140, 0.5674])\n",
      "Valid Idx 3 | Loss tensor([0.5054, 0.6460, 0.4815, 0.4585, 0.4455])\n",
      "\n",
      "************** Batch 580 in 3.681689500808716 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4360, 0.3921, 0.4180, 0.3836]) \n",
      "Test Loss tensor([0.4357, 0.3919, 0.4149, 0.3828])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3994, 0.4531, 0.2969, 0.4543, 0.4427]) \n",
      "Test Loss tensor([0.4011, 0.4541, 0.2925, 0.4546, 0.4472])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3929, 0.4974, 0.4507, 0.3784, 0.3591, 0.4577, 0.2969]) \n",
      "Test Loss tensor([0.3947, 0.4961, 0.4501, 0.3779, 0.3622, 0.4597, 0.2919])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5175, 0.3988, 0.6411, 0.4886, 0.4156, 0.4231, 0.3159]) \n",
      "Test Loss tensor([0.5191, 0.4009, 0.6476, 0.4887, 0.4138, 0.4208, 0.3126])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4095, 0.4239, 0.4881, 0.6288, 0.4979, 0.4258, 0.3878, 0.2960]) \n",
      "Test Loss tensor([0.4085, 0.4263, 0.4849, 0.6345, 0.4938, 0.4290, 0.3884, 0.2915])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4553, 0.4373, 0.3160, 0.4861, 0.4166, 0.5732])\n",
      "Valid Idx 3 | Loss tensor([0.5035, 0.6514, 0.4793, 0.4573, 0.4505])\n",
      "Gradients: Input 0.0010946050751954317 | Message 0.02316485345363617 | Update 0.06773358583450317 | Output 0.4121461510658264\n",
      "\n",
      "************** Batch 584 in 3.701909065246582 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4372, 0.3866, 0.4127, 0.3823]) \n",
      "Test Loss tensor([0.4338, 0.3920, 0.4143, 0.3824])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3979, 0.4545, 0.2932, 0.4540, 0.4450]) \n",
      "Test Loss tensor([0.4017, 0.4538, 0.2874, 0.4538, 0.4475])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3954, 0.4952, 0.4504, 0.3734, 0.3588, 0.4589, 0.2930]) \n",
      "Test Loss tensor([0.3955, 0.4936, 0.4489, 0.3787, 0.3627, 0.4592, 0.2877])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5208, 0.4042, 0.6494, 0.4875, 0.4153, 0.4237, 0.3110]) \n",
      "Test Loss tensor([0.5188, 0.4021, 0.6542, 0.4872, 0.4148, 0.4216, 0.3075])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4084, 0.4265, 0.4838, 0.6346, 0.4938, 0.4296, 0.3876, 0.2921]) \n",
      "Test Loss tensor([0.4086, 0.4273, 0.4821, 0.6391, 0.4924, 0.4301, 0.3887, 0.2867])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4545, 0.4391, 0.3128, 0.4848, 0.4172, 0.5760])\n",
      "Valid Idx 3 | Loss tensor([0.5021, 0.6576, 0.4779, 0.4563, 0.4531])\n",
      "\n",
      "************** Batch 588 in 3.6875619888305664 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4338, 0.3892, 0.4178, 0.3784]) \n",
      "Test Loss tensor([0.4333, 0.3891, 0.4099, 0.3793])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4035, 0.4545, 0.2873, 0.4515, 0.4491]) \n",
      "Test Loss tensor([0.4018, 0.4538, 0.2830, 0.4524, 0.4484])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3960, 0.4939, 0.4493, 0.3783, 0.3654, 0.4599, 0.2872]) \n",
      "Test Loss tensor([0.3945, 0.4919, 0.4467, 0.3761, 0.3621, 0.4593, 0.2826])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5183, 0.4009, 0.6549, 0.4874, 0.4147, 0.4217, 0.3088]) \n",
      "Test Loss tensor([0.5185, 0.4032, 0.6605, 0.4850, 0.4146, 0.4234, 0.3042])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4090, 0.4260, 0.4830, 0.6430, 0.4915, 0.4307, 0.3910, 0.2868]) \n",
      "Test Loss tensor([0.4085, 0.4281, 0.4805, 0.6461, 0.4904, 0.4317, 0.3886, 0.2818])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4557, 0.4421, 0.3085, 0.4835, 0.4189, 0.5810])\n",
      "Valid Idx 3 | Loss tensor([0.4996, 0.6641, 0.4771, 0.4555, 0.4531])\n",
      "\n",
      "************** Batch 592 in 3.6990859508514404 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4310, 0.3929, 0.4117, 0.3827]) \n",
      "Test Loss tensor([0.4335, 0.3889, 0.4086, 0.3774])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4016, 0.4534, 0.2837, 0.4527, 0.4511]) \n",
      "Test Loss tensor([0.4029, 0.4535, 0.2779, 0.4526, 0.4508])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3931, 0.4909, 0.4471, 0.3787, 0.3579, 0.4589, 0.2831]) \n",
      "Test Loss tensor([0.3947, 0.4903, 0.4461, 0.3753, 0.3631, 0.4599, 0.2772])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5190, 0.4018, 0.6634, 0.4850, 0.4122, 0.4245, 0.3041]) \n",
      "Test Loss tensor([0.5181, 0.4047, 0.6667, 0.4841, 0.4152, 0.4247, 0.2999])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4090, 0.4287, 0.4813, 0.6485, 0.4887, 0.4314, 0.3856, 0.2809]) \n",
      "Test Loss tensor([0.4085, 0.4293, 0.4790, 0.6522, 0.4883, 0.4329, 0.3890, 0.2764])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4549, 0.4437, 0.3048, 0.4820, 0.4211, 0.5849])\n",
      "Valid Idx 3 | Loss tensor([0.4987, 0.6711, 0.4765, 0.4547, 0.4584])\n",
      "\n",
      "************** Batch 596 in 3.6681060791015625 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4344, 0.3914, 0.4153, 0.3753]) \n",
      "Test Loss tensor([0.4325, 0.3889, 0.4067, 0.3772])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4015, 0.4545, 0.2777, 0.4531, 0.4484]) \n",
      "Test Loss tensor([0.4015, 0.4544, 0.2723, 0.4520, 0.4539])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3949, 0.4899, 0.4449, 0.3751, 0.3656, 0.4607, 0.2774]) \n",
      "Test Loss tensor([0.3944, 0.4891, 0.4446, 0.3732, 0.3634, 0.4595, 0.2718])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5190, 0.4078, 0.6672, 0.4814, 0.4163, 0.4237, 0.3000]) \n",
      "Test Loss tensor([0.5178, 0.4073, 0.6732, 0.4820, 0.4154, 0.4251, 0.2947])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4093, 0.4299, 0.4771, 0.6516, 0.4879, 0.4329, 0.3863, 0.2765]) \n",
      "Test Loss tensor([0.4084, 0.4298, 0.4758, 0.6579, 0.4864, 0.4338, 0.3898, 0.2711])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4541, 0.4460, 0.3002, 0.4804, 0.4214, 0.5888])\n",
      "Valid Idx 3 | Loss tensor([0.4966, 0.6781, 0.4748, 0.4542, 0.4626])\n",
      "\n",
      "************** Batch 600 in 3.7252585887908936 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4329, 0.3862, 0.4068, 0.3734]) \n",
      "Test Loss tensor([0.4323, 0.3877, 0.4063, 0.3761])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4009, 0.4541, 0.2731, 0.4502, 0.4520]) \n",
      "Test Loss tensor([0.4006, 0.4535, 0.2666, 0.4508, 0.4505])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3956, 0.4885, 0.4462, 0.3711, 0.3631, 0.4607, 0.2724]) \n",
      "Test Loss tensor([0.3930, 0.4873, 0.4424, 0.3735, 0.3638, 0.4594, 0.2665])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5160, 0.4083, 0.6750, 0.4821, 0.4176, 0.4276, 0.2949]) \n",
      "Test Loss tensor([0.5184, 0.4072, 0.6804, 0.4805, 0.4147, 0.4271, 0.2906])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4088, 0.4290, 0.4751, 0.6585, 0.4855, 0.4346, 0.3911, 0.2710]) \n",
      "Test Loss tensor([0.4079, 0.4307, 0.4733, 0.6644, 0.4847, 0.4351, 0.3892, 0.2653])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4550, 0.4489, 0.2959, 0.4788, 0.4228, 0.5930])\n",
      "Valid Idx 3 | Loss tensor([0.4953, 0.6857, 0.4737, 0.4521, 0.4673])\n",
      "Gradients: Input 0.00199683103710413 | Message 0.026651587337255478 | Update 0.07713748514652252 | Output 0.5107868313789368\n",
      "\n",
      "************** Batch 604 in 3.811990976333618 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4335, 0.3880, 0.4047, 0.3700]) \n",
      "Test Loss tensor([0.4310, 0.3865, 0.4040, 0.3734])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4008, 0.4536, 0.2666, 0.4482, 0.4503]) \n",
      "Test Loss tensor([0.4011, 0.4536, 0.2605, 0.4494, 0.4513])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3932, 0.4881, 0.4451, 0.3724, 0.3653, 0.4602, 0.2656]) \n",
      "Test Loss tensor([0.3924, 0.4854, 0.4409, 0.3700, 0.3633, 0.4599, 0.2598])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5180, 0.4083, 0.6804, 0.4788, 0.4134, 0.4303, 0.2902]) \n",
      "Test Loss tensor([0.5185, 0.4100, 0.6876, 0.4785, 0.4155, 0.4290, 0.2859])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4071, 0.4295, 0.4735, 0.6676, 0.4841, 0.4351, 0.3870, 0.2650]) \n",
      "Test Loss tensor([0.4069, 0.4309, 0.4706, 0.6722, 0.4825, 0.4357, 0.3879, 0.2588])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4542, 0.4509, 0.2915, 0.4772, 0.4262, 0.5988])\n",
      "Valid Idx 3 | Loss tensor([0.4932, 0.6946, 0.4729, 0.4512, 0.4713])\n",
      "\n",
      "************** Batch 608 in 3.820209503173828 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4331, 0.3886, 0.4020, 0.3733]) \n",
      "Test Loss tensor([0.4314, 0.3845, 0.4017, 0.3724])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4000, 0.4534, 0.2614, 0.4477, 0.4578]) \n",
      "Test Loss tensor([0.4004, 0.4541, 0.2546, 0.4491, 0.4546])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3945, 0.4862, 0.4439, 0.3694, 0.3657, 0.4611, 0.2607]) \n",
      "Test Loss tensor([0.3924, 0.4841, 0.4404, 0.3685, 0.3641, 0.4606, 0.2533])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5184, 0.4109, 0.6871, 0.4786, 0.4133, 0.4249, 0.2841]) \n",
      "Test Loss tensor([0.5189, 0.4110, 0.6954, 0.4773, 0.4157, 0.4316, 0.2792])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4062, 0.4302, 0.4715, 0.6741, 0.4827, 0.4364, 0.3920, 0.2578]) \n",
      "Test Loss tensor([0.4060, 0.4311, 0.4680, 0.6790, 0.4805, 0.4363, 0.3888, 0.2530])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4542, 0.4532, 0.2864, 0.4759, 0.4269, 0.6033])\n",
      "Valid Idx 3 | Loss tensor([0.4923, 0.7024, 0.4719, 0.4509, 0.4803])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 612 in 3.774143695831299 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4300, 0.3860, 0.4014, 0.3742]) \n",
      "Test Loss tensor([0.4298, 0.3856, 0.3997, 0.3715])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4012, 0.4528, 0.2548, 0.4456, 0.4546]) \n",
      "Test Loss tensor([0.4008, 0.4535, 0.2484, 0.4469, 0.4541])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3904, 0.4826, 0.4401, 0.3690, 0.3629, 0.4600, 0.2523]) \n",
      "Test Loss tensor([0.3909, 0.4823, 0.4377, 0.3667, 0.3645, 0.4608, 0.2470])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5195, 0.4099, 0.6954, 0.4772, 0.4163, 0.4305, 0.2791]) \n",
      "Test Loss tensor([0.5195, 0.4124, 0.7030, 0.4758, 0.4163, 0.4324, 0.2745])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4074, 0.4306, 0.4667, 0.6841, 0.4801, 0.4369, 0.3888, 0.2520]) \n",
      "Test Loss tensor([0.4053, 0.4313, 0.4652, 0.6864, 0.4780, 0.4367, 0.3907, 0.2465])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4543, 0.4561, 0.2812, 0.4750, 0.4298, 0.6094])\n",
      "Valid Idx 3 | Loss tensor([0.4904, 0.7110, 0.4718, 0.4500, 0.4826])\n",
      "\n",
      "************** Batch 616 in 3.7588083744049072 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4272, 0.3882, 0.4013, 0.3772]) \n",
      "Test Loss tensor([0.4284, 0.3852, 0.3977, 0.3724])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4008, 0.4543, 0.2481, 0.4480, 0.4524]) \n",
      "Test Loss tensor([0.4011, 0.4528, 0.2425, 0.4465, 0.4564])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3917, 0.4828, 0.4354, 0.3654, 0.3626, 0.4609, 0.2469]) \n",
      "Test Loss tensor([0.3888, 0.4806, 0.4361, 0.3663, 0.3643, 0.4612, 0.2406])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5197, 0.4125, 0.7044, 0.4751, 0.4170, 0.4318, 0.2756]) \n",
      "Test Loss tensor([0.5200, 0.4149, 0.7115, 0.4740, 0.4177, 0.4343, 0.2695])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4049, 0.4296, 0.4651, 0.6838, 0.4784, 0.4373, 0.3906, 0.2469]) \n",
      "Test Loss tensor([0.4043, 0.4306, 0.4622, 0.6937, 0.4762, 0.4372, 0.3912, 0.2402])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4550, 0.4594, 0.2761, 0.4742, 0.4304, 0.6138])\n",
      "Valid Idx 3 | Loss tensor([0.4889, 0.7195, 0.4706, 0.4478, 0.4886])\n",
      "\n",
      "************** Batch 620 in 3.7179036140441895 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4292, 0.3862, 0.3951, 0.3729]) \n",
      "Test Loss tensor([0.4273, 0.3823, 0.3953, 0.3685])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4013, 0.4552, 0.2414, 0.4445, 0.4580]) \n",
      "Test Loss tensor([0.4010, 0.4527, 0.2356, 0.4453, 0.4583])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3885, 0.4819, 0.4363, 0.3677, 0.3659, 0.4614, 0.2419]) \n",
      "Test Loss tensor([0.3872, 0.4794, 0.4344, 0.3646, 0.3652, 0.4613, 0.2344])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5196, 0.4157, 0.7094, 0.4745, 0.4174, 0.4359, 0.2717]) \n",
      "Test Loss tensor([0.5206, 0.4161, 0.7204, 0.4716, 0.4169, 0.4360, 0.2633])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4050, 0.4299, 0.4622, 0.6925, 0.4749, 0.4363, 0.3870, 0.2408]) \n",
      "Test Loss tensor([0.4031, 0.4302, 0.4591, 0.7012, 0.4730, 0.4375, 0.3902, 0.2342])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4549, 0.4624, 0.2706, 0.4720, 0.4334, 0.6210])\n",
      "Valid Idx 3 | Loss tensor([0.4868, 0.7282, 0.4701, 0.4466, 0.4927])\n",
      "Gradients: Input 0.003575138282030821 | Message 0.020664755254983902 | Update 0.09638440608978271 | Output 0.7435475587844849\n",
      "\n",
      "************** Batch 624 in 3.718938112258911 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4310, 0.3797, 0.3973, 0.3715]) \n",
      "Test Loss tensor([0.4264, 0.3861, 0.3931, 0.3698])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3973, 0.4528, 0.2349, 0.4464, 0.4578]) \n",
      "Test Loss tensor([0.3998, 0.4531, 0.2289, 0.4436, 0.4578])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3866, 0.4782, 0.4359, 0.3652, 0.3659, 0.4601, 0.2338]) \n",
      "Test Loss tensor([0.3858, 0.4774, 0.4323, 0.3628, 0.3650, 0.4617, 0.2279])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5206, 0.4144, 0.7195, 0.4711, 0.4187, 0.4370, 0.2629]) \n",
      "Test Loss tensor([0.5214, 0.4185, 0.7281, 0.4696, 0.4163, 0.4385, 0.2582])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4030, 0.4302, 0.4592, 0.7030, 0.4744, 0.4376, 0.3911, 0.2339]) \n",
      "Test Loss tensor([0.4019, 0.4303, 0.4554, 0.7098, 0.4706, 0.4376, 0.3908, 0.2272])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.4655, 0.2665, 0.4694, 0.4344, 0.6274])\n",
      "Valid Idx 3 | Loss tensor([0.4856, 0.7389, 0.4686, 0.4453, 0.4984])\n",
      "\n",
      "************** Batch 628 in 3.6982998847961426 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4241, 0.3848, 0.3943, 0.3647]) \n",
      "Test Loss tensor([0.4253, 0.3834, 0.3912, 0.3687])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4007, 0.4527, 0.2311, 0.4459, 0.4695]) \n",
      "Test Loss tensor([0.4014, 0.4520, 0.2221, 0.4417, 0.4594])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3843, 0.4768, 0.4347, 0.3630, 0.3659, 0.4619, 0.2279]) \n",
      "Test Loss tensor([0.3826, 0.4756, 0.4305, 0.3612, 0.3651, 0.4619, 0.2211])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5207, 0.4174, 0.7259, 0.4702, 0.4158, 0.4384, 0.2603]) \n",
      "Test Loss tensor([0.5223, 0.4195, 0.7371, 0.4676, 0.4174, 0.4405, 0.2525])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4013, 0.4302, 0.4563, 0.7129, 0.4685, 0.4372, 0.3924, 0.2274]) \n",
      "Test Loss tensor([0.4000, 0.4278, 0.4521, 0.7171, 0.4679, 0.4373, 0.3911, 0.2207])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4548, 0.4678, 0.2619, 0.4684, 0.4370, 0.6320])\n",
      "Valid Idx 3 | Loss tensor([0.4830, 0.7482, 0.4685, 0.4452, 0.5045])\n",
      "\n",
      "************** Batch 632 in 3.7279295921325684 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4279, 0.3849, 0.3930, 0.3588]) \n",
      "Test Loss tensor([0.4240, 0.3830, 0.3887, 0.3680])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4005, 0.4536, 0.2235, 0.4424, 0.4693]) \n",
      "Test Loss tensor([0.3994, 0.4522, 0.2152, 0.4411, 0.4627])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3823, 0.4749, 0.4323, 0.3689, 0.3659, 0.4632, 0.2213]) \n",
      "Test Loss tensor([0.3801, 0.4737, 0.4288, 0.3613, 0.3655, 0.4624, 0.2136])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5224, 0.4223, 0.7402, 0.4685, 0.4151, 0.4409, 0.2512]) \n",
      "Test Loss tensor([0.5233, 0.4228, 0.7465, 0.4659, 0.4174, 0.4426, 0.2466])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.4010, 0.4254, 0.4528, 0.7167, 0.4691, 0.4387, 0.3923, 0.2205]) \n",
      "Test Loss tensor([0.3985, 0.4272, 0.4496, 0.7268, 0.4650, 0.4364, 0.3905, 0.2133])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.4716, 0.2567, 0.4670, 0.4401, 0.6387])\n",
      "Valid Idx 3 | Loss tensor([0.4809, 0.7571, 0.4675, 0.4431, 0.5110])\n",
      "\n",
      "************** Batch 636 in 3.6673264503479004 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4255, 0.3778, 0.3855, 0.3626]) \n",
      "Test Loss tensor([0.4217, 0.3804, 0.3860, 0.3653])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4000, 0.4529, 0.2157, 0.4404, 0.4618]) \n",
      "Test Loss tensor([0.4000, 0.4524, 0.2082, 0.4399, 0.4635])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3795, 0.4738, 0.4322, 0.3546, 0.3646, 0.4623, 0.2139]) \n",
      "Test Loss tensor([0.3779, 0.4725, 0.4267, 0.3601, 0.3666, 0.4621, 0.2066])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5233, 0.4240, 0.7470, 0.4653, 0.4179, 0.4412, 0.2505]) \n",
      "Test Loss tensor([0.5244, 0.4245, 0.7558, 0.4645, 0.4163, 0.4451, 0.2410])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3983, 0.4276, 0.4493, 0.7271, 0.4646, 0.4362, 0.3854, 0.2131]) \n",
      "Test Loss tensor([0.3960, 0.4244, 0.4462, 0.7352, 0.4618, 0.4348, 0.3914, 0.2062])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4554, 0.4750, 0.2498, 0.4654, 0.4416, 0.6450])\n",
      "Valid Idx 3 | Loss tensor([0.4790, 0.7685, 0.4689, 0.4427, 0.5159])\n",
      "\n",
      "************** Batch 640 in 3.712817430496216 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4220, 0.3797, 0.3846, 0.3633]) \n",
      "Test Loss tensor([0.4224, 0.3810, 0.3840, 0.3648])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4011, 0.4542, 0.2083, 0.4395, 0.4643]) \n",
      "Test Loss tensor([0.3995, 0.4525, 0.2011, 0.4388, 0.4682])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3767, 0.4702, 0.4266, 0.3587, 0.3706, 0.4646, 0.2063]) \n",
      "Test Loss tensor([0.3746, 0.4706, 0.4246, 0.3570, 0.3666, 0.4629, 0.1991])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5249, 0.4262, 0.7598, 0.4645, 0.4184, 0.4419, 0.2374]) \n",
      "Test Loss tensor([0.5264, 0.4261, 0.7660, 0.4621, 0.4179, 0.4469, 0.2347])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3956, 0.4232, 0.4460, 0.7342, 0.4612, 0.4354, 0.3949, 0.2052]) \n",
      "Test Loss tensor([0.3926, 0.4208, 0.4431, 0.7442, 0.4588, 0.4323, 0.3919, 0.1990])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4565, 0.4789, 0.2448, 0.4644, 0.4442, 0.6551])\n",
      "Valid Idx 3 | Loss tensor([0.4778, 0.7792, 0.4697, 0.4417, 0.5242])\n",
      "Gradients: Input 0.006456879898905754 | Message 0.03060401976108551 | Update 0.09958537667989731 | Output 0.8848052024841309\n",
      "\n",
      "************** Batch 644 in 3.722564458847046 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4220, 0.3873, 0.3790, 0.3692]) \n",
      "Test Loss tensor([0.4189, 0.3791, 0.3797, 0.3614])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4048, 0.4544, 0.2016, 0.4396, 0.4657]) \n",
      "Test Loss tensor([0.3988, 0.4530, 0.1935, 0.4365, 0.4713])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3706, 0.4727, 0.4255, 0.3534, 0.3651, 0.4646, 0.2001]) \n",
      "Test Loss tensor([0.3700, 0.4700, 0.4226, 0.3544, 0.3665, 0.4632, 0.1917])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5278, 0.4259, 0.7669, 0.4630, 0.4180, 0.4454, 0.2350]) \n",
      "Test Loss tensor([0.5275, 0.4272, 0.7757, 0.4605, 0.4182, 0.4490, 0.2289])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3926, 0.4204, 0.4430, 0.7405, 0.4599, 0.4327, 0.3968, 0.1990]) \n",
      "Test Loss tensor([0.3896, 0.4174, 0.4396, 0.7540, 0.4559, 0.4295, 0.3923, 0.1916])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4562, 0.4819, 0.2407, 0.4622, 0.4473, 0.6604])\n",
      "Valid Idx 3 | Loss tensor([0.4764, 0.7900, 0.4704, 0.4401, 0.5271])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 648 in 3.6949801445007324 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4224, 0.3773, 0.3795, 0.3639]) \n",
      "Test Loss tensor([0.4170, 0.3786, 0.3782, 0.3607])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4000, 0.4519, 0.1942, 0.4379, 0.4696]) \n",
      "Test Loss tensor([0.3992, 0.4528, 0.1857, 0.4359, 0.4697])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3660, 0.4711, 0.4209, 0.3501, 0.3655, 0.4639, 0.1924]) \n",
      "Test Loss tensor([0.3649, 0.4684, 0.4197, 0.3553, 0.3671, 0.4637, 0.1840])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5291, 0.4266, 0.7777, 0.4621, 0.4199, 0.4487, 0.2261]) \n",
      "Test Loss tensor([0.5297, 0.4299, 0.7859, 0.4589, 0.4174, 0.4517, 0.2240])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3892, 0.4180, 0.4393, 0.7515, 0.4568, 0.4294, 0.3920, 0.1912]) \n",
      "Test Loss tensor([0.3855, 0.4123, 0.4355, 0.7619, 0.4528, 0.4260, 0.3915, 0.1838])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4566, 0.4856, 0.2339, 0.4607, 0.4502, 0.6666])\n",
      "Valid Idx 3 | Loss tensor([0.4757, 0.8021, 0.4720, 0.4392, 0.5375])\n",
      "\n",
      "************** Batch 652 in 3.6889030933380127 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4159, 0.3776, 0.3765, 0.3624]) \n",
      "Test Loss tensor([0.4164, 0.3804, 0.3755, 0.3626])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3976, 0.4538, 0.1857, 0.4357, 0.4788]) \n",
      "Test Loss tensor([0.3977, 0.4530, 0.1780, 0.4338, 0.4713])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3665, 0.4696, 0.4187, 0.3464, 0.3629, 0.4648, 0.1839]) \n",
      "Test Loss tensor([0.3591, 0.4670, 0.4190, 0.3546, 0.3672, 0.4651, 0.1762])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5303, 0.4295, 0.7870, 0.4580, 0.4161, 0.4522, 0.2201]) \n",
      "Test Loss tensor([0.5318, 0.4311, 0.7965, 0.4576, 0.4173, 0.4538, 0.2153])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3871, 0.4130, 0.4369, 0.7572, 0.4528, 0.4259, 0.3939, 0.1833]) \n",
      "Test Loss tensor([0.3819, 0.4068, 0.4322, 0.7720, 0.4497, 0.4213, 0.3928, 0.1757])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4562, 0.4898, 0.2280, 0.4599, 0.4519, 0.6745])\n",
      "Valid Idx 3 | Loss tensor([0.4747, 0.8139, 0.4746, 0.4374, 0.5469])\n",
      "\n",
      "************** Batch 656 in 3.7284891605377197 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4157, 0.3796, 0.3759, 0.3672]) \n",
      "Test Loss tensor([0.4138, 0.3796, 0.3727, 0.3646])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3988, 0.4519, 0.1779, 0.4321, 0.4787]) \n",
      "Test Loss tensor([0.3976, 0.4535, 0.1706, 0.4319, 0.4763])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3594, 0.4678, 0.4195, 0.3575, 0.3681, 0.4646, 0.1753]) \n",
      "Test Loss tensor([0.3541, 0.4665, 0.4155, 0.3559, 0.3671, 0.4663, 0.1682])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5313, 0.4321, 0.7971, 0.4570, 0.4166, 0.4544, 0.2154]) \n",
      "Test Loss tensor([0.5342, 0.4325, 0.8079, 0.4561, 0.4167, 0.4563, 0.2081])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3808, 0.4058, 0.4319, 0.7738, 0.4504, 0.4212, 0.3942, 0.1754]) \n",
      "Test Loss tensor([0.3767, 0.3999, 0.4291, 0.7815, 0.4453, 0.4154, 0.3943, 0.1680])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4576, 0.4932, 0.2220, 0.4577, 0.4550, 0.6817])\n",
      "Valid Idx 3 | Loss tensor([0.4746, 0.8269, 0.4772, 0.4370, 0.5535])\n",
      "\n",
      "************** Batch 660 in 3.6801490783691406 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4142, 0.3812, 0.3754, 0.3583]) \n",
      "Test Loss tensor([0.4103, 0.3801, 0.3693, 0.3596])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3956, 0.4512, 0.1700, 0.4300, 0.4811]) \n",
      "Test Loss tensor([0.3979, 0.4528, 0.1619, 0.4301, 0.4780])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3535, 0.4658, 0.4147, 0.3541, 0.3695, 0.4650, 0.1681]) \n",
      "Test Loss tensor([0.3466, 0.4650, 0.4135, 0.3545, 0.3672, 0.4672, 0.1603])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5352, 0.4345, 0.8050, 0.4541, 0.4152, 0.4560, 0.2085]) \n",
      "Test Loss tensor([0.5364, 0.4331, 0.8180, 0.4546, 0.4162, 0.4596, 0.2039])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3761, 0.3985, 0.4291, 0.7794, 0.4452, 0.4147, 0.3947, 0.1680]) \n",
      "Test Loss tensor([0.3713, 0.3924, 0.4258, 0.7923, 0.4419, 0.4092, 0.3943, 0.1597])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4587, 0.4974, 0.2168, 0.4562, 0.4575, 0.6890])\n",
      "Valid Idx 3 | Loss tensor([0.4748, 0.8399, 0.4802, 0.4363, 0.5636])\n",
      "Gradients: Input 0.011452998034656048 | Message 0.03210659325122833 | Update 0.11757873743772507 | Output 1.2257758378982544\n",
      "\n",
      "************** Batch 664 in 3.7575230598449707 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4105, 0.3787, 0.3680, 0.3494]) \n",
      "Test Loss tensor([0.4087, 0.3802, 0.3667, 0.3579])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4016, 0.4552, 0.1616, 0.4306, 0.4830]) \n",
      "Test Loss tensor([0.3993, 0.4544, 0.1537, 0.4297, 0.4784])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3464, 0.4639, 0.4140, 0.3499, 0.3672, 0.4660, 0.1601]) \n",
      "Test Loss tensor([0.3402, 0.4633, 0.4112, 0.3505, 0.3682, 0.4685, 0.1516])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5368, 0.4290, 0.8187, 0.4544, 0.4149, 0.4626, 0.2016]) \n",
      "Test Loss tensor([0.5378, 0.4334, 0.8297, 0.4535, 0.4157, 0.4616, 0.1972])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3736, 0.3918, 0.4249, 0.7892, 0.4415, 0.4106, 0.3928, 0.1594]) \n",
      "Test Loss tensor([0.3656, 0.3840, 0.4221, 0.8017, 0.4376, 0.4020, 0.3940, 0.1512])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4583, 0.5009, 0.2089, 0.4547, 0.4595, 0.6992])\n",
      "Valid Idx 3 | Loss tensor([0.4755, 0.8538, 0.4837, 0.4340, 0.5686])\n",
      "\n",
      "************** Batch 668 in 3.7050182819366455 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4096, 0.3710, 0.3669, 0.3644]) \n",
      "Test Loss tensor([0.4055, 0.3751, 0.3634, 0.3608])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4014, 0.4538, 0.1534, 0.4315, 0.4807]) \n",
      "Test Loss tensor([0.3982, 0.4539, 0.1449, 0.4271, 0.4871])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3392, 0.4632, 0.4105, 0.3467, 0.3680, 0.4702, 0.1515]) \n",
      "Test Loss tensor([0.3314, 0.4619, 0.4091, 0.3492, 0.3694, 0.4694, 0.1436])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5374, 0.4331, 0.8271, 0.4531, 0.4147, 0.4604, 0.1910]) \n",
      "Test Loss tensor([0.5401, 0.4329, 0.8404, 0.4514, 0.4163, 0.4651, 0.1892])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3662, 0.3838, 0.4226, 0.7993, 0.4379, 0.4022, 0.3928, 0.1517]) \n",
      "Test Loss tensor([0.3592, 0.3743, 0.4193, 0.8115, 0.4334, 0.3938, 0.3952, 0.1429])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4598, 0.5040, 0.2038, 0.4522, 0.4630, 0.7061])\n",
      "Valid Idx 3 | Loss tensor([0.4776, 0.8664, 0.4887, 0.4330, 0.5788])\n",
      "\n",
      "************** Batch 672 in 3.6942105293273926 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4063, 0.3782, 0.3696, 0.3511]) \n",
      "Test Loss tensor([0.4016, 0.3784, 0.3611, 0.3611])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3980, 0.4527, 0.1456, 0.4273, 0.4912]) \n",
      "Test Loss tensor([0.3980, 0.4554, 0.1374, 0.4258, 0.4877])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3312, 0.4616, 0.4096, 0.3428, 0.3657, 0.4699, 0.1424]) \n",
      "Test Loss tensor([0.3214, 0.4599, 0.4080, 0.3496, 0.3679, 0.4717, 0.1348])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5410, 0.4344, 0.8407, 0.4533, 0.4173, 0.4649, 0.1847]) \n",
      "Test Loss tensor([0.5409, 0.4311, 0.8514, 0.4499, 0.4165, 0.4669, 0.1832])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3587, 0.3726, 0.4174, 0.8128, 0.4339, 0.3947, 0.3996, 0.1427]) \n",
      "Test Loss tensor([0.3513, 0.3638, 0.4161, 0.8214, 0.4288, 0.3845, 0.3956, 0.1345])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4590, 0.5084, 0.1973, 0.4490, 0.4652, 0.7169])\n",
      "Valid Idx 3 | Loss tensor([0.4809, 0.8808, 0.4915, 0.4318, 0.5913])\n",
      "\n",
      "************** Batch 676 in 3.767075538635254 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.4010, 0.3736, 0.3622, 0.3562]) \n",
      "Test Loss tensor([0.3977, 0.3779, 0.3568, 0.3561])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3997, 0.4537, 0.1376, 0.4236, 0.4945]) \n",
      "Test Loss tensor([0.4014, 0.4560, 0.1288, 0.4244, 0.4901])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3207, 0.4599, 0.4044, 0.3459, 0.3669, 0.4712, 0.1358]) \n",
      "Test Loss tensor([0.3117, 0.4573, 0.4049, 0.3479, 0.3694, 0.4740, 0.1269])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5413, 0.4334, 0.8547, 0.4501, 0.4143, 0.4680, 0.1810]) \n",
      "Test Loss tensor([0.5413, 0.4303, 0.8624, 0.4481, 0.4157, 0.4697, 0.1770])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3524, 0.3628, 0.4145, 0.8224, 0.4286, 0.3870, 0.3914, 0.1346]) \n",
      "Test Loss tensor([0.3434, 0.3517, 0.4136, 0.8324, 0.4233, 0.3747, 0.3977, 0.1266])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4606, 0.5115, 0.1918, 0.4462, 0.4684, 0.7239])\n",
      "Valid Idx 3 | Loss tensor([0.4854, 0.8956, 0.4969, 0.4316, 0.5979])\n",
      "\n",
      "************** Batch 680 in 4.00983190536499 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3998, 0.3742, 0.3556, 0.3552]) \n",
      "Test Loss tensor([0.3932, 0.3791, 0.3537, 0.3556])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3976, 0.4534, 0.1295, 0.4229, 0.4990]) \n",
      "Test Loss tensor([0.3983, 0.4576, 0.1210, 0.4219, 0.4956])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3141, 0.4562, 0.4068, 0.3399, 0.3686, 0.4742, 0.1275]) \n",
      "Test Loss tensor([0.3009, 0.4539, 0.4031, 0.3491, 0.3693, 0.4765, 0.1185])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5426, 0.4253, 0.8658, 0.4475, 0.4149, 0.4735, 0.1691]) \n",
      "Test Loss tensor([0.5408, 0.4259, 0.8723, 0.4457, 0.4161, 0.4723, 0.1715])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3415, 0.3499, 0.4133, 0.8331, 0.4225, 0.3757, 0.3939, 0.1257]) \n",
      "Test Loss tensor([0.3343, 0.3391, 0.4107, 0.8421, 0.4175, 0.3642, 0.3984, 0.1183])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4617, 0.5147, 0.1869, 0.4423, 0.4711, 0.7330])\n",
      "Valid Idx 3 | Loss tensor([0.4899, 0.9089, 0.5027, 0.4320, 0.6099])\n",
      "Gradients: Input 0.02023065835237503 | Message 0.03931688517332077 | Update 0.1611093282699585 | Output 1.6226588487625122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 684 in 3.7576911449432373 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3931, 0.3888, 0.3534, 0.3596]) \n",
      "Test Loss tensor([0.3891, 0.3797, 0.3509, 0.3555])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3960, 0.4574, 0.1237, 0.4211, 0.5054]) \n",
      "Test Loss tensor([0.3994, 0.4578, 0.1137, 0.4193, 0.5035])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.3004, 0.4543, 0.4020, 0.3464, 0.3730, 0.4753, 0.1196]) \n",
      "Test Loss tensor([0.2895, 0.4495, 0.4002, 0.3515, 0.3713, 0.4790, 0.1106])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5412, 0.4260, 0.8750, 0.4460, 0.4169, 0.4721, 0.1677]) \n",
      "Test Loss tensor([0.5384, 0.4221, 0.8833, 0.4431, 0.4163, 0.4747, 0.1629])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3331, 0.3383, 0.4080, 0.8383, 0.4160, 0.3662, 0.3994, 0.1185]) \n",
      "Test Loss tensor([0.3236, 0.3245, 0.4078, 0.8507, 0.4115, 0.3534, 0.3990, 0.1104])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4619, 0.5170, 0.1818, 0.4381, 0.4734, 0.7410])\n",
      "Valid Idx 3 | Loss tensor([0.4963, 0.9224, 0.5065, 0.4314, 0.6209])\n",
      "\n",
      "************** Batch 688 in 3.4161083698272705 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3869, 0.3835, 0.3499, 0.3479]) \n",
      "Test Loss tensor([0.3838, 0.3816, 0.3472, 0.3599])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3996, 0.4583, 0.1136, 0.4190, 0.4994]) \n",
      "Test Loss tensor([0.4007, 0.4610, 0.1057, 0.4184, 0.5017])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2919, 0.4501, 0.4023, 0.3418, 0.3693, 0.4794, 0.1110]) \n",
      "Test Loss tensor([0.2797, 0.4442, 0.4004, 0.3481, 0.3737, 0.4831, 0.1035])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5387, 0.4210, 0.8834, 0.4434, 0.4160, 0.4741, 0.1596]) \n",
      "Test Loss tensor([0.5343, 0.4158, 0.8926, 0.4406, 0.4171, 0.4786, 0.1567])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3247, 0.3224, 0.4074, 0.8491, 0.4115, 0.3534, 0.3971, 0.1106]) \n",
      "Test Loss tensor([0.3134, 0.3091, 0.4049, 0.8588, 0.4051, 0.3421, 0.4011, 0.1026])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4631, 0.5193, 0.1761, 0.4341, 0.4745, 0.7501])\n",
      "Valid Idx 3 | Loss tensor([0.5014, 0.9372, 0.5102, 0.4311, 0.6303])\n",
      "\n",
      "************** Batch 692 in 3.9469268321990967 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3839, 0.3823, 0.3521, 0.3523]) \n",
      "Test Loss tensor([0.3792, 0.3831, 0.3450, 0.3588])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4049, 0.4595, 0.1049, 0.4190, 0.4966]) \n",
      "Test Loss tensor([0.4012, 0.4620, 0.0985, 0.4165, 0.5080])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2796, 0.4433, 0.4013, 0.3514, 0.3710, 0.4830, 0.1029]) \n",
      "Test Loss tensor([0.2679, 0.4369, 0.3995, 0.3499, 0.3752, 0.4883, 0.0961])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5368, 0.4213, 0.8956, 0.4405, 0.4184, 0.4812, 0.1563]) \n",
      "Test Loss tensor([0.5276, 0.4102, 0.9020, 0.4366, 0.4178, 0.4808, 0.1502])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3129, 0.3090, 0.4062, 0.8618, 0.4053, 0.3408, 0.4016, 0.1026]) \n",
      "Test Loss tensor([0.3025, 0.2936, 0.4009, 0.8652, 0.3985, 0.3304, 0.4020, 0.0952])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4633, 0.5209, 0.1711, 0.4283, 0.4765, 0.7563])\n",
      "Valid Idx 3 | Loss tensor([0.5077, 0.9497, 0.5157, 0.4319, 0.6362])\n",
      "\n",
      "************** Batch 696 in 4.662116765975952 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3805, 0.3738, 0.3458, 0.3735]) \n",
      "Test Loss tensor([0.3735, 0.3769, 0.3408, 0.3570])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3985, 0.4590, 0.0999, 0.4149, 0.5280]) \n",
      "Test Loss tensor([0.4009, 0.4644, 0.0916, 0.4132, 0.5140])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2679, 0.4339, 0.3997, 0.3451, 0.3772, 0.4866, 0.0963]) \n",
      "Test Loss tensor([0.2590, 0.4290, 0.3978, 0.3532, 0.3786, 0.4929, 0.0892])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5294, 0.4113, 0.8997, 0.4350, 0.4219, 0.4826, 0.1481]) \n",
      "Test Loss tensor([0.5188, 0.4042, 0.9080, 0.4323, 0.4194, 0.4838, 0.1480])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.3021, 0.2931, 0.4023, 0.8693, 0.3984, 0.3319, 0.4017, 0.0956]) \n",
      "Test Loss tensor([0.2916, 0.2766, 0.3970, 0.8738, 0.3915, 0.3193, 0.4031, 0.0884])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4642, 0.5230, 0.1677, 0.4215, 0.4798, 0.7608])\n",
      "Valid Idx 3 | Loss tensor([0.5119, 0.9620, 0.5179, 0.4328, 0.6474])\n",
      "\n",
      "************** Batch 700 in 3.769070625305176 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3745, 0.3782, 0.3411, 0.3521]) \n",
      "Test Loss tensor([0.3683, 0.3831, 0.3377, 0.3591])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3951, 0.4638, 0.0922, 0.4126, 0.5288]) \n",
      "Test Loss tensor([0.4035, 0.4664, 0.0852, 0.4118, 0.5200])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2574, 0.4299, 0.3985, 0.3551, 0.3765, 0.4937, 0.0891]) \n",
      "Test Loss tensor([0.2490, 0.4215, 0.3966, 0.3521, 0.3784, 0.4980, 0.0824])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5192, 0.4012, 0.9067, 0.4337, 0.4171, 0.4814, 0.1509]) \n",
      "Test Loss tensor([0.5069, 0.3948, 0.9141, 0.4271, 0.4198, 0.4863, 0.1412])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2925, 0.2773, 0.3968, 0.8728, 0.3924, 0.3171, 0.3996, 0.0886]) \n",
      "Test Loss tensor([0.2805, 0.2610, 0.3926, 0.8799, 0.3847, 0.3091, 0.4055, 0.0816])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4638, 0.5251, 0.1635, 0.4156, 0.4782, 0.7676])\n",
      "Valid Idx 3 | Loss tensor([0.5157, 0.9748, 0.5191, 0.4335, 0.6546])\n",
      "Gradients: Input 0.03014364093542099 | Message 0.07434700429439545 | Update 0.2448142170906067 | Output 2.178067684173584\n",
      "\n",
      "************** Batch 704 in 3.8493294715881348 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3676, 0.3758, 0.3341, 0.3594]) \n",
      "Test Loss tensor([0.3632, 0.3815, 0.3338, 0.3578])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4046, 0.4650, 0.0859, 0.4112, 0.5209]) \n",
      "Test Loss tensor([0.4046, 0.4691, 0.0781, 0.4086, 0.5239])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2489, 0.4200, 0.3962, 0.3582, 0.3784, 0.4978, 0.0828]) \n",
      "Test Loss tensor([0.2429, 0.4119, 0.3961, 0.3523, 0.3806, 0.5023, 0.0761])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.5074, 0.3966, 0.9136, 0.4279, 0.4198, 0.4842, 0.1423]) \n",
      "Test Loss tensor([0.4923, 0.3882, 0.9200, 0.4228, 0.4215, 0.4894, 0.1346])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2806, 0.2604, 0.3921, 0.8830, 0.3839, 0.3083, 0.4012, 0.0809]) \n",
      "Test Loss tensor([0.2701, 0.2464, 0.3867, 0.8849, 0.3774, 0.2987, 0.4061, 0.0751])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4636, 0.5256, 0.1594, 0.4085, 0.4794, 0.7702])\n",
      "Valid Idx 3 | Loss tensor([0.5196, 0.9849, 0.5197, 0.4346, 0.6658])\n",
      "\n",
      "************** Batch 708 in 3.8156795501708984 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3633, 0.3761, 0.3343, 0.3517]) \n",
      "Test Loss tensor([0.3585, 0.3830, 0.3294, 0.3603])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4067, 0.4702, 0.0789, 0.4117, 0.5263]) \n",
      "Test Loss tensor([0.4049, 0.4726, 0.0725, 0.4068, 0.5280])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2411, 0.4113, 0.3991, 0.3524, 0.3761, 0.5010, 0.0770]) \n",
      "Test Loss tensor([0.2348, 0.4028, 0.3941, 0.3546, 0.3828, 0.5080, 0.0702])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4934, 0.3912, 0.9209, 0.4226, 0.4225, 0.4887, 0.1385]) \n",
      "Test Loss tensor([0.4756, 0.3796, 0.9233, 0.4179, 0.4227, 0.4923, 0.1311])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2711, 0.2479, 0.3861, 0.8787, 0.3782, 0.2989, 0.4053, 0.0745]) \n",
      "Test Loss tensor([0.2602, 0.2336, 0.3800, 0.8890, 0.3704, 0.2906, 0.4079, 0.0691])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4655, 0.5279, 0.1549, 0.4024, 0.4796, 0.7779])\n",
      "Valid Idx 3 | Loss tensor([0.5209, 0.9944, 0.5194, 0.4354, 0.6761])\n",
      "\n",
      "************** Batch 712 in 3.8022351264953613 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3598, 0.3764, 0.3282, 0.3553]) \n",
      "Test Loss tensor([0.3532, 0.3831, 0.3271, 0.3637])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4049, 0.4671, 0.0729, 0.4057, 0.5319]) \n",
      "Test Loss tensor([0.4096, 0.4748, 0.0673, 0.4052, 0.5319])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2350, 0.4020, 0.3962, 0.3551, 0.3835, 0.5092, 0.0702]) \n",
      "Test Loss tensor([0.2268, 0.3938, 0.3930, 0.3556, 0.3865, 0.5139, 0.0645])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4758, 0.3818, 0.9262, 0.4205, 0.4267, 0.4953, 0.1250]) \n",
      "Test Loss tensor([0.4582, 0.3703, 0.9242, 0.4122, 0.4229, 0.4953, 0.1257])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2608, 0.2324, 0.3785, 0.8864, 0.3693, 0.2884, 0.4106, 0.0689]) \n",
      "Test Loss tensor([0.2516, 0.2221, 0.3727, 0.8894, 0.3636, 0.2813, 0.4088, 0.0634])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4638, 0.5293, 0.1518, 0.3953, 0.4811, 0.7831])\n",
      "Valid Idx 3 | Loss tensor([0.5217, 1.0054, 0.5176, 0.4363, 0.6806])\n",
      "\n",
      "************** Batch 716 in 3.385791778564453 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3545, 0.3852, 0.3285, 0.3663]) \n",
      "Test Loss tensor([0.3498, 0.3781, 0.3229, 0.3624])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4074, 0.4748, 0.0670, 0.4041, 0.5322]) \n",
      "Test Loss tensor([0.4114, 0.4770, 0.0622, 0.4032, 0.5346])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2247, 0.3934, 0.3899, 0.3481, 0.3862, 0.5140, 0.0656]) \n",
      "Test Loss tensor([0.2189, 0.3851, 0.3922, 0.3569, 0.3876, 0.5194, 0.0594])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4583, 0.3708, 0.9294, 0.4114, 0.4222, 0.4937, 0.1199]) \n",
      "Test Loss tensor([0.4420, 0.3621, 0.9239, 0.4063, 0.4260, 0.4982, 0.1228])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2524, 0.2218, 0.3715, 0.8911, 0.3633, 0.2814, 0.4093, 0.0637]) \n",
      "Test Loss tensor([0.2431, 0.2100, 0.3659, 0.8908, 0.3570, 0.2727, 0.4092, 0.0582])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4655, 0.5299, 0.1474, 0.3887, 0.4793, 0.7820])\n",
      "Valid Idx 3 | Loss tensor([0.5207, 1.0140, 0.5154, 0.4378, 0.6906])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 720 in 3.3507723808288574 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3503, 0.3805, 0.3244, 0.3612]) \n",
      "Test Loss tensor([0.3451, 0.3804, 0.3213, 0.3612])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4163, 0.4779, 0.0618, 0.4030, 0.5243]) \n",
      "Test Loss tensor([0.4183, 0.4797, 0.0583, 0.4018, 0.5382])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2179, 0.3867, 0.3887, 0.3534, 0.3811, 0.5218, 0.0631]) \n",
      "Test Loss tensor([0.2132, 0.3763, 0.3924, 0.3550, 0.3906, 0.5247, 0.0555])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4423, 0.3673, 0.9242, 0.4075, 0.4294, 0.4967, 0.1216]) \n",
      "Test Loss tensor([0.4259, 0.3533, 0.9219, 0.4007, 0.4299, 0.5006, 0.1202])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2449, 0.2107, 0.3664, 0.8910, 0.3565, 0.2736, 0.4089, 0.0578]) \n",
      "Test Loss tensor([0.2360, 0.2001, 0.3582, 0.8908, 0.3505, 0.2668, 0.4101, 0.0535])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4659, 0.5306, 0.1442, 0.3815, 0.4782, 0.7869])\n",
      "Valid Idx 3 | Loss tensor([0.5198, 1.0203, 0.5139, 0.4384, 0.6927])\n",
      "Gradients: Input 0.032062072306871414 | Message 0.09032118320465088 | Update 0.2654721140861511 | Output 2.4915173053741455\n",
      "\n",
      "************** Batch 724 in 3.3128905296325684 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3438, 0.3901, 0.3220, 0.3608]) \n",
      "Test Loss tensor([0.3401, 0.3810, 0.3185, 0.3642])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4167, 0.4780, 0.0569, 0.3981, 0.5410]) \n",
      "Test Loss tensor([0.4195, 0.4810, 0.0543, 0.3990, 0.5495])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2100, 0.3770, 0.3908, 0.3525, 0.3906, 0.5282, 0.0558]) \n",
      "Test Loss tensor([0.2063, 0.3686, 0.3919, 0.3608, 0.3932, 0.5307, 0.0516])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4224, 0.3511, 0.9213, 0.4012, 0.4263, 0.5040, 0.1200]) \n",
      "Test Loss tensor([0.4099, 0.3438, 0.9170, 0.3948, 0.4305, 0.5026, 0.1154])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2370, 0.1979, 0.3583, 0.8887, 0.3509, 0.2660, 0.4139, 0.0542]) \n",
      "Test Loss tensor([0.2299, 0.1891, 0.3518, 0.8885, 0.3445, 0.2599, 0.4114, 0.0494])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4649, 0.5309, 0.1428, 0.3759, 0.4776, 0.7851])\n",
      "Valid Idx 3 | Loss tensor([0.5180, 1.0261, 0.5081, 0.4426, 0.7047])\n",
      "\n",
      "************** Batch 728 in 3.401529312133789 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3396, 0.3719, 0.3180, 0.3531]) \n",
      "Test Loss tensor([0.3390, 0.3758, 0.3145, 0.3616])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4098, 0.4803, 0.0549, 0.3959, 0.5655]) \n",
      "Test Loss tensor([0.4254, 0.4844, 0.0511, 0.3963, 0.5444])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2092, 0.3696, 0.3902, 0.3619, 0.4027, 0.5352, 0.0512]) \n",
      "Test Loss tensor([0.2030, 0.3609, 0.3928, 0.3619, 0.3957, 0.5360, 0.0481])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.4111, 0.3479, 0.9172, 0.3970, 0.4289, 0.5029, 0.1180]) \n",
      "Test Loss tensor([0.3957, 0.3357, 0.9099, 0.3896, 0.4312, 0.5048, 0.1128])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2296, 0.1901, 0.3499, 0.8869, 0.3442, 0.2579, 0.4134, 0.0493]) \n",
      "Test Loss tensor([0.2246, 0.1804, 0.3451, 0.8826, 0.3392, 0.2548, 0.4115, 0.0459])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4624, 0.5328, 0.1386, 0.3698, 0.4750, 0.7818])\n",
      "Valid Idx 3 | Loss tensor([0.5137, 1.0323, 0.5024, 0.4422, 0.7042])\n",
      "\n",
      "************** Batch 732 in 3.331862449645996 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3406, 0.3731, 0.3141, 0.3702]) \n",
      "Test Loss tensor([0.3352, 0.3763, 0.3116, 0.3587])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4240, 0.4835, 0.0506, 0.3922, 0.5596]) \n",
      "Test Loss tensor([0.4275, 0.4849, 0.0476, 0.3934, 0.5535])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2040, 0.3607, 0.3955, 0.3677, 0.3920, 0.5365, 0.0503]) \n",
      "Test Loss tensor([0.1979, 0.3538, 0.3914, 0.3654, 0.3972, 0.5403, 0.0453])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.3952, 0.3345, 0.9090, 0.3865, 0.4315, 0.5067, 0.1106]) \n",
      "Test Loss tensor([0.3846, 0.3286, 0.9003, 0.3848, 0.4327, 0.5096, 0.1109])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2276, 0.1775, 0.3448, 0.8865, 0.3386, 0.2531, 0.4176, 0.0460]) \n",
      "Test Loss tensor([0.2195, 0.1713, 0.3395, 0.8776, 0.3337, 0.2499, 0.4131, 0.0425])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4631, 0.5335, 0.1375, 0.3646, 0.4733, 0.7770])\n",
      "Valid Idx 3 | Loss tensor([0.5092, 1.0352, 0.4976, 0.4458, 0.7096])\n",
      "\n",
      "************** Batch 736 in 3.3726842403411865 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3344, 0.3738, 0.3111, 0.3627]) \n",
      "Test Loss tensor([0.3311, 0.3779, 0.3097, 0.3627])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4322, 0.4884, 0.0466, 0.3949, 0.5480]) \n",
      "Test Loss tensor([0.4300, 0.4859, 0.0451, 0.3928, 0.5569])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.2043, 0.3538, 0.3930, 0.3670, 0.4037, 0.5442, 0.0448]) \n",
      "Test Loss tensor([0.1923, 0.3475, 0.3907, 0.3611, 0.4015, 0.5453, 0.0427])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.3837, 0.3274, 0.8984, 0.3866, 0.4378, 0.5059, 0.1125]) \n",
      "Test Loss tensor([0.3732, 0.3210, 0.8865, 0.3800, 0.4347, 0.5118, 0.1075])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2208, 0.1727, 0.3397, 0.8754, 0.3331, 0.2521, 0.4156, 0.0434]) \n",
      "Test Loss tensor([0.2154, 0.1635, 0.3351, 0.8706, 0.3287, 0.2468, 0.4133, 0.0396])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4625, 0.5337, 0.1341, 0.3596, 0.4693, 0.7751])\n",
      "Valid Idx 3 | Loss tensor([0.5043, 1.0357, 0.4886, 0.4485, 0.7154])\n",
      "\n",
      "************** Batch 740 in 3.2893247604370117 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3301, 0.3821, 0.3123, 0.3622]) \n",
      "Test Loss tensor([0.3284, 0.3709, 0.3059, 0.3621])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4320, 0.4888, 0.0457, 0.3909, 0.5453]) \n",
      "Test Loss tensor([0.4353, 0.4880, 0.0429, 0.3900, 0.5620])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1916, 0.3465, 0.3877, 0.3494, 0.3930, 0.5456, 0.0420]) \n",
      "Test Loss tensor([0.1898, 0.3408, 0.3918, 0.3669, 0.4026, 0.5508, 0.0403])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.3726, 0.3176, 0.8841, 0.3800, 0.4391, 0.5104, 0.1073]) \n",
      "Test Loss tensor([0.3622, 0.3124, 0.8706, 0.3748, 0.4370, 0.5133, 0.1044])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2133, 0.1619, 0.3327, 0.8792, 0.3270, 0.2447, 0.4152, 0.0401]) \n",
      "Test Loss tensor([0.2110, 0.1561, 0.3294, 0.8602, 0.3232, 0.2417, 0.4148, 0.0372])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4608, 0.5357, 0.1315, 0.3538, 0.4690, 0.7651])\n",
      "Valid Idx 3 | Loss tensor([0.4982, 1.0358, 0.4811, 0.4498, 0.7121])\n",
      "Gradients: Input 0.03441717475652695 | Message 0.09713609516620636 | Update 0.2449672669172287 | Output 2.6656606197357178\n",
      "\n",
      "************** Batch 744 in 3.302861213684082 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3308, 0.3749, 0.3031, 0.3794]) \n",
      "Test Loss tensor([0.3247, 0.3731, 0.3038, 0.3602])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4433, 0.4889, 0.0417, 0.3897, 0.5664]) \n",
      "Test Loss tensor([0.4413, 0.4893, 0.0408, 0.3883, 0.5592])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1946, 0.3402, 0.3900, 0.3594, 0.4010, 0.5479, 0.0402]) \n",
      "Test Loss tensor([0.1869, 0.3346, 0.3901, 0.3636, 0.4087, 0.5534, 0.0389])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.3627, 0.3135, 0.8717, 0.3739, 0.4377, 0.5131, 0.1002]) \n",
      "Test Loss tensor([0.3535, 0.3072, 0.8516, 0.3704, 0.4381, 0.5181, 0.1041])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2097, 0.1544, 0.3280, 0.8619, 0.3242, 0.2435, 0.4146, 0.0372]) \n",
      "Test Loss tensor([0.2081, 0.1501, 0.3255, 0.8487, 0.3188, 0.2394, 0.4163, 0.0353])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4628, 0.5383, 0.1324, 0.3501, 0.4663, 0.7565])\n",
      "Valid Idx 3 | Loss tensor([0.4877, 1.0341, 0.4696, 0.4521, 0.7125])\n",
      "\n",
      "************** Batch 748 in 3.300715923309326 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3275, 0.3594, 0.3032, 0.3469]) \n",
      "Test Loss tensor([0.3237, 0.3675, 0.3013, 0.3589])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4435, 0.4882, 0.0396, 0.3888, 0.5621]) \n",
      "Test Loss tensor([0.4462, 0.4890, 0.0394, 0.3858, 0.5610])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1845, 0.3356, 0.3914, 0.3568, 0.4051, 0.5511, 0.0403]) \n",
      "Test Loss tensor([0.1838, 0.3292, 0.3877, 0.3630, 0.4110, 0.5557, 0.0370])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.3540, 0.3062, 0.8554, 0.3697, 0.4419, 0.5194, 0.1053]) \n",
      "Test Loss tensor([0.3457, 0.3031, 0.8277, 0.3656, 0.4401, 0.5223, 0.1018])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2093, 0.1505, 0.3279, 0.8483, 0.3187, 0.2401, 0.4149, 0.0351]) \n",
      "Test Loss tensor([0.2063, 0.1440, 0.3208, 0.8360, 0.3131, 0.2384, 0.4151, 0.0334])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4598, 0.5406, 0.1286, 0.3461, 0.4609, 0.7508])\n",
      "Valid Idx 3 | Loss tensor([0.4744, 1.0314, 0.4537, 0.4526, 0.7103])\n",
      "\n",
      "************** Batch 752 in 3.3031907081604004 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3258, 0.3653, 0.2993, 0.3693]) \n",
      "Test Loss tensor([0.3187, 0.3594, 0.2981, 0.3544])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4428, 0.4860, 0.0407, 0.3847, 0.5757]) \n",
      "Test Loss tensor([0.4498, 0.4877, 0.0376, 0.3832, 0.5686])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1868, 0.3291, 0.3919, 0.3738, 0.4155, 0.5581, 0.0365]) \n",
      "Test Loss tensor([0.1828, 0.3234, 0.3867, 0.3708, 0.4154, 0.5564, 0.0366])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.3436, 0.3028, 0.8297, 0.3646, 0.4406, 0.5233, 0.1029]) \n",
      "Test Loss tensor([0.3355, 0.2977, 0.7978, 0.3602, 0.4402, 0.5257, 0.1024])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2067, 0.1429, 0.3216, 0.8378, 0.3136, 0.2362, 0.4218, 0.0333]) \n",
      "Test Loss tensor([0.2055, 0.1393, 0.3171, 0.8216, 0.3086, 0.2378, 0.4173, 0.0322])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4575, 0.5471, 0.1287, 0.3421, 0.4605, 0.7333])\n",
      "Valid Idx 3 | Loss tensor([0.4578, 1.0251, 0.4383, 0.4550, 0.7109])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 756 in 3.312621593475342 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3168, 0.3598, 0.3000, 0.3689]) \n",
      "Test Loss tensor([0.3182, 0.3557, 0.2962, 0.3542])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4545, 0.4831, 0.0383, 0.3833, 0.5667]) \n",
      "Test Loss tensor([0.4514, 0.4860, 0.0371, 0.3807, 0.5707])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1828, 0.3232, 0.3818, 0.3667, 0.4228, 0.5555, 0.0385]) \n",
      "Test Loss tensor([0.1797, 0.3182, 0.3856, 0.3660, 0.4196, 0.5556, 0.0350])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.3365, 0.3008, 0.7999, 0.3621, 0.4446, 0.5221, 0.1010]) \n",
      "Test Loss tensor([0.3270, 0.2940, 0.7629, 0.3541, 0.4413, 0.5299, 0.1011])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2056, 0.1381, 0.3162, 0.8240, 0.3074, 0.2356, 0.4198, 0.0316]) \n",
      "Test Loss tensor([0.2058, 0.1346, 0.3126, 0.8067, 0.3041, 0.2396, 0.4176, 0.0309])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4583, 0.5523, 0.1280, 0.3386, 0.4597, 0.7179])\n",
      "Valid Idx 3 | Loss tensor([0.4393, 1.0184, 0.4185, 0.4595, 0.7043])\n",
      "\n",
      "************** Batch 760 in 3.2683002948760986 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3224, 0.3417, 0.2891, 0.3589]) \n",
      "Test Loss tensor([0.3158, 0.3420, 0.2929, 0.3529])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4592, 0.4831, 0.0375, 0.3811, 0.5702]) \n",
      "Test Loss tensor([0.4588, 0.4838, 0.0359, 0.3799, 0.5689])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1839, 0.3177, 0.3806, 0.3515, 0.4171, 0.5514, 0.0355]) \n",
      "Test Loss tensor([0.1799, 0.3133, 0.3836, 0.3673, 0.4206, 0.5517, 0.0343])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.3277, 0.2983, 0.7640, 0.3532, 0.4414, 0.5280, 0.0998]) \n",
      "Test Loss tensor([0.3187, 0.2893, 0.7201, 0.3496, 0.4434, 0.5343, 0.0975])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2051, 0.1346, 0.3108, 0.8024, 0.3036, 0.2393, 0.4234, 0.0307]) \n",
      "Test Loss tensor([0.2066, 0.1298, 0.3086, 0.7904, 0.2989, 0.2405, 0.4192, 0.0299])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4545, 0.5576, 0.1242, 0.3342, 0.4570, 0.7033])\n",
      "Valid Idx 3 | Loss tensor([0.4170, 1.0089, 0.3998, 0.4592, 0.6977])\n",
      "Gradients: Input 0.04590163379907608 | Message 0.1684066653251648 | Update 0.3396487534046173 | Output 2.7426981925964355\n",
      "\n",
      "************** Batch 764 in 3.2606680393218994 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3189, 0.3357, 0.2926, 0.3529]) \n",
      "Test Loss tensor([0.3133, 0.3313, 0.2915, 0.3505])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4626, 0.4808, 0.0373, 0.3771, 0.5802]) \n",
      "Test Loss tensor([0.4647, 0.4791, 0.0349, 0.3769, 0.5750])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1784, 0.3129, 0.3853, 0.3747, 0.4259, 0.5489, 0.0323]) \n",
      "Test Loss tensor([0.1816, 0.3081, 0.3825, 0.3674, 0.4258, 0.5498, 0.0342])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.3199, 0.2924, 0.7244, 0.3515, 0.4408, 0.5317, 0.0972]) \n",
      "Test Loss tensor([0.3114, 0.2869, 0.6727, 0.3438, 0.4440, 0.5382, 0.0985])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2096, 0.1303, 0.3093, 0.7868, 0.2986, 0.2476, 0.4159, 0.0298]) \n",
      "Test Loss tensor([0.2079, 0.1267, 0.3047, 0.7688, 0.2950, 0.2415, 0.4201, 0.0288])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4575, 0.5649, 0.1203, 0.3301, 0.4535, 0.6858])\n",
      "Valid Idx 3 | Loss tensor([0.3943, 0.9975, 0.3816, 0.4597, 0.6917])\n",
      "\n",
      "************** Batch 768 in 3.2694284915924072 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3117, 0.3440, 0.2893, 0.3579]) \n",
      "Test Loss tensor([0.3136, 0.3155, 0.2892, 0.3464])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4677, 0.4817, 0.0331, 0.3798, 0.5650]) \n",
      "Test Loss tensor([0.4688, 0.4768, 0.0335, 0.3735, 0.5765])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1879, 0.3091, 0.3825, 0.3705, 0.4276, 0.5517, 0.0342]) \n",
      "Test Loss tensor([0.1833, 0.3032, 0.3803, 0.3621, 0.4318, 0.5479, 0.0335])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.3122, 0.2785, 0.6657, 0.3432, 0.4453, 0.5367, 0.0948]) \n",
      "Test Loss tensor([0.3052, 0.2828, 0.6168, 0.3391, 0.4453, 0.5439, 0.0964])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2066, 0.1268, 0.3036, 0.7680, 0.2942, 0.2439, 0.4186, 0.0285]) \n",
      "Test Loss tensor([0.2100, 0.1222, 0.3008, 0.7492, 0.2904, 0.2432, 0.4202, 0.0274])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4552, 0.5698, 0.1207, 0.3261, 0.4528, 0.6607])\n",
      "Valid Idx 3 | Loss tensor([0.3737, 0.9845, 0.3657, 0.4618, 0.6833])\n",
      "\n",
      "************** Batch 772 in 3.268691062927246 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3209, 0.3095, 0.2874, 0.3383]) \n",
      "Test Loss tensor([0.3099, 0.3060, 0.2858, 0.3388])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4750, 0.4829, 0.0343, 0.3772, 0.5709]) \n",
      "Test Loss tensor([0.4723, 0.4765, 0.0322, 0.3732, 0.5797])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1818, 0.3039, 0.3774, 0.3588, 0.4277, 0.5460, 0.0329]) \n",
      "Test Loss tensor([0.1828, 0.2981, 0.3815, 0.3692, 0.4332, 0.5453, 0.0323])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.3049, 0.2817, 0.6183, 0.3373, 0.4438, 0.5414, 0.0951]) \n",
      "Test Loss tensor([0.2993, 0.2788, 0.5580, 0.3347, 0.4447, 0.5497, 0.0934])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2127, 0.1221, 0.3021, 0.7509, 0.2913, 0.2467, 0.4228, 0.0271]) \n",
      "Test Loss tensor([0.2113, 0.1181, 0.2975, 0.7243, 0.2866, 0.2438, 0.4227, 0.0261])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4532, 0.5736, 0.1179, 0.3221, 0.4524, 0.6291])\n",
      "Valid Idx 3 | Loss tensor([0.3546, 0.9702, 0.3516, 0.4677, 0.6748])\n",
      "\n",
      "************** Batch 776 in 3.2403571605682373 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3102, 0.3117, 0.2838, 0.3473]) \n",
      "Test Loss tensor([0.3109, 0.2905, 0.2840, 0.3368])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4733, 0.4730, 0.0326, 0.3729, 0.5920]) \n",
      "Test Loss tensor([0.4737, 0.4736, 0.0314, 0.3707, 0.5860])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1817, 0.2964, 0.3832, 0.3677, 0.4234, 0.5441, 0.0318]) \n",
      "Test Loss tensor([0.1814, 0.2930, 0.3782, 0.3673, 0.4404, 0.5425, 0.0310])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2998, 0.2743, 0.5589, 0.3330, 0.4387, 0.5439, 0.0943]) \n",
      "Test Loss tensor([0.2931, 0.2768, 0.5052, 0.3303, 0.4463, 0.5516, 0.0908])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2098, 0.1171, 0.2964, 0.7268, 0.2857, 0.2443, 0.4236, 0.0263]) \n",
      "Test Loss tensor([0.2130, 0.1125, 0.2943, 0.6995, 0.2825, 0.2446, 0.4234, 0.0246])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4535, 0.5789, 0.1170, 0.3191, 0.4484, 0.5950])\n",
      "Valid Idx 3 | Loss tensor([0.3391, 0.9557, 0.3395, 0.4704, 0.6665])\n",
      "\n",
      "************** Batch 780 in 3.2603960037231445 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3102, 0.2844, 0.2858, 0.3377]) \n",
      "Test Loss tensor([0.3092, 0.2890, 0.2830, 0.3316])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4803, 0.4811, 0.0305, 0.3781, 0.5706]) \n",
      "Test Loss tensor([0.4817, 0.4773, 0.0292, 0.3715, 0.5889])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1743, 0.2937, 0.3747, 0.3657, 0.4366, 0.5438, 0.0307]) \n",
      "Test Loss tensor([0.1784, 0.2880, 0.3798, 0.3698, 0.4451, 0.5421, 0.0301])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2946, 0.2810, 0.5048, 0.3287, 0.4519, 0.5487, 0.0887]) \n",
      "Test Loss tensor([0.2875, 0.2750, 0.4596, 0.3257, 0.4472, 0.5539, 0.0885])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2141, 0.1136, 0.2955, 0.7006, 0.2821, 0.2497, 0.4207, 0.0247]) \n",
      "Test Loss tensor([0.2118, 0.1073, 0.2914, 0.6685, 0.2783, 0.2425, 0.4233, 0.0228])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4525, 0.5817, 0.1126, 0.3174, 0.4469, 0.5556])\n",
      "Valid Idx 3 | Loss tensor([0.3277, 0.9376, 0.3316, 0.4743, 0.6443])\n",
      "Gradients: Input 0.05521208047866821 | Message 0.16147483885288239 | Update 0.32692238688468933 | Output 2.446251153945923\n",
      "\n",
      "************** Batch 784 in 3.2471420764923096 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3110, 0.2734, 0.2824, 0.3318]) \n",
      "Test Loss tensor([0.3092, 0.2853, 0.2802, 0.3294])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4784, 0.4683, 0.0300, 0.3697, 0.6115]) \n",
      "Test Loss tensor([0.4870, 0.4785, 0.0278, 0.3683, 0.5946])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1807, 0.2877, 0.3839, 0.3775, 0.4416, 0.5420, 0.0311]) \n",
      "Test Loss tensor([0.1767, 0.2835, 0.3783, 0.3723, 0.4492, 0.5460, 0.0287])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2867, 0.2770, 0.4570, 0.3270, 0.4541, 0.5612, 0.0824]) \n",
      "Test Loss tensor([0.2830, 0.2690, 0.4251, 0.3233, 0.4497, 0.5562, 0.0824])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2149, 0.1059, 0.2934, 0.6665, 0.2783, 0.2416, 0.4248, 0.0230]) \n",
      "Test Loss tensor([0.2077, 0.1007, 0.2877, 0.6366, 0.2741, 0.2386, 0.4246, 0.0210])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4525, 0.5820, 0.1124, 0.3132, 0.4459, 0.5126])\n",
      "Valid Idx 3 | Loss tensor([0.3220, 0.9230, 0.3256, 0.4795, 0.6372])\n",
      "\n",
      "************** Batch 788 in 3.2760062217712402 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3021, 0.2789, 0.2813, 0.3246]) \n",
      "Test Loss tensor([0.3047, 0.2882, 0.2793, 0.3254])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4872, 0.4754, 0.0261, 0.3672, 0.5992]) \n",
      "Test Loss tensor([0.4910, 0.4816, 0.0260, 0.3674, 0.5994])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1801, 0.2834, 0.3766, 0.3765, 0.4499, 0.5471, 0.0289]) \n",
      "Test Loss tensor([0.1725, 0.2788, 0.3807, 0.3729, 0.4519, 0.5516, 0.0266])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2817, 0.2708, 0.4240, 0.3244, 0.4535, 0.5549, 0.0880]) \n",
      "Test Loss tensor([0.2784, 0.2686, 0.4009, 0.3199, 0.4517, 0.5577, 0.0806])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2059, 0.1007, 0.2883, 0.6380, 0.2750, 0.2338, 0.4254, 0.0207]) \n",
      "Test Loss tensor([0.2009, 0.0941, 0.2845, 0.5998, 0.2703, 0.2334, 0.4244, 0.0190])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4554, 0.5771, 0.1101, 0.3097, 0.4407, 0.4793])\n",
      "Valid Idx 3 | Loss tensor([0.3188, 0.9081, 0.3210, 0.4839, 0.6236])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 792 in 3.4306488037109375 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3063, 0.2860, 0.2747, 0.3228]) \n",
      "Test Loss tensor([0.3048, 0.2869, 0.2766, 0.3226])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4897, 0.4771, 0.0251, 0.3667, 0.6173]) \n",
      "Test Loss tensor([0.4927, 0.4853, 0.0241, 0.3639, 0.6089])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1726, 0.2785, 0.3777, 0.3677, 0.4550, 0.5527, 0.0264]) \n",
      "Test Loss tensor([0.1665, 0.2741, 0.3797, 0.3755, 0.4510, 0.5563, 0.0259])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2797, 0.2613, 0.4015, 0.3188, 0.4520, 0.5510, 0.0777]) \n",
      "Test Loss tensor([0.2738, 0.2636, 0.3801, 0.3186, 0.4537, 0.5604, 0.0779])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.2020, 0.0961, 0.2841, 0.5947, 0.2692, 0.2336, 0.4271, 0.0191]) \n",
      "Test Loss tensor([0.1921, 0.0873, 0.2817, 0.5577, 0.2663, 0.2275, 0.4293, 0.0171])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4561, 0.5719, 0.1100, 0.3067, 0.4415, 0.4443])\n",
      "Valid Idx 3 | Loss tensor([0.3166, 0.8919, 0.3155, 0.4912, 0.6193])\n",
      "\n",
      "************** Batch 796 in 3.3049395084381104 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3015, 0.2917, 0.2776, 0.3300]) \n",
      "Test Loss tensor([0.3008, 0.2858, 0.2736, 0.3169])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4973, 0.4854, 0.0238, 0.3612, 0.6128]) \n",
      "Test Loss tensor([0.4990, 0.4876, 0.0227, 0.3633, 0.6179])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1606, 0.2738, 0.3837, 0.3663, 0.4525, 0.5549, 0.0245]) \n",
      "Test Loss tensor([0.1632, 0.2698, 0.3827, 0.3776, 0.4534, 0.5609, 0.0239])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2745, 0.2620, 0.3801, 0.3196, 0.4521, 0.5621, 0.0767]) \n",
      "Test Loss tensor([0.2699, 0.2613, 0.3652, 0.3152, 0.4543, 0.5618, 0.0767])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1902, 0.0880, 0.2824, 0.5573, 0.2651, 0.2205, 0.4333, 0.0168]) \n",
      "Test Loss tensor([0.1828, 0.0812, 0.2769, 0.5141, 0.2626, 0.2214, 0.4311, 0.0154])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4569, 0.5659, 0.1101, 0.3025, 0.4406, 0.4194])\n",
      "Valid Idx 3 | Loss tensor([0.3153, 0.8719, 0.3137, 0.4963, 0.6142])\n",
      "\n",
      "************** Batch 800 in 3.3393940925598145 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3047, 0.2854, 0.2747, 0.3247]) \n",
      "Test Loss tensor([0.2977, 0.2890, 0.2711, 0.3057])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4950, 0.4866, 0.0228, 0.3594, 0.6278]) \n",
      "Test Loss tensor([0.5014, 0.4918, 0.0212, 0.3617, 0.6261])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1604, 0.2690, 0.3806, 0.3732, 0.4483, 0.5639, 0.0259]) \n",
      "Test Loss tensor([0.1592, 0.2659, 0.3827, 0.3792, 0.4537, 0.5666, 0.0228])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2698, 0.2662, 0.3660, 0.3155, 0.4547, 0.5656, 0.0752]) \n",
      "Test Loss tensor([0.2653, 0.2611, 0.3506, 0.3116, 0.4559, 0.5630, 0.0740])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1807, 0.0830, 0.2780, 0.5154, 0.2628, 0.2227, 0.4284, 0.0157]) \n",
      "Test Loss tensor([0.1732, 0.0777, 0.2740, 0.4687, 0.2582, 0.2170, 0.4320, 0.0142])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4588, 0.5615, 0.1065, 0.3011, 0.4396, 0.3962])\n",
      "Valid Idx 3 | Loss tensor([0.3123, 0.8500, 0.3077, 0.4984, 0.6073])\n",
      "Gradients: Input 0.03504268452525139 | Message 0.17049677670001984 | Update 0.35866332054138184 | Output 2.3993663787841797\n",
      "\n",
      "************** Batch 804 in 3.275068521499634 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.3013, 0.2882, 0.2704, 0.3032]) \n",
      "Test Loss tensor([0.2981, 0.2893, 0.2688, 0.3013])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4863, 0.4892, 0.0203, 0.3609, 0.6372]) \n",
      "Test Loss tensor([0.5075, 0.4961, 0.0208, 0.3605, 0.6340])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1596, 0.2653, 0.3854, 0.3823, 0.4515, 0.5678, 0.0228]) \n",
      "Test Loss tensor([0.1555, 0.2614, 0.3828, 0.3827, 0.4608, 0.5686, 0.0216])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2673, 0.2645, 0.3499, 0.3146, 0.4537, 0.5599, 0.0731]) \n",
      "Test Loss tensor([0.2607, 0.2568, 0.3368, 0.3078, 0.4574, 0.5664, 0.0711])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1717, 0.0772, 0.2726, 0.4713, 0.2574, 0.2204, 0.4261, 0.0143]) \n",
      "Test Loss tensor([0.1654, 0.0742, 0.2694, 0.4250, 0.2543, 0.2154, 0.4336, 0.0129])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4551, 0.5574, 0.1078, 0.2977, 0.4397, 0.3783])\n",
      "Valid Idx 3 | Loss tensor([0.3063, 0.8216, 0.3030, 0.5010, 0.5959])\n",
      "\n",
      "************** Batch 808 in 3.2651419639587402 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2943, 0.2722, 0.2712, 0.3005]) \n",
      "Test Loss tensor([0.2955, 0.2895, 0.2667, 0.2927])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5077, 0.4983, 0.0178, 0.3636, 0.6317]) \n",
      "Test Loss tensor([0.5046, 0.4973, 0.0194, 0.3590, 0.6502])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1537, 0.2605, 0.3777, 0.3782, 0.4695, 0.5745, 0.0198]) \n",
      "Test Loss tensor([0.1542, 0.2572, 0.3806, 0.3843, 0.4624, 0.5665, 0.0209])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2611, 0.2678, 0.3474, 0.3121, 0.4570, 0.5663, 0.0683]) \n",
      "Test Loss tensor([0.2564, 0.2586, 0.3247, 0.3045, 0.4614, 0.5691, 0.0703])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1655, 0.0739, 0.2683, 0.4231, 0.2548, 0.2150, 0.4361, 0.0130]) \n",
      "Test Loss tensor([0.1591, 0.0717, 0.2660, 0.3840, 0.2504, 0.2144, 0.4356, 0.0121])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4573, 0.5558, 0.1052, 0.2932, 0.4449, 0.3580])\n",
      "Valid Idx 3 | Loss tensor([0.2989, 0.7850, 0.2972, 0.5029, 0.5853])\n",
      "\n",
      "************** Batch 812 in 3.2606918811798096 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2947, 0.2973, 0.2628, 0.2864]) \n",
      "Test Loss tensor([0.2939, 0.2911, 0.2619, 0.2841])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5051, 0.4971, 0.0179, 0.3587, 0.6483]) \n",
      "Test Loss tensor([0.5075, 0.4956, 0.0183, 0.3586, 0.6669])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1603, 0.2571, 0.3815, 0.3808, 0.4630, 0.5724, 0.0230]) \n",
      "Test Loss tensor([0.1546, 0.2531, 0.3797, 0.3903, 0.4698, 0.5614, 0.0211])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2565, 0.2539, 0.3233, 0.3024, 0.4612, 0.5707, 0.0734]) \n",
      "Test Loss tensor([0.2523, 0.2585, 0.3112, 0.3015, 0.4602, 0.5723, 0.0680])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1569, 0.0694, 0.2642, 0.3847, 0.2499, 0.2108, 0.4374, 0.0119]) \n",
      "Test Loss tensor([0.1542, 0.0699, 0.2621, 0.3498, 0.2462, 0.2121, 0.4365, 0.0113])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4577, 0.5589, 0.1040, 0.2903, 0.4467, 0.3421])\n",
      "Valid Idx 3 | Loss tensor([0.2893, 0.7438, 0.2914, 0.5010, 0.5802])\n",
      "\n",
      "************** Batch 816 in 3.2517497539520264 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2978, 0.2859, 0.2611, 0.2855]) \n",
      "Test Loss tensor([0.2930, 0.2945, 0.2612, 0.2764])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5091, 0.4975, 0.0181, 0.3559, 0.6693]) \n",
      "Test Loss tensor([0.5114, 0.4944, 0.0182, 0.3550, 0.6864])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1539, 0.2531, 0.3732, 0.3870, 0.4588, 0.5580, 0.0203]) \n",
      "Test Loss tensor([0.1495, 0.2487, 0.3750, 0.3876, 0.4695, 0.5568, 0.0198])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2518, 0.2550, 0.3103, 0.3043, 0.4605, 0.5678, 0.0698]) \n",
      "Test Loss tensor([0.2487, 0.2598, 0.3011, 0.2951, 0.4641, 0.5776, 0.0685])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1541, 0.0703, 0.2623, 0.3448, 0.2464, 0.2120, 0.4365, 0.0113]) \n",
      "Test Loss tensor([0.1494, 0.0675, 0.2579, 0.3209, 0.2426, 0.2149, 0.4403, 0.0106])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4613, 0.5660, 0.1045, 0.2877, 0.4479, 0.3276])\n",
      "Valid Idx 3 | Loss tensor([0.2808, 0.6957, 0.2865, 0.4976, 0.5724])\n",
      "\n",
      "************** Batch 820 in 3.231553077697754 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2985, 0.2923, 0.2545, 0.2691]) \n",
      "Test Loss tensor([0.2925, 0.2917, 0.2577, 0.2695])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5062, 0.4949, 0.0183, 0.3546, 0.6940]) \n",
      "Test Loss tensor([0.5112, 0.4949, 0.0173, 0.3557, 0.6980])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1550, 0.2492, 0.3797, 0.3833, 0.4784, 0.5598, 0.0217]) \n",
      "Test Loss tensor([0.1521, 0.2445, 0.3729, 0.3867, 0.4760, 0.5513, 0.0201])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2473, 0.2600, 0.3002, 0.2940, 0.4610, 0.5781, 0.0678]) \n",
      "Test Loss tensor([0.2436, 0.2561, 0.2917, 0.2922, 0.4670, 0.5816, 0.0678])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1485, 0.0646, 0.2564, 0.3240, 0.2418, 0.2168, 0.4345, 0.0106]) \n",
      "Test Loss tensor([0.1449, 0.0673, 0.2542, 0.3004, 0.2385, 0.2144, 0.4426, 0.0102])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4605, 0.5712, 0.1005, 0.2839, 0.4536, 0.3163])\n",
      "Valid Idx 3 | Loss tensor([0.2752, 0.6560, 0.2823, 0.4967, 0.5657])\n",
      "Gradients: Input 0.051681581884622574 | Message 0.06184377893805504 | Update 0.15505856275558472 | Output 2.111271858215332\n",
      "\n",
      "************** Batch 824 in 3.2667787075042725 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2923, 0.2932, 0.2578, 0.2756]) \n",
      "Test Loss tensor([0.2905, 0.2943, 0.2548, 0.2675])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5204, 0.4949, 0.0174, 0.3572, 0.6968]) \n",
      "Test Loss tensor([0.5127, 0.4944, 0.0163, 0.3528, 0.7138])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1579, 0.2445, 0.3703, 0.3821, 0.4844, 0.5503, 0.0199]) \n",
      "Test Loss tensor([0.1513, 0.2405, 0.3727, 0.3879, 0.4767, 0.5487, 0.0191])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2433, 0.2636, 0.2911, 0.2910, 0.4572, 0.5793, 0.0699]) \n",
      "Test Loss tensor([0.2401, 0.2592, 0.2840, 0.2894, 0.4645, 0.5849, 0.0674])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1436, 0.0653, 0.2544, 0.3013, 0.2380, 0.2146, 0.4403, 0.0100]) \n",
      "Test Loss tensor([0.1393, 0.0649, 0.2501, 0.2864, 0.2343, 0.2137, 0.4408, 0.0094])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4628, 0.5732, 0.1020, 0.2811, 0.4563, 0.3080])\n",
      "Valid Idx 3 | Loss tensor([0.2703, 0.6228, 0.2779, 0.4981, 0.5668])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 828 in 3.2474989891052246 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2987, 0.2959, 0.2552, 0.2796]) \n",
      "Test Loss tensor([0.2905, 0.2934, 0.2529, 0.2644])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5049, 0.4971, 0.0174, 0.3549, 0.7146]) \n",
      "Test Loss tensor([0.5112, 0.4988, 0.0164, 0.3535, 0.7235])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1490, 0.2413, 0.3670, 0.3811, 0.4704, 0.5492, 0.0194]) \n",
      "Test Loss tensor([0.1470, 0.2365, 0.3732, 0.3931, 0.4794, 0.5476, 0.0179])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2397, 0.2539, 0.2826, 0.2832, 0.4671, 0.5913, 0.0651]) \n",
      "Test Loss tensor([0.2362, 0.2573, 0.2768, 0.2847, 0.4626, 0.5887, 0.0650])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1384, 0.0643, 0.2504, 0.2849, 0.2350, 0.2215, 0.4345, 0.0093]) \n",
      "Test Loss tensor([0.1320, 0.0624, 0.2460, 0.2752, 0.2301, 0.2118, 0.4448, 0.0089])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4636, 0.5745, 0.1015, 0.2762, 0.4588, 0.2986])\n",
      "Valid Idx 3 | Loss tensor([0.2683, 0.6064, 0.2745, 0.4981, 0.5657])\n",
      "\n",
      "************** Batch 832 in 3.2889652252197266 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2870, 0.2948, 0.2507, 0.2539]) \n",
      "Test Loss tensor([0.2846, 0.2943, 0.2509, 0.2615])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5032, 0.4935, 0.0151, 0.3568, 0.7274]) \n",
      "Test Loss tensor([0.5096, 0.4982, 0.0150, 0.3498, 0.7346])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1450, 0.2375, 0.3567, 0.3651, 0.4791, 0.5546, 0.0205]) \n",
      "Test Loss tensor([0.1457, 0.2321, 0.3710, 0.3931, 0.4773, 0.5498, 0.0177])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2362, 0.2658, 0.2803, 0.2863, 0.4706, 0.5850, 0.0696]) \n",
      "Test Loss tensor([0.2324, 0.2567, 0.2726, 0.2835, 0.4658, 0.5904, 0.0655])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1317, 0.0643, 0.2458, 0.2758, 0.2295, 0.2131, 0.4396, 0.0088]) \n",
      "Test Loss tensor([0.1251, 0.0613, 0.2418, 0.2680, 0.2262, 0.2075, 0.4433, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4627, 0.5723, 0.0995, 0.2742, 0.4580, 0.2946])\n",
      "Valid Idx 3 | Loss tensor([0.2677, 0.6014, 0.2728, 0.5009, 0.5663])\n",
      "\n",
      "************** Batch 836 in 3.264645576477051 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2893, 0.3076, 0.2537, 0.2659]) \n",
      "Test Loss tensor([0.2852, 0.2949, 0.2488, 0.2640])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5127, 0.4976, 0.0156, 0.3519, 0.7294]) \n",
      "Test Loss tensor([0.5170, 0.5044, 0.0151, 0.3499, 0.7335])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1475, 0.2326, 0.3679, 0.3730, 0.4799, 0.5561, 0.0166]) \n",
      "Test Loss tensor([0.1417, 0.2278, 0.3707, 0.3907, 0.4757, 0.5517, 0.0167])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2301, 0.2580, 0.2701, 0.2841, 0.4698, 0.5904, 0.0667]) \n",
      "Test Loss tensor([0.2289, 0.2526, 0.2665, 0.2825, 0.4705, 0.5943, 0.0633])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1256, 0.0615, 0.2407, 0.2677, 0.2248, 0.2102, 0.4478, 0.0086]) \n",
      "Test Loss tensor([0.1186, 0.0597, 0.2383, 0.2629, 0.2218, 0.2034, 0.4458, 0.0077])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4696, 0.5709, 0.0988, 0.2712, 0.4579, 0.2911])\n",
      "Valid Idx 3 | Loss tensor([0.2702, 0.6099, 0.2721, 0.4987, 0.5669])\n",
      "\n",
      "************** Batch 840 in 3.2543587684631348 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2838, 0.2955, 0.2521, 0.2687]) \n",
      "Test Loss tensor([0.2805, 0.2930, 0.2452, 0.2591])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5094, 0.5047, 0.0144, 0.3527, 0.7274]) \n",
      "Test Loss tensor([0.5134, 0.5058, 0.0141, 0.3497, 0.7404])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1445, 0.2294, 0.3741, 0.3878, 0.4717, 0.5532, 0.0193]) \n",
      "Test Loss tensor([0.1380, 0.2237, 0.3717, 0.3887, 0.4750, 0.5566, 0.0162])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2297, 0.2568, 0.2664, 0.2809, 0.4702, 0.5889, 0.0641]) \n",
      "Test Loss tensor([0.2245, 0.2539, 0.2640, 0.2813, 0.4721, 0.5941, 0.0630])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1178, 0.0595, 0.2389, 0.2616, 0.2225, 0.2059, 0.4437, 0.0082]) \n",
      "Test Loss tensor([0.1122, 0.0581, 0.2348, 0.2584, 0.2182, 0.1972, 0.4504, 0.0074])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4718, 0.5699, 0.0973, 0.2686, 0.4589, 0.2877])\n",
      "Valid Idx 3 | Loss tensor([0.2748, 0.6329, 0.2699, 0.5052, 0.5747])\n",
      "Gradients: Input 0.07085834443569183 | Message 0.06868872046470642 | Update 0.10045330226421356 | Output 1.8271520137786865\n",
      "\n",
      "************** Batch 844 in 3.256568670272827 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2864, 0.2898, 0.2466, 0.2651]) \n",
      "Test Loss tensor([0.2799, 0.2956, 0.2461, 0.2595])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5131, 0.5072, 0.0126, 0.3588, 0.7482]) \n",
      "Test Loss tensor([0.5180, 0.5135, 0.0140, 0.3487, 0.7345])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1432, 0.2237, 0.3698, 0.3944, 0.4661, 0.5463, 0.0192]) \n",
      "Test Loss tensor([0.1384, 0.2198, 0.3705, 0.3903, 0.4721, 0.5561, 0.0165])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2244, 0.2611, 0.2625, 0.2765, 0.4697, 0.5944, 0.0602]) \n",
      "Test Loss tensor([0.2217, 0.2585, 0.2621, 0.2786, 0.4736, 0.5991, 0.0625])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1092, 0.0586, 0.2342, 0.2574, 0.2160, 0.2036, 0.4496, 0.0070]) \n",
      "Test Loss tensor([0.1063, 0.0565, 0.2306, 0.2537, 0.2143, 0.1934, 0.4541, 0.0071])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4782, 0.5708, 0.0954, 0.2656, 0.4586, 0.2828])\n",
      "Valid Idx 3 | Loss tensor([0.2784, 0.6626, 0.2669, 0.5047, 0.5755])\n",
      "\n",
      "************** Batch 848 in 3.2491111755371094 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2756, 0.2972, 0.2460, 0.2696]) \n",
      "Test Loss tensor([0.2796, 0.2955, 0.2425, 0.2645])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5133, 0.5170, 0.0129, 0.3480, 0.7422]) \n",
      "Test Loss tensor([0.5144, 0.5144, 0.0135, 0.3475, 0.7375])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1387, 0.2197, 0.3595, 0.3864, 0.4771, 0.5547, 0.0187]) \n",
      "Test Loss tensor([0.1363, 0.2160, 0.3672, 0.3855, 0.4715, 0.5550, 0.0160])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2216, 0.2595, 0.2627, 0.2787, 0.4781, 0.6004, 0.0590]) \n",
      "Test Loss tensor([0.2179, 0.2554, 0.2555, 0.2776, 0.4754, 0.6014, 0.0602])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1063, 0.0599, 0.2298, 0.2544, 0.2143, 0.1961, 0.4440, 0.0070]) \n",
      "Test Loss tensor([0.1040, 0.0565, 0.2273, 0.2507, 0.2101, 0.1911, 0.4513, 0.0067])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4781, 0.5771, 0.0993, 0.2635, 0.4633, 0.2833])\n",
      "Valid Idx 3 | Loss tensor([0.2843, 0.6882, 0.2663, 0.5052, 0.5800])\n",
      "\n",
      "************** Batch 852 in 3.25020170211792 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2805, 0.2913, 0.2421, 0.2701]) \n",
      "Test Loss tensor([0.2793, 0.2920, 0.2410, 0.2630])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5146, 0.5133, 0.0141, 0.3505, 0.7312]) \n",
      "Test Loss tensor([0.5101, 0.5096, 0.0135, 0.3452, 0.7383])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1374, 0.2155, 0.3623, 0.3819, 0.4680, 0.5536, 0.0162]) \n",
      "Test Loss tensor([0.1322, 0.2119, 0.3636, 0.3821, 0.4672, 0.5514, 0.0146])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2186, 0.2633, 0.2578, 0.2772, 0.4706, 0.5957, 0.0598]) \n",
      "Test Loss tensor([0.2143, 0.2595, 0.2565, 0.2733, 0.4756, 0.6103, 0.0623])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1018, 0.0567, 0.2253, 0.2525, 0.2100, 0.1893, 0.4555, 0.0064]) \n",
      "Test Loss tensor([0.1028, 0.0559, 0.2233, 0.2483, 0.2062, 0.1892, 0.4559, 0.0066])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4792, 0.5882, 0.0941, 0.2593, 0.4615, 0.2778])\n",
      "Valid Idx 3 | Loss tensor([0.2868, 0.7207, 0.2621, 0.5047, 0.5876])\n",
      "\n",
      "************** Batch 856 in 3.2472569942474365 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2770, 0.2858, 0.2465, 0.2662]) \n",
      "Test Loss tensor([0.2785, 0.2908, 0.2383, 0.2619])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5067, 0.5073, 0.0120, 0.3475, 0.7384]) \n",
      "Test Loss tensor([0.5125, 0.5115, 0.0131, 0.3460, 0.7337])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1375, 0.2115, 0.3696, 0.3892, 0.4748, 0.5561, 0.0169]) \n",
      "Test Loss tensor([0.1358, 0.2081, 0.3636, 0.3825, 0.4683, 0.5439, 0.0153])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2127, 0.2535, 0.2540, 0.2742, 0.4777, 0.6072, 0.0595]) \n",
      "Test Loss tensor([0.2109, 0.2572, 0.2540, 0.2709, 0.4780, 0.6165, 0.0616])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1013, 0.0552, 0.2226, 0.2479, 0.2052, 0.1910, 0.4544, 0.0063]) \n",
      "Test Loss tensor([0.1023, 0.0563, 0.2208, 0.2454, 0.2025, 0.1901, 0.4567, 0.0066])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4822, 0.5998, 0.0955, 0.2566, 0.4653, 0.2769])\n",
      "Valid Idx 3 | Loss tensor([0.2869, 0.7507, 0.2624, 0.5057, 0.5934])\n",
      "\n",
      "************** Batch 860 in 3.2607502937316895 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2850, 0.2934, 0.2390, 0.2687]) \n",
      "Test Loss tensor([0.2769, 0.2932, 0.2367, 0.2652])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5232, 0.5125, 0.0115, 0.3426, 0.7337]) \n",
      "Test Loss tensor([0.5148, 0.5088, 0.0140, 0.3441, 0.7251])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1340, 0.2084, 0.3578, 0.3846, 0.4706, 0.5476, 0.0172]) \n",
      "Test Loss tensor([0.1331, 0.2045, 0.3594, 0.3776, 0.4698, 0.5394, 0.0152])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2110, 0.2497, 0.2499, 0.2684, 0.4813, 0.6189, 0.0547]) \n",
      "Test Loss tensor([0.2076, 0.2522, 0.2498, 0.2695, 0.4776, 0.6194, 0.0600])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1023, 0.0548, 0.2190, 0.2440, 0.2019, 0.1888, 0.4546, 0.0066]) \n",
      "Test Loss tensor([0.1032, 0.0566, 0.2168, 0.2427, 0.1985, 0.1893, 0.4581, 0.0068])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4852, 0.6158, 0.0975, 0.2536, 0.4644, 0.2757])\n",
      "Valid Idx 3 | Loss tensor([0.2872, 0.7810, 0.2611, 0.5000, 0.6010])\n",
      "Gradients: Input 0.06626133620738983 | Message 0.05909553915262222 | Update 0.09349757432937622 | Output 1.7174534797668457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 864 in 3.2693779468536377 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2782, 0.2917, 0.2356, 0.2663]) \n",
      "Test Loss tensor([0.2752, 0.2925, 0.2348, 0.2663])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5147, 0.5035, 0.0124, 0.3442, 0.7295]) \n",
      "Test Loss tensor([0.5139, 0.5077, 0.0136, 0.3452, 0.7226])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1377, 0.2050, 0.3616, 0.3745, 0.4661, 0.5401, 0.0155]) \n",
      "Test Loss tensor([0.1310, 0.2010, 0.3565, 0.3774, 0.4641, 0.5372, 0.0151])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2064, 0.2497, 0.2447, 0.2658, 0.4806, 0.6240, 0.0599]) \n",
      "Test Loss tensor([0.2040, 0.2571, 0.2484, 0.2650, 0.4813, 0.6262, 0.0608])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1029, 0.0560, 0.2190, 0.2428, 0.1976, 0.1828, 0.4643, 0.0067]) \n",
      "Test Loss tensor([0.1031, 0.0563, 0.2131, 0.2398, 0.1951, 0.1896, 0.4621, 0.0070])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4871, 0.6295, 0.0981, 0.2512, 0.4690, 0.2740])\n",
      "Valid Idx 3 | Loss tensor([0.2919, 0.8115, 0.2605, 0.4991, 0.5994])\n",
      "\n",
      "************** Batch 868 in 3.35539174079895 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2697, 0.3004, 0.2340, 0.2509]) \n",
      "Test Loss tensor([0.2729, 0.2915, 0.2334, 0.2662])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5195, 0.5112, 0.0144, 0.3481, 0.7059]) \n",
      "Test Loss tensor([0.5070, 0.5063, 0.0139, 0.3397, 0.7161])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1347, 0.2015, 0.3506, 0.3720, 0.4674, 0.5394, 0.0148]) \n",
      "Test Loss tensor([0.1338, 0.1972, 0.3544, 0.3715, 0.4615, 0.5348, 0.0152])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2042, 0.2518, 0.2495, 0.2658, 0.4896, 0.6215, 0.0604]) \n",
      "Test Loss tensor([0.2013, 0.2574, 0.2468, 0.2633, 0.4793, 0.6319, 0.0602])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1022, 0.0550, 0.2124, 0.2412, 0.1953, 0.1878, 0.4701, 0.0068]) \n",
      "Test Loss tensor([0.1031, 0.0566, 0.2099, 0.2382, 0.1916, 0.1869, 0.4627, 0.0070])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4877, 0.6382, 0.0959, 0.2486, 0.4731, 0.2722])\n",
      "Valid Idx 3 | Loss tensor([0.2983, 0.8438, 0.2621, 0.4933, 0.6080])\n",
      "\n",
      "************** Batch 872 in 3.340221881866455 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2759, 0.2794, 0.2345, 0.2683]) \n",
      "Test Loss tensor([0.2736, 0.2901, 0.2326, 0.2662])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5126, 0.5043, 0.0128, 0.3419, 0.7192]) \n",
      "Test Loss tensor([0.5056, 0.5034, 0.0137, 0.3386, 0.7116])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1351, 0.1966, 0.3497, 0.3687, 0.4566, 0.5420, 0.0153]) \n",
      "Test Loss tensor([0.1326, 0.1937, 0.3537, 0.3733, 0.4617, 0.5368, 0.0155])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.2018, 0.2624, 0.2484, 0.2659, 0.4816, 0.6285, 0.0649]) \n",
      "Test Loss tensor([0.1985, 0.2585, 0.2449, 0.2621, 0.4850, 0.6363, 0.0609])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1072, 0.0575, 0.2116, 0.2381, 0.1915, 0.1917, 0.4548, 0.0067]) \n",
      "Test Loss tensor([0.1025, 0.0572, 0.2073, 0.2355, 0.1877, 0.1853, 0.4635, 0.0071])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4918, 0.6472, 0.0970, 0.2452, 0.4733, 0.2713])\n",
      "Valid Idx 3 | Loss tensor([0.3093, 0.8746, 0.2637, 0.4913, 0.6143])\n",
      "\n",
      "************** Batch 876 in 3.207914352416992 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2080, 0.2052, 0.1802, 0.2144]) \n",
      "Test Loss tensor([0.2709, 0.2857, 0.2309, 0.2689])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3874, 0.3899, 0.0100, 0.2603, 0.5141]) \n",
      "Test Loss tensor([0.5041, 0.5059, 0.0139, 0.3385, 0.6978])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0990, 0.1446, 0.2713, 0.2862, 0.3412, 0.4004, 0.0113]) \n",
      "Test Loss tensor([0.1325, 0.1899, 0.3526, 0.3710, 0.4561, 0.5348, 0.0156])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1489, 0.1960, 0.1804, 0.2000, 0.3607, 0.4756, 0.0441]) \n",
      "Test Loss tensor([0.1958, 0.2575, 0.2452, 0.2608, 0.4824, 0.6387, 0.0617])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0761, 0.0430, 0.1542, 0.1766, 0.1411, 0.1413, 0.3478, 0.0053]) \n",
      "Test Loss tensor([0.1012, 0.0576, 0.2036, 0.2349, 0.1843, 0.1826, 0.4668, 0.0073])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4925, 0.6530, 0.0976, 0.2440, 0.4708, 0.2695])\n",
      "Valid Idx 3 | Loss tensor([0.3230, 0.9008, 0.2646, 0.4937, 0.6295])\n",
      "\n",
      "************** Batch 0 in 3.417825222015381 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2732, 0.2759, 0.2298, 0.2690]) \n",
      "Test Loss tensor([0.2719, 0.2881, 0.2286, 0.2782])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5139, 0.5117, 0.0149, 0.3401, 0.6992]) \n",
      "Test Loss tensor([0.5018, 0.5061, 0.0141, 0.3375, 0.6936])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1356, 0.1906, 0.3426, 0.3667, 0.4511, 0.5361, 0.0150]) \n",
      "Test Loss tensor([0.1272, 0.1863, 0.3513, 0.3696, 0.4543, 0.5371, 0.0149])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1977, 0.2592, 0.2461, 0.2586, 0.4846, 0.6403, 0.0606]) \n",
      "Test Loss tensor([0.1931, 0.2566, 0.2451, 0.2609, 0.4875, 0.6431, 0.0603])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0990, 0.0580, 0.2020, 0.2352, 0.1851, 0.1831, 0.4632, 0.0071]) \n",
      "Test Loss tensor([0.1006, 0.0576, 0.2000, 0.2322, 0.1811, 0.1794, 0.4675, 0.0073])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4930, 0.6602, 0.0962, 0.2401, 0.4735, 0.2677])\n",
      "Valid Idx 3 | Loss tensor([0.3397, 0.9202, 0.2680, 0.4894, 0.6354])\n",
      "Gradients: Input 0.04660322889685631 | Message 0.04777030646800995 | Update 0.08630945533514023 | Output 1.5497775077819824\n",
      "\n",
      "************** Batch 4 in 3.374492883682251 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2745, 0.2647, 0.2292, 0.2734]) \n",
      "Test Loss tensor([0.2693, 0.2904, 0.2270, 0.2736])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5091, 0.5047, 0.0127, 0.3348, 0.6846]) \n",
      "Test Loss tensor([0.5014, 0.5072, 0.0145, 0.3370, 0.6860])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1304, 0.1876, 0.3450, 0.3580, 0.4528, 0.5384, 0.0138]) \n",
      "Test Loss tensor([0.1283, 0.1833, 0.3491, 0.3699, 0.4523, 0.5358, 0.0154])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1910, 0.2559, 0.2451, 0.2574, 0.4882, 0.6473, 0.0592]) \n",
      "Test Loss tensor([0.1908, 0.2584, 0.2434, 0.2597, 0.4856, 0.6475, 0.0604])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1007, 0.0549, 0.2005, 0.2303, 0.1820, 0.1811, 0.4653, 0.0076]) \n",
      "Test Loss tensor([0.0994, 0.0572, 0.1977, 0.2314, 0.1774, 0.1776, 0.4668, 0.0074])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4977, 0.6663, 0.0978, 0.2383, 0.4754, 0.2707])\n",
      "Valid Idx 3 | Loss tensor([0.3522, 0.9397, 0.2678, 0.4890, 0.6484])\n",
      "\n",
      "************** Batch 8 in 3.3138046264648438 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2669, 0.2886, 0.2241, 0.2691]) \n",
      "Test Loss tensor([0.2690, 0.2885, 0.2263, 0.2797])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4944, 0.5085, 0.0135, 0.3351, 0.6897]) \n",
      "Test Loss tensor([0.5001, 0.5079, 0.0144, 0.3338, 0.6807])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1242, 0.1835, 0.3455, 0.3631, 0.4491, 0.5376, 0.0164]) \n",
      "Test Loss tensor([0.1282, 0.1800, 0.3486, 0.3722, 0.4495, 0.5346, 0.0158])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1887, 0.2556, 0.2454, 0.2621, 0.4858, 0.6520, 0.0635]) \n",
      "Test Loss tensor([0.1876, 0.2574, 0.2430, 0.2556, 0.4892, 0.6539, 0.0605])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0996, 0.0579, 0.1957, 0.2280, 0.1766, 0.1765, 0.4690, 0.0076]) \n",
      "Test Loss tensor([0.0989, 0.0584, 0.1940, 0.2286, 0.1743, 0.1765, 0.4714, 0.0078])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.4999, 0.6769, 0.0971, 0.2356, 0.4768, 0.2702])\n",
      "Valid Idx 3 | Loss tensor([0.3587, 0.9487, 0.2696, 0.4807, 0.6516])\n",
      "\n",
      "************** Batch 12 in 3.3027467727661133 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2660, 0.2946, 0.2287, 0.2832]) \n",
      "Test Loss tensor([0.2668, 0.2855, 0.2213, 0.2759])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4889, 0.5040, 0.0138, 0.3398, 0.6858]) \n",
      "Test Loss tensor([0.4976, 0.5071, 0.0151, 0.3347, 0.6719])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1249, 0.1790, 0.3502, 0.3674, 0.4518, 0.5366, 0.0160]) \n",
      "Test Loss tensor([0.1285, 0.1767, 0.3451, 0.3689, 0.4463, 0.5305, 0.0160])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1861, 0.2598, 0.2438, 0.2568, 0.4827, 0.6606, 0.0576]) \n",
      "Test Loss tensor([0.1852, 0.2579, 0.2419, 0.2531, 0.4855, 0.6591, 0.0622])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.1006, 0.0578, 0.1926, 0.2280, 0.1742, 0.1740, 0.4699, 0.0076]) \n",
      "Test Loss tensor([0.0998, 0.0589, 0.1909, 0.2265, 0.1712, 0.1779, 0.4740, 0.0080])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5012, 0.6872, 0.0970, 0.2329, 0.4796, 0.2688])\n",
      "Valid Idx 3 | Loss tensor([0.3577, 0.9550, 0.2710, 0.4810, 0.6551])\n",
      "\n",
      "************** Batch 16 in 3.3181560039520264 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2676, 0.2877, 0.2259, 0.2861]) \n",
      "Test Loss tensor([0.2638, 0.2889, 0.2201, 0.2772])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4947, 0.5016, 0.0153, 0.3356, 0.6799]) \n",
      "Test Loss tensor([0.4949, 0.5065, 0.0150, 0.3340, 0.6679])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1292, 0.1775, 0.3463, 0.3837, 0.4530, 0.5323, 0.0148]) \n",
      "Test Loss tensor([0.1270, 0.1733, 0.3410, 0.3678, 0.4490, 0.5327, 0.0160])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1840, 0.2594, 0.2392, 0.2535, 0.4847, 0.6474, 0.0613]) \n",
      "Test Loss tensor([0.1822, 0.2615, 0.2431, 0.2515, 0.4895, 0.6631, 0.0635])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0967, 0.0580, 0.1873, 0.2301, 0.1710, 0.1793, 0.4753, 0.0083]) \n",
      "Test Loss tensor([0.0991, 0.0600, 0.1886, 0.2235, 0.1679, 0.1767, 0.4756, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5038, 0.6993, 0.0987, 0.2291, 0.4843, 0.2698])\n",
      "Valid Idx 3 | Loss tensor([0.3600, 0.9613, 0.2690, 0.4756, 0.6591])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 20 in 3.2987489700317383 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2727, 0.2880, 0.2214, 0.2876]) \n",
      "Test Loss tensor([0.2641, 0.2840, 0.2202, 0.2786])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4868, 0.5099, 0.0161, 0.3318, 0.6636]) \n",
      "Test Loss tensor([0.4940, 0.5081, 0.0151, 0.3336, 0.6641])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1282, 0.1740, 0.3423, 0.3623, 0.4484, 0.5388, 0.0176]) \n",
      "Test Loss tensor([0.1286, 0.1703, 0.3423, 0.3689, 0.4497, 0.5282, 0.0160])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1830, 0.2575, 0.2468, 0.2581, 0.4919, 0.6630, 0.0588]) \n",
      "Test Loss tensor([0.1806, 0.2627, 0.2423, 0.2495, 0.4883, 0.6683, 0.0626])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0980, 0.0578, 0.1875, 0.2222, 0.1675, 0.1814, 0.4820, 0.0087]) \n",
      "Test Loss tensor([0.0989, 0.0603, 0.1854, 0.2227, 0.1643, 0.1747, 0.4753, 0.0085])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5047, 0.7015, 0.0995, 0.2279, 0.4844, 0.2690])\n",
      "Valid Idx 3 | Loss tensor([0.3624, 0.9659, 0.2672, 0.4728, 0.6631])\n",
      "Gradients: Input 0.02714015729725361 | Message 0.039335012435913086 | Update 0.09174831211566925 | Output 1.3829090595245361\n",
      "\n",
      "************** Batch 24 in 3.324259042739868 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2621, 0.2831, 0.2184, 0.2796]) \n",
      "Test Loss tensor([0.2609, 0.2895, 0.2191, 0.2757])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5054, 0.5078, 0.0156, 0.3356, 0.6541]) \n",
      "Test Loss tensor([0.4915, 0.5100, 0.0155, 0.3332, 0.6612])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1302, 0.1696, 0.3323, 0.3671, 0.4467, 0.5293, 0.0184]) \n",
      "Test Loss tensor([0.1297, 0.1671, 0.3382, 0.3684, 0.4450, 0.5287, 0.0172])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1801, 0.2626, 0.2427, 0.2533, 0.4947, 0.6750, 0.0624]) \n",
      "Test Loss tensor([0.1775, 0.2664, 0.2411, 0.2461, 0.4890, 0.6696, 0.0620])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0997, 0.0612, 0.1833, 0.2195, 0.1641, 0.1751, 0.4760, 0.0082]) \n",
      "Test Loss tensor([0.0960, 0.0610, 0.1828, 0.2196, 0.1613, 0.1728, 0.4810, 0.0088])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5072, 0.7110, 0.0976, 0.2261, 0.4882, 0.2681])\n",
      "Valid Idx 3 | Loss tensor([0.3690, 0.9717, 0.2666, 0.4723, 0.6650])\n",
      "\n",
      "************** Batch 28 in 3.3065366744995117 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2610, 0.2903, 0.2199, 0.2791]) \n",
      "Test Loss tensor([0.2629, 0.2855, 0.2190, 0.2826])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4877, 0.5065, 0.0157, 0.3263, 0.6601]) \n",
      "Test Loss tensor([0.4937, 0.5116, 0.0155, 0.3340, 0.6526])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1307, 0.1654, 0.3435, 0.3716, 0.4461, 0.5249, 0.0164]) \n",
      "Test Loss tensor([0.1262, 0.1641, 0.3338, 0.3636, 0.4461, 0.5264, 0.0175])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1793, 0.2717, 0.2427, 0.2502, 0.4939, 0.6798, 0.0621]) \n",
      "Test Loss tensor([0.1753, 0.2666, 0.2399, 0.2440, 0.4936, 0.6750, 0.0630])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0945, 0.0608, 0.1808, 0.2191, 0.1610, 0.1755, 0.4776, 0.0087]) \n",
      "Test Loss tensor([0.0946, 0.0606, 0.1800, 0.2167, 0.1585, 0.1700, 0.4822, 0.0091])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5073, 0.7123, 0.0981, 0.2221, 0.4886, 0.2682])\n",
      "Valid Idx 3 | Loss tensor([0.3713, 0.9729, 0.2649, 0.4680, 0.6630])\n",
      "\n",
      "************** Batch 32 in 3.3065831661224365 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2614, 0.2861, 0.2172, 0.2767]) \n",
      "Test Loss tensor([0.2592, 0.2871, 0.2148, 0.2799])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4963, 0.5114, 0.0146, 0.3382, 0.6502]) \n",
      "Test Loss tensor([0.4869, 0.5137, 0.0162, 0.3326, 0.6518])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1326, 0.1634, 0.3364, 0.3806, 0.4362, 0.5263, 0.0156]) \n",
      "Test Loss tensor([0.1295, 0.1604, 0.3347, 0.3641, 0.4456, 0.5280, 0.0169])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1740, 0.2670, 0.2432, 0.2453, 0.4918, 0.6739, 0.0618]) \n",
      "Test Loss tensor([0.1720, 0.2604, 0.2380, 0.2428, 0.4916, 0.6759, 0.0623])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0924, 0.0587, 0.1762, 0.2150, 0.1567, 0.1674, 0.4935, 0.0089]) \n",
      "Test Loss tensor([0.0920, 0.0614, 0.1774, 0.2148, 0.1550, 0.1679, 0.4843, 0.0092])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5070, 0.7164, 0.0994, 0.2199, 0.4909, 0.2679])\n",
      "Valid Idx 3 | Loss tensor([0.3715, 0.9719, 0.2622, 0.4687, 0.6697])\n",
      "\n",
      "************** Batch 36 in 3.30945086479187 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2575, 0.2899, 0.2140, 0.2751]) \n",
      "Test Loss tensor([0.2613, 0.2818, 0.2137, 0.2801])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4790, 0.5085, 0.0164, 0.3247, 0.6557]) \n",
      "Test Loss tensor([0.4915, 0.5164, 0.0159, 0.3324, 0.6518])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1228, 0.1602, 0.3306, 0.3563, 0.4408, 0.5200, 0.0169]) \n",
      "Test Loss tensor([0.1259, 0.1572, 0.3353, 0.3672, 0.4451, 0.5294, 0.0170])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1731, 0.2637, 0.2376, 0.2427, 0.4982, 0.6769, 0.0634]) \n",
      "Test Loss tensor([0.1698, 0.2624, 0.2370, 0.2406, 0.4901, 0.6838, 0.0634])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0949, 0.0606, 0.1789, 0.2134, 0.1531, 0.1727, 0.4733, 0.0092]) \n",
      "Test Loss tensor([0.0906, 0.0611, 0.1731, 0.2139, 0.1512, 0.1661, 0.4899, 0.0092])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5096, 0.7174, 0.0993, 0.2184, 0.4978, 0.2663])\n",
      "Valid Idx 3 | Loss tensor([0.3744, 0.9740, 0.2607, 0.4669, 0.6693])\n",
      "\n",
      "************** Batch 40 in 3.322805643081665 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2581, 0.2985, 0.2086, 0.2703]) \n",
      "Test Loss tensor([0.2605, 0.2842, 0.2137, 0.2777])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4847, 0.5082, 0.0174, 0.3202, 0.6672]) \n",
      "Test Loss tensor([0.4902, 0.5178, 0.0165, 0.3309, 0.6514])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1245, 0.1581, 0.3309, 0.3681, 0.4466, 0.5366, 0.0163]) \n",
      "Test Loss tensor([0.1265, 0.1541, 0.3333, 0.3644, 0.4411, 0.5278, 0.0170])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1688, 0.2610, 0.2349, 0.2443, 0.4944, 0.6717, 0.0630]) \n",
      "Test Loss tensor([0.1669, 0.2649, 0.2342, 0.2390, 0.4943, 0.6852, 0.0629])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0914, 0.0619, 0.1745, 0.2146, 0.1516, 0.1676, 0.4833, 0.0093]) \n",
      "Test Loss tensor([0.0877, 0.0610, 0.1708, 0.2103, 0.1480, 0.1642, 0.4880, 0.0094])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5083, 0.7208, 0.0994, 0.2149, 0.4942, 0.2659])\n",
      "Valid Idx 3 | Loss tensor([0.3749, 0.9749, 0.2545, 0.4689, 0.6701])\n",
      "Gradients: Input 0.028761889785528183 | Message 0.035549595952034 | Update 0.09386736154556274 | Output 1.2790919542312622\n",
      "\n",
      "************** Batch 44 in 3.3181726932525635 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2610, 0.2814, 0.2138, 0.2795]) \n",
      "Test Loss tensor([0.2605, 0.2831, 0.2134, 0.2776])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4818, 0.5158, 0.0169, 0.3311, 0.6492]) \n",
      "Test Loss tensor([0.4858, 0.5168, 0.0163, 0.3289, 0.6587])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1318, 0.1538, 0.3357, 0.3575, 0.4340, 0.5221, 0.0174]) \n",
      "Test Loss tensor([0.1242, 0.1510, 0.3326, 0.3617, 0.4462, 0.5285, 0.0173])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1650, 0.2682, 0.2379, 0.2387, 0.5001, 0.6744, 0.0631]) \n",
      "Test Loss tensor([0.1638, 0.2638, 0.2313, 0.2371, 0.4954, 0.6880, 0.0630])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0875, 0.0609, 0.1689, 0.2099, 0.1487, 0.1674, 0.4807, 0.0098]) \n",
      "Test Loss tensor([0.0844, 0.0606, 0.1675, 0.2069, 0.1443, 0.1613, 0.4918, 0.0095])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5157, 0.7202, 0.0968, 0.2127, 0.4969, 0.2617])\n",
      "Valid Idx 3 | Loss tensor([0.3837, 0.9745, 0.2574, 0.4654, 0.6684])\n",
      "\n",
      "************** Batch 48 in 3.3175768852233887 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2627, 0.2715, 0.2128, 0.2869]) \n",
      "Test Loss tensor([0.2572, 0.2806, 0.2099, 0.2719])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4898, 0.5071, 0.0165, 0.3215, 0.6632]) \n",
      "Test Loss tensor([0.4830, 0.5166, 0.0162, 0.3275, 0.6557])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1241, 0.1504, 0.3251, 0.3553, 0.4418, 0.5310, 0.0166]) \n",
      "Test Loss tensor([0.1245, 0.1474, 0.3324, 0.3657, 0.4434, 0.5300, 0.0176])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1674, 0.2663, 0.2344, 0.2373, 0.4981, 0.6894, 0.0602]) \n",
      "Test Loss tensor([0.1620, 0.2657, 0.2316, 0.2356, 0.4997, 0.6923, 0.0636])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0839, 0.0615, 0.1667, 0.2057, 0.1441, 0.1612, 0.4873, 0.0095]) \n",
      "Test Loss tensor([0.0818, 0.0605, 0.1647, 0.2039, 0.1412, 0.1570, 0.4947, 0.0093])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5130, 0.7180, 0.0994, 0.2097, 0.4984, 0.2610])\n",
      "Valid Idx 3 | Loss tensor([0.3889, 0.9747, 0.2538, 0.4726, 0.6707])\n",
      "\n",
      "************** Batch 52 in 3.3082895278930664 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2534, 0.2769, 0.2126, 0.2762]) \n",
      "Test Loss tensor([0.2567, 0.2825, 0.2086, 0.2722])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4833, 0.5153, 0.0150, 0.3266, 0.6576]) \n",
      "Test Loss tensor([0.4860, 0.5213, 0.0161, 0.3287, 0.6608])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1276, 0.1466, 0.3358, 0.3539, 0.4388, 0.5296, 0.0186]) \n",
      "Test Loss tensor([0.1222, 0.1440, 0.3281, 0.3564, 0.4429, 0.5269, 0.0170])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1616, 0.2646, 0.2347, 0.2351, 0.5050, 0.6873, 0.0642]) \n",
      "Test Loss tensor([0.1592, 0.2679, 0.2268, 0.2327, 0.5014, 0.6943, 0.0628])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0825, 0.0610, 0.1673, 0.2065, 0.1421, 0.1579, 0.4875, 0.0091]) \n",
      "Test Loss tensor([0.0801, 0.0602, 0.1620, 0.2019, 0.1382, 0.1546, 0.4955, 0.0092])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5142, 0.7216, 0.1007, 0.2063, 0.5023, 0.2614])\n",
      "Valid Idx 3 | Loss tensor([0.3894, 0.9741, 0.2513, 0.4692, 0.6695])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 56 in 3.326828956604004 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2675, 0.2692, 0.2106, 0.2742]) \n",
      "Test Loss tensor([0.2569, 0.2852, 0.2074, 0.2703])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4829, 0.5337, 0.0164, 0.3314, 0.6425]) \n",
      "Test Loss tensor([0.4874, 0.5236, 0.0165, 0.3265, 0.6582])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1319, 0.1436, 0.3190, 0.3643, 0.4403, 0.5220, 0.0203]) \n",
      "Test Loss tensor([0.1218, 0.1409, 0.3302, 0.3635, 0.4435, 0.5290, 0.0168])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1657, 0.2734, 0.2270, 0.2362, 0.5005, 0.6865, 0.0639]) \n",
      "Test Loss tensor([0.1557, 0.2659, 0.2233, 0.2306, 0.5011, 0.6951, 0.0634])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0780, 0.0597, 0.1623, 0.2038, 0.1391, 0.1566, 0.4850, 0.0089]) \n",
      "Test Loss tensor([0.0769, 0.0594, 0.1585, 0.1993, 0.1352, 0.1535, 0.4965, 0.0091])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5169, 0.7264, 0.0988, 0.2050, 0.5040, 0.2569])\n",
      "Valid Idx 3 | Loss tensor([0.3828, 0.9726, 0.2455, 0.4711, 0.6734])\n",
      "\n",
      "************** Batch 60 in 3.3198354244232178 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2587, 0.2800, 0.2136, 0.2795]) \n",
      "Test Loss tensor([0.2598, 0.2821, 0.2090, 0.2725])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4771, 0.5230, 0.0159, 0.3237, 0.6600]) \n",
      "Test Loss tensor([0.4841, 0.5243, 0.0163, 0.3284, 0.6613])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1154, 0.1412, 0.3305, 0.3514, 0.4418, 0.5289, 0.0179]) \n",
      "Test Loss tensor([0.1236, 0.1374, 0.3303, 0.3627, 0.4444, 0.5301, 0.0175])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1567, 0.2720, 0.2245, 0.2300, 0.4973, 0.6963, 0.0590]) \n",
      "Test Loss tensor([0.1529, 0.2672, 0.2187, 0.2278, 0.5009, 0.7019, 0.0620])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0783, 0.0623, 0.1617, 0.1952, 0.1349, 0.1532, 0.5012, 0.0096]) \n",
      "Test Loss tensor([0.0757, 0.0598, 0.1559, 0.1946, 0.1317, 0.1500, 0.4996, 0.0091])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5171, 0.7244, 0.0995, 0.2021, 0.5063, 0.2537])\n",
      "Valid Idx 3 | Loss tensor([0.3738, 0.9701, 0.2432, 0.4716, 0.6660])\n",
      "Gradients: Input 0.025619708001613617 | Message 0.030084049329161644 | Update 0.08183363080024719 | Output 1.0288667678833008\n",
      "\n",
      "************** Batch 64 in 3.3525283336639404 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2505, 0.2865, 0.2063, 0.2688]) \n",
      "Test Loss tensor([0.2541, 0.2810, 0.2053, 0.2678])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4825, 0.5230, 0.0143, 0.3234, 0.6544]) \n",
      "Test Loss tensor([0.4850, 0.5246, 0.0164, 0.3284, 0.6624])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1186, 0.1367, 0.3282, 0.3488, 0.4416, 0.5248, 0.0174]) \n",
      "Test Loss tensor([0.1246, 0.1341, 0.3307, 0.3622, 0.4468, 0.5314, 0.0175])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1540, 0.2614, 0.2147, 0.2269, 0.4914, 0.7126, 0.0642]) \n",
      "Test Loss tensor([0.1516, 0.2689, 0.2149, 0.2267, 0.5055, 0.7074, 0.0623])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0755, 0.0621, 0.1548, 0.1997, 0.1325, 0.1569, 0.5067, 0.0096]) \n",
      "Test Loss tensor([0.0735, 0.0594, 0.1527, 0.1913, 0.1286, 0.1487, 0.5026, 0.0090])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5195, 0.7288, 0.1002, 0.2016, 0.5084, 0.2532])\n",
      "Valid Idx 3 | Loss tensor([0.3716, 0.9671, 0.2405, 0.4690, 0.6672])\n",
      "\n",
      "************** Batch 68 in 3.3609659671783447 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2613, 0.2745, 0.2095, 0.2816]) \n",
      "Test Loss tensor([0.2537, 0.2806, 0.2039, 0.2633])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4977, 0.5380, 0.0193, 0.3347, 0.6511]) \n",
      "Test Loss tensor([0.4821, 0.5269, 0.0159, 0.3271, 0.6632])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1298, 0.1349, 0.3370, 0.3688, 0.4440, 0.5341, 0.0185]) \n",
      "Test Loss tensor([0.1223, 0.1312, 0.3260, 0.3556, 0.4464, 0.5309, 0.0173])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1524, 0.2689, 0.2149, 0.2307, 0.5085, 0.7025, 0.0634]) \n",
      "Test Loss tensor([0.1477, 0.2665, 0.2120, 0.2245, 0.5068, 0.7089, 0.0616])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0723, 0.0584, 0.1510, 0.1877, 0.1278, 0.1453, 0.5099, 0.0087]) \n",
      "Test Loss tensor([0.0712, 0.0593, 0.1487, 0.1886, 0.1253, 0.1452, 0.5066, 0.0090])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5191, 0.7257, 0.0988, 0.1992, 0.5111, 0.2522])\n",
      "Valid Idx 3 | Loss tensor([0.3767, 0.9701, 0.2370, 0.4776, 0.6710])\n",
      "\n",
      "************** Batch 72 in 3.371919870376587 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2474, 0.2829, 0.1992, 0.2689]) \n",
      "Test Loss tensor([0.2528, 0.2802, 0.2033, 0.2648])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4843, 0.5202, 0.0164, 0.3231, 0.6673]) \n",
      "Test Loss tensor([0.4815, 0.5273, 0.0165, 0.3260, 0.6701])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1187, 0.1321, 0.3176, 0.3495, 0.4491, 0.5265, 0.0202]) \n",
      "Test Loss tensor([0.1235, 0.1281, 0.3269, 0.3610, 0.4489, 0.5330, 0.0173])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1494, 0.2823, 0.2217, 0.2343, 0.5009, 0.7113, 0.0655]) \n",
      "Test Loss tensor([0.1454, 0.2663, 0.2095, 0.2232, 0.5052, 0.7113, 0.0624])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0762, 0.0565, 0.1530, 0.1849, 0.1258, 0.1402, 0.5167, 0.0087]) \n",
      "Test Loss tensor([0.0686, 0.0587, 0.1461, 0.1851, 0.1223, 0.1416, 0.5086, 0.0088])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5224, 0.7256, 0.1000, 0.1958, 0.5129, 0.2494])\n",
      "Valid Idx 3 | Loss tensor([0.3820, 0.9684, 0.2383, 0.4740, 0.6699])\n",
      "\n",
      "************** Batch 76 in 3.378398895263672 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2539, 0.2876, 0.2063, 0.2636]) \n",
      "Test Loss tensor([0.2512, 0.2848, 0.2022, 0.2616])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4884, 0.5409, 0.0155, 0.3322, 0.6554]) \n",
      "Test Loss tensor([0.4910, 0.5330, 0.0162, 0.3296, 0.6672])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1189, 0.1282, 0.3194, 0.3525, 0.4409, 0.5400, 0.0175]) \n",
      "Test Loss tensor([0.1235, 0.1246, 0.3272, 0.3577, 0.4435, 0.5367, 0.0171])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1446, 0.2667, 0.2114, 0.2190, 0.5072, 0.7035, 0.0603]) \n",
      "Test Loss tensor([0.1434, 0.2681, 0.2058, 0.2222, 0.5083, 0.7111, 0.0621])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0674, 0.0546, 0.1465, 0.1842, 0.1224, 0.1388, 0.5168, 0.0093]) \n",
      "Test Loss tensor([0.0671, 0.0567, 0.1434, 0.1832, 0.1194, 0.1376, 0.5118, 0.0087])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5198, 0.7248, 0.1009, 0.1933, 0.5178, 0.2439])\n",
      "Valid Idx 3 | Loss tensor([0.3953, 0.9710, 0.2376, 0.4726, 0.6691])\n",
      "\n",
      "************** Batch 80 in 3.3837473392486572 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2411, 0.2817, 0.1947, 0.2520]) \n",
      "Test Loss tensor([0.2539, 0.2840, 0.2047, 0.2633])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4814, 0.5276, 0.0166, 0.3186, 0.6794]) \n",
      "Test Loss tensor([0.4838, 0.5352, 0.0162, 0.3234, 0.6646])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1246, 0.1241, 0.3357, 0.3699, 0.4536, 0.5405, 0.0174]) \n",
      "Test Loss tensor([0.1217, 0.1216, 0.3271, 0.3584, 0.4469, 0.5381, 0.0174])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1440, 0.2705, 0.2081, 0.2209, 0.5126, 0.7190, 0.0605]) \n",
      "Test Loss tensor([0.1402, 0.2653, 0.2027, 0.2193, 0.5121, 0.7158, 0.0610])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0667, 0.0596, 0.1439, 0.1818, 0.1202, 0.1393, 0.5053, 0.0087]) \n",
      "Test Loss tensor([0.0658, 0.0566, 0.1399, 0.1803, 0.1163, 0.1355, 0.5141, 0.0087])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5215, 0.7240, 0.0988, 0.1918, 0.5167, 0.2443])\n",
      "Valid Idx 3 | Loss tensor([0.4029, 0.9711, 0.2363, 0.4773, 0.6723])\n",
      "Gradients: Input 0.027596941217780113 | Message 0.023084517568349838 | Update 0.06876999139785767 | Output 0.9211375117301941\n",
      "\n",
      "************** Batch 84 in 3.397991180419922 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2522, 0.2773, 0.2113, 0.2731]) \n",
      "Test Loss tensor([0.2522, 0.2820, 0.1991, 0.2605])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4787, 0.5252, 0.0152, 0.3161, 0.6696]) \n",
      "Test Loss tensor([0.4827, 0.5378, 0.0163, 0.3261, 0.6644])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1310, 0.1219, 0.3298, 0.3653, 0.4541, 0.5421, 0.0179]) \n",
      "Test Loss tensor([0.1205, 0.1187, 0.3286, 0.3576, 0.4432, 0.5398, 0.0167])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1394, 0.2588, 0.2094, 0.2223, 0.5159, 0.7138, 0.0567]) \n",
      "Test Loss tensor([0.1385, 0.2690, 0.2013, 0.2189, 0.5097, 0.7202, 0.0627])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0664, 0.0584, 0.1424, 0.1793, 0.1165, 0.1303, 0.5190, 0.0087]) \n",
      "Test Loss tensor([0.0645, 0.0559, 0.1388, 0.1776, 0.1139, 0.1317, 0.5154, 0.0087])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5207, 0.7253, 0.1003, 0.1882, 0.5192, 0.2421])\n",
      "Valid Idx 3 | Loss tensor([0.4112, 0.9725, 0.2376, 0.4772, 0.6730])\n",
      "\n",
      "************** Batch 88 in 3.3593881130218506 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2540, 0.2654, 0.1993, 0.2687]) \n",
      "Test Loss tensor([0.2523, 0.2846, 0.2018, 0.2620])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4778, 0.5326, 0.0156, 0.3202, 0.6628]) \n",
      "Test Loss tensor([0.4878, 0.5345, 0.0163, 0.3250, 0.6688])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1226, 0.1186, 0.3284, 0.3592, 0.4459, 0.5385, 0.0160]) \n",
      "Test Loss tensor([0.1241, 0.1156, 0.3266, 0.3574, 0.4438, 0.5405, 0.0179])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1376, 0.2729, 0.2003, 0.2232, 0.5091, 0.7159, 0.0627]) \n",
      "Test Loss tensor([0.1360, 0.2678, 0.1983, 0.2168, 0.5119, 0.7224, 0.0621])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0631, 0.0537, 0.1357, 0.1716, 0.1137, 0.1337, 0.5189, 0.0085]) \n",
      "Test Loss tensor([0.0652, 0.0558, 0.1357, 0.1741, 0.1112, 0.1330, 0.5215, 0.0087])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5247, 0.7328, 0.0990, 0.1862, 0.5215, 0.2429])\n",
      "Valid Idx 3 | Loss tensor([0.4112, 0.9712, 0.2347, 0.4806, 0.6735])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 92 in 3.3616232872009277 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2476, 0.2817, 0.1999, 0.2703]) \n",
      "Test Loss tensor([0.2487, 0.2797, 0.1962, 0.2576])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4794, 0.5478, 0.0154, 0.3307, 0.6575]) \n",
      "Test Loss tensor([0.4861, 0.5385, 0.0165, 0.3252, 0.6640])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1153, 0.1162, 0.3216, 0.3518, 0.4332, 0.5249, 0.0159]) \n",
      "Test Loss tensor([0.1205, 0.1130, 0.3253, 0.3564, 0.4477, 0.5470, 0.0177])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1375, 0.2772, 0.1940, 0.2074, 0.5042, 0.7239, 0.0575]) \n",
      "Test Loss tensor([0.1331, 0.2703, 0.1944, 0.2163, 0.5134, 0.7292, 0.0618])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0621, 0.0567, 0.1345, 0.1758, 0.1111, 0.1330, 0.5320, 0.0084]) \n",
      "Test Loss tensor([0.0646, 0.0560, 0.1322, 0.1709, 0.1081, 0.1313, 0.5213, 0.0089])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5288, 0.7399, 0.1002, 0.1833, 0.5207, 0.2407])\n",
      "Valid Idx 3 | Loss tensor([0.4068, 0.9712, 0.2359, 0.4716, 0.6753])\n",
      "\n",
      "************** Batch 96 in 3.382300853729248 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2410, 0.2876, 0.1961, 0.2551]) \n",
      "Test Loss tensor([0.2515, 0.2816, 0.1986, 0.2646])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.5031, 0.5502, 0.0156, 0.3303, 0.6470]) \n",
      "Test Loss tensor([0.4832, 0.5392, 0.0168, 0.3262, 0.6640])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1215, 0.1130, 0.3149, 0.3440, 0.4444, 0.5488, 0.0171]) \n",
      "Test Loss tensor([0.1209, 0.1104, 0.3245, 0.3581, 0.4432, 0.5415, 0.0174])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1336, 0.2722, 0.1973, 0.2190, 0.5176, 0.7243, 0.0633]) \n",
      "Test Loss tensor([0.1303, 0.2709, 0.1927, 0.2125, 0.5130, 0.7317, 0.0604])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0670, 0.0559, 0.1310, 0.1715, 0.1086, 0.1349, 0.5059, 0.0089]) \n",
      "Test Loss tensor([0.0640, 0.0568, 0.1301, 0.1692, 0.1057, 0.1304, 0.5209, 0.0090])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5239, 0.7455, 0.0984, 0.1814, 0.5266, 0.2397])\n",
      "Valid Idx 3 | Loss tensor([0.4065, 0.9717, 0.2309, 0.4755, 0.6778])\n",
      "\n",
      "************** Batch 100 in 3.382830858230591 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2591, 0.2758, 0.1957, 0.2680]) \n",
      "Test Loss tensor([0.2488, 0.2808, 0.1957, 0.2602])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4915, 0.5269, 0.0157, 0.3266, 0.6710]) \n",
      "Test Loss tensor([0.4821, 0.5402, 0.0163, 0.3253, 0.6613])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1208, 0.1103, 0.3233, 0.3536, 0.4364, 0.5441, 0.0188]) \n",
      "Test Loss tensor([0.1201, 0.1074, 0.3252, 0.3589, 0.4419, 0.5436, 0.0185])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1305, 0.2615, 0.1928, 0.2155, 0.5060, 0.7239, 0.0630]) \n",
      "Test Loss tensor([0.1274, 0.2684, 0.1900, 0.2117, 0.5172, 0.7377, 0.0612])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0620, 0.0576, 0.1300, 0.1703, 0.1058, 0.1301, 0.5266, 0.0089]) \n",
      "Test Loss tensor([0.0646, 0.0560, 0.1270, 0.1667, 0.1033, 0.1292, 0.5258, 0.0092])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5219, 0.7521, 0.0990, 0.1801, 0.5282, 0.2339])\n",
      "Valid Idx 3 | Loss tensor([0.4096, 0.9709, 0.2333, 0.4729, 0.6740])\n",
      "Gradients: Input 0.028968151658773422 | Message 0.024131782352924347 | Update 0.057834990322589874 | Output 0.6976423859596252\n",
      "\n",
      "************** Batch 104 in 3.3690319061279297 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2493, 0.2804, 0.1956, 0.2682]) \n",
      "Test Loss tensor([0.2434, 0.2817, 0.1965, 0.2605])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4958, 0.5322, 0.0173, 0.3238, 0.6692]) \n",
      "Test Loss tensor([0.4863, 0.5391, 0.0166, 0.3264, 0.6600])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1229, 0.1081, 0.3322, 0.3761, 0.4370, 0.5532, 0.0176]) \n",
      "Test Loss tensor([0.1234, 0.1050, 0.3248, 0.3564, 0.4417, 0.5431, 0.0183])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1298, 0.2824, 0.1866, 0.2150, 0.5233, 0.7448, 0.0624]) \n",
      "Test Loss tensor([0.1260, 0.2703, 0.1873, 0.2079, 0.5132, 0.7447, 0.0616])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0647, 0.0577, 0.1286, 0.1671, 0.1027, 0.1302, 0.5200, 0.0089]) \n",
      "Test Loss tensor([0.0645, 0.0569, 0.1239, 0.1640, 0.1005, 0.1284, 0.5276, 0.0093])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5233, 0.7558, 0.1001, 0.1791, 0.5263, 0.2352])\n",
      "Valid Idx 3 | Loss tensor([0.4189, 0.9718, 0.2348, 0.4762, 0.6782])\n",
      "\n",
      "************** Batch 108 in 3.3728442192077637 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2399, 0.2860, 0.1997, 0.2652]) \n",
      "Test Loss tensor([0.2455, 0.2785, 0.1958, 0.2621])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4790, 0.5350, 0.0166, 0.3261, 0.6660]) \n",
      "Test Loss tensor([0.4849, 0.5444, 0.0172, 0.3248, 0.6554])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1161, 0.1043, 0.3292, 0.3533, 0.4406, 0.5531, 0.0187]) \n",
      "Test Loss tensor([0.1228, 0.1025, 0.3241, 0.3574, 0.4404, 0.5431, 0.0184])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1251, 0.2632, 0.1884, 0.2063, 0.5060, 0.7346, 0.0668]) \n",
      "Test Loss tensor([0.1230, 0.2689, 0.1853, 0.2092, 0.5216, 0.7449, 0.0628])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0618, 0.0574, 0.1229, 0.1650, 0.1012, 0.1263, 0.5261, 0.0099]) \n",
      "Test Loss tensor([0.0641, 0.0559, 0.1221, 0.1616, 0.0984, 0.1252, 0.5302, 0.0094])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5254, 0.7581, 0.1011, 0.1780, 0.5306, 0.2358])\n",
      "Valid Idx 3 | Loss tensor([0.4335, 0.9743, 0.2382, 0.4702, 0.6777])\n",
      "\n",
      "************** Batch 112 in 3.3772213459014893 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2512, 0.2766, 0.1975, 0.2600]) \n",
      "Test Loss tensor([0.2449, 0.2804, 0.1956, 0.2610])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4802, 0.5417, 0.0159, 0.3177, 0.6625]) \n",
      "Test Loss tensor([0.4816, 0.5456, 0.0172, 0.3266, 0.6518])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1247, 0.1024, 0.3219, 0.3523, 0.4405, 0.5455, 0.0188]) \n",
      "Test Loss tensor([0.1207, 0.0998, 0.3225, 0.3577, 0.4367, 0.5480, 0.0186])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1260, 0.2707, 0.1863, 0.2069, 0.5222, 0.7441, 0.0623]) \n",
      "Test Loss tensor([0.1205, 0.2710, 0.1823, 0.2074, 0.5181, 0.7490, 0.0619])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0629, 0.0583, 0.1220, 0.1616, 0.0976, 0.1245, 0.5241, 0.0098]) \n",
      "Test Loss tensor([0.0635, 0.0560, 0.1203, 0.1596, 0.0959, 0.1240, 0.5287, 0.0095])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5221, 0.7574, 0.1005, 0.1760, 0.5334, 0.2330])\n",
      "Valid Idx 3 | Loss tensor([0.4509, 0.9767, 0.2403, 0.4723, 0.6792])\n",
      "\n",
      "************** Batch 116 in 3.3791050910949707 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2465, 0.2820, 0.1922, 0.2615]) \n",
      "Test Loss tensor([0.2453, 0.2779, 0.1946, 0.2617])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4767, 0.5377, 0.0158, 0.3199, 0.6554]) \n",
      "Test Loss tensor([0.4815, 0.5431, 0.0170, 0.3212, 0.6565])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1191, 0.0995, 0.3264, 0.3684, 0.4359, 0.5596, 0.0181]) \n",
      "Test Loss tensor([0.1193, 0.0977, 0.3194, 0.3507, 0.4360, 0.5513, 0.0175])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1217, 0.2661, 0.1856, 0.2019, 0.5115, 0.7368, 0.0656]) \n",
      "Test Loss tensor([0.1199, 0.2712, 0.1838, 0.2054, 0.5194, 0.7492, 0.0626])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0642, 0.0570, 0.1181, 0.1559, 0.0968, 0.1269, 0.5240, 0.0097]) \n",
      "Test Loss tensor([0.0640, 0.0553, 0.1174, 0.1570, 0.0947, 0.1230, 0.5355, 0.0095])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5276, 0.7599, 0.1002, 0.1740, 0.5316, 0.2340])\n",
      "Valid Idx 3 | Loss tensor([0.4704, 0.9787, 0.2448, 0.4706, 0.6802])\n",
      "\n",
      "************** Batch 120 in 3.361898183822632 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2373, 0.2754, 0.1935, 0.2614]) \n",
      "Test Loss tensor([0.2437, 0.2774, 0.1933, 0.2614])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4856, 0.5397, 0.0151, 0.3220, 0.6513]) \n",
      "Test Loss tensor([0.4822, 0.5492, 0.0173, 0.3254, 0.6477])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1190, 0.0975, 0.3217, 0.3637, 0.4383, 0.5643, 0.0204]) \n",
      "Test Loss tensor([0.1214, 0.0955, 0.3221, 0.3529, 0.4335, 0.5519, 0.0178])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1220, 0.2666, 0.1806, 0.2064, 0.5157, 0.7566, 0.0661]) \n",
      "Test Loss tensor([0.1177, 0.2691, 0.1810, 0.2058, 0.5169, 0.7517, 0.0604])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0626, 0.0576, 0.1153, 0.1550, 0.0935, 0.1197, 0.5328, 0.0097]) \n",
      "Test Loss tensor([0.0640, 0.0553, 0.1155, 0.1553, 0.0917, 0.1208, 0.5376, 0.0096])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5270, 0.7610, 0.1000, 0.1736, 0.5307, 0.2327])\n",
      "Valid Idx 3 | Loss tensor([0.4835, 0.9818, 0.2473, 0.4739, 0.6887])\n",
      "Gradients: Input 0.023832829669117928 | Message 0.02023755945265293 | Update 0.04954245686531067 | Output 0.5272647142410278\n",
      "\n",
      "************** Batch 124 in 3.3890902996063232 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2400, 0.2697, 0.1946, 0.2570]) \n",
      "Test Loss tensor([0.2438, 0.2814, 0.1937, 0.2633])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4856, 0.5581, 0.0176, 0.3400, 0.6462]) \n",
      "Test Loss tensor([0.4794, 0.5468, 0.0177, 0.3224, 0.6516])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1202, 0.0956, 0.3232, 0.3458, 0.4228, 0.5554, 0.0212]) \n",
      "Test Loss tensor([0.1212, 0.0931, 0.3206, 0.3556, 0.4325, 0.5539, 0.0186])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1159, 0.2619, 0.1743, 0.2103, 0.5198, 0.7476, 0.0588]) \n",
      "Test Loss tensor([0.1152, 0.2710, 0.1776, 0.2032, 0.5211, 0.7539, 0.0619])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0632, 0.0570, 0.1176, 0.1527, 0.0916, 0.1218, 0.5282, 0.0096]) \n",
      "Test Loss tensor([0.0632, 0.0554, 0.1133, 0.1536, 0.0903, 0.1210, 0.5409, 0.0100])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5262, 0.7664, 0.1014, 0.1722, 0.5361, 0.2328])\n",
      "Valid Idx 3 | Loss tensor([0.4854, 0.9825, 0.2482, 0.4704, 0.6832])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 128 in 3.4033641815185547 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2358, 0.2737, 0.1988, 0.2634]) \n",
      "Test Loss tensor([0.2434, 0.2794, 0.1919, 0.2633])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4925, 0.5520, 0.0163, 0.3164, 0.6523]) \n",
      "Test Loss tensor([0.4818, 0.5491, 0.0178, 0.3243, 0.6468])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1151, 0.0933, 0.3255, 0.3615, 0.4316, 0.5584, 0.0190]) \n",
      "Test Loss tensor([0.1195, 0.0912, 0.3186, 0.3547, 0.4310, 0.5515, 0.0193])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1149, 0.2688, 0.1915, 0.2088, 0.5243, 0.7552, 0.0657]) \n",
      "Test Loss tensor([0.1143, 0.2742, 0.1781, 0.2035, 0.5189, 0.7588, 0.0630])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0609, 0.0540, 0.1128, 0.1555, 0.0890, 0.1219, 0.5375, 0.0099]) \n",
      "Test Loss tensor([0.0644, 0.0566, 0.1117, 0.1506, 0.0884, 0.1204, 0.5409, 0.0103])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5290, 0.7763, 0.1004, 0.1690, 0.5355, 0.2296])\n",
      "Valid Idx 3 | Loss tensor([0.4778, 0.9819, 0.2477, 0.4745, 0.6882])\n",
      "\n",
      "************** Batch 132 in 3.3581604957580566 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2440, 0.2740, 0.1990, 0.2653]) \n",
      "Test Loss tensor([0.2397, 0.2808, 0.1944, 0.2615])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4836, 0.5513, 0.0191, 0.3171, 0.6381]) \n",
      "Test Loss tensor([0.4795, 0.5527, 0.0185, 0.3268, 0.6429])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1223, 0.0917, 0.3204, 0.3600, 0.4394, 0.5506, 0.0184]) \n",
      "Test Loss tensor([0.1227, 0.0897, 0.3186, 0.3539, 0.4336, 0.5526, 0.0192])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1105, 0.2614, 0.1796, 0.2078, 0.5258, 0.7626, 0.0626]) \n",
      "Test Loss tensor([0.1109, 0.2724, 0.1738, 0.2020, 0.5216, 0.7650, 0.0620])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0651, 0.0537, 0.1167, 0.1482, 0.0887, 0.1223, 0.5327, 0.0108]) \n",
      "Test Loss tensor([0.0646, 0.0567, 0.1097, 0.1486, 0.0863, 0.1200, 0.5417, 0.0104])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5302, 0.7841, 0.1020, 0.1685, 0.5354, 0.2312])\n",
      "Valid Idx 3 | Loss tensor([0.4656, 0.9782, 0.2446, 0.4644, 0.6849])\n",
      "\n",
      "************** Batch 136 in 3.3772945404052734 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2430, 0.2810, 0.1931, 0.2646]) \n",
      "Test Loss tensor([0.2392, 0.2771, 0.1912, 0.2606])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4805, 0.5505, 0.0172, 0.3256, 0.6333]) \n",
      "Test Loss tensor([0.4820, 0.5485, 0.0185, 0.3260, 0.6464])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1238, 0.0895, 0.3186, 0.3537, 0.4322, 0.5522, 0.0160]) \n",
      "Test Loss tensor([0.1215, 0.0880, 0.3158, 0.3528, 0.4306, 0.5519, 0.0199])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1133, 0.2747, 0.1794, 0.1984, 0.5228, 0.7587, 0.0693]) \n",
      "Test Loss tensor([0.1097, 0.2701, 0.1724, 0.2012, 0.5233, 0.7649, 0.0626])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0666, 0.0555, 0.1104, 0.1451, 0.0853, 0.1223, 0.5364, 0.0108]) \n",
      "Test Loss tensor([0.0648, 0.0566, 0.1068, 0.1468, 0.0846, 0.1218, 0.5425, 0.0106])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5269, 0.7902, 0.1035, 0.1694, 0.5425, 0.2281])\n",
      "Valid Idx 3 | Loss tensor([0.4599, 0.9756, 0.2423, 0.4715, 0.6847])\n",
      "\n",
      "************** Batch 140 in 3.380326747894287 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2439, 0.2694, 0.1936, 0.2683]) \n",
      "Test Loss tensor([0.2400, 0.2769, 0.1927, 0.2601])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4813, 0.5496, 0.0183, 0.3194, 0.6555]) \n",
      "Test Loss tensor([0.4772, 0.5488, 0.0190, 0.3222, 0.6468])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1195, 0.0876, 0.3153, 0.3518, 0.4300, 0.5395, 0.0197]) \n",
      "Test Loss tensor([0.1236, 0.0860, 0.3173, 0.3544, 0.4287, 0.5518, 0.0208])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1076, 0.2733, 0.1733, 0.1983, 0.4983, 0.7731, 0.0626]) \n",
      "Test Loss tensor([0.1089, 0.2772, 0.1739, 0.1999, 0.5249, 0.7689, 0.0653])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0652, 0.0569, 0.1051, 0.1445, 0.0850, 0.1244, 0.5384, 0.0102]) \n",
      "Test Loss tensor([0.0649, 0.0567, 0.1062, 0.1457, 0.0830, 0.1199, 0.5468, 0.0106])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5272, 0.7930, 0.1035, 0.1681, 0.5403, 0.2282])\n",
      "Valid Idx 3 | Loss tensor([0.4664, 0.9764, 0.2461, 0.4641, 0.6854])\n",
      "Gradients: Input 0.03211330994963646 | Message 0.040014609694480896 | Update 0.05846349522471428 | Output 0.35239484906196594\n",
      "\n",
      "************** Batch 144 in 3.4354655742645264 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2455, 0.2814, 0.1963, 0.2698]) \n",
      "Test Loss tensor([0.2376, 0.2803, 0.1917, 0.2590])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4838, 0.5516, 0.0169, 0.3297, 0.6533]) \n",
      "Test Loss tensor([0.4809, 0.5537, 0.0182, 0.3238, 0.6434])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1231, 0.0867, 0.3189, 0.3563, 0.4267, 0.5535, 0.0174]) \n",
      "Test Loss tensor([0.1231, 0.0847, 0.3152, 0.3509, 0.4328, 0.5514, 0.0194])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1094, 0.2765, 0.1809, 0.2071, 0.5385, 0.7736, 0.0681]) \n",
      "Test Loss tensor([0.1061, 0.2768, 0.1697, 0.2000, 0.5225, 0.7714, 0.0643])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0649, 0.0558, 0.1073, 0.1458, 0.0828, 0.1183, 0.5538, 0.0103]) \n",
      "Test Loss tensor([0.0634, 0.0554, 0.1035, 0.1418, 0.0818, 0.1207, 0.5455, 0.0108])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5278, 0.7848, 0.1049, 0.1666, 0.5409, 0.2267])\n",
      "Valid Idx 3 | Loss tensor([0.4726, 0.9764, 0.2491, 0.4665, 0.6862])\n",
      "\n",
      "************** Batch 148 in 3.388920783996582 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2394, 0.2669, 0.2020, 0.2705]) \n",
      "Test Loss tensor([0.2393, 0.2817, 0.1910, 0.2629])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4947, 0.5493, 0.0180, 0.3281, 0.6406]) \n",
      "Test Loss tensor([0.4815, 0.5547, 0.0181, 0.3242, 0.6414])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1270, 0.0852, 0.3144, 0.3552, 0.4218, 0.5686, 0.0172]) \n",
      "Test Loss tensor([0.1222, 0.0831, 0.3169, 0.3559, 0.4294, 0.5533, 0.0200])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1024, 0.2705, 0.1668, 0.2030, 0.5135, 0.7645, 0.0645]) \n",
      "Test Loss tensor([0.1055, 0.2735, 0.1694, 0.1999, 0.5194, 0.7696, 0.0634])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0641, 0.0573, 0.1028, 0.1423, 0.0817, 0.1180, 0.5373, 0.0106]) \n",
      "Test Loss tensor([0.0631, 0.0552, 0.1033, 0.1417, 0.0806, 0.1165, 0.5449, 0.0107])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5305, 0.7806, 0.1029, 0.1635, 0.5391, 0.2294])\n",
      "Valid Idx 3 | Loss tensor([0.4928, 0.9772, 0.2529, 0.4731, 0.6883])\n",
      "\n",
      "************** Batch 152 in 3.3927810192108154 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2288, 0.2827, 0.1919, 0.2630]) \n",
      "Test Loss tensor([0.2392, 0.2761, 0.1906, 0.2636])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4851, 0.5454, 0.0183, 0.3211, 0.6383]) \n",
      "Test Loss tensor([0.4814, 0.5607, 0.0191, 0.3270, 0.6379])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1165, 0.0830, 0.3200, 0.3501, 0.4335, 0.5639, 0.0177]) \n",
      "Test Loss tensor([0.1188, 0.0816, 0.3164, 0.3513, 0.4240, 0.5535, 0.0197])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1067, 0.2725, 0.1689, 0.2032, 0.5206, 0.7664, 0.0667]) \n",
      "Test Loss tensor([0.1054, 0.2743, 0.1682, 0.1991, 0.5266, 0.7693, 0.0637])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0635, 0.0548, 0.1062, 0.1403, 0.0799, 0.1180, 0.5366, 0.0103]) \n",
      "Test Loss tensor([0.0620, 0.0549, 0.1019, 0.1390, 0.0794, 0.1155, 0.5490, 0.0105])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5304, 0.7692, 0.1027, 0.1651, 0.5401, 0.2254])\n",
      "Valid Idx 3 | Loss tensor([0.5055, 0.9793, 0.2562, 0.4695, 0.6887])\n",
      "\n",
      "************** Batch 156 in 3.3710174560546875 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2450, 0.2734, 0.1989, 0.2706]) \n",
      "Test Loss tensor([0.2411, 0.2799, 0.1897, 0.2613])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4761, 0.5497, 0.0171, 0.3214, 0.6591]) \n",
      "Test Loss tensor([0.4851, 0.5596, 0.0185, 0.3268, 0.6423])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1209, 0.0817, 0.3055, 0.3527, 0.4236, 0.5500, 0.0194]) \n",
      "Test Loss tensor([0.1206, 0.0806, 0.3207, 0.3550, 0.4270, 0.5573, 0.0194])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1081, 0.2806, 0.1732, 0.1975, 0.5252, 0.7690, 0.0673]) \n",
      "Test Loss tensor([0.1044, 0.2753, 0.1659, 0.2003, 0.5235, 0.7685, 0.0637])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0627, 0.0536, 0.1008, 0.1387, 0.0783, 0.1160, 0.5456, 0.0105]) \n",
      "Test Loss tensor([0.0609, 0.0542, 0.1009, 0.1366, 0.0782, 0.1130, 0.5472, 0.0104])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5319, 0.7655, 0.1038, 0.1634, 0.5425, 0.2251])\n",
      "Valid Idx 3 | Loss tensor([0.5129, 0.9772, 0.2575, 0.4712, 0.6878])\n",
      "\n",
      "************** Batch 160 in 3.3538730144500732 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2431, 0.2919, 0.1922, 0.2649]) \n",
      "Test Loss tensor([0.2389, 0.2792, 0.1900, 0.2576])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4734, 0.5469, 0.0195, 0.3184, 0.6606]) \n",
      "Test Loss tensor([0.4825, 0.5593, 0.0189, 0.3253, 0.6411])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1145, 0.0801, 0.3149, 0.3593, 0.4200, 0.5601, 0.0185]) \n",
      "Test Loss tensor([0.1182, 0.0795, 0.3137, 0.3501, 0.4245, 0.5499, 0.0195])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1020, 0.2750, 0.1751, 0.2052, 0.5211, 0.7704, 0.0624]) \n",
      "Test Loss tensor([0.1032, 0.2725, 0.1644, 0.1986, 0.5271, 0.7684, 0.0625])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0638, 0.0533, 0.1002, 0.1388, 0.0780, 0.1150, 0.5472, 0.0108]) \n",
      "Test Loss tensor([0.0606, 0.0536, 0.0980, 0.1363, 0.0773, 0.1103, 0.5499, 0.0105])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5283, 0.7654, 0.1038, 0.1629, 0.5413, 0.2233])\n",
      "Valid Idx 3 | Loss tensor([0.5106, 0.9761, 0.2524, 0.4697, 0.6850])\n",
      "Gradients: Input 0.023746173828840256 | Message 0.020531954243779182 | Update 0.04049012437462807 | Output 0.3326623737812042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 164 in 3.379483699798584 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2289, 0.2767, 0.1865, 0.2597]) \n",
      "Test Loss tensor([0.2386, 0.2776, 0.1926, 0.2608])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4772, 0.5774, 0.0187, 0.3261, 0.6456]) \n",
      "Test Loss tensor([0.4818, 0.5597, 0.0187, 0.3224, 0.6452])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1205, 0.0796, 0.3078, 0.3486, 0.4212, 0.5561, 0.0204]) \n",
      "Test Loss tensor([0.1221, 0.0783, 0.3158, 0.3538, 0.4259, 0.5547, 0.0202])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1029, 0.2744, 0.1665, 0.1914, 0.5247, 0.7773, 0.0613]) \n",
      "Test Loss tensor([0.1021, 0.2749, 0.1629, 0.1990, 0.5254, 0.7709, 0.0626])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0575, 0.0537, 0.1003, 0.1344, 0.0772, 0.1128, 0.5519, 0.0110]) \n",
      "Test Loss tensor([0.0609, 0.0543, 0.0979, 0.1341, 0.0761, 0.1103, 0.5483, 0.0106])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5301, 0.7648, 0.1018, 0.1614, 0.5431, 0.2212])\n",
      "Valid Idx 3 | Loss tensor([0.5003, 0.9743, 0.2495, 0.4698, 0.6854])\n",
      "\n",
      "************** Batch 168 in 3.3769001960754395 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2404, 0.2697, 0.1983, 0.2698]) \n",
      "Test Loss tensor([0.2390, 0.2795, 0.1911, 0.2602])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4739, 0.5548, 0.0200, 0.3285, 0.6476]) \n",
      "Test Loss tensor([0.4812, 0.5628, 0.0194, 0.3260, 0.6475])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1212, 0.0784, 0.3083, 0.3512, 0.4147, 0.5534, 0.0210]) \n",
      "Test Loss tensor([0.1211, 0.0773, 0.3141, 0.3490, 0.4227, 0.5505, 0.0207])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1041, 0.2781, 0.1659, 0.2035, 0.5267, 0.7704, 0.0584]) \n",
      "Test Loss tensor([0.1003, 0.2733, 0.1601, 0.1992, 0.5241, 0.7709, 0.0634])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0636, 0.0519, 0.0998, 0.1312, 0.0776, 0.1128, 0.5440, 0.0105]) \n",
      "Test Loss tensor([0.0608, 0.0533, 0.0972, 0.1329, 0.0751, 0.1105, 0.5494, 0.0107])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5254, 0.7650, 0.1039, 0.1610, 0.5476, 0.2204])\n",
      "Valid Idx 3 | Loss tensor([0.4927, 0.9717, 0.2457, 0.4688, 0.6793])\n",
      "\n",
      "************** Batch 172 in 3.3792293071746826 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2333, 0.2818, 0.1885, 0.2570]) \n",
      "Test Loss tensor([0.2359, 0.2800, 0.1916, 0.2555])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4764, 0.5560, 0.0173, 0.3189, 0.6453]) \n",
      "Test Loss tensor([0.4786, 0.5613, 0.0192, 0.3257, 0.6467])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1194, 0.0768, 0.3214, 0.3554, 0.4229, 0.5571, 0.0199]) \n",
      "Test Loss tensor([0.1221, 0.0765, 0.3141, 0.3518, 0.4293, 0.5531, 0.0210])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1022, 0.2735, 0.1646, 0.2040, 0.5334, 0.7684, 0.0675]) \n",
      "Test Loss tensor([0.1010, 0.2735, 0.1587, 0.1961, 0.5227, 0.7746, 0.0626])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0626, 0.0520, 0.0996, 0.1272, 0.0761, 0.1098, 0.5418, 0.0112]) \n",
      "Test Loss tensor([0.0601, 0.0535, 0.0961, 0.1304, 0.0744, 0.1103, 0.5520, 0.0107])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5363, 0.7668, 0.1026, 0.1602, 0.5469, 0.2182])\n",
      "Valid Idx 3 | Loss tensor([0.4804, 0.9713, 0.2434, 0.4637, 0.6787])\n",
      "\n",
      "************** Batch 176 in 3.3841474056243896 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2305, 0.2727, 0.1861, 0.2583]) \n",
      "Test Loss tensor([0.2347, 0.2794, 0.1884, 0.2579])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4858, 0.5615, 0.0180, 0.3291, 0.6367]) \n",
      "Test Loss tensor([0.4760, 0.5608, 0.0197, 0.3226, 0.6481])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1187, 0.0769, 0.3185, 0.3549, 0.4325, 0.5626, 0.0213]) \n",
      "Test Loss tensor([0.1206, 0.0759, 0.3095, 0.3468, 0.4237, 0.5480, 0.0207])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.1025, 0.2753, 0.1644, 0.1972, 0.5165, 0.7691, 0.0598]) \n",
      "Test Loss tensor([0.0992, 0.2747, 0.1591, 0.1969, 0.5257, 0.7730, 0.0644])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0610, 0.0529, 0.1011, 0.1310, 0.0741, 0.1115, 0.5374, 0.0114]) \n",
      "Test Loss tensor([0.0597, 0.0527, 0.0959, 0.1295, 0.0736, 0.1096, 0.5513, 0.0105])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5246, 0.7658, 0.1004, 0.1577, 0.5455, 0.2157])\n",
      "Valid Idx 3 | Loss tensor([0.4792, 0.9710, 0.2417, 0.4657, 0.6828])\n",
      "\n",
      "************** Batch 180 in 3.377107620239258 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2315, 0.2818, 0.1824, 0.2428]) \n",
      "Test Loss tensor([0.2360, 0.2787, 0.1898, 0.2554])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4721, 0.5491, 0.0188, 0.3124, 0.6568]) \n",
      "Test Loss tensor([0.4761, 0.5618, 0.0198, 0.3235, 0.6498])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1269, 0.0748, 0.3036, 0.3495, 0.4245, 0.5535, 0.0213]) \n",
      "Test Loss tensor([0.1208, 0.0749, 0.3118, 0.3486, 0.4257, 0.5506, 0.0214])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0972, 0.2702, 0.1491, 0.1912, 0.5183, 0.7671, 0.0639]) \n",
      "Test Loss tensor([0.0970, 0.2725, 0.1570, 0.1971, 0.5260, 0.7726, 0.0626])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0594, 0.0550, 0.0930, 0.1347, 0.0733, 0.1102, 0.5519, 0.0104]) \n",
      "Test Loss tensor([0.0583, 0.0531, 0.0936, 0.1288, 0.0730, 0.1080, 0.5549, 0.0106])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5342, 0.7647, 0.1023, 0.1573, 0.5438, 0.2189])\n",
      "Valid Idx 3 | Loss tensor([0.4798, 0.9690, 0.2412, 0.4703, 0.6865])\n",
      "Gradients: Input 0.028511617332696915 | Message 0.021022353321313858 | Update 0.035811424255371094 | Output 0.28076082468032837\n",
      "\n",
      "************** Batch 184 in 3.379162311553955 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2305, 0.2677, 0.1960, 0.2538]) \n",
      "Test Loss tensor([0.2360, 0.2792, 0.1886, 0.2557])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4778, 0.5505, 0.0187, 0.3256, 0.6620]) \n",
      "Test Loss tensor([0.4765, 0.5629, 0.0187, 0.3216, 0.6485])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1122, 0.0748, 0.3045, 0.3438, 0.4255, 0.5524, 0.0199]) \n",
      "Test Loss tensor([0.1212, 0.0741, 0.3078, 0.3471, 0.4259, 0.5532, 0.0199])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0979, 0.2740, 0.1480, 0.1897, 0.5286, 0.7730, 0.0639]) \n",
      "Test Loss tensor([0.0966, 0.2701, 0.1535, 0.1946, 0.5248, 0.7717, 0.0641])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0591, 0.0525, 0.0935, 0.1256, 0.0715, 0.1036, 0.5553, 0.0107]) \n",
      "Test Loss tensor([0.0586, 0.0515, 0.0932, 0.1281, 0.0726, 0.1060, 0.5550, 0.0103])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5335, 0.7624, 0.1022, 0.1548, 0.5495, 0.2177])\n",
      "Valid Idx 3 | Loss tensor([0.4827, 0.9725, 0.2442, 0.4721, 0.6879])\n",
      "\n",
      "************** Batch 188 in 3.4015121459960938 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2431, 0.2724, 0.1942, 0.2633]) \n",
      "Test Loss tensor([0.2362, 0.2776, 0.1872, 0.2530])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4736, 0.5534, 0.0200, 0.3202, 0.6631]) \n",
      "Test Loss tensor([0.4806, 0.5642, 0.0186, 0.3258, 0.6435])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1245, 0.0737, 0.3136, 0.3561, 0.4208, 0.5443, 0.0238]) \n",
      "Test Loss tensor([0.1222, 0.0732, 0.3091, 0.3493, 0.4244, 0.5522, 0.0211])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0979, 0.2621, 0.1527, 0.1830, 0.5247, 0.7718, 0.0651]) \n",
      "Test Loss tensor([0.0960, 0.2721, 0.1543, 0.1967, 0.5282, 0.7748, 0.0611])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0584, 0.0533, 0.0927, 0.1280, 0.0721, 0.1060, 0.5514, 0.0116]) \n",
      "Test Loss tensor([0.0588, 0.0514, 0.0916, 0.1255, 0.0714, 0.1040, 0.5557, 0.0104])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5328, 0.7590, 0.1040, 0.1578, 0.5499, 0.2194])\n",
      "Valid Idx 3 | Loss tensor([0.4868, 0.9733, 0.2468, 0.4653, 0.6900])\n",
      "\n",
      "************** Batch 192 in 3.544877767562866 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2392, 0.2729, 0.1957, 0.2681]) \n",
      "Test Loss tensor([0.2352, 0.2795, 0.1906, 0.2569])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4746, 0.5754, 0.0192, 0.3308, 0.6353]) \n",
      "Test Loss tensor([0.4754, 0.5627, 0.0188, 0.3220, 0.6468])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1222, 0.0728, 0.3077, 0.3566, 0.4291, 0.5621, 0.0228]) \n",
      "Test Loss tensor([0.1214, 0.0728, 0.3106, 0.3499, 0.4212, 0.5501, 0.0201])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0954, 0.2704, 0.1541, 0.1962, 0.5259, 0.7580, 0.0600]) \n",
      "Test Loss tensor([0.0963, 0.2749, 0.1567, 0.1952, 0.5247, 0.7719, 0.0625])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0565, 0.0509, 0.0900, 0.1244, 0.0725, 0.1033, 0.5513, 0.0102]) \n",
      "Test Loss tensor([0.0577, 0.0509, 0.0910, 0.1247, 0.0709, 0.1023, 0.5478, 0.0101])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5287, 0.7566, 0.1028, 0.1551, 0.5510, 0.2178])\n",
      "Valid Idx 3 | Loss tensor([0.4864, 0.9753, 0.2460, 0.4688, 0.6888])\n",
      "\n",
      "************** Batch 196 in 3.4907617568969727 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2346, 0.2766, 0.1867, 0.2478]) \n",
      "Test Loss tensor([0.2331, 0.2784, 0.1891, 0.2542])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4746, 0.5561, 0.0171, 0.3239, 0.6535]) \n",
      "Test Loss tensor([0.4723, 0.5619, 0.0183, 0.3233, 0.6506])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1200, 0.0722, 0.2940, 0.3431, 0.4229, 0.5586, 0.0232]) \n",
      "Test Loss tensor([0.1238, 0.0720, 0.3078, 0.3508, 0.4226, 0.5519, 0.0210])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0979, 0.2806, 0.1520, 0.1971, 0.5278, 0.7836, 0.0582]) \n",
      "Test Loss tensor([0.0954, 0.2722, 0.1540, 0.1947, 0.5257, 0.7762, 0.0609])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0559, 0.0494, 0.0887, 0.1236, 0.0706, 0.1046, 0.5541, 0.0102]) \n",
      "Test Loss tensor([0.0569, 0.0509, 0.0903, 0.1232, 0.0707, 0.1016, 0.5545, 0.0100])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5319, 0.7522, 0.1036, 0.1552, 0.5458, 0.2183])\n",
      "Valid Idx 3 | Loss tensor([0.4908, 0.9785, 0.2463, 0.4668, 0.6932])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 200 in 3.4562036991119385 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2467, 0.2735, 0.1960, 0.2640]) \n",
      "Test Loss tensor([0.2330, 0.2792, 0.1886, 0.2556])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4720, 0.5516, 0.0165, 0.3222, 0.6582]) \n",
      "Test Loss tensor([0.4770, 0.5646, 0.0187, 0.3237, 0.6477])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1232, 0.0718, 0.3001, 0.3465, 0.4260, 0.5607, 0.0219]) \n",
      "Test Loss tensor([0.1202, 0.0713, 0.3073, 0.3472, 0.4197, 0.5536, 0.0212])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0981, 0.2828, 0.1536, 0.2035, 0.5271, 0.7702, 0.0642]) \n",
      "Test Loss tensor([0.0954, 0.2739, 0.1559, 0.1945, 0.5244, 0.7729, 0.0618])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0576, 0.0479, 0.0908, 0.1228, 0.0712, 0.1055, 0.5466, 0.0101]) \n",
      "Test Loss tensor([0.0563, 0.0498, 0.0901, 0.1227, 0.0706, 0.1010, 0.5557, 0.0099])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5357, 0.7490, 0.1024, 0.1559, 0.5476, 0.2169])\n",
      "Valid Idx 3 | Loss tensor([0.4978, 0.9813, 0.2461, 0.4716, 0.6937])\n",
      "Gradients: Input 0.029821880161762238 | Message 0.02623172104358673 | Update 0.0315987691283226 | Output 0.2080199122428894\n",
      "\n",
      "************** Batch 204 in 3.456371545791626 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2261, 0.2765, 0.1925, 0.2537]) \n",
      "Test Loss tensor([0.2325, 0.2757, 0.1874, 0.2567])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4653, 0.5580, 0.0176, 0.3145, 0.6398]) \n",
      "Test Loss tensor([0.4749, 0.5694, 0.0183, 0.3232, 0.6420])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1213, 0.0710, 0.3020, 0.3451, 0.4209, 0.5495, 0.0213]) \n",
      "Test Loss tensor([0.1191, 0.0709, 0.3080, 0.3461, 0.4227, 0.5513, 0.0207])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0961, 0.2687, 0.1527, 0.1824, 0.5268, 0.7699, 0.0609]) \n",
      "Test Loss tensor([0.0952, 0.2762, 0.1571, 0.1961, 0.5264, 0.7729, 0.0633])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0551, 0.0497, 0.0902, 0.1253, 0.0698, 0.0974, 0.5564, 0.0101]) \n",
      "Test Loss tensor([0.0561, 0.0492, 0.0892, 0.1225, 0.0702, 0.0997, 0.5509, 0.0098])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5349, 0.7412, 0.1028, 0.1537, 0.5480, 0.2203])\n",
      "Valid Idx 3 | Loss tensor([0.5003, 0.9826, 0.2525, 0.4665, 0.6917])\n",
      "\n",
      "************** Batch 208 in 3.2612247467041016 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2307, 0.2657, 0.1832, 0.2573]) \n",
      "Test Loss tensor([0.2356, 0.2766, 0.1896, 0.2579])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4768, 0.5594, 0.0182, 0.3193, 0.6441]) \n",
      "Test Loss tensor([0.4755, 0.5661, 0.0190, 0.3237, 0.6481])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1241, 0.0716, 0.3115, 0.3687, 0.4370, 0.5647, 0.0199]) \n",
      "Test Loss tensor([0.1187, 0.0702, 0.3078, 0.3505, 0.4170, 0.5521, 0.0198])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0941, 0.2727, 0.1586, 0.1984, 0.5261, 0.7716, 0.0640]) \n",
      "Test Loss tensor([0.0946, 0.2700, 0.1551, 0.1949, 0.5264, 0.7710, 0.0601])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0536, 0.0476, 0.0870, 0.1198, 0.0690, 0.0990, 0.5621, 0.0095]) \n",
      "Test Loss tensor([0.0549, 0.0484, 0.0891, 0.1205, 0.0696, 0.0976, 0.5492, 0.0098])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5366, 0.7368, 0.1011, 0.1527, 0.5452, 0.2174])\n",
      "Valid Idx 3 | Loss tensor([0.5082, 0.9824, 0.2534, 0.4629, 0.6967])\n",
      "\n",
      "************** Batch 212 in 3.2685670852661133 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2349, 0.2671, 0.1889, 0.2491]) \n",
      "Test Loss tensor([0.2352, 0.2781, 0.1908, 0.2572])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4715, 0.5817, 0.0186, 0.3267, 0.6387]) \n",
      "Test Loss tensor([0.4716, 0.5669, 0.0185, 0.3208, 0.6431])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1176, 0.0710, 0.3062, 0.3421, 0.4187, 0.5494, 0.0216]) \n",
      "Test Loss tensor([0.1210, 0.0698, 0.3065, 0.3468, 0.4201, 0.5519, 0.0203])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0948, 0.2680, 0.1556, 0.1895, 0.5193, 0.7722, 0.0553]) \n",
      "Test Loss tensor([0.0945, 0.2717, 0.1564, 0.1962, 0.5244, 0.7648, 0.0611])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0535, 0.0460, 0.0898, 0.1161, 0.0691, 0.0968, 0.5593, 0.0102]) \n",
      "Test Loss tensor([0.0550, 0.0495, 0.0866, 0.1200, 0.0696, 0.0973, 0.5514, 0.0098])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5321, 0.7263, 0.1037, 0.1547, 0.5494, 0.2195])\n",
      "Valid Idx 3 | Loss tensor([0.5157, 0.9854, 0.2573, 0.4668, 0.6965])\n",
      "\n",
      "************** Batch 216 in 3.272444486618042 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2414, 0.2749, 0.1911, 0.2564]) \n",
      "Test Loss tensor([0.2341, 0.2786, 0.1863, 0.2554])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4877, 0.5747, 0.0168, 0.3276, 0.6384]) \n",
      "Test Loss tensor([0.4739, 0.5661, 0.0191, 0.3218, 0.6455])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1184, 0.0701, 0.3033, 0.3454, 0.4279, 0.5604, 0.0209]) \n",
      "Test Loss tensor([0.1189, 0.0697, 0.3077, 0.3485, 0.4202, 0.5542, 0.0210])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0958, 0.2749, 0.1543, 0.1953, 0.5263, 0.7642, 0.0583]) \n",
      "Test Loss tensor([0.0945, 0.2702, 0.1563, 0.1967, 0.5243, 0.7614, 0.0622])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0542, 0.0476, 0.0886, 0.1226, 0.0685, 0.0973, 0.5481, 0.0095]) \n",
      "Test Loss tensor([0.0554, 0.0490, 0.0877, 0.1188, 0.0696, 0.0954, 0.5551, 0.0099])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5309, 0.7209, 0.1015, 0.1558, 0.5485, 0.2208])\n",
      "Valid Idx 3 | Loss tensor([0.5208, 0.9865, 0.2579, 0.4655, 0.6990])\n",
      "\n",
      "************** Batch 220 in 3.2922792434692383 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2326, 0.2802, 0.1898, 0.2639]) \n",
      "Test Loss tensor([0.2301, 0.2750, 0.1873, 0.2547])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4839, 0.5556, 0.0165, 0.3197, 0.6630]) \n",
      "Test Loss tensor([0.4728, 0.5697, 0.0190, 0.3254, 0.6415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1286, 0.0696, 0.3121, 0.3654, 0.4302, 0.5675, 0.0202]) \n",
      "Test Loss tensor([0.1207, 0.0694, 0.3047, 0.3478, 0.4165, 0.5510, 0.0214])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0919, 0.2627, 0.1505, 0.1915, 0.5163, 0.7669, 0.0527]) \n",
      "Test Loss tensor([0.0940, 0.2711, 0.1529, 0.1939, 0.5239, 0.7661, 0.0617])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0555, 0.0479, 0.0872, 0.1169, 0.0682, 0.0948, 0.5518, 0.0097]) \n",
      "Test Loss tensor([0.0553, 0.0479, 0.0876, 0.1186, 0.0692, 0.0969, 0.5565, 0.0103])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5310, 0.7205, 0.1034, 0.1519, 0.5521, 0.2216])\n",
      "Valid Idx 3 | Loss tensor([0.4992, 0.9808, 0.2515, 0.4599, 0.6934])\n",
      "Gradients: Input 0.04239059239625931 | Message 0.03525461256504059 | Update 0.07128959894180298 | Output 0.27814263105392456\n",
      "\n",
      "************** Batch 224 in 3.276869058609009 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2298, 0.2731, 0.1925, 0.2612]) \n",
      "Test Loss tensor([0.2275, 0.2800, 0.1887, 0.2509])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4701, 0.5741, 0.0183, 0.3278, 0.6329]) \n",
      "Test Loss tensor([0.4733, 0.5706, 0.0195, 0.3267, 0.6386])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1208, 0.0690, 0.2975, 0.3456, 0.4136, 0.5428, 0.0208]) \n",
      "Test Loss tensor([0.1217, 0.0691, 0.3040, 0.3457, 0.4165, 0.5498, 0.0216])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0961, 0.2660, 0.1542, 0.1903, 0.5076, 0.7726, 0.0616]) \n",
      "Test Loss tensor([0.0942, 0.2697, 0.1547, 0.1952, 0.5277, 0.7643, 0.0620])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0566, 0.0482, 0.0886, 0.1195, 0.0692, 0.0962, 0.5497, 0.0105]) \n",
      "Test Loss tensor([0.0557, 0.0483, 0.0873, 0.1179, 0.0687, 0.0959, 0.5522, 0.0103])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5331, 0.7262, 0.1027, 0.1512, 0.5525, 0.2211])\n",
      "Valid Idx 3 | Loss tensor([0.4730, 0.9777, 0.2419, 0.4612, 0.6933])\n",
      "\n",
      "************** Batch 228 in 3.267000675201416 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2287, 0.2759, 0.1848, 0.2593]) \n",
      "Test Loss tensor([0.2292, 0.2763, 0.1890, 0.2551])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4882, 0.5804, 0.0202, 0.3352, 0.6315]) \n",
      "Test Loss tensor([0.4677, 0.5646, 0.0192, 0.3225, 0.6439])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1160, 0.0702, 0.3031, 0.3502, 0.4338, 0.5474, 0.0218]) \n",
      "Test Loss tensor([0.1233, 0.0694, 0.3032, 0.3477, 0.4206, 0.5523, 0.0224])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0948, 0.2769, 0.1550, 0.1958, 0.5339, 0.7663, 0.0651]) \n",
      "Test Loss tensor([0.0923, 0.2708, 0.1509, 0.1898, 0.5248, 0.7680, 0.0611])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0563, 0.0511, 0.0860, 0.1196, 0.0686, 0.0979, 0.5555, 0.0107]) \n",
      "Test Loss tensor([0.0565, 0.0489, 0.0870, 0.1160, 0.0691, 0.0966, 0.5537, 0.0105])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5272, 0.7238, 0.1023, 0.1517, 0.5526, 0.2171])\n",
      "Valid Idx 3 | Loss tensor([0.4643, 0.9754, 0.2380, 0.4602, 0.6919])\n",
      "\n",
      "************** Batch 232 in 3.2725722789764404 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2271, 0.2829, 0.1820, 0.2517]) \n",
      "Test Loss tensor([0.2316, 0.2758, 0.1910, 0.2577])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4820, 0.5821, 0.0209, 0.3294, 0.6233]) \n",
      "Test Loss tensor([0.4703, 0.5677, 0.0198, 0.3245, 0.6439])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1278, 0.0695, 0.2971, 0.3336, 0.4135, 0.5481, 0.0238]) \n",
      "Test Loss tensor([0.1208, 0.0690, 0.3000, 0.3442, 0.4186, 0.5478, 0.0222])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0943, 0.2721, 0.1554, 0.1875, 0.5263, 0.7642, 0.0654]) \n",
      "Test Loss tensor([0.0923, 0.2653, 0.1513, 0.1915, 0.5219, 0.7634, 0.0608])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0576, 0.0473, 0.0859, 0.1163, 0.0675, 0.0998, 0.5482, 0.0111]) \n",
      "Test Loss tensor([0.0565, 0.0487, 0.0855, 0.1154, 0.0689, 0.0967, 0.5528, 0.0105])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5298, 0.7190, 0.1000, 0.1504, 0.5468, 0.2170])\n",
      "Valid Idx 3 | Loss tensor([0.4660, 0.9748, 0.2442, 0.4578, 0.6923])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 236 in 3.290757894515991 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2215, 0.2787, 0.1937, 0.2500]) \n",
      "Test Loss tensor([0.2291, 0.2781, 0.1877, 0.2547])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4598, 0.5439, 0.0185, 0.3116, 0.6505]) \n",
      "Test Loss tensor([0.4749, 0.5708, 0.0199, 0.3222, 0.6426])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1268, 0.0685, 0.2874, 0.3333, 0.4163, 0.5518, 0.0248]) \n",
      "Test Loss tensor([0.1228, 0.0684, 0.3015, 0.3432, 0.4194, 0.5506, 0.0227])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0927, 0.2702, 0.1543, 0.1918, 0.5274, 0.7638, 0.0608]) \n",
      "Test Loss tensor([0.0936, 0.2724, 0.1532, 0.1930, 0.5248, 0.7645, 0.0619])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0567, 0.0507, 0.0864, 0.1136, 0.0675, 0.0941, 0.5619, 0.0107]) \n",
      "Test Loss tensor([0.0559, 0.0484, 0.0855, 0.1149, 0.0683, 0.0947, 0.5524, 0.0106])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5321, 0.7106, 0.1014, 0.1523, 0.5533, 0.2182])\n",
      "Valid Idx 3 | Loss tensor([0.4697, 0.9759, 0.2417, 0.4619, 0.6935])\n",
      "\n",
      "************** Batch 240 in 3.269976854324341 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2338, 0.2618, 0.1954, 0.2570]) \n",
      "Test Loss tensor([0.2291, 0.2745, 0.1913, 0.2529])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4613, 0.5600, 0.0202, 0.3087, 0.6526]) \n",
      "Test Loss tensor([0.4671, 0.5731, 0.0205, 0.3232, 0.6403])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1204, 0.0683, 0.2991, 0.3434, 0.4084, 0.5440, 0.0206]) \n",
      "Test Loss tensor([0.1245, 0.0682, 0.3006, 0.3429, 0.4177, 0.5462, 0.0228])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0918, 0.2634, 0.1501, 0.1940, 0.5247, 0.7588, 0.0615]) \n",
      "Test Loss tensor([0.0930, 0.2742, 0.1527, 0.1941, 0.5190, 0.7607, 0.0624])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0571, 0.0461, 0.0867, 0.1175, 0.0679, 0.0968, 0.5437, 0.0102]) \n",
      "Test Loss tensor([0.0546, 0.0482, 0.0849, 0.1137, 0.0686, 0.0947, 0.5465, 0.0106])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5316, 0.6999, 0.1020, 0.1514, 0.5507, 0.2174])\n",
      "Valid Idx 3 | Loss tensor([0.4727, 0.9769, 0.2397, 0.4593, 0.6937])\n",
      "Gradients: Input 0.024870337918400764 | Message 0.019302502274513245 | Update 0.030816420912742615 | Output 0.1864796280860901\n",
      "\n",
      "************** Batch 244 in 3.278895378112793 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2219, 0.2797, 0.1905, 0.2444]) \n",
      "Test Loss tensor([0.2300, 0.2750, 0.1898, 0.2545])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4643, 0.5721, 0.0190, 0.3252, 0.6473]) \n",
      "Test Loss tensor([0.4697, 0.5711, 0.0193, 0.3233, 0.6424])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1280, 0.0679, 0.2949, 0.3437, 0.4217, 0.5526, 0.0229]) \n",
      "Test Loss tensor([0.1214, 0.0680, 0.3002, 0.3409, 0.4131, 0.5510, 0.0215])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0934, 0.2636, 0.1556, 0.1924, 0.5177, 0.7633, 0.0640]) \n",
      "Test Loss tensor([0.0918, 0.2655, 0.1510, 0.1935, 0.5246, 0.7561, 0.0598])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0548, 0.0471, 0.0840, 0.1137, 0.0679, 0.0928, 0.5528, 0.0102]) \n",
      "Test Loss tensor([0.0547, 0.0483, 0.0848, 0.1125, 0.0687, 0.0933, 0.5488, 0.0105])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5342, 0.6855, 0.1019, 0.1523, 0.5488, 0.2185])\n",
      "Valid Idx 3 | Loss tensor([0.4842, 0.9781, 0.2462, 0.4573, 0.6965])\n",
      "\n",
      "************** Batch 248 in 3.2847347259521484 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2319, 0.2723, 0.1895, 0.2489]) \n",
      "Test Loss tensor([0.2316, 0.2747, 0.1885, 0.2557])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4872, 0.5757, 0.0176, 0.3239, 0.6349]) \n",
      "Test Loss tensor([0.4700, 0.5750, 0.0193, 0.3232, 0.6417])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1230, 0.0675, 0.2916, 0.3388, 0.4203, 0.5599, 0.0223]) \n",
      "Test Loss tensor([0.1220, 0.0675, 0.3032, 0.3462, 0.4138, 0.5473, 0.0218])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0933, 0.2738, 0.1503, 0.1930, 0.5218, 0.7555, 0.0671]) \n",
      "Test Loss tensor([0.0920, 0.2664, 0.1516, 0.1944, 0.5200, 0.7518, 0.0601])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0539, 0.0460, 0.0832, 0.1089, 0.0700, 0.0910, 0.5630, 0.0103]) \n",
      "Test Loss tensor([0.0540, 0.0474, 0.0840, 0.1108, 0.0686, 0.0916, 0.5482, 0.0103])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5317, 0.6765, 0.1013, 0.1532, 0.5491, 0.2173])\n",
      "Valid Idx 3 | Loss tensor([0.4847, 0.9800, 0.2437, 0.4602, 0.6978])\n",
      "\n",
      "************** Batch 252 in 3.2892873287200928 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2295, 0.2687, 0.1781, 0.2519]) \n",
      "Test Loss tensor([0.2314, 0.2799, 0.1903, 0.2553])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4758, 0.5701, 0.0192, 0.3219, 0.6382]) \n",
      "Test Loss tensor([0.4661, 0.5725, 0.0196, 0.3193, 0.6433])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1274, 0.0671, 0.3123, 0.3622, 0.4099, 0.5436, 0.0228]) \n",
      "Test Loss tensor([0.1174, 0.0670, 0.2977, 0.3430, 0.4149, 0.5502, 0.0216])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0902, 0.2480, 0.1501, 0.1900, 0.5102, 0.7506, 0.0571]) \n",
      "Test Loss tensor([0.0908, 0.2649, 0.1509, 0.1913, 0.5214, 0.7503, 0.0608])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0540, 0.0482, 0.0877, 0.1108, 0.0699, 0.0904, 0.5496, 0.0102]) \n",
      "Test Loss tensor([0.0542, 0.0477, 0.0836, 0.1114, 0.0682, 0.0907, 0.5465, 0.0102])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5349, 0.6701, 0.1029, 0.1497, 0.5498, 0.2213])\n",
      "Valid Idx 3 | Loss tensor([0.4840, 0.9792, 0.2438, 0.4620, 0.6970])\n",
      "\n",
      "************** Batch 256 in 3.266592025756836 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2315, 0.2691, 0.1908, 0.2597]) \n",
      "Test Loss tensor([0.2239, 0.2761, 0.1886, 0.2542])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4684, 0.5734, 0.0192, 0.3164, 0.6404]) \n",
      "Test Loss tensor([0.4688, 0.5789, 0.0190, 0.3238, 0.6402])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1187, 0.0666, 0.2938, 0.3488, 0.4170, 0.5441, 0.0228]) \n",
      "Test Loss tensor([0.1210, 0.0667, 0.3021, 0.3470, 0.4148, 0.5474, 0.0221])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0901, 0.2588, 0.1504, 0.1937, 0.5263, 0.7460, 0.0680]) \n",
      "Test Loss tensor([0.0907, 0.2636, 0.1503, 0.1938, 0.5212, 0.7430, 0.0605])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0523, 0.0483, 0.0817, 0.1144, 0.0692, 0.0928, 0.5392, 0.0100]) \n",
      "Test Loss tensor([0.0527, 0.0464, 0.0836, 0.1116, 0.0684, 0.0892, 0.5461, 0.0100])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5339, 0.6595, 0.1019, 0.1497, 0.5488, 0.2194])\n",
      "Valid Idx 3 | Loss tensor([0.4909, 0.9780, 0.2493, 0.4577, 0.6932])\n",
      "\n",
      "************** Batch 260 in 3.279330253601074 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2200, 0.2719, 0.1886, 0.2428]) \n",
      "Test Loss tensor([0.2245, 0.2773, 0.1878, 0.2501])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4576, 0.5706, 0.0201, 0.3139, 0.6406]) \n",
      "Test Loss tensor([0.4665, 0.5810, 0.0194, 0.3244, 0.6405])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1232, 0.0662, 0.3034, 0.3503, 0.4082, 0.5469, 0.0235]) \n",
      "Test Loss tensor([0.1211, 0.0664, 0.2980, 0.3456, 0.4133, 0.5478, 0.0226])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0942, 0.2543, 0.1480, 0.2043, 0.5103, 0.7386, 0.0647]) \n",
      "Test Loss tensor([0.0910, 0.2658, 0.1500, 0.1950, 0.5189, 0.7417, 0.0594])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0531, 0.0443, 0.0840, 0.1106, 0.0684, 0.0885, 0.5477, 0.0099]) \n",
      "Test Loss tensor([0.0522, 0.0460, 0.0829, 0.1089, 0.0682, 0.0886, 0.5468, 0.0100])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5308, 0.6480, 0.1000, 0.1496, 0.5479, 0.2215])\n",
      "Valid Idx 3 | Loss tensor([0.4924, 0.9803, 0.2458, 0.4628, 0.7014])\n",
      "Gradients: Input 0.027125904336571693 | Message 0.01782630942761898 | Update 0.03219255059957504 | Output 0.23246361315250397\n",
      "\n",
      "************** Batch 264 in 3.2863242626190186 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2308, 0.2704, 0.1887, 0.2526]) \n",
      "Test Loss tensor([0.2237, 0.2793, 0.1879, 0.2521])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4729, 0.5792, 0.0202, 0.3119, 0.6421]) \n",
      "Test Loss tensor([0.4639, 0.5789, 0.0196, 0.3170, 0.6443])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1239, 0.0659, 0.3031, 0.3397, 0.4069, 0.5665, 0.0225]) \n",
      "Test Loss tensor([0.1222, 0.0657, 0.2972, 0.3454, 0.4117, 0.5447, 0.0225])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0902, 0.2654, 0.1480, 0.1902, 0.5130, 0.7328, 0.0547]) \n",
      "Test Loss tensor([0.0916, 0.2656, 0.1505, 0.1916, 0.5210, 0.7366, 0.0603])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0575, 0.0475, 0.0845, 0.1098, 0.0701, 0.0924, 0.5292, 0.0102]) \n",
      "Test Loss tensor([0.0525, 0.0452, 0.0827, 0.1078, 0.0683, 0.0882, 0.5437, 0.0099])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5316, 0.6452, 0.1025, 0.1487, 0.5485, 0.2212])\n",
      "Valid Idx 3 | Loss tensor([0.4841, 0.9793, 0.2468, 0.4586, 0.6969])\n",
      "\n",
      "************** Batch 268 in 3.2853846549987793 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2293, 0.2664, 0.1917, 0.2648]) \n",
      "Test Loss tensor([0.2240, 0.2748, 0.1877, 0.2504])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4725, 0.5915, 0.0182, 0.3181, 0.6334]) \n",
      "Test Loss tensor([0.4644, 0.5821, 0.0196, 0.3206, 0.6366])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1248, 0.0655, 0.3050, 0.3487, 0.4153, 0.5371, 0.0239]) \n",
      "Test Loss tensor([0.1244, 0.0656, 0.2959, 0.3404, 0.4114, 0.5448, 0.0223])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0892, 0.2733, 0.1497, 0.2010, 0.5222, 0.7227, 0.0571]) \n",
      "Test Loss tensor([0.0909, 0.2649, 0.1514, 0.1945, 0.5153, 0.7344, 0.0607])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0522, 0.0488, 0.0851, 0.1095, 0.0689, 0.0880, 0.5377, 0.0105]) \n",
      "Test Loss tensor([0.0525, 0.0464, 0.0820, 0.1075, 0.0681, 0.0871, 0.5399, 0.0100])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5331, 0.6405, 0.1015, 0.1492, 0.5478, 0.2221])\n",
      "Valid Idx 3 | Loss tensor([0.4826, 0.9817, 0.2442, 0.4561, 0.6980])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 272 in 3.3460614681243896 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2247, 0.2691, 0.1899, 0.2556]) \n",
      "Test Loss tensor([0.2278, 0.2746, 0.1879, 0.2560])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4675, 0.5752, 0.0168, 0.3204, 0.6535]) \n",
      "Test Loss tensor([0.4664, 0.5831, 0.0194, 0.3228, 0.6389])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1145, 0.0653, 0.2898, 0.3317, 0.4013, 0.5399, 0.0223]) \n",
      "Test Loss tensor([0.1221, 0.0652, 0.2935, 0.3399, 0.4140, 0.5434, 0.0231])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0900, 0.2686, 0.1472, 0.1927, 0.5151, 0.7414, 0.0610]) \n",
      "Test Loss tensor([0.0908, 0.2641, 0.1493, 0.1926, 0.5162, 0.7350, 0.0605])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0521, 0.0443, 0.0832, 0.1066, 0.0709, 0.0902, 0.5477, 0.0097]) \n",
      "Test Loss tensor([0.0521, 0.0447, 0.0820, 0.1079, 0.0688, 0.0888, 0.5408, 0.0099])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5336, 0.6372, 0.1012, 0.1491, 0.5540, 0.2249])\n",
      "Valid Idx 3 | Loss tensor([0.4692, 0.9804, 0.2375, 0.4591, 0.7033])\n",
      "\n",
      "************** Batch 276 in 3.2968640327453613 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2325, 0.2631, 0.1943, 0.2558]) \n",
      "Test Loss tensor([0.2223, 0.2781, 0.1870, 0.2537])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4515, 0.5707, 0.0199, 0.3135, 0.6562]) \n",
      "Test Loss tensor([0.4653, 0.5762, 0.0193, 0.3171, 0.6460])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1248, 0.0655, 0.2963, 0.3396, 0.4138, 0.5489, 0.0226]) \n",
      "Test Loss tensor([0.1238, 0.0650, 0.2908, 0.3398, 0.4120, 0.5412, 0.0231])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0925, 0.2659, 0.1469, 0.1871, 0.5216, 0.7287, 0.0628]) \n",
      "Test Loss tensor([0.0883, 0.2625, 0.1474, 0.1930, 0.5142, 0.7321, 0.0597])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0503, 0.0446, 0.0829, 0.1070, 0.0672, 0.0909, 0.5440, 0.0099]) \n",
      "Test Loss tensor([0.0515, 0.0453, 0.0796, 0.1058, 0.0673, 0.0881, 0.5355, 0.0098])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5316, 0.6408, 0.1031, 0.1494, 0.5535, 0.2219])\n",
      "Valid Idx 3 | Loss tensor([0.4473, 0.9792, 0.2348, 0.4513, 0.7006])\n",
      "\n",
      "************** Batch 280 in 3.3145382404327393 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2274, 0.2813, 0.1903, 0.2573]) \n",
      "Test Loss tensor([0.2268, 0.2756, 0.1888, 0.2547])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4536, 0.5690, 0.0209, 0.3143, 0.6541]) \n",
      "Test Loss tensor([0.4660, 0.5848, 0.0200, 0.3235, 0.6378])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1192, 0.0642, 0.2815, 0.3344, 0.4028, 0.5425, 0.0221]) \n",
      "Test Loss tensor([0.1213, 0.0644, 0.2971, 0.3476, 0.4123, 0.5457, 0.0217])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0910, 0.2703, 0.1442, 0.1940, 0.5174, 0.7219, 0.0526]) \n",
      "Test Loss tensor([0.0884, 0.2648, 0.1504, 0.1912, 0.5138, 0.7274, 0.0605])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0538, 0.0450, 0.0800, 0.1078, 0.0681, 0.0886, 0.5499, 0.0095]) \n",
      "Test Loss tensor([0.0523, 0.0445, 0.0805, 0.1057, 0.0679, 0.0880, 0.5397, 0.0099])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5308, 0.6297, 0.1011, 0.1486, 0.5476, 0.2228])\n",
      "Valid Idx 3 | Loss tensor([0.4475, 0.9790, 0.2369, 0.4519, 0.7028])\n",
      "Gradients: Input 0.045373521745204926 | Message 0.04900696873664856 | Update 0.061718277633190155 | Output 0.17313507199287415\n",
      "\n",
      "************** Batch 284 in 3.280714988708496 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2298, 0.2752, 0.1930, 0.2691]) \n",
      "Test Loss tensor([0.2229, 0.2752, 0.1889, 0.2547])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4546, 0.5848, 0.0183, 0.3193, 0.6408]) \n",
      "Test Loss tensor([0.4663, 0.5914, 0.0192, 0.3247, 0.6377])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1199, 0.0644, 0.2956, 0.3380, 0.4162, 0.5520, 0.0203]) \n",
      "Test Loss tensor([0.1223, 0.0638, 0.2971, 0.3398, 0.4066, 0.5434, 0.0223])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0879, 0.2625, 0.1444, 0.1994, 0.5231, 0.7233, 0.0643]) \n",
      "Test Loss tensor([0.0883, 0.2633, 0.1509, 0.1925, 0.5150, 0.7182, 0.0605])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0497, 0.0430, 0.0802, 0.1064, 0.0672, 0.0906, 0.5309, 0.0093]) \n",
      "Test Loss tensor([0.0507, 0.0432, 0.0802, 0.1041, 0.0688, 0.0863, 0.5333, 0.0094])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5336, 0.6098, 0.1012, 0.1480, 0.5477, 0.2251])\n",
      "Valid Idx 3 | Loss tensor([0.4676, 0.9844, 0.2384, 0.4566, 0.7051])\n",
      "\n",
      "************** Batch 288 in 3.343817949295044 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2369, 0.2592, 0.1889, 0.2719]) \n",
      "Test Loss tensor([0.2246, 0.2772, 0.1891, 0.2574])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4628, 0.5843, 0.0202, 0.3171, 0.6382]) \n",
      "Test Loss tensor([0.4618, 0.5883, 0.0196, 0.3224, 0.6363])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1178, 0.0628, 0.2968, 0.3357, 0.4142, 0.5455, 0.0209]) \n",
      "Test Loss tensor([0.1215, 0.0634, 0.2925, 0.3403, 0.4081, 0.5419, 0.0222])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0879, 0.2611, 0.1551, 0.1947, 0.5251, 0.7079, 0.0583]) \n",
      "Test Loss tensor([0.0893, 0.2600, 0.1491, 0.1942, 0.5141, 0.7115, 0.0583])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0497, 0.0424, 0.0776, 0.1053, 0.0689, 0.0867, 0.5333, 0.0098]) \n",
      "Test Loss tensor([0.0499, 0.0433, 0.0792, 0.1041, 0.0694, 0.0849, 0.5293, 0.0094])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5323, 0.5979, 0.1008, 0.1476, 0.5463, 0.2280])\n",
      "Valid Idx 3 | Loss tensor([0.4816, 0.9861, 0.2456, 0.4563, 0.7068])\n",
      "\n",
      "************** Batch 292 in 3.3010988235473633 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2130, 0.2793, 0.1782, 0.2490]) \n",
      "Test Loss tensor([0.2258, 0.2751, 0.1907, 0.2565])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4553, 0.5914, 0.0173, 0.3113, 0.6302]) \n",
      "Test Loss tensor([0.4622, 0.5944, 0.0191, 0.3207, 0.6388])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1211, 0.0649, 0.2873, 0.3255, 0.4054, 0.5381, 0.0210]) \n",
      "Test Loss tensor([0.1223, 0.0629, 0.2946, 0.3446, 0.4098, 0.5432, 0.0228])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0900, 0.2701, 0.1521, 0.1958, 0.5208, 0.7125, 0.0579]) \n",
      "Test Loss tensor([0.0888, 0.2606, 0.1477, 0.1929, 0.5123, 0.7091, 0.0592])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0523, 0.0412, 0.0788, 0.1006, 0.0688, 0.0853, 0.5345, 0.0091]) \n",
      "Test Loss tensor([0.0494, 0.0433, 0.0791, 0.1029, 0.0686, 0.0843, 0.5270, 0.0094])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5334, 0.5901, 0.1000, 0.1483, 0.5455, 0.2276])\n",
      "Valid Idx 3 | Loss tensor([0.4697, 0.9820, 0.2399, 0.4616, 0.7104])\n",
      "\n",
      "************** Batch 296 in 3.2874488830566406 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2164, 0.2745, 0.1894, 0.2532]) \n",
      "Test Loss tensor([0.2254, 0.2787, 0.1894, 0.2553])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4653, 0.5796, 0.0184, 0.3152, 0.6485]) \n",
      "Test Loss tensor([0.4613, 0.5870, 0.0192, 0.3192, 0.6372])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1265, 0.0628, 0.2918, 0.3366, 0.4093, 0.5425, 0.0238]) \n",
      "Test Loss tensor([0.1231, 0.0628, 0.2899, 0.3391, 0.4087, 0.5402, 0.0234])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0894, 0.2636, 0.1500, 0.1915, 0.5165, 0.7188, 0.0524]) \n",
      "Test Loss tensor([0.0882, 0.2651, 0.1459, 0.1907, 0.5081, 0.7102, 0.0613])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0512, 0.0448, 0.0829, 0.1036, 0.0693, 0.0846, 0.5234, 0.0094]) \n",
      "Test Loss tensor([0.0506, 0.0430, 0.0792, 0.1013, 0.0685, 0.0859, 0.5295, 0.0095])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5336, 0.6025, 0.1011, 0.1475, 0.5503, 0.2260])\n",
      "Valid Idx 3 | Loss tensor([0.4349, 0.9767, 0.2298, 0.4565, 0.7065])\n",
      "\n",
      "************** Batch 300 in 3.2988808155059814 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2165, 0.2892, 0.1869, 0.2476]) \n",
      "Test Loss tensor([0.2213, 0.2787, 0.1881, 0.2521])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4603, 0.5947, 0.0204, 0.3195, 0.6401]) \n",
      "Test Loss tensor([0.4614, 0.5928, 0.0196, 0.3235, 0.6328])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1204, 0.0624, 0.2981, 0.3552, 0.3997, 0.5497, 0.0221]) \n",
      "Test Loss tensor([0.1266, 0.0622, 0.2898, 0.3430, 0.4111, 0.5435, 0.0238])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0906, 0.2610, 0.1427, 0.1893, 0.5165, 0.6949, 0.0625]) \n",
      "Test Loss tensor([0.0867, 0.2625, 0.1428, 0.1893, 0.5115, 0.7096, 0.0584])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0482, 0.0404, 0.0759, 0.0970, 0.0659, 0.0866, 0.5363, 0.0099]) \n",
      "Test Loss tensor([0.0500, 0.0436, 0.0783, 0.1010, 0.0680, 0.0862, 0.5234, 0.0095])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5339, 0.5941, 0.0992, 0.1464, 0.5484, 0.2265])\n",
      "Valid Idx 3 | Loss tensor([0.4208, 0.9739, 0.2273, 0.4527, 0.7032])\n",
      "Gradients: Input 0.04679838567972183 | Message 0.03470297530293465 | Update 0.04228658229112625 | Output 0.18611422181129456\n",
      "\n",
      "************** Batch 304 in 3.270270586013794 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2145, 0.2629, 0.1902, 0.2599]) \n",
      "Test Loss tensor([0.2192, 0.2737, 0.1869, 0.2523])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4701, 0.5893, 0.0184, 0.3226, 0.6358]) \n",
      "Test Loss tensor([0.4629, 0.5889, 0.0198, 0.3226, 0.6423])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1326, 0.0619, 0.3144, 0.3582, 0.4122, 0.5554, 0.0227]) \n",
      "Test Loss tensor([0.1237, 0.0619, 0.2882, 0.3427, 0.4093, 0.5389, 0.0237])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0879, 0.2560, 0.1446, 0.1883, 0.5176, 0.7096, 0.0586]) \n",
      "Test Loss tensor([0.0857, 0.2598, 0.1426, 0.1896, 0.5081, 0.7031, 0.0598])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0471, 0.0450, 0.0753, 0.1023, 0.0678, 0.0888, 0.5210, 0.0099]) \n",
      "Test Loss tensor([0.0495, 0.0430, 0.0770, 0.0992, 0.0682, 0.0863, 0.5254, 0.0096])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5323, 0.5954, 0.1014, 0.1498, 0.5516, 0.2269])\n",
      "Valid Idx 3 | Loss tensor([0.4039, 0.9719, 0.2254, 0.4528, 0.7046])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 308 in 3.291778802871704 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2259, 0.2739, 0.1839, 0.2474]) \n",
      "Test Loss tensor([0.2199, 0.2765, 0.1898, 0.2543])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4592, 0.5871, 0.0210, 0.3177, 0.6349]) \n",
      "Test Loss tensor([0.4609, 0.5942, 0.0197, 0.3228, 0.6388])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1195, 0.0628, 0.2858, 0.3306, 0.4059, 0.5422, 0.0233]) \n",
      "Test Loss tensor([0.1251, 0.0611, 0.2901, 0.3386, 0.4068, 0.5406, 0.0236])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0867, 0.2592, 0.1435, 0.1878, 0.5002, 0.7194, 0.0601]) \n",
      "Test Loss tensor([0.0852, 0.2606, 0.1424, 0.1904, 0.5052, 0.6997, 0.0596])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0523, 0.0424, 0.0810, 0.1002, 0.0666, 0.0870, 0.5236, 0.0098]) \n",
      "Test Loss tensor([0.0504, 0.0443, 0.0781, 0.0991, 0.0685, 0.0859, 0.5185, 0.0095])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5331, 0.5784, 0.1008, 0.1466, 0.5501, 0.2288])\n",
      "Valid Idx 3 | Loss tensor([0.4146, 0.9724, 0.2228, 0.4525, 0.7021])\n",
      "\n",
      "************** Batch 312 in 3.2919986248016357 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2201, 0.2691, 0.1885, 0.2560]) \n",
      "Test Loss tensor([0.2187, 0.2751, 0.1888, 0.2550])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4589, 0.5911, 0.0180, 0.3044, 0.6415]) \n",
      "Test Loss tensor([0.4601, 0.6025, 0.0198, 0.3225, 0.6383])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1196, 0.0615, 0.2868, 0.3393, 0.3923, 0.5357, 0.0220]) \n",
      "Test Loss tensor([0.1229, 0.0607, 0.2913, 0.3424, 0.4063, 0.5466, 0.0230])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0846, 0.2613, 0.1385, 0.1963, 0.5154, 0.7065, 0.0576]) \n",
      "Test Loss tensor([0.0873, 0.2571, 0.1441, 0.1919, 0.5025, 0.6878, 0.0586])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0475, 0.0422, 0.0791, 0.1007, 0.0669, 0.0860, 0.5305, 0.0091]) \n",
      "Test Loss tensor([0.0489, 0.0431, 0.0765, 0.0966, 0.0703, 0.0843, 0.5150, 0.0092])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5377, 0.5582, 0.1005, 0.1469, 0.5434, 0.2313])\n",
      "Valid Idx 3 | Loss tensor([0.4457, 0.9773, 0.2316, 0.4611, 0.7112])\n",
      "\n",
      "************** Batch 316 in 3.2851908206939697 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2147, 0.2615, 0.1901, 0.2598]) \n",
      "Test Loss tensor([0.2185, 0.2779, 0.1877, 0.2539])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4479, 0.5961, 0.0204, 0.3106, 0.6287]) \n",
      "Test Loss tensor([0.4563, 0.6074, 0.0201, 0.3198, 0.6390])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1195, 0.0607, 0.2910, 0.3448, 0.3982, 0.5352, 0.0208]) \n",
      "Test Loss tensor([0.1234, 0.0600, 0.2890, 0.3398, 0.4049, 0.5380, 0.0230])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0857, 0.2589, 0.1413, 0.1900, 0.5151, 0.6809, 0.0511]) \n",
      "Test Loss tensor([0.0862, 0.2577, 0.1440, 0.1932, 0.5060, 0.6800, 0.0587])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0485, 0.0403, 0.0763, 0.0961, 0.0705, 0.0860, 0.5091, 0.0095]) \n",
      "Test Loss tensor([0.0475, 0.0429, 0.0760, 0.0975, 0.0716, 0.0836, 0.5134, 0.0091])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5360, 0.5420, 0.0993, 0.1461, 0.5418, 0.2334])\n",
      "Valid Idx 3 | Loss tensor([0.4689, 0.9799, 0.2326, 0.4578, 0.7082])\n",
      "\n",
      "************** Batch 320 in 3.311030149459839 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2206, 0.2826, 0.1904, 0.2532]) \n",
      "Test Loss tensor([0.2168, 0.2787, 0.1899, 0.2534])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4595, 0.6045, 0.0181, 0.3152, 0.6433]) \n",
      "Test Loss tensor([0.4630, 0.6073, 0.0194, 0.3204, 0.6360])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1194, 0.0597, 0.2830, 0.3395, 0.4021, 0.5432, 0.0228]) \n",
      "Test Loss tensor([0.1244, 0.0592, 0.2876, 0.3376, 0.4056, 0.5399, 0.0233])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0835, 0.2512, 0.1460, 0.1940, 0.5053, 0.6773, 0.0600]) \n",
      "Test Loss tensor([0.0848, 0.2576, 0.1410, 0.1908, 0.5074, 0.6794, 0.0578])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0456, 0.0396, 0.0719, 0.0971, 0.0715, 0.0836, 0.5093, 0.0093]) \n",
      "Test Loss tensor([0.0490, 0.0419, 0.0755, 0.0959, 0.0713, 0.0835, 0.5083, 0.0091])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5345, 0.5474, 0.1009, 0.1479, 0.5450, 0.2328])\n",
      "Valid Idx 3 | Loss tensor([0.4458, 0.9780, 0.2292, 0.4515, 0.7094])\n",
      "Gradients: Input 0.057862088084220886 | Message 0.05851946771144867 | Update 0.10534203052520752 | Output 0.24905577301979065\n",
      "\n",
      "************** Batch 324 in 3.2979793548583984 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2164, 0.2659, 0.1889, 0.2592]) \n",
      "Test Loss tensor([0.2152, 0.2737, 0.1869, 0.2538])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4663, 0.5945, 0.0201, 0.3105, 0.6453]) \n",
      "Test Loss tensor([0.4581, 0.5963, 0.0199, 0.3159, 0.6385])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1206, 0.0592, 0.2771, 0.3317, 0.4138, 0.5483, 0.0205]) \n",
      "Test Loss tensor([0.1264, 0.0590, 0.2881, 0.3401, 0.4048, 0.5412, 0.0236])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0841, 0.2538, 0.1389, 0.1898, 0.5001, 0.6799, 0.0609]) \n",
      "Test Loss tensor([0.0833, 0.2627, 0.1404, 0.1864, 0.5030, 0.6870, 0.0588])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0477, 0.0436, 0.0766, 0.0998, 0.0701, 0.0859, 0.5009, 0.0093]) \n",
      "Test Loss tensor([0.0487, 0.0427, 0.0746, 0.0945, 0.0690, 0.0864, 0.5056, 0.0092])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5389, 0.5627, 0.0979, 0.1439, 0.5454, 0.2281])\n",
      "Valid Idx 3 | Loss tensor([0.4005, 0.9697, 0.2191, 0.4513, 0.7103])\n",
      "\n",
      "************** Batch 328 in 3.2799839973449707 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2172, 0.2803, 0.1926, 0.2604]) \n",
      "Test Loss tensor([0.2197, 0.2775, 0.1894, 0.2579])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4616, 0.6044, 0.0185, 0.3195, 0.6315]) \n",
      "Test Loss tensor([0.4625, 0.5997, 0.0201, 0.3237, 0.6325])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1267, 0.0599, 0.2797, 0.3384, 0.4043, 0.5419, 0.0236]) \n",
      "Test Loss tensor([0.1281, 0.0582, 0.2842, 0.3377, 0.4072, 0.5397, 0.0243])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0857, 0.2592, 0.1364, 0.1918, 0.4940, 0.6853, 0.0621]) \n",
      "Test Loss tensor([0.0826, 0.2596, 0.1380, 0.1850, 0.5023, 0.6863, 0.0584])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0484, 0.0424, 0.0735, 0.0904, 0.0710, 0.0889, 0.5110, 0.0090]) \n",
      "Test Loss tensor([0.0492, 0.0427, 0.0734, 0.0927, 0.0688, 0.0876, 0.5022, 0.0092])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5363, 0.5704, 0.1012, 0.1454, 0.5484, 0.2316])\n",
      "Valid Idx 3 | Loss tensor([0.3773, 0.9666, 0.2147, 0.4500, 0.7060])\n",
      "\n",
      "************** Batch 332 in 3.3019168376922607 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2044, 0.2753, 0.1860, 0.2482]) \n",
      "Test Loss tensor([0.2160, 0.2761, 0.1876, 0.2544])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4717, 0.6038, 0.0199, 0.3172, 0.6279]) \n",
      "Test Loss tensor([0.4576, 0.6037, 0.0205, 0.3217, 0.6313])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1264, 0.0584, 0.2853, 0.3367, 0.4065, 0.5406, 0.0223]) \n",
      "Test Loss tensor([0.1235, 0.0575, 0.2803, 0.3355, 0.4056, 0.5388, 0.0240])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0804, 0.2507, 0.1330, 0.1817, 0.4919, 0.6870, 0.0562]) \n",
      "Test Loss tensor([0.0829, 0.2592, 0.1396, 0.1848, 0.5010, 0.6806, 0.0589])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0496, 0.0426, 0.0737, 0.0925, 0.0687, 0.0891, 0.5158, 0.0093]) \n",
      "Test Loss tensor([0.0496, 0.0418, 0.0737, 0.0930, 0.0690, 0.0872, 0.5010, 0.0091])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5307, 0.5704, 0.0998, 0.1461, 0.5455, 0.2293])\n",
      "Valid Idx 3 | Loss tensor([0.3864, 0.9689, 0.2188, 0.4506, 0.7134])\n",
      "\n",
      "************** Batch 336 in 3.2695939540863037 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2087, 0.2753, 0.1889, 0.2610]) \n",
      "Test Loss tensor([0.2133, 0.2784, 0.1889, 0.2547])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4537, 0.6014, 0.0208, 0.3203, 0.6305]) \n",
      "Test Loss tensor([0.4528, 0.6028, 0.0198, 0.3198, 0.6354])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1251, 0.0585, 0.2908, 0.3491, 0.3946, 0.5414, 0.0256]) \n",
      "Test Loss tensor([0.1249, 0.0568, 0.2853, 0.3392, 0.3995, 0.5365, 0.0240])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0806, 0.2559, 0.1378, 0.1844, 0.4993, 0.6724, 0.0573]) \n",
      "Test Loss tensor([0.0820, 0.2573, 0.1385, 0.1843, 0.4963, 0.6729, 0.0575])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0516, 0.0444, 0.0771, 0.0921, 0.0674, 0.0883, 0.4933, 0.0095]) \n",
      "Test Loss tensor([0.0477, 0.0423, 0.0731, 0.0924, 0.0712, 0.0869, 0.4952, 0.0091])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5314, 0.5551, 0.1021, 0.1469, 0.5470, 0.2378])\n",
      "Valid Idx 3 | Loss tensor([0.4188, 0.9738, 0.2213, 0.4486, 0.7102])\n",
      "\n",
      "************** Batch 340 in 3.275819778442383 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2072, 0.2740, 0.1917, 0.2533]) \n",
      "Test Loss tensor([0.2156, 0.2753, 0.1881, 0.2559])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4483, 0.6066, 0.0191, 0.3187, 0.6296]) \n",
      "Test Loss tensor([0.4561, 0.6040, 0.0200, 0.3183, 0.6346])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1235, 0.0557, 0.3003, 0.3583, 0.4028, 0.5534, 0.0238]) \n",
      "Test Loss tensor([0.1228, 0.0562, 0.2882, 0.3426, 0.4017, 0.5411, 0.0234])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0826, 0.2516, 0.1382, 0.1862, 0.5017, 0.6768, 0.0581]) \n",
      "Test Loss tensor([0.0817, 0.2572, 0.1387, 0.1865, 0.4997, 0.6703, 0.0579])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0497, 0.0408, 0.0731, 0.0927, 0.0706, 0.0895, 0.4925, 0.0093]) \n",
      "Test Loss tensor([0.0482, 0.0419, 0.0719, 0.0906, 0.0712, 0.0863, 0.4979, 0.0090])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5362, 0.5473, 0.0982, 0.1438, 0.5427, 0.2367])\n",
      "Valid Idx 3 | Loss tensor([0.4233, 0.9754, 0.2226, 0.4508, 0.7148])\n",
      "Gradients: Input 0.044182855635881424 | Message 0.04078368842601776 | Update 0.07214514911174774 | Output 0.20428086817264557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 344 in 3.3167941570281982 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2051, 0.2785, 0.1876, 0.2548]) \n",
      "Test Loss tensor([0.2167, 0.2763, 0.1870, 0.2572])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4563, 0.6035, 0.0190, 0.3098, 0.6368]) \n",
      "Test Loss tensor([0.4597, 0.6063, 0.0208, 0.3202, 0.6316])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1265, 0.0564, 0.2845, 0.3388, 0.3974, 0.5358, 0.0263]) \n",
      "Test Loss tensor([0.1255, 0.0555, 0.2857, 0.3420, 0.4006, 0.5410, 0.0243])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0805, 0.2646, 0.1366, 0.1881, 0.4996, 0.6673, 0.0610]) \n",
      "Test Loss tensor([0.0811, 0.2559, 0.1360, 0.1828, 0.4973, 0.6735, 0.0571])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0449, 0.0443, 0.0724, 0.0906, 0.0698, 0.0864, 0.4913, 0.0089]) \n",
      "Test Loss tensor([0.0489, 0.0426, 0.0722, 0.0904, 0.0700, 0.0884, 0.4953, 0.0091])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5344, 0.5641, 0.0989, 0.1446, 0.5446, 0.2362])\n",
      "Valid Idx 3 | Loss tensor([0.3938, 0.9698, 0.2155, 0.4497, 0.7103])\n",
      "\n",
      "************** Batch 348 in 3.3059327602386475 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2004, 0.2714, 0.1896, 0.2504]) \n",
      "Test Loss tensor([0.2135, 0.2787, 0.1883, 0.2568])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4696, 0.6145, 0.0207, 0.3209, 0.6160]) \n",
      "Test Loss tensor([0.4603, 0.6045, 0.0201, 0.3214, 0.6300])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1182, 0.0558, 0.2802, 0.3489, 0.3882, 0.5328, 0.0246]) \n",
      "Test Loss tensor([0.1269, 0.0550, 0.2844, 0.3394, 0.4022, 0.5429, 0.0245])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0827, 0.2675, 0.1336, 0.1777, 0.4891, 0.6718, 0.0597]) \n",
      "Test Loss tensor([0.0789, 0.2539, 0.1340, 0.1783, 0.4913, 0.6780, 0.0572])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0494, 0.0450, 0.0748, 0.0927, 0.0691, 0.0865, 0.4964, 0.0094]) \n",
      "Test Loss tensor([0.0488, 0.0435, 0.0707, 0.0874, 0.0683, 0.0898, 0.4907, 0.0091])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5350, 0.5761, 0.1004, 0.1457, 0.5492, 0.2343])\n",
      "Valid Idx 3 | Loss tensor([0.3704, 0.9654, 0.2137, 0.4491, 0.7147])\n",
      "\n",
      "************** Batch 352 in 3.3325071334838867 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2105, 0.2689, 0.1827, 0.2584]) \n",
      "Test Loss tensor([0.2115, 0.2747, 0.1886, 0.2496])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4700, 0.6029, 0.0210, 0.3182, 0.6425]) \n",
      "Test Loss tensor([0.4603, 0.6082, 0.0203, 0.3236, 0.6308])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1217, 0.0547, 0.2861, 0.3433, 0.4044, 0.5287, 0.0233]) \n",
      "Test Loss tensor([0.1255, 0.0544, 0.2845, 0.3421, 0.4017, 0.5432, 0.0235])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0820, 0.2612, 0.1307, 0.1853, 0.5071, 0.6789, 0.0587]) \n",
      "Test Loss tensor([0.0783, 0.2555, 0.1303, 0.1789, 0.4944, 0.6796, 0.0570])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0481, 0.0422, 0.0727, 0.0859, 0.0698, 0.0921, 0.4894, 0.0089]) \n",
      "Test Loss tensor([0.0486, 0.0411, 0.0701, 0.0868, 0.0689, 0.0915, 0.4906, 0.0090])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5337, 0.5751, 0.1020, 0.1449, 0.5493, 0.2384])\n",
      "Valid Idx 3 | Loss tensor([0.3743, 0.9638, 0.2118, 0.4464, 0.7127])\n",
      "\n",
      "************** Batch 356 in 3.2957773208618164 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2140, 0.2680, 0.1900, 0.2631]) \n",
      "Test Loss tensor([0.2096, 0.2777, 0.1873, 0.2544])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4585, 0.5989, 0.0187, 0.3198, 0.6338]) \n",
      "Test Loss tensor([0.4559, 0.6060, 0.0202, 0.3190, 0.6317])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1307, 0.0556, 0.2900, 0.3448, 0.4078, 0.5541, 0.0271]) \n",
      "Test Loss tensor([0.1268, 0.0536, 0.2843, 0.3381, 0.3988, 0.5417, 0.0244])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0786, 0.2388, 0.1272, 0.1727, 0.4928, 0.6729, 0.0579]) \n",
      "Test Loss tensor([0.0798, 0.2534, 0.1327, 0.1771, 0.4933, 0.6727, 0.0578])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0481, 0.0402, 0.0678, 0.0848, 0.0706, 0.0918, 0.4877, 0.0090]) \n",
      "Test Loss tensor([0.0476, 0.0412, 0.0684, 0.0867, 0.0698, 0.0902, 0.4907, 0.0090])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5350, 0.5665, 0.0996, 0.1442, 0.5472, 0.2369])\n",
      "Valid Idx 3 | Loss tensor([0.4013, 0.9691, 0.2182, 0.4479, 0.7169])\n",
      "\n",
      "************** Batch 360 in 3.277892827987671 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2152, 0.2747, 0.1884, 0.2575]) \n",
      "Test Loss tensor([0.2112, 0.2743, 0.1878, 0.2559])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4605, 0.6014, 0.0214, 0.3135, 0.6341]) \n",
      "Test Loss tensor([0.4566, 0.6063, 0.0206, 0.3203, 0.6334])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1239, 0.0545, 0.2853, 0.3378, 0.4034, 0.5403, 0.0240]) \n",
      "Test Loss tensor([0.1250, 0.0529, 0.2837, 0.3393, 0.3952, 0.5415, 0.0232])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0771, 0.2614, 0.1351, 0.1773, 0.4954, 0.6545, 0.0582]) \n",
      "Test Loss tensor([0.0771, 0.2523, 0.1296, 0.1782, 0.4889, 0.6651, 0.0567])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0464, 0.0437, 0.0682, 0.0851, 0.0707, 0.0921, 0.4896, 0.0092]) \n",
      "Test Loss tensor([0.0473, 0.0411, 0.0687, 0.0864, 0.0705, 0.0893, 0.4909, 0.0088])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5371, 0.5596, 0.0993, 0.1425, 0.5464, 0.2405])\n",
      "Valid Idx 3 | Loss tensor([0.4233, 0.9709, 0.2266, 0.4436, 0.7126])\n",
      "Gradients: Input 0.02876695990562439 | Message 0.02233314700424671 | Update 0.03227454423904419 | Output 0.15236453711986542\n",
      "\n",
      "************** Batch 364 in 3.290027618408203 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2227, 0.2803, 0.2014, 0.2627]) \n",
      "Test Loss tensor([0.2123, 0.2773, 0.1883, 0.2533])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4356, 0.5919, 0.0203, 0.3053, 0.6481]) \n",
      "Test Loss tensor([0.4559, 0.6046, 0.0204, 0.3207, 0.6359])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1240, 0.0532, 0.2791, 0.3318, 0.3928, 0.5545, 0.0229]) \n",
      "Test Loss tensor([0.1221, 0.0522, 0.2796, 0.3375, 0.3972, 0.5443, 0.0232])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0810, 0.2506, 0.1339, 0.1773, 0.4957, 0.6699, 0.0599]) \n",
      "Test Loss tensor([0.0775, 0.2543, 0.1290, 0.1765, 0.4919, 0.6644, 0.0561])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0481, 0.0405, 0.0715, 0.0844, 0.0697, 0.0932, 0.4907, 0.0091]) \n",
      "Test Loss tensor([0.0472, 0.0409, 0.0681, 0.0851, 0.0703, 0.0907, 0.4892, 0.0087])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5348, 0.5597, 0.0976, 0.1426, 0.5435, 0.2443])\n",
      "Valid Idx 3 | Loss tensor([0.4215, 0.9682, 0.2254, 0.4470, 0.7175])\n",
      "\n",
      "************** Batch 368 in 3.2673192024230957 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2111, 0.2708, 0.1826, 0.2579]) \n",
      "Test Loss tensor([0.2091, 0.2763, 0.1896, 0.2553])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4604, 0.5999, 0.0191, 0.3194, 0.6387]) \n",
      "Test Loss tensor([0.4564, 0.6022, 0.0205, 0.3204, 0.6375])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1234, 0.0523, 0.2856, 0.3347, 0.3911, 0.5519, 0.0244]) \n",
      "Test Loss tensor([0.1240, 0.0516, 0.2800, 0.3388, 0.3961, 0.5438, 0.0239])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0755, 0.2480, 0.1278, 0.1781, 0.4882, 0.6667, 0.0559]) \n",
      "Test Loss tensor([0.0762, 0.2539, 0.1262, 0.1724, 0.4918, 0.6643, 0.0570])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0435, 0.0416, 0.0674, 0.0836, 0.0708, 0.0899, 0.4832, 0.0087]) \n",
      "Test Loss tensor([0.0477, 0.0419, 0.0673, 0.0826, 0.0685, 0.0913, 0.4855, 0.0087])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5336, 0.5740, 0.0978, 0.1416, 0.5511, 0.2356])\n",
      "Valid Idx 3 | Loss tensor([0.3900, 0.9612, 0.2158, 0.4419, 0.7117])\n",
      "\n",
      "************** Batch 372 in 3.28642201423645 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2068, 0.2801, 0.1878, 0.2548]) \n",
      "Test Loss tensor([0.2074, 0.2765, 0.1872, 0.2535])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4533, 0.5976, 0.0201, 0.3119, 0.6395]) \n",
      "Test Loss tensor([0.4581, 0.6000, 0.0196, 0.3198, 0.6401])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1178, 0.0524, 0.2880, 0.3512, 0.3795, 0.5373, 0.0239]) \n",
      "Test Loss tensor([0.1273, 0.0514, 0.2784, 0.3352, 0.3976, 0.5439, 0.0240])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0769, 0.2512, 0.1319, 0.1783, 0.4810, 0.6644, 0.0553]) \n",
      "Test Loss tensor([0.0745, 0.2586, 0.1231, 0.1696, 0.4856, 0.6724, 0.0580])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0488, 0.0421, 0.0677, 0.0844, 0.0703, 0.0927, 0.4849, 0.0086]) \n",
      "Test Loss tensor([0.0477, 0.0428, 0.0661, 0.0816, 0.0663, 0.0950, 0.4839, 0.0088])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5322, 0.5824, 0.1003, 0.1425, 0.5538, 0.2383])\n",
      "Valid Idx 3 | Loss tensor([0.3545, 0.9546, 0.2072, 0.4366, 0.7079])\n",
      "\n",
      "************** Batch 376 in 3.262080430984497 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2097, 0.2842, 0.1829, 0.2502]) \n",
      "Test Loss tensor([0.2085, 0.2747, 0.1883, 0.2551])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4696, 0.6030, 0.0196, 0.3201, 0.6386]) \n",
      "Test Loss tensor([0.4557, 0.6046, 0.0206, 0.3218, 0.6371])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1324, 0.0513, 0.2760, 0.3392, 0.3895, 0.5384, 0.0235]) \n",
      "Test Loss tensor([0.1274, 0.0505, 0.2786, 0.3385, 0.3944, 0.5407, 0.0233])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0757, 0.2609, 0.1242, 0.1659, 0.4900, 0.6731, 0.0580]) \n",
      "Test Loss tensor([0.0749, 0.2545, 0.1229, 0.1703, 0.4833, 0.6628, 0.0567])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0440, 0.0411, 0.0643, 0.0794, 0.0666, 0.0930, 0.4879, 0.0096]) \n",
      "Test Loss tensor([0.0477, 0.0421, 0.0660, 0.0805, 0.0673, 0.0946, 0.4846, 0.0087])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5343, 0.5753, 0.0979, 0.1393, 0.5512, 0.2361])\n",
      "Valid Idx 3 | Loss tensor([0.3638, 0.9557, 0.2042, 0.4422, 0.7150])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 380 in 3.3106484413146973 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2123, 0.2751, 0.1898, 0.2542]) \n",
      "Test Loss tensor([0.2066, 0.2781, 0.1882, 0.2513])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4578, 0.6018, 0.0184, 0.3197, 0.6407]) \n",
      "Test Loss tensor([0.4617, 0.6084, 0.0199, 0.3231, 0.6332])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1237, 0.0502, 0.2762, 0.3447, 0.3941, 0.5358, 0.0234]) \n",
      "Test Loss tensor([0.1244, 0.0500, 0.2818, 0.3409, 0.3935, 0.5458, 0.0230])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0752, 0.2541, 0.1233, 0.1680, 0.4856, 0.6615, 0.0589]) \n",
      "Test Loss tensor([0.0745, 0.2546, 0.1247, 0.1715, 0.4849, 0.6551, 0.0569])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0462, 0.0403, 0.0669, 0.0814, 0.0687, 0.0935, 0.4923, 0.0087]) \n",
      "Test Loss tensor([0.0469, 0.0415, 0.0657, 0.0795, 0.0694, 0.0920, 0.4835, 0.0085])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5336, 0.5617, 0.0979, 0.1416, 0.5481, 0.2394])\n",
      "Valid Idx 3 | Loss tensor([0.4062, 0.9653, 0.2167, 0.4425, 0.7197])\n",
      "Gradients: Input 0.051266610622406006 | Message 0.04260920733213425 | Update 0.058484867215156555 | Output 0.06983090937137604\n",
      "\n",
      "************** Batch 384 in 3.3095133304595947 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2051, 0.2787, 0.1905, 0.2433]) \n",
      "Test Loss tensor([0.2064, 0.2783, 0.1872, 0.2553])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4566, 0.6214, 0.0199, 0.3283, 0.6310]) \n",
      "Test Loss tensor([0.4566, 0.6145, 0.0194, 0.3209, 0.6310])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1309, 0.0501, 0.2869, 0.3488, 0.3904, 0.5531, 0.0229]) \n",
      "Test Loss tensor([0.1224, 0.0492, 0.2837, 0.3397, 0.3914, 0.5478, 0.0233])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0739, 0.2490, 0.1216, 0.1760, 0.4838, 0.6510, 0.0557]) \n",
      "Test Loss tensor([0.0753, 0.2530, 0.1248, 0.1764, 0.4828, 0.6423, 0.0560])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0465, 0.0442, 0.0653, 0.0800, 0.0680, 0.0906, 0.4918, 0.0087]) \n",
      "Test Loss tensor([0.0453, 0.0417, 0.0645, 0.0791, 0.0716, 0.0898, 0.4871, 0.0082])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5376, 0.5393, 0.0980, 0.1413, 0.5468, 0.2462])\n",
      "Valid Idx 3 | Loss tensor([0.4438, 0.9701, 0.2276, 0.4430, 0.7199])\n",
      "\n",
      "************** Batch 388 in 3.4633867740631104 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2082, 0.2863, 0.1895, 0.2539]) \n",
      "Test Loss tensor([0.2055, 0.2790, 0.1878, 0.2529])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4579, 0.6139, 0.0186, 0.3126, 0.6271]) \n",
      "Test Loss tensor([0.4578, 0.6148, 0.0197, 0.3210, 0.6340])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1217, 0.0495, 0.2853, 0.3442, 0.4038, 0.5622, 0.0217]) \n",
      "Test Loss tensor([0.1245, 0.0488, 0.2839, 0.3422, 0.3882, 0.5470, 0.0230])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0784, 0.2513, 0.1279, 0.1709, 0.5000, 0.6448, 0.0513]) \n",
      "Test Loss tensor([0.0760, 0.2515, 0.1258, 0.1770, 0.4816, 0.6404, 0.0551])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0422, 0.0402, 0.0648, 0.0772, 0.0716, 0.0856, 0.4775, 0.0079]) \n",
      "Test Loss tensor([0.0451, 0.0409, 0.0649, 0.0778, 0.0716, 0.0891, 0.4828, 0.0082])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5347, 0.5364, 0.0978, 0.1411, 0.5412, 0.2468])\n",
      "Valid Idx 3 | Loss tensor([0.4498, 0.9685, 0.2315, 0.4394, 0.7181])\n",
      "\n",
      "************** Batch 392 in 3.7496068477630615 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2141, 0.2680, 0.1863, 0.2672]) \n",
      "Test Loss tensor([0.2068, 0.2764, 0.1859, 0.2549])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4476, 0.6093, 0.0215, 0.3181, 0.6433]) \n",
      "Test Loss tensor([0.4559, 0.6102, 0.0201, 0.3219, 0.6359])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1231, 0.0491, 0.2839, 0.3361, 0.3841, 0.5587, 0.0229]) \n",
      "Test Loss tensor([0.1234, 0.0487, 0.2737, 0.3341, 0.3874, 0.5435, 0.0231])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0752, 0.2543, 0.1233, 0.1792, 0.4940, 0.6406, 0.0516]) \n",
      "Test Loss tensor([0.0742, 0.2575, 0.1232, 0.1685, 0.4831, 0.6499, 0.0560])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0445, 0.0446, 0.0665, 0.0801, 0.0690, 0.0876, 0.4800, 0.0079]) \n",
      "Test Loss tensor([0.0467, 0.0417, 0.0637, 0.0768, 0.0690, 0.0928, 0.4836, 0.0084])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5374, 0.5527, 0.0980, 0.1396, 0.5465, 0.2438])\n",
      "Valid Idx 3 | Loss tensor([0.4119, 0.9594, 0.2195, 0.4429, 0.7205])\n",
      "\n",
      "************** Batch 396 in 3.5255343914031982 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2019, 0.2897, 0.1822, 0.2559]) \n",
      "Test Loss tensor([0.2094, 0.2740, 0.1893, 0.2542])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4408, 0.5921, 0.0207, 0.3199, 0.6511]) \n",
      "Test Loss tensor([0.4572, 0.6090, 0.0205, 0.3199, 0.6334])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1239, 0.0491, 0.2934, 0.3511, 0.3834, 0.5579, 0.0256]) \n",
      "Test Loss tensor([0.1248, 0.0483, 0.2832, 0.3413, 0.3859, 0.5443, 0.0236])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0758, 0.2562, 0.1172, 0.1686, 0.4814, 0.6513, 0.0551]) \n",
      "Test Loss tensor([0.0727, 0.2559, 0.1192, 0.1663, 0.4809, 0.6535, 0.0576])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0470, 0.0396, 0.0615, 0.0792, 0.0708, 0.0928, 0.4811, 0.0083]) \n",
      "Test Loss tensor([0.0472, 0.0422, 0.0633, 0.0754, 0.0663, 0.0959, 0.4797, 0.0086])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5326, 0.5673, 0.0980, 0.1389, 0.5529, 0.2382])\n",
      "Valid Idx 3 | Loss tensor([0.3751, 0.9469, 0.2078, 0.4431, 0.7171])\n",
      "\n",
      "************** Batch 400 in 4.213750123977661 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2153, 0.2744, 0.1893, 0.2534]) \n",
      "Test Loss tensor([0.2103, 0.2762, 0.1899, 0.2601])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4630, 0.5932, 0.0187, 0.3195, 0.6427]) \n",
      "Test Loss tensor([0.4580, 0.6079, 0.0202, 0.3210, 0.6346])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1291, 0.0501, 0.2830, 0.3368, 0.3913, 0.5538, 0.0261]) \n",
      "Test Loss tensor([0.1265, 0.0476, 0.2772, 0.3364, 0.3821, 0.5411, 0.0238])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0705, 0.2498, 0.1231, 0.1646, 0.4884, 0.6567, 0.0587]) \n",
      "Test Loss tensor([0.0718, 0.2534, 0.1169, 0.1683, 0.4785, 0.6479, 0.0581])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0467, 0.0413, 0.0669, 0.0750, 0.0685, 0.0931, 0.4717, 0.0084]) \n",
      "Test Loss tensor([0.0464, 0.0416, 0.0624, 0.0750, 0.0669, 0.0973, 0.4821, 0.0086])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5364, 0.5636, 0.0995, 0.1409, 0.5493, 0.2397])\n",
      "Valid Idx 3 | Loss tensor([0.3881, 0.9487, 0.2093, 0.4419, 0.7159])\n",
      "Gradients: Input 0.06718672811985016 | Message 0.05943641439080238 | Update 0.0874478667974472 | Output 0.018597567453980446\n",
      "\n",
      "************** Batch 404 in 4.248676061630249 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2157, 0.2739, 0.1857, 0.2525]) \n",
      "Test Loss tensor([0.2049, 0.2762, 0.1887, 0.2545])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4608, 0.6068, 0.0197, 0.3220, 0.6400]) \n",
      "Test Loss tensor([0.4556, 0.6072, 0.0204, 0.3182, 0.6366])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1244, 0.0475, 0.2750, 0.3421, 0.3787, 0.5399, 0.0217]) \n",
      "Test Loss tensor([0.1241, 0.0471, 0.2798, 0.3411, 0.3850, 0.5502, 0.0233])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0713, 0.2505, 0.1199, 0.1672, 0.4887, 0.6531, 0.0566]) \n",
      "Test Loss tensor([0.0722, 0.2543, 0.1162, 0.1669, 0.4830, 0.6371, 0.0558])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0515, 0.0417, 0.0652, 0.0730, 0.0678, 0.0942, 0.4899, 0.0085]) \n",
      "Test Loss tensor([0.0458, 0.0422, 0.0624, 0.0728, 0.0690, 0.0952, 0.4807, 0.0085])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5363, 0.5484, 0.0977, 0.1400, 0.5473, 0.2445])\n",
      "Valid Idx 3 | Loss tensor([0.4225, 0.9529, 0.2166, 0.4432, 0.7223])\n",
      "\n",
      "************** Batch 408 in 3.969028949737549 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2047, 0.2708, 0.1929, 0.2555]) \n",
      "Test Loss tensor([0.2088, 0.2760, 0.1882, 0.2566])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4507, 0.6076, 0.0187, 0.3065, 0.6230]) \n",
      "Test Loss tensor([0.4570, 0.6156, 0.0203, 0.3213, 0.6322])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1272, 0.0465, 0.2796, 0.3392, 0.3820, 0.5602, 0.0236]) \n",
      "Test Loss tensor([0.1224, 0.0467, 0.2772, 0.3364, 0.3789, 0.5461, 0.0233])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0756, 0.2669, 0.1195, 0.1701, 0.4785, 0.6594, 0.0504]) \n",
      "Test Loss tensor([0.0748, 0.2577, 0.1176, 0.1679, 0.4798, 0.6278, 0.0564])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0469, 0.0401, 0.0651, 0.0763, 0.0681, 0.0958, 0.4844, 0.0084]) \n",
      "Test Loss tensor([0.0447, 0.0414, 0.0625, 0.0730, 0.0709, 0.0938, 0.4821, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5352, 0.5280, 0.0985, 0.1419, 0.5456, 0.2479])\n",
      "Valid Idx 3 | Loss tensor([0.4621, 0.9529, 0.2295, 0.4453, 0.7197])\n",
      "\n",
      "************** Batch 412 in 4.0773985385894775 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2036, 0.2802, 0.1897, 0.2525]) \n",
      "Test Loss tensor([0.2061, 0.2749, 0.1878, 0.2563])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4556, 0.6260, 0.0219, 0.3248, 0.6114]) \n",
      "Test Loss tensor([0.4557, 0.6142, 0.0204, 0.3166, 0.6299])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1238, 0.0470, 0.2682, 0.3236, 0.3775, 0.5472, 0.0253]) \n",
      "Test Loss tensor([0.1244, 0.0465, 0.2805, 0.3375, 0.3772, 0.5447, 0.0234])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0756, 0.2506, 0.1210, 0.1797, 0.4780, 0.6352, 0.0533]) \n",
      "Test Loss tensor([0.0738, 0.2546, 0.1175, 0.1702, 0.4809, 0.6224, 0.0546])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0472, 0.0399, 0.0636, 0.0736, 0.0686, 0.0924, 0.4831, 0.0081]) \n",
      "Test Loss tensor([0.0448, 0.0418, 0.0618, 0.0720, 0.0716, 0.0919, 0.4821, 0.0082])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5409, 0.5179, 0.0988, 0.1421, 0.5448, 0.2513])\n",
      "Valid Idx 3 | Loss tensor([0.4835, 0.9578, 0.2351, 0.4455, 0.7227])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 416 in 3.7201671600341797 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2202, 0.2717, 0.1815, 0.2567]) \n",
      "Test Loss tensor([0.2044, 0.2788, 0.1883, 0.2538])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4477, 0.6042, 0.0191, 0.3142, 0.6470]) \n",
      "Test Loss tensor([0.4560, 0.6111, 0.0202, 0.3203, 0.6340])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1171, 0.0464, 0.2712, 0.3267, 0.3649, 0.5349, 0.0232]) \n",
      "Test Loss tensor([0.1222, 0.0460, 0.2784, 0.3406, 0.3741, 0.5434, 0.0229])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0724, 0.2458, 0.1185, 0.1679, 0.4678, 0.6151, 0.0561]) \n",
      "Test Loss tensor([0.0748, 0.2562, 0.1181, 0.1682, 0.4871, 0.6243, 0.0564])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0436, 0.0439, 0.0600, 0.0736, 0.0702, 0.0975, 0.4749, 0.0082]) \n",
      "Test Loss tensor([0.0450, 0.0407, 0.0616, 0.0717, 0.0704, 0.0944, 0.4801, 0.0081])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5388, 0.5223, 0.0980, 0.1421, 0.5472, 0.2489])\n",
      "Valid Idx 3 | Loss tensor([0.4691, 0.9514, 0.2258, 0.4418, 0.7210])\n",
      "\n",
      "************** Batch 420 in 3.508338451385498 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2126, 0.2544, 0.1916, 0.2623]) \n",
      "Test Loss tensor([0.2038, 0.2758, 0.1885, 0.2558])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4641, 0.6026, 0.0180, 0.3300, 0.6386]) \n",
      "Test Loss tensor([0.4587, 0.6094, 0.0211, 0.3186, 0.6334])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1149, 0.0465, 0.2870, 0.3444, 0.3861, 0.5478, 0.0204]) \n",
      "Test Loss tensor([0.1230, 0.0460, 0.2805, 0.3410, 0.3732, 0.5435, 0.0231])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0720, 0.2565, 0.1124, 0.1613, 0.4874, 0.6207, 0.0572]) \n",
      "Test Loss tensor([0.0745, 0.2564, 0.1161, 0.1683, 0.4795, 0.6254, 0.0569])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0436, 0.0401, 0.0584, 0.0711, 0.0687, 0.0933, 0.4875, 0.0080]) \n",
      "Test Loss tensor([0.0462, 0.0419, 0.0618, 0.0705, 0.0688, 0.0943, 0.4812, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5374, 0.5302, 0.0988, 0.1412, 0.5455, 0.2498])\n",
      "Valid Idx 3 | Loss tensor([0.4604, 0.9510, 0.2239, 0.4405, 0.7194])\n",
      "Gradients: Input 0.04299662262201309 | Message 0.01803971827030182 | Update 0.02212618663907051 | Output 0.06676925718784332\n",
      "\n",
      "************** Batch 424 in 3.569589853286743 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2109, 0.2755, 0.1931, 0.2587]) \n",
      "Test Loss tensor([0.2064, 0.2786, 0.1894, 0.2560])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4382, 0.6114, 0.0180, 0.3101, 0.6400]) \n",
      "Test Loss tensor([0.4568, 0.6024, 0.0208, 0.3160, 0.6388])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1175, 0.0457, 0.2785, 0.3408, 0.3709, 0.5502, 0.0238]) \n",
      "Test Loss tensor([0.1243, 0.0454, 0.2750, 0.3378, 0.3738, 0.5464, 0.0241])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0748, 0.2603, 0.1195, 0.1640, 0.4900, 0.6336, 0.0637]) \n",
      "Test Loss tensor([0.0728, 0.2563, 0.1140, 0.1640, 0.4810, 0.6313, 0.0566])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0479, 0.0439, 0.0630, 0.0711, 0.0684, 0.0958, 0.4769, 0.0081]) \n",
      "Test Loss tensor([0.0458, 0.0416, 0.0606, 0.0698, 0.0668, 0.0972, 0.4790, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5361, 0.5389, 0.0985, 0.1396, 0.5462, 0.2479])\n",
      "Valid Idx 3 | Loss tensor([0.4352, 0.9415, 0.2133, 0.4378, 0.7195])\n",
      "\n",
      "************** Batch 428 in 3.5367143154144287 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2014, 0.2846, 0.1864, 0.2568]) \n",
      "Test Loss tensor([0.2030, 0.2766, 0.1849, 0.2534])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4569, 0.6023, 0.0189, 0.3177, 0.6430]) \n",
      "Test Loss tensor([0.4499, 0.6006, 0.0208, 0.3174, 0.6391])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1235, 0.0456, 0.2609, 0.3254, 0.3752, 0.5443, 0.0242]) \n",
      "Test Loss tensor([0.1242, 0.0450, 0.2768, 0.3360, 0.3721, 0.5428, 0.0232])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0758, 0.2562, 0.1137, 0.1640, 0.4731, 0.6284, 0.0513]) \n",
      "Test Loss tensor([0.0720, 0.2559, 0.1124, 0.1635, 0.4751, 0.6267, 0.0559])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0457, 0.0409, 0.0617, 0.0661, 0.0659, 0.0956, 0.4839, 0.0082]) \n",
      "Test Loss tensor([0.0459, 0.0411, 0.0604, 0.0694, 0.0672, 0.0967, 0.4779, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5393, 0.5378, 0.0970, 0.1410, 0.5431, 0.2508])\n",
      "Valid Idx 3 | Loss tensor([0.4502, 0.9433, 0.2163, 0.4375, 0.7200])\n",
      "\n",
      "************** Batch 432 in 3.4896745681762695 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2078, 0.2809, 0.1784, 0.2476]) \n",
      "Test Loss tensor([0.2029, 0.2717, 0.1889, 0.2564])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4488, 0.6057, 0.0204, 0.3210, 0.6234]) \n",
      "Test Loss tensor([0.4526, 0.6080, 0.0198, 0.3179, 0.6317])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1273, 0.0450, 0.2764, 0.3445, 0.3625, 0.5409, 0.0253]) \n",
      "Test Loss tensor([0.1225, 0.0449, 0.2751, 0.3375, 0.3670, 0.5419, 0.0229])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0695, 0.2601, 0.1172, 0.1657, 0.4738, 0.6170, 0.0541]) \n",
      "Test Loss tensor([0.0751, 0.2546, 0.1126, 0.1662, 0.4785, 0.6147, 0.0560])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0440, 0.0429, 0.0611, 0.0735, 0.0655, 0.0967, 0.4734, 0.0080]) \n",
      "Test Loss tensor([0.0438, 0.0409, 0.0600, 0.0693, 0.0697, 0.0940, 0.4772, 0.0081])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5412, 0.5196, 0.0978, 0.1439, 0.5439, 0.2594])\n",
      "Valid Idx 3 | Loss tensor([0.4920, 0.9527, 0.2315, 0.4397, 0.7240])\n",
      "\n",
      "************** Batch 436 in 3.6473422050476074 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2106, 0.2829, 0.1833, 0.2614]) \n",
      "Test Loss tensor([0.2027, 0.2736, 0.1891, 0.2571])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4557, 0.6142, 0.0224, 0.3148, 0.6251]) \n",
      "Test Loss tensor([0.4549, 0.6087, 0.0199, 0.3161, 0.6337])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1291, 0.0450, 0.2751, 0.3341, 0.3742, 0.5651, 0.0245]) \n",
      "Test Loss tensor([0.1214, 0.0441, 0.2774, 0.3377, 0.3691, 0.5480, 0.0226])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0721, 0.2492, 0.1181, 0.1664, 0.4733, 0.6173, 0.0553]) \n",
      "Test Loss tensor([0.0748, 0.2489, 0.1118, 0.1657, 0.4847, 0.6091, 0.0525])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0432, 0.0408, 0.0601, 0.0731, 0.0709, 0.0960, 0.4832, 0.0083]) \n",
      "Test Loss tensor([0.0435, 0.0403, 0.0587, 0.0691, 0.0709, 0.0932, 0.4816, 0.0081])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5409, 0.5084, 0.0960, 0.1411, 0.5407, 0.2643])\n",
      "Valid Idx 3 | Loss tensor([0.5140, 0.9561, 0.2365, 0.4427, 0.7293])\n",
      "\n",
      "************** Batch 440 in 3.5040178298950195 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2055, 0.2771, 0.1899, 0.2512]) \n",
      "Test Loss tensor([0.2008, 0.2748, 0.1848, 0.2523])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4355, 0.5952, 0.0205, 0.3096, 0.6267]) \n",
      "Test Loss tensor([0.4554, 0.6071, 0.0203, 0.3153, 0.6298])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1142, 0.0440, 0.2862, 0.3448, 0.3525, 0.5426, 0.0220]) \n",
      "Test Loss tensor([0.1202, 0.0441, 0.2782, 0.3342, 0.3678, 0.5471, 0.0224])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0706, 0.2534, 0.1155, 0.1591, 0.4894, 0.6114, 0.0467]) \n",
      "Test Loss tensor([0.0758, 0.2550, 0.1141, 0.1685, 0.4803, 0.6029, 0.0559])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0432, 0.0426, 0.0599, 0.0682, 0.0719, 0.0924, 0.4757, 0.0079]) \n",
      "Test Loss tensor([0.0435, 0.0401, 0.0600, 0.0681, 0.0702, 0.0922, 0.4796, 0.0081])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5445, 0.5001, 0.0986, 0.1413, 0.5339, 0.2657])\n",
      "Valid Idx 3 | Loss tensor([0.5205, 0.9542, 0.2395, 0.4376, 0.7291])\n",
      "Gradients: Input 0.03231683745980263 | Message 0.02034013904631138 | Update 0.027713019400835037 | Output 0.0931374579668045\n",
      "\n",
      "************** Batch 444 in 3.3145930767059326 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2025, 0.2747, 0.1904, 0.2488]) \n",
      "Test Loss tensor([0.2010, 0.2767, 0.1889, 0.2561])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4463, 0.5936, 0.0172, 0.3099, 0.6506]) \n",
      "Test Loss tensor([0.4551, 0.6079, 0.0206, 0.3185, 0.6308])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1217, 0.0433, 0.2761, 0.3351, 0.3711, 0.5493, 0.0245]) \n",
      "Test Loss tensor([0.1224, 0.0441, 0.2753, 0.3386, 0.3641, 0.5462, 0.0233])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0739, 0.2575, 0.1102, 0.1644, 0.4852, 0.6090, 0.0598]) \n",
      "Test Loss tensor([0.0751, 0.2538, 0.1136, 0.1667, 0.4797, 0.6098, 0.0550])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0412, 0.0377, 0.0583, 0.0674, 0.0698, 0.0952, 0.4854, 0.0076]) \n",
      "Test Loss tensor([0.0449, 0.0405, 0.0585, 0.0667, 0.0683, 0.0938, 0.4769, 0.0079])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5418, 0.5111, 0.0988, 0.1424, 0.5412, 0.2653])\n",
      "Valid Idx 3 | Loss tensor([0.4962, 0.9468, 0.2244, 0.4399, 0.7258])\n",
      "\n",
      "************** Batch 448 in 3.6357815265655518 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2040, 0.2887, 0.1987, 0.2525]) \n",
      "Test Loss tensor([0.1974, 0.2778, 0.1833, 0.2484])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4621, 0.6063, 0.0205, 0.3147, 0.6307]) \n",
      "Test Loss tensor([0.4529, 0.6030, 0.0201, 0.3177, 0.6369])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1232, 0.0435, 0.2838, 0.3434, 0.3663, 0.5581, 0.0224]) \n",
      "Test Loss tensor([0.1241, 0.0435, 0.2748, 0.3361, 0.3597, 0.5431, 0.0240])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0770, 0.2548, 0.1145, 0.1779, 0.4820, 0.6132, 0.0524]) \n",
      "Test Loss tensor([0.0729, 0.2557, 0.1111, 0.1607, 0.4784, 0.6103, 0.0538])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0439, 0.0403, 0.0584, 0.0677, 0.0696, 0.0944, 0.4765, 0.0078]) \n",
      "Test Loss tensor([0.0449, 0.0418, 0.0587, 0.0661, 0.0664, 0.0957, 0.4823, 0.0082])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5399, 0.5118, 0.0956, 0.1421, 0.5398, 0.2612])\n",
      "Valid Idx 3 | Loss tensor([0.4766, 0.9316, 0.2165, 0.4371, 0.7244])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 452 in 3.7257275581359863 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1993, 0.2833, 0.1906, 0.2495]) \n",
      "Test Loss tensor([0.1985, 0.2779, 0.1851, 0.2526])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4581, 0.5910, 0.0205, 0.3098, 0.6553]) \n",
      "Test Loss tensor([0.4525, 0.6103, 0.0210, 0.3208, 0.6288])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1222, 0.0443, 0.2884, 0.3448, 0.3555, 0.5533, 0.0239]) \n",
      "Test Loss tensor([0.1227, 0.0435, 0.2735, 0.3329, 0.3589, 0.5419, 0.0236])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0738, 0.2470, 0.1059, 0.1632, 0.4790, 0.6070, 0.0526]) \n",
      "Test Loss tensor([0.0762, 0.2522, 0.1111, 0.1671, 0.4794, 0.5934, 0.0549])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0444, 0.0389, 0.0599, 0.0645, 0.0683, 0.0962, 0.4953, 0.0078]) \n",
      "Test Loss tensor([0.0438, 0.0422, 0.0578, 0.0651, 0.0693, 0.0928, 0.4784, 0.0080])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5421, 0.4929, 0.0978, 0.1426, 0.5382, 0.2660])\n",
      "Valid Idx 3 | Loss tensor([0.5136, 0.9406, 0.2240, 0.4439, 0.7304])\n",
      "\n",
      "************** Batch 456 in 3.726356029510498 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2027, 0.2663, 0.1858, 0.2605]) \n",
      "Test Loss tensor([0.1984, 0.2767, 0.1853, 0.2510])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4551, 0.6109, 0.0203, 0.3114, 0.6250]) \n",
      "Test Loss tensor([0.4516, 0.6145, 0.0201, 0.3150, 0.6308])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1193, 0.0427, 0.2643, 0.3249, 0.3518, 0.5417, 0.0232]) \n",
      "Test Loss tensor([0.1220, 0.0431, 0.2769, 0.3393, 0.3586, 0.5460, 0.0231])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0762, 0.2579, 0.1136, 0.1622, 0.4838, 0.6064, 0.0571]) \n",
      "Test Loss tensor([0.0777, 0.2496, 0.1110, 0.1691, 0.4753, 0.5797, 0.0548])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0410, 0.0441, 0.0545, 0.0628, 0.0712, 0.0938, 0.4939, 0.0077]) \n",
      "Test Loss tensor([0.0434, 0.0404, 0.0579, 0.0654, 0.0725, 0.0894, 0.4827, 0.0080])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5440, 0.4686, 0.0958, 0.1417, 0.5345, 0.2730])\n",
      "Valid Idx 3 | Loss tensor([0.5554, 0.9435, 0.2431, 0.4431, 0.7297])\n",
      "\n",
      "************** Batch 460 in 3.722275733947754 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2029, 0.2687, 0.1874, 0.2616]) \n",
      "Test Loss tensor([0.1995, 0.2784, 0.1870, 0.2565])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4527, 0.6128, 0.0196, 0.3013, 0.6264]) \n",
      "Test Loss tensor([0.4495, 0.6099, 0.0206, 0.3185, 0.6327])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1273, 0.0424, 0.2940, 0.3523, 0.3615, 0.5671, 0.0217]) \n",
      "Test Loss tensor([0.1246, 0.0429, 0.2710, 0.3315, 0.3550, 0.5392, 0.0242])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0752, 0.2652, 0.1131, 0.1667, 0.4840, 0.5782, 0.0492]) \n",
      "Test Loss tensor([0.0773, 0.2546, 0.1094, 0.1661, 0.4812, 0.5842, 0.0556])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0418, 0.0397, 0.0587, 0.0679, 0.0728, 0.0947, 0.4720, 0.0078]) \n",
      "Test Loss tensor([0.0427, 0.0412, 0.0568, 0.0638, 0.0708, 0.0911, 0.4830, 0.0079])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5402, 0.4742, 0.0976, 0.1451, 0.5337, 0.2756])\n",
      "Valid Idx 3 | Loss tensor([0.5312, 0.9292, 0.2263, 0.4389, 0.7226])\n",
      "Gradients: Input 0.08218246698379517 | Message 0.09009495377540588 | Update 0.15611478686332703 | Output 0.1494574248790741\n",
      "\n",
      "************** Batch 464 in 3.700202703475952 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2087, 0.2758, 0.1845, 0.2538]) \n",
      "Test Loss tensor([0.1977, 0.2775, 0.1857, 0.2476])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4580, 0.6089, 0.0215, 0.3131, 0.6373]) \n",
      "Test Loss tensor([0.4492, 0.6040, 0.0209, 0.3181, 0.6391])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1261, 0.0430, 0.2798, 0.3359, 0.3511, 0.5463, 0.0254]) \n",
      "Test Loss tensor([0.1258, 0.0426, 0.2703, 0.3309, 0.3566, 0.5442, 0.0249])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0775, 0.2649, 0.1079, 0.1658, 0.4680, 0.5867, 0.0558]) \n",
      "Test Loss tensor([0.0737, 0.2556, 0.1042, 0.1608, 0.4757, 0.5902, 0.0544])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0424, 0.0418, 0.0616, 0.0682, 0.0685, 0.0923, 0.4706, 0.0079]) \n",
      "Test Loss tensor([0.0450, 0.0426, 0.0571, 0.0624, 0.0673, 0.0949, 0.4805, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5417, 0.4926, 0.0956, 0.1422, 0.5339, 0.2675])\n",
      "Valid Idx 3 | Loss tensor([0.4877, 0.9145, 0.2051, 0.4350, 0.7244])\n",
      "\n",
      "************** Batch 468 in 3.512969970703125 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1946, 0.2799, 0.1860, 0.2492]) \n",
      "Test Loss tensor([0.1973, 0.2781, 0.1831, 0.2512])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4369, 0.6099, 0.0223, 0.3148, 0.6303]) \n",
      "Test Loss tensor([0.4507, 0.6077, 0.0216, 0.3205, 0.6364])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1262, 0.0423, 0.2750, 0.3342, 0.3520, 0.5371, 0.0227]) \n",
      "Test Loss tensor([0.1265, 0.0419, 0.2741, 0.3367, 0.3538, 0.5445, 0.0246])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0708, 0.2563, 0.1079, 0.1660, 0.4841, 0.5935, 0.0587]) \n",
      "Test Loss tensor([0.0745, 0.2547, 0.1045, 0.1615, 0.4775, 0.5846, 0.0550])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0452, 0.0425, 0.0616, 0.0631, 0.0670, 0.0951, 0.4743, 0.0085]) \n",
      "Test Loss tensor([0.0431, 0.0420, 0.0558, 0.0612, 0.0676, 0.0958, 0.4760, 0.0081])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5448, 0.4840, 0.0991, 0.1414, 0.5352, 0.2718])\n",
      "Valid Idx 3 | Loss tensor([0.4963, 0.9095, 0.2060, 0.4391, 0.7267])\n",
      "\n",
      "************** Batch 472 in 3.785823345184326 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2043, 0.2789, 0.1807, 0.2571]) \n",
      "Test Loss tensor([0.1975, 0.2779, 0.1838, 0.2516])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4577, 0.6118, 0.0233, 0.3169, 0.6255]) \n",
      "Test Loss tensor([0.4470, 0.6089, 0.0207, 0.3148, 0.6363])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1254, 0.0421, 0.2673, 0.3376, 0.3619, 0.5446, 0.0246]) \n",
      "Test Loss tensor([0.1233, 0.0419, 0.2728, 0.3324, 0.3485, 0.5454, 0.0232])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0737, 0.2516, 0.1031, 0.1667, 0.4776, 0.5905, 0.0580]) \n",
      "Test Loss tensor([0.0775, 0.2571, 0.1079, 0.1647, 0.4777, 0.5684, 0.0550])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0428, 0.0408, 0.0575, 0.0611, 0.0689, 0.0955, 0.4788, 0.0083]) \n",
      "Test Loss tensor([0.0430, 0.0410, 0.0572, 0.0620, 0.0730, 0.0906, 0.4810, 0.0078])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5463, 0.4588, 0.0956, 0.1463, 0.5275, 0.2778])\n",
      "Valid Idx 3 | Loss tensor([0.5625, 0.9238, 0.2323, 0.4435, 0.7330])\n",
      "\n",
      "************** Batch 476 in 3.588153600692749 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2000, 0.2873, 0.1797, 0.2469]) \n",
      "Test Loss tensor([0.1991, 0.2753, 0.1871, 0.2546])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4473, 0.6076, 0.0211, 0.3102, 0.6243]) \n",
      "Test Loss tensor([0.4456, 0.6083, 0.0207, 0.3138, 0.6327])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1216, 0.0417, 0.2746, 0.3349, 0.3375, 0.5363, 0.0236]) \n",
      "Test Loss tensor([0.1244, 0.0415, 0.2702, 0.3319, 0.3464, 0.5410, 0.0242])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0812, 0.2589, 0.1055, 0.1668, 0.4742, 0.5637, 0.0500]) \n",
      "Test Loss tensor([0.0778, 0.2496, 0.1054, 0.1669, 0.4802, 0.5683, 0.0538])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0406, 0.0400, 0.0563, 0.0594, 0.0721, 0.0887, 0.4906, 0.0079]) \n",
      "Test Loss tensor([0.0430, 0.0406, 0.0567, 0.0621, 0.0731, 0.0916, 0.4809, 0.0078])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5459, 0.4574, 0.0958, 0.1451, 0.5279, 0.2799])\n",
      "Valid Idx 3 | Loss tensor([0.5711, 0.9241, 0.2355, 0.4412, 0.7297])\n",
      "\n",
      "************** Batch 480 in 3.9062435626983643 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2000, 0.2887, 0.1832, 0.2559]) \n",
      "Test Loss tensor([0.1963, 0.2803, 0.1844, 0.2502])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4427, 0.5952, 0.0218, 0.3008, 0.6523]) \n",
      "Test Loss tensor([0.4484, 0.6095, 0.0212, 0.3141, 0.6316])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1247, 0.0424, 0.2732, 0.3347, 0.3417, 0.5407, 0.0253]) \n",
      "Test Loss tensor([0.1247, 0.0412, 0.2710, 0.3361, 0.3451, 0.5409, 0.0239])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0840, 0.2590, 0.1083, 0.1709, 0.4741, 0.5575, 0.0562]) \n",
      "Test Loss tensor([0.0776, 0.2522, 0.1043, 0.1631, 0.4753, 0.5641, 0.0545])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0484, 0.0417, 0.0593, 0.0590, 0.0714, 0.0908, 0.4771, 0.0082]) \n",
      "Test Loss tensor([0.0422, 0.0407, 0.0552, 0.0600, 0.0714, 0.0932, 0.4844, 0.0080])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5420, 0.4688, 0.0949, 0.1435, 0.5272, 0.2804])\n",
      "Valid Idx 3 | Loss tensor([0.5607, 0.9179, 0.2284, 0.4392, 0.7312])\n",
      "Gradients: Input 0.0366947315633297 | Message 0.04178651422262192 | Update 0.06763586401939392 | Output 0.114352747797966\n",
      "\n",
      "************** Batch 484 in 4.026463031768799 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1973, 0.2801, 0.1803, 0.2465]) \n",
      "Test Loss tensor([0.1934, 0.2778, 0.1855, 0.2501])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4470, 0.6054, 0.0211, 0.3137, 0.6263]) \n",
      "Test Loss tensor([0.4476, 0.6000, 0.0215, 0.3136, 0.6331])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1272, 0.0418, 0.2776, 0.3373, 0.3370, 0.5433, 0.0254]) \n",
      "Test Loss tensor([0.1253, 0.0409, 0.2713, 0.3340, 0.3450, 0.5418, 0.0246])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0745, 0.2616, 0.1045, 0.1616, 0.4789, 0.5611, 0.0538]) \n",
      "Test Loss tensor([0.0775, 0.2550, 0.1036, 0.1623, 0.4744, 0.5679, 0.0541])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0436, 0.0448, 0.0551, 0.0622, 0.0701, 0.0944, 0.4688, 0.0077]) \n",
      "Test Loss tensor([0.0429, 0.0416, 0.0542, 0.0608, 0.0703, 0.0949, 0.4776, 0.0081])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5465, 0.4706, 0.0962, 0.1442, 0.5294, 0.2805])\n",
      "Valid Idx 3 | Loss tensor([0.5566, 0.9143, 0.2242, 0.4394, 0.7267])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 488 in 4.227091312408447 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1959, 0.2648, 0.1885, 0.2520]) \n",
      "Test Loss tensor([0.1970, 0.2770, 0.1870, 0.2530])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4368, 0.6119, 0.0201, 0.3109, 0.6369]) \n",
      "Test Loss tensor([0.4436, 0.6067, 0.0214, 0.3149, 0.6312])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1262, 0.0405, 0.2679, 0.3191, 0.3372, 0.5467, 0.0264]) \n",
      "Test Loss tensor([0.1233, 0.0405, 0.2712, 0.3349, 0.3425, 0.5443, 0.0234])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0780, 0.2514, 0.1073, 0.1662, 0.4697, 0.5777, 0.0541]) \n",
      "Test Loss tensor([0.0770, 0.2557, 0.1041, 0.1643, 0.4761, 0.5638, 0.0534])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0426, 0.0424, 0.0499, 0.0581, 0.0703, 0.0934, 0.4818, 0.0077]) \n",
      "Test Loss tensor([0.0415, 0.0426, 0.0546, 0.0587, 0.0716, 0.0939, 0.4780, 0.0079])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5480, 0.4658, 0.0970, 0.1433, 0.5308, 0.2839])\n",
      "Valid Idx 3 | Loss tensor([0.5813, 0.9174, 0.2362, 0.4450, 0.7356])\n",
      "\n",
      "************** Batch 492 in 4.2911906242370605 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1983, 0.2727, 0.1875, 0.2554]) \n",
      "Test Loss tensor([0.1941, 0.2767, 0.1871, 0.2512])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4410, 0.5948, 0.0195, 0.3106, 0.6371]) \n",
      "Test Loss tensor([0.4451, 0.6086, 0.0205, 0.3128, 0.6307])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1293, 0.0416, 0.2815, 0.3434, 0.3338, 0.5487, 0.0227]) \n",
      "Test Loss tensor([0.1235, 0.0402, 0.2728, 0.3373, 0.3416, 0.5451, 0.0233])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0748, 0.2624, 0.1032, 0.1680, 0.4697, 0.5653, 0.0497]) \n",
      "Test Loss tensor([0.0793, 0.2555, 0.1054, 0.1669, 0.4751, 0.5498, 0.0524])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0445, 0.0429, 0.0574, 0.0592, 0.0722, 0.0952, 0.4771, 0.0081]) \n",
      "Test Loss tensor([0.0415, 0.0407, 0.0538, 0.0583, 0.0750, 0.0917, 0.4778, 0.0075])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5482, 0.4493, 0.0956, 0.1444, 0.5230, 0.2939])\n",
      "Valid Idx 3 | Loss tensor([0.6261, 0.9326, 0.2609, 0.4424, 0.7340])\n",
      "\n",
      "************** Batch 496 in 4.349545478820801 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1922, 0.2803, 0.1802, 0.2532]) \n",
      "Test Loss tensor([0.1941, 0.2758, 0.1890, 0.2541])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4441, 0.6089, 0.0213, 0.3131, 0.6309]) \n",
      "Test Loss tensor([0.4441, 0.6101, 0.0208, 0.3084, 0.6309])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1232, 0.0393, 0.2514, 0.3170, 0.3345, 0.5382, 0.0232]) \n",
      "Test Loss tensor([0.1213, 0.0398, 0.2741, 0.3397, 0.3386, 0.5448, 0.0224])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0813, 0.2479, 0.1084, 0.1640, 0.4846, 0.5464, 0.0572]) \n",
      "Test Loss tensor([0.0786, 0.2499, 0.1049, 0.1658, 0.4769, 0.5414, 0.0541])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0402, 0.0409, 0.0548, 0.0604, 0.0721, 0.0855, 0.4706, 0.0075]) \n",
      "Test Loss tensor([0.0407, 0.0394, 0.0534, 0.0576, 0.0765, 0.0893, 0.4790, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5470, 0.4393, 0.0979, 0.1473, 0.5257, 0.2968])\n",
      "Valid Idx 3 | Loss tensor([0.6471, 0.9361, 0.2821, 0.4452, 0.7429])\n",
      "\n",
      "************** Batch 500 in 4.44138240814209 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1956, 0.2844, 0.1900, 0.2548]) \n",
      "Test Loss tensor([0.1923, 0.2763, 0.1877, 0.2550])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4514, 0.6187, 0.0200, 0.3120, 0.6187]) \n",
      "Test Loss tensor([0.4439, 0.6037, 0.0202, 0.3109, 0.6311])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1277, 0.0403, 0.2743, 0.3392, 0.3362, 0.5492, 0.0243]) \n",
      "Test Loss tensor([0.1249, 0.0395, 0.2711, 0.3363, 0.3357, 0.5405, 0.0227])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0817, 0.2473, 0.1072, 0.1622, 0.4657, 0.5435, 0.0547]) \n",
      "Test Loss tensor([0.0770, 0.2570, 0.1054, 0.1658, 0.4728, 0.5486, 0.0535])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0396, 0.0381, 0.0523, 0.0559, 0.0769, 0.0904, 0.4926, 0.0074]) \n",
      "Test Loss tensor([0.0410, 0.0409, 0.0531, 0.0578, 0.0734, 0.0922, 0.4738, 0.0073])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5452, 0.4567, 0.0951, 0.1452, 0.5267, 0.2923])\n",
      "Valid Idx 3 | Loss tensor([0.6278, 0.9286, 0.2670, 0.4459, 0.7419])\n",
      "Gradients: Input 0.06188932806253433 | Message 0.05928809940814972 | Update 0.10221844166517258 | Output 0.08173984289169312\n",
      "\n",
      "************** Batch 504 in 3.9364864826202393 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1952, 0.2746, 0.1866, 0.2657]) \n",
      "Test Loss tensor([0.1917, 0.2765, 0.1852, 0.2527])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4427, 0.5996, 0.0208, 0.3043, 0.6293]) \n",
      "Test Loss tensor([0.4465, 0.6025, 0.0216, 0.3076, 0.6291])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1284, 0.0398, 0.2771, 0.3411, 0.3344, 0.5476, 0.0234]) \n",
      "Test Loss tensor([0.1269, 0.0392, 0.2677, 0.3360, 0.3336, 0.5420, 0.0239])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0779, 0.2421, 0.1013, 0.1613, 0.4687, 0.5521, 0.0509]) \n",
      "Test Loss tensor([0.0746, 0.2526, 0.0990, 0.1597, 0.4712, 0.5564, 0.0535])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0386, 0.0356, 0.0537, 0.0546, 0.0730, 0.0885, 0.4788, 0.0071]) \n",
      "Test Loss tensor([0.0421, 0.0423, 0.0532, 0.0559, 0.0690, 0.0966, 0.4737, 0.0076])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5455, 0.4778, 0.0973, 0.1459, 0.5247, 0.2918])\n",
      "Valid Idx 3 | Loss tensor([0.5900, 0.9095, 0.2358, 0.4383, 0.7358])\n",
      "\n",
      "************** Batch 508 in 3.9234116077423096 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1904, 0.2776, 0.1960, 0.2603]) \n",
      "Test Loss tensor([0.1896, 0.2802, 0.1866, 0.2527])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4444, 0.6126, 0.0191, 0.3179, 0.6257]) \n",
      "Test Loss tensor([0.4460, 0.6050, 0.0211, 0.3105, 0.6285])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1253, 0.0398, 0.2587, 0.3253, 0.3338, 0.5414, 0.0221]) \n",
      "Test Loss tensor([0.1262, 0.0386, 0.2653, 0.3328, 0.3317, 0.5375, 0.0236])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0742, 0.2482, 0.1044, 0.1632, 0.4741, 0.5365, 0.0528]) \n",
      "Test Loss tensor([0.0744, 0.2531, 0.0993, 0.1589, 0.4720, 0.5474, 0.0520])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0463, 0.0416, 0.0545, 0.0625, 0.0687, 0.0988, 0.4637, 0.0074]) \n",
      "Test Loss tensor([0.0410, 0.0400, 0.0523, 0.0560, 0.0702, 0.0940, 0.4728, 0.0075])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5448, 0.4649, 0.0975, 0.1449, 0.5240, 0.2941])\n",
      "Valid Idx 3 | Loss tensor([0.6135, 0.9098, 0.2523, 0.4399, 0.7350])\n",
      "\n",
      "************** Batch 512 in 3.9670770168304443 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.2013, 0.2770, 0.1881, 0.2612]) \n",
      "Test Loss tensor([0.1892, 0.2768, 0.1889, 0.2539])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4479, 0.6133, 0.0216, 0.3051, 0.6298]) \n",
      "Test Loss tensor([0.4427, 0.6077, 0.0204, 0.3076, 0.6273])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1262, 0.0387, 0.2720, 0.3487, 0.3331, 0.5471, 0.0230]) \n",
      "Test Loss tensor([0.1256, 0.0386, 0.2700, 0.3348, 0.3298, 0.5413, 0.0227])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0722, 0.2616, 0.0917, 0.1638, 0.4784, 0.5381, 0.0538]) \n",
      "Test Loss tensor([0.0778, 0.2515, 0.1007, 0.1619, 0.4744, 0.5345, 0.0534])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0414, 0.0426, 0.0513, 0.0528, 0.0696, 0.0939, 0.4721, 0.0072]) \n",
      "Test Loss tensor([0.0416, 0.0406, 0.0523, 0.0549, 0.0732, 0.0910, 0.4748, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5456, 0.4437, 0.0960, 0.1476, 0.5194, 0.3025])\n",
      "Valid Idx 3 | Loss tensor([0.6534, 0.9202, 0.2780, 0.4409, 0.7393])\n",
      "\n",
      "************** Batch 516 in 3.9077160358428955 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1939, 0.2677, 0.1876, 0.2591]) \n",
      "Test Loss tensor([0.1879, 0.2784, 0.1878, 0.2522])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4593, 0.6203, 0.0210, 0.3194, 0.6209]) \n",
      "Test Loss tensor([0.4465, 0.6062, 0.0207, 0.3068, 0.6322])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1256, 0.0393, 0.2724, 0.3259, 0.3264, 0.5393, 0.0214]) \n",
      "Test Loss tensor([0.1228, 0.0381, 0.2664, 0.3366, 0.3260, 0.5373, 0.0231])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0758, 0.2597, 0.1045, 0.1716, 0.4728, 0.5379, 0.0574]) \n",
      "Test Loss tensor([0.0785, 0.2511, 0.0994, 0.1630, 0.4733, 0.5290, 0.0527])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0398, 0.0405, 0.0516, 0.0569, 0.0743, 0.0915, 0.4712, 0.0072]) \n",
      "Test Loss tensor([0.0402, 0.0411, 0.0511, 0.0539, 0.0744, 0.0907, 0.4764, 0.0070])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5447, 0.4384, 0.0951, 0.1486, 0.5197, 0.3062])\n",
      "Valid Idx 3 | Loss tensor([0.6684, 0.9185, 0.2903, 0.4435, 0.7430])\n",
      "\n",
      "************** Batch 520 in 3.9629571437835693 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1923, 0.2641, 0.1909, 0.2712]) \n",
      "Test Loss tensor([0.1867, 0.2738, 0.1878, 0.2551])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4444, 0.5984, 0.0187, 0.2952, 0.6397]) \n",
      "Test Loss tensor([0.4456, 0.6076, 0.0203, 0.3078, 0.6279])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1228, 0.0386, 0.2649, 0.3306, 0.3223, 0.5204, 0.0226]) \n",
      "Test Loss tensor([0.1259, 0.0379, 0.2718, 0.3353, 0.3263, 0.5395, 0.0234])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0763, 0.2565, 0.0928, 0.1574, 0.4822, 0.5206, 0.0562]) \n",
      "Test Loss tensor([0.0777, 0.2536, 0.1001, 0.1634, 0.4717, 0.5259, 0.0524])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0402, 0.0399, 0.0518, 0.0540, 0.0756, 0.0907, 0.4750, 0.0072]) \n",
      "Test Loss tensor([0.0402, 0.0405, 0.0516, 0.0534, 0.0753, 0.0906, 0.4764, 0.0071])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5512, 0.4400, 0.0957, 0.1487, 0.5154, 0.3089])\n",
      "Valid Idx 3 | Loss tensor([0.6769, 0.9171, 0.2995, 0.4463, 0.7448])\n",
      "Gradients: Input 0.03760601580142975 | Message 0.01973738893866539 | Update 0.02456985041499138 | Output 0.04477336257696152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 524 in 3.9154751300811768 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1820, 0.2824, 0.1822, 0.2491]) \n",
      "Test Loss tensor([0.1852, 0.2761, 0.1879, 0.2538])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4495, 0.6205, 0.0200, 0.3055, 0.6224]) \n",
      "Test Loss tensor([0.4445, 0.6068, 0.0210, 0.3087, 0.6270])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1235, 0.0383, 0.2730, 0.3329, 0.3348, 0.5582, 0.0238]) \n",
      "Test Loss tensor([0.1276, 0.0377, 0.2701, 0.3363, 0.3229, 0.5374, 0.0230])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0804, 0.2460, 0.0963, 0.1640, 0.4739, 0.5218, 0.0463]) \n",
      "Test Loss tensor([0.0787, 0.2520, 0.0997, 0.1645, 0.4723, 0.5278, 0.0544])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0382, 0.0413, 0.0469, 0.0537, 0.0750, 0.0893, 0.4720, 0.0071]) \n",
      "Test Loss tensor([0.0406, 0.0409, 0.0506, 0.0529, 0.0727, 0.0935, 0.4729, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5488, 0.4504, 0.0963, 0.1445, 0.5211, 0.3079])\n",
      "Valid Idx 3 | Loss tensor([0.6643, 0.9061, 0.2845, 0.4443, 0.7399])\n",
      "\n",
      "************** Batch 528 in 3.947307825088501 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1856, 0.2900, 0.1857, 0.2511]) \n",
      "Test Loss tensor([0.1850, 0.2751, 0.1864, 0.2530])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4341, 0.5987, 0.0196, 0.2922, 0.6384]) \n",
      "Test Loss tensor([0.4440, 0.6082, 0.0212, 0.3110, 0.6232])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1269, 0.0376, 0.2658, 0.3271, 0.3234, 0.5280, 0.0233]) \n",
      "Test Loss tensor([0.1306, 0.0375, 0.2708, 0.3363, 0.3201, 0.5350, 0.0234])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0785, 0.2513, 0.0982, 0.1615, 0.4621, 0.5276, 0.0541]) \n",
      "Test Loss tensor([0.0779, 0.2563, 0.0989, 0.1612, 0.4739, 0.5311, 0.0519])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0419, 0.0419, 0.0522, 0.0546, 0.0737, 0.0912, 0.4660, 0.0071]) \n",
      "Test Loss tensor([0.0406, 0.0410, 0.0502, 0.0521, 0.0708, 0.0922, 0.4714, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5483, 0.4531, 0.0954, 0.1490, 0.5176, 0.3128])\n",
      "Valid Idx 3 | Loss tensor([0.6699, 0.9044, 0.2849, 0.4343, 0.7372])\n",
      "\n",
      "************** Batch 532 in 3.9424307346343994 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1832, 0.2765, 0.1923, 0.2553]) \n",
      "Test Loss tensor([0.1831, 0.2752, 0.1871, 0.2543])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4367, 0.5983, 0.0204, 0.3075, 0.6447]) \n",
      "Test Loss tensor([0.4397, 0.5992, 0.0210, 0.3072, 0.6334])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1261, 0.0378, 0.2700, 0.3272, 0.3134, 0.5381, 0.0221]) \n",
      "Test Loss tensor([0.1289, 0.0373, 0.2681, 0.3346, 0.3205, 0.5368, 0.0232])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0734, 0.2630, 0.0988, 0.1572, 0.4735, 0.5306, 0.0466]) \n",
      "Test Loss tensor([0.0799, 0.2531, 0.0980, 0.1649, 0.4687, 0.5187, 0.0522])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0420, 0.0411, 0.0534, 0.0537, 0.0734, 0.0918, 0.4765, 0.0070]) \n",
      "Test Loss tensor([0.0393, 0.0418, 0.0498, 0.0521, 0.0742, 0.0901, 0.4707, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5480, 0.4428, 0.0956, 0.1495, 0.5189, 0.3161])\n",
      "Valid Idx 3 | Loss tensor([0.6977, 0.9085, 0.3152, 0.4428, 0.7464])\n",
      "\n",
      "************** Batch 536 in 4.0592429637908936 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1816, 0.2741, 0.1826, 0.2575]) \n",
      "Test Loss tensor([0.1793, 0.2763, 0.1862, 0.2559])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4394, 0.6118, 0.0191, 0.3052, 0.6305]) \n",
      "Test Loss tensor([0.4416, 0.6091, 0.0205, 0.3078, 0.6276])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1248, 0.0379, 0.2639, 0.3362, 0.3169, 0.5376, 0.0225]) \n",
      "Test Loss tensor([0.1249, 0.0373, 0.2668, 0.3326, 0.3156, 0.5317, 0.0224])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0781, 0.2610, 0.0998, 0.1598, 0.4827, 0.5130, 0.0499]) \n",
      "Test Loss tensor([0.0831, 0.2484, 0.0976, 0.1619, 0.4694, 0.5077, 0.0517])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0408, 0.0404, 0.0510, 0.0508, 0.0738, 0.0911, 0.4772, 0.0069]) \n",
      "Test Loss tensor([0.0392, 0.0413, 0.0497, 0.0520, 0.0767, 0.0884, 0.4745, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5511, 0.4278, 0.0976, 0.1508, 0.5120, 0.3233])\n",
      "Valid Idx 3 | Loss tensor([0.7312, 0.9186, 0.3469, 0.4437, 0.7491])\n",
      "\n",
      "************** Batch 540 in 4.045924186706543 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1811, 0.2587, 0.1929, 0.2558]) \n",
      "Test Loss tensor([0.1792, 0.2784, 0.1881, 0.2552])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4302, 0.6092, 0.0216, 0.2991, 0.6220]) \n",
      "Test Loss tensor([0.4371, 0.6070, 0.0211, 0.3036, 0.6272])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1271, 0.0371, 0.2642, 0.3273, 0.3136, 0.5262, 0.0222]) \n",
      "Test Loss tensor([0.1271, 0.0366, 0.2677, 0.3336, 0.3138, 0.5341, 0.0230])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0810, 0.2455, 0.0946, 0.1655, 0.4773, 0.5093, 0.0501]) \n",
      "Test Loss tensor([0.0827, 0.2513, 0.0980, 0.1639, 0.4708, 0.5072, 0.0511])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0413, 0.0393, 0.0541, 0.0540, 0.0762, 0.0857, 0.4669, 0.0070]) \n",
      "Test Loss tensor([0.0391, 0.0414, 0.0488, 0.0501, 0.0774, 0.0890, 0.4770, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5508, 0.4300, 0.0971, 0.1501, 0.5120, 0.3270])\n",
      "Valid Idx 3 | Loss tensor([0.7362, 0.9109, 0.3496, 0.4445, 0.7443])\n",
      "Gradients: Input 0.05822055786848068 | Message 0.0587800070643425 | Update 0.09522024542093277 | Output 0.079094298183918\n",
      "\n",
      "************** Batch 544 in 4.076293468475342 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1842, 0.2661, 0.1927, 0.2637]) \n",
      "Test Loss tensor([0.1763, 0.2743, 0.1841, 0.2516])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4309, 0.6015, 0.0222, 0.3041, 0.6335]) \n",
      "Test Loss tensor([0.4346, 0.6007, 0.0222, 0.3062, 0.6352])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1225, 0.0374, 0.2642, 0.3320, 0.3193, 0.5445, 0.0229]) \n",
      "Test Loss tensor([0.1311, 0.0364, 0.2680, 0.3326, 0.3086, 0.5358, 0.0235])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0838, 0.2462, 0.0959, 0.1632, 0.4765, 0.4987, 0.0472]) \n",
      "Test Loss tensor([0.0799, 0.2570, 0.0956, 0.1598, 0.4702, 0.5139, 0.0512])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0425, 0.0458, 0.0503, 0.0510, 0.0778, 0.0861, 0.4800, 0.0070]) \n",
      "Test Loss tensor([0.0403, 0.0423, 0.0499, 0.0503, 0.0733, 0.0940, 0.4718, 0.0076])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5480, 0.4494, 0.0973, 0.1500, 0.5161, 0.3191])\n",
      "Valid Idx 3 | Loss tensor([0.7158, 0.8985, 0.3182, 0.4425, 0.7453])\n",
      "\n",
      "************** Batch 548 in 4.042321443557739 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1783, 0.2781, 0.1830, 0.2505]) \n",
      "Test Loss tensor([0.1739, 0.2767, 0.1856, 0.2530])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4533, 0.6009, 0.0211, 0.3167, 0.6273]) \n",
      "Test Loss tensor([0.4377, 0.5976, 0.0218, 0.3032, 0.6306])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1359, 0.0365, 0.2741, 0.3247, 0.3071, 0.5416, 0.0264]) \n",
      "Test Loss tensor([0.1324, 0.0363, 0.2693, 0.3311, 0.3073, 0.5304, 0.0240])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0818, 0.2526, 0.0927, 0.1631, 0.4763, 0.5085, 0.0457]) \n",
      "Test Loss tensor([0.0805, 0.2544, 0.0944, 0.1585, 0.4681, 0.5063, 0.0526])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0430, 0.0475, 0.0509, 0.0528, 0.0735, 0.0946, 0.4738, 0.0076]) \n",
      "Test Loss tensor([0.0414, 0.0431, 0.0487, 0.0499, 0.0743, 0.0927, 0.4705, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5472, 0.4499, 0.0971, 0.1510, 0.5148, 0.3195])\n",
      "Valid Idx 3 | Loss tensor([0.7348, 0.8952, 0.3396, 0.4422, 0.7445])\n",
      "\n",
      "************** Batch 552 in 4.208192825317383 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1775, 0.2870, 0.1834, 0.2510]) \n",
      "Test Loss tensor([0.1707, 0.2739, 0.1850, 0.2532])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4325, 0.5977, 0.0199, 0.2980, 0.6402]) \n",
      "Test Loss tensor([0.4342, 0.6025, 0.0218, 0.3014, 0.6306])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1294, 0.0365, 0.2630, 0.3266, 0.3058, 0.5386, 0.0223]) \n",
      "Test Loss tensor([0.1295, 0.0361, 0.2756, 0.3366, 0.3066, 0.5266, 0.0229])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0793, 0.2651, 0.0930, 0.1615, 0.4806, 0.5108, 0.0514]) \n",
      "Test Loss tensor([0.0834, 0.2525, 0.0942, 0.1633, 0.4656, 0.4918, 0.0529])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0397, 0.0455, 0.0505, 0.0504, 0.0738, 0.0925, 0.4673, 0.0078]) \n",
      "Test Loss tensor([0.0393, 0.0422, 0.0476, 0.0489, 0.0795, 0.0880, 0.4712, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5503, 0.4282, 0.0968, 0.1508, 0.5105, 0.3287])\n",
      "Valid Idx 3 | Loss tensor([0.7735, 0.9047, 0.3964, 0.4507, 0.7481])\n",
      "\n",
      "************** Batch 556 in 4.158083438873291 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1758, 0.2597, 0.1837, 0.2580]) \n",
      "Test Loss tensor([0.1706, 0.2748, 0.1854, 0.2519])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4339, 0.6043, 0.0206, 0.3042, 0.6402]) \n",
      "Test Loss tensor([0.4343, 0.6052, 0.0215, 0.3008, 0.6304])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1280, 0.0361, 0.2698, 0.3398, 0.3103, 0.5381, 0.0236]) \n",
      "Test Loss tensor([0.1269, 0.0358, 0.2705, 0.3365, 0.3040, 0.5282, 0.0232])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0816, 0.2561, 0.1018, 0.1686, 0.4682, 0.5072, 0.0563]) \n",
      "Test Loss tensor([0.0840, 0.2550, 0.0918, 0.1626, 0.4689, 0.4915, 0.0509])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0418, 0.0440, 0.0497, 0.0509, 0.0810, 0.0893, 0.4799, 0.0075]) \n",
      "Test Loss tensor([0.0384, 0.0423, 0.0474, 0.0472, 0.0812, 0.0885, 0.4688, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5461, 0.4313, 0.0953, 0.1534, 0.5105, 0.3280])\n",
      "Valid Idx 3 | Loss tensor([0.7943, 0.9055, 0.4186, 0.4515, 0.7516])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 560 in 4.067586421966553 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1773, 0.2688, 0.1922, 0.2636]) \n",
      "Test Loss tensor([0.1653, 0.2735, 0.1833, 0.2505])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4359, 0.6024, 0.0202, 0.3029, 0.6310]) \n",
      "Test Loss tensor([0.4361, 0.6026, 0.0222, 0.3042, 0.6273])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1250, 0.0353, 0.2639, 0.3334, 0.3003, 0.5283, 0.0247]) \n",
      "Test Loss tensor([0.1346, 0.0355, 0.2672, 0.3325, 0.3009, 0.5250, 0.0239])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0808, 0.2590, 0.0932, 0.1646, 0.4725, 0.4834, 0.0477]) \n",
      "Test Loss tensor([0.0801, 0.2574, 0.0920, 0.1580, 0.4670, 0.4999, 0.0524])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0389, 0.0411, 0.0479, 0.0463, 0.0828, 0.0917, 0.4802, 0.0070]) \n",
      "Test Loss tensor([0.0393, 0.0433, 0.0458, 0.0469, 0.0752, 0.0933, 0.4678, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5456, 0.4541, 0.0957, 0.1493, 0.5143, 0.3185])\n",
      "Valid Idx 3 | Loss tensor([0.7827, 0.8891, 0.3979, 0.4514, 0.7530])\n",
      "Gradients: Input 0.0799027532339096 | Message 0.0821850523352623 | Update 0.1325293630361557 | Output 0.07675443589687347\n",
      "\n",
      "************** Batch 564 in 4.053565979003906 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1728, 0.2719, 0.1851, 0.2459]) \n",
      "Test Loss tensor([0.1654, 0.2747, 0.1835, 0.2509])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4377, 0.5982, 0.0206, 0.3041, 0.6351]) \n",
      "Test Loss tensor([0.4373, 0.6027, 0.0229, 0.3038, 0.6264])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1401, 0.0353, 0.2812, 0.3445, 0.3020, 0.5454, 0.0242]) \n",
      "Test Loss tensor([0.1336, 0.0350, 0.2699, 0.3323, 0.2982, 0.5248, 0.0238])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0830, 0.2543, 0.0904, 0.1551, 0.4664, 0.5137, 0.0486]) \n",
      "Test Loss tensor([0.0829, 0.2560, 0.0895, 0.1583, 0.4642, 0.4967, 0.0507])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0359, 0.0397, 0.0439, 0.0481, 0.0753, 0.0914, 0.4650, 0.0071]) \n",
      "Test Loss tensor([0.0408, 0.0440, 0.0463, 0.0474, 0.0761, 0.0920, 0.4644, 0.0073])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5440, 0.4566, 0.0953, 0.1499, 0.5151, 0.3182])\n",
      "Valid Idx 3 | Loss tensor([0.7988, 0.8890, 0.4184, 0.4516, 0.7504])\n",
      "\n",
      "************** Batch 568 in 4.023575067520142 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1670, 0.2728, 0.1802, 0.2606]) \n",
      "Test Loss tensor([0.1630, 0.2739, 0.1879, 0.2561])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4452, 0.6023, 0.0206, 0.3118, 0.6256]) \n",
      "Test Loss tensor([0.4316, 0.5975, 0.0214, 0.2987, 0.6313])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1297, 0.0354, 0.2688, 0.3218, 0.2934, 0.5293, 0.0232]) \n",
      "Test Loss tensor([0.1291, 0.0354, 0.2694, 0.3370, 0.2952, 0.5183, 0.0226])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0850, 0.2489, 0.0932, 0.1569, 0.4670, 0.5073, 0.0487]) \n",
      "Test Loss tensor([0.0833, 0.2511, 0.0890, 0.1622, 0.4696, 0.4854, 0.0509])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0394, 0.0473, 0.0481, 0.0474, 0.0762, 0.0917, 0.4711, 0.0071]) \n",
      "Test Loss tensor([0.0380, 0.0433, 0.0463, 0.0467, 0.0826, 0.0866, 0.4635, 0.0070])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5492, 0.4447, 0.0976, 0.1509, 0.5112, 0.3236])\n",
      "Valid Idx 3 | Loss tensor([0.8318, 0.9023, 0.4802, 0.4578, 0.7575])\n",
      "\n",
      "************** Batch 572 in 4.081551790237427 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1676, 0.2701, 0.1837, 0.2504]) \n",
      "Test Loss tensor([0.1617, 0.2722, 0.1853, 0.2563])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4442, 0.6047, 0.0209, 0.2977, 0.6239]) \n",
      "Test Loss tensor([0.4387, 0.6028, 0.0217, 0.2986, 0.6331])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1323, 0.0355, 0.2735, 0.3331, 0.2951, 0.5049, 0.0237]) \n",
      "Test Loss tensor([0.1282, 0.0351, 0.2678, 0.3317, 0.2938, 0.5118, 0.0233])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0823, 0.2463, 0.0839, 0.1595, 0.4676, 0.4865, 0.0482]) \n",
      "Test Loss tensor([0.0845, 0.2539, 0.0889, 0.1615, 0.4690, 0.4832, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0387, 0.0399, 0.0474, 0.0466, 0.0839, 0.0858, 0.4754, 0.0073]) \n",
      "Test Loss tensor([0.0377, 0.0435, 0.0460, 0.0447, 0.0827, 0.0857, 0.4633, 0.0069])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5516, 0.4470, 0.0965, 0.1518, 0.5098, 0.3250])\n",
      "Valid Idx 3 | Loss tensor([0.8439, 0.9063, 0.5050, 0.4613, 0.7563])\n",
      "\n",
      "************** Batch 576 in 4.03599214553833 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1528, 0.2803, 0.1932, 0.2580]) \n",
      "Test Loss tensor([0.1593, 0.2734, 0.1841, 0.2515])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4244, 0.5917, 0.0222, 0.2928, 0.6455]) \n",
      "Test Loss tensor([0.4310, 0.5951, 0.0226, 0.2974, 0.6288])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1316, 0.0359, 0.2664, 0.3401, 0.2877, 0.5241, 0.0245]) \n",
      "Test Loss tensor([0.1359, 0.0344, 0.2729, 0.3359, 0.2890, 0.5139, 0.0240])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0881, 0.2547, 0.0939, 0.1577, 0.4676, 0.4787, 0.0522]) \n",
      "Test Loss tensor([0.0774, 0.2591, 0.0861, 0.1552, 0.4638, 0.4998, 0.0517])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0396, 0.0398, 0.0454, 0.0453, 0.0842, 0.0858, 0.4702, 0.0072]) \n",
      "Test Loss tensor([0.0401, 0.0451, 0.0454, 0.0448, 0.0754, 0.0926, 0.4601, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5479, 0.4822, 0.0970, 0.1503, 0.5157, 0.3126])\n",
      "Valid Idx 3 | Loss tensor([0.8219, 0.8863, 0.4657, 0.4548, 0.7549])\n",
      "\n",
      "************** Batch 580 in 3.9753661155700684 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1596, 0.2737, 0.1820, 0.2549]) \n",
      "Test Loss tensor([0.1596, 0.2728, 0.1856, 0.2546])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4404, 0.5992, 0.0231, 0.3106, 0.6230]) \n",
      "Test Loss tensor([0.4345, 0.5937, 0.0219, 0.3024, 0.6272])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1326, 0.0341, 0.2573, 0.3247, 0.2916, 0.5170, 0.0221]) \n",
      "Test Loss tensor([0.1381, 0.0339, 0.2698, 0.3317, 0.2849, 0.5110, 0.0236])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0813, 0.2700, 0.0875, 0.1622, 0.4765, 0.5024, 0.0523]) \n",
      "Test Loss tensor([0.0754, 0.2604, 0.0839, 0.1516, 0.4643, 0.4975, 0.0526])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0367, 0.0430, 0.0457, 0.0427, 0.0782, 0.0922, 0.4520, 0.0069]) \n",
      "Test Loss tensor([0.0397, 0.0457, 0.0444, 0.0440, 0.0734, 0.0933, 0.4602, 0.0071])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5470, 0.4893, 0.0996, 0.1486, 0.5195, 0.3107])\n",
      "Valid Idx 3 | Loss tensor([0.8250, 0.8809, 0.4723, 0.4497, 0.7521])\n",
      "Gradients: Input 0.06631530076265335 | Message 0.060067810118198395 | Update 0.08256126940250397 | Output 0.07614870369434357\n",
      "\n",
      "************** Batch 584 in 4.0433759689331055 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1577, 0.2702, 0.1837, 0.2604]) \n",
      "Test Loss tensor([0.1578, 0.2717, 0.1859, 0.2582])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4375, 0.6008, 0.0223, 0.3043, 0.6242]) \n",
      "Test Loss tensor([0.4387, 0.6049, 0.0213, 0.3007, 0.6246])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1377, 0.0345, 0.2693, 0.3372, 0.2803, 0.5225, 0.0239]) \n",
      "Test Loss tensor([0.1265, 0.0353, 0.2702, 0.3353, 0.2887, 0.4979, 0.0213])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0742, 0.2568, 0.0832, 0.1491, 0.4599, 0.4894, 0.0462]) \n",
      "Test Loss tensor([0.0837, 0.2515, 0.0851, 0.1660, 0.4685, 0.4677, 0.0517])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0429, 0.0445, 0.0462, 0.0438, 0.0709, 0.0963, 0.4585, 0.0071]) \n",
      "Test Loss tensor([0.0370, 0.0419, 0.0443, 0.0435, 0.0885, 0.0789, 0.4609, 0.0065])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5519, 0.4399, 0.0983, 0.1542, 0.5051, 0.3219])\n",
      "Valid Idx 3 | Loss tensor([0.8902, 0.9200, 0.5974, 0.4738, 0.7678])\n",
      "\n",
      "************** Batch 588 in 4.046826362609863 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1679, 0.2544, 0.1957, 0.2749]) \n",
      "Test Loss tensor([0.1572, 0.2732, 0.1834, 0.2511])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4261, 0.5926, 0.0185, 0.2876, 0.6319]) \n",
      "Test Loss tensor([0.4298, 0.5967, 0.0217, 0.2981, 0.6315])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1251, 0.0358, 0.2715, 0.3387, 0.2978, 0.5056, 0.0212]) \n",
      "Test Loss tensor([0.1306, 0.0345, 0.2699, 0.3339, 0.2817, 0.4983, 0.0219])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0769, 0.2535, 0.0849, 0.1629, 0.4849, 0.4619, 0.0535]) \n",
      "Test Loss tensor([0.0805, 0.2526, 0.0836, 0.1618, 0.4663, 0.4779, 0.0517])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0368, 0.0446, 0.0478, 0.0440, 0.0871, 0.0801, 0.4508, 0.0067]) \n",
      "Test Loss tensor([0.0380, 0.0423, 0.0438, 0.0431, 0.0816, 0.0829, 0.4637, 0.0066])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5455, 0.4652, 0.0965, 0.1508, 0.5101, 0.3165])\n",
      "Valid Idx 3 | Loss tensor([0.8696, 0.8985, 0.5681, 0.4693, 0.7644])\n",
      "\n",
      "************** Batch 592 in 4.083760976791382 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1604, 0.2647, 0.1897, 0.2692]) \n",
      "Test Loss tensor([0.1537, 0.2759, 0.1801, 0.2506])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4317, 0.6078, 0.0182, 0.3154, 0.6211]) \n",
      "Test Loss tensor([0.4385, 0.5975, 0.0219, 0.3009, 0.6271])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1272, 0.0345, 0.2831, 0.3411, 0.2810, 0.5035, 0.0206]) \n",
      "Test Loss tensor([0.1350, 0.0337, 0.2669, 0.3317, 0.2769, 0.4965, 0.0229])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0762, 0.2603, 0.0817, 0.1577, 0.4856, 0.4779, 0.0539]) \n",
      "Test Loss tensor([0.0757, 0.2588, 0.0799, 0.1556, 0.4664, 0.4834, 0.0507])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0323, 0.0408, 0.0455, 0.0410, 0.0790, 0.0830, 0.4678, 0.0064]) \n",
      "Test Loss tensor([0.0388, 0.0449, 0.0436, 0.0424, 0.0737, 0.0856, 0.4594, 0.0069])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5452, 0.4866, 0.0973, 0.1500, 0.5102, 0.3076])\n",
      "Valid Idx 3 | Loss tensor([0.8522, 0.8758, 0.5409, 0.4671, 0.7604])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 596 in 4.09153413772583 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1615, 0.2639, 0.1882, 0.2719]) \n",
      "Test Loss tensor([0.1531, 0.2741, 0.1846, 0.2534])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4340, 0.5981, 0.0224, 0.2993, 0.6243]) \n",
      "Test Loss tensor([0.4317, 0.5951, 0.0212, 0.2964, 0.6341])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1281, 0.0345, 0.2654, 0.3359, 0.2817, 0.5043, 0.0240]) \n",
      "Test Loss tensor([0.1309, 0.0338, 0.2674, 0.3334, 0.2782, 0.4885, 0.0216])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0768, 0.2615, 0.0784, 0.1549, 0.4613, 0.4828, 0.0520]) \n",
      "Test Loss tensor([0.0774, 0.2540, 0.0801, 0.1584, 0.4665, 0.4691, 0.0510])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0370, 0.0459, 0.0420, 0.0444, 0.0730, 0.0864, 0.4693, 0.0076]) \n",
      "Test Loss tensor([0.0375, 0.0441, 0.0433, 0.0415, 0.0798, 0.0803, 0.4576, 0.0064])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5463, 0.4691, 0.0974, 0.1508, 0.5065, 0.3112])\n",
      "Valid Idx 3 | Loss tensor([0.8799, 0.8880, 0.5955, 0.4776, 0.7652])\n",
      "\n",
      "************** Batch 600 in 4.084433555603027 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1563, 0.2683, 0.1809, 0.2517]) \n",
      "Test Loss tensor([0.1536, 0.2708, 0.1838, 0.2589])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4379, 0.5915, 0.0214, 0.2978, 0.6440]) \n",
      "Test Loss tensor([0.4346, 0.5993, 0.0218, 0.2999, 0.6312])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1279, 0.0340, 0.2698, 0.3290, 0.2804, 0.4950, 0.0205]) \n",
      "Test Loss tensor([0.1302, 0.0343, 0.2732, 0.3391, 0.2777, 0.4869, 0.0211])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0782, 0.2462, 0.0831, 0.1640, 0.4697, 0.4677, 0.0540]) \n",
      "Test Loss tensor([0.0807, 0.2554, 0.0797, 0.1650, 0.4674, 0.4587, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0365, 0.0415, 0.0393, 0.0423, 0.0805, 0.0806, 0.4666, 0.0068]) \n",
      "Test Loss tensor([0.0375, 0.0425, 0.0429, 0.0410, 0.0847, 0.0759, 0.4649, 0.0063])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5492, 0.4567, 0.0974, 0.1536, 0.5046, 0.3148])\n",
      "Valid Idx 3 | Loss tensor([0.8943, 0.8938, 0.6363, 0.4865, 0.7698])\n",
      "Gradients: Input 0.054162055253982544 | Message 0.03549199551343918 | Update 0.052720487117767334 | Output 0.035374969244003296\n",
      "\n",
      "************** Batch 604 in 4.129020929336548 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1621, 0.2772, 0.1816, 0.2519]) \n",
      "Test Loss tensor([0.1512, 0.2712, 0.1803, 0.2496])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4374, 0.5957, 0.0228, 0.3006, 0.6270]) \n",
      "Test Loss tensor([0.4315, 0.5881, 0.0220, 0.2971, 0.6349])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1283, 0.0342, 0.2705, 0.3445, 0.2734, 0.4881, 0.0199]) \n",
      "Test Loss tensor([0.1349, 0.0330, 0.2711, 0.3363, 0.2711, 0.4816, 0.0214])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0723, 0.2496, 0.0790, 0.1646, 0.4775, 0.4661, 0.0484]) \n",
      "Test Loss tensor([0.0747, 0.2582, 0.0779, 0.1581, 0.4684, 0.4682, 0.0513])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0346, 0.0408, 0.0417, 0.0375, 0.0851, 0.0777, 0.4537, 0.0067]) \n",
      "Test Loss tensor([0.0380, 0.0445, 0.0415, 0.0404, 0.0765, 0.0824, 0.4591, 0.0066])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5461, 0.4856, 0.0977, 0.1518, 0.5063, 0.3050])\n",
      "Valid Idx 3 | Loss tensor([0.8766, 0.8709, 0.6024, 0.4830, 0.7671])\n",
      "\n",
      "************** Batch 608 in 4.07030987739563 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1523, 0.2828, 0.1755, 0.2498]) \n",
      "Test Loss tensor([0.1509, 0.2711, 0.1823, 0.2514])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4402, 0.5966, 0.0216, 0.2952, 0.6319]) \n",
      "Test Loss tensor([0.4313, 0.5846, 0.0222, 0.2983, 0.6356])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1307, 0.0349, 0.2752, 0.3355, 0.2804, 0.4922, 0.0211]) \n",
      "Test Loss tensor([0.1381, 0.0330, 0.2706, 0.3368, 0.2656, 0.4770, 0.0226])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0707, 0.2608, 0.0711, 0.1526, 0.4616, 0.4776, 0.0509]) \n",
      "Test Loss tensor([0.0739, 0.2607, 0.0757, 0.1558, 0.4685, 0.4704, 0.0516])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0389, 0.0460, 0.0428, 0.0367, 0.0763, 0.0832, 0.4750, 0.0071]) \n",
      "Test Loss tensor([0.0377, 0.0451, 0.0425, 0.0390, 0.0738, 0.0843, 0.4564, 0.0068])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5399, 0.4942, 0.0981, 0.1501, 0.5041, 0.2969])\n",
      "Valid Idx 3 | Loss tensor([0.8705, 0.8559, 0.6011, 0.4802, 0.7664])\n",
      "\n",
      "************** Batch 612 in 3.986731767654419 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1549, 0.2618, 0.1919, 0.2614]) \n",
      "Test Loss tensor([0.1505, 0.2661, 0.1866, 0.2575])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4219, 0.5846, 0.0243, 0.2923, 0.6320]) \n",
      "Test Loss tensor([0.4309, 0.5833, 0.0216, 0.2914, 0.6339])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1418, 0.0329, 0.2811, 0.3478, 0.2688, 0.4913, 0.0224]) \n",
      "Test Loss tensor([0.1339, 0.0340, 0.2740, 0.3351, 0.2694, 0.4663, 0.0209])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0721, 0.2637, 0.0753, 0.1652, 0.4619, 0.4870, 0.0465]) \n",
      "Test Loss tensor([0.0751, 0.2584, 0.0764, 0.1569, 0.4685, 0.4575, 0.0505])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0371, 0.0453, 0.0401, 0.0408, 0.0718, 0.0844, 0.4574, 0.0063]) \n",
      "Test Loss tensor([0.0373, 0.0440, 0.0419, 0.0393, 0.0800, 0.0781, 0.4565, 0.0062])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5482, 0.4802, 0.0975, 0.1533, 0.5000, 0.3031])\n",
      "Valid Idx 3 | Loss tensor([0.8932, 0.8672, 0.6527, 0.4857, 0.7640])\n",
      "\n",
      "************** Batch 616 in 4.122319459915161 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1486, 0.2618, 0.1771, 0.2562]) \n",
      "Test Loss tensor([0.1461, 0.2685, 0.1840, 0.2505])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4456, 0.5967, 0.0196, 0.2970, 0.6286]) \n",
      "Test Loss tensor([0.4347, 0.5841, 0.0223, 0.2934, 0.6373])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1340, 0.0338, 0.2839, 0.3446, 0.2635, 0.4654, 0.0201]) \n",
      "Test Loss tensor([0.1342, 0.0336, 0.2745, 0.3379, 0.2642, 0.4606, 0.0214])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0756, 0.2558, 0.0761, 0.1581, 0.4682, 0.4606, 0.0468]) \n",
      "Test Loss tensor([0.0736, 0.2589, 0.0713, 0.1563, 0.4687, 0.4600, 0.0513])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0390, 0.0437, 0.0424, 0.0383, 0.0797, 0.0779, 0.4555, 0.0062]) \n",
      "Test Loss tensor([0.0379, 0.0438, 0.0415, 0.0388, 0.0788, 0.0793, 0.4534, 0.0064])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5492, 0.4908, 0.0986, 0.1544, 0.4948, 0.2999])\n",
      "Valid Idx 3 | Loss tensor([0.8893, 0.8529, 0.6565, 0.4943, 0.7725])\n",
      "\n",
      "************** Batch 620 in 4.066510200500488 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1469, 0.2595, 0.1773, 0.2415]) \n",
      "Test Loss tensor([0.1467, 0.2647, 0.1851, 0.2571])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4461, 0.5978, 0.0221, 0.2910, 0.6182]) \n",
      "Test Loss tensor([0.4312, 0.5839, 0.0220, 0.2900, 0.6374])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1345, 0.0333, 0.2631, 0.3264, 0.2711, 0.4641, 0.0205]) \n",
      "Test Loss tensor([0.1334, 0.0342, 0.2740, 0.3347, 0.2617, 0.4485, 0.0202])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0699, 0.2544, 0.0718, 0.1503, 0.4690, 0.4525, 0.0480]) \n",
      "Test Loss tensor([0.0741, 0.2561, 0.0731, 0.1578, 0.4649, 0.4509, 0.0484])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0394, 0.0444, 0.0447, 0.0404, 0.0798, 0.0786, 0.4558, 0.0062]) \n",
      "Test Loss tensor([0.0378, 0.0439, 0.0399, 0.0377, 0.0824, 0.0752, 0.4548, 0.0065])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5421, 0.4887, 0.0994, 0.1533, 0.4971, 0.2984])\n",
      "Valid Idx 3 | Loss tensor([0.9031, 0.8579, 0.6849, 0.5004, 0.7699])\n",
      "Gradients: Input 0.08137239515781403 | Message 0.050258807837963104 | Update 0.06667464971542358 | Output 0.041929081082344055\n",
      "\n",
      "************** Batch 624 in 4.046695947647095 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1421, 0.2641, 0.1884, 0.2571]) \n",
      "Test Loss tensor([0.1456, 0.2572, 0.1825, 0.2504])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4465, 0.5920, 0.0221, 0.3026, 0.6132]) \n",
      "Test Loss tensor([0.4349, 0.5806, 0.0229, 0.2957, 0.6354])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1354, 0.0343, 0.2766, 0.3322, 0.2633, 0.4497, 0.0205]) \n",
      "Test Loss tensor([0.1370, 0.0331, 0.2697, 0.3347, 0.2581, 0.4458, 0.0214])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0690, 0.2590, 0.0735, 0.1584, 0.4609, 0.4541, 0.0550]) \n",
      "Test Loss tensor([0.0692, 0.2613, 0.0702, 0.1523, 0.4660, 0.4560, 0.0505])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0366, 0.0422, 0.0418, 0.0369, 0.0850, 0.0736, 0.4480, 0.0066]) \n",
      "Test Loss tensor([0.0385, 0.0455, 0.0405, 0.0376, 0.0774, 0.0793, 0.4543, 0.0065])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5405, 0.5101, 0.0993, 0.1499, 0.5029, 0.2899])\n",
      "Valid Idx 3 | Loss tensor([0.8896, 0.8351, 0.6647, 0.4982, 0.7725])\n",
      "\n",
      "************** Batch 628 in 4.064073801040649 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1475, 0.2641, 0.1712, 0.2548]) \n",
      "Test Loss tensor([0.1438, 0.2574, 0.1824, 0.2520])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4465, 0.5795, 0.0212, 0.2928, 0.6389]) \n",
      "Test Loss tensor([0.4370, 0.5794, 0.0221, 0.2891, 0.6354])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1450, 0.0339, 0.2704, 0.3413, 0.2607, 0.4440, 0.0223]) \n",
      "Test Loss tensor([0.1334, 0.0339, 0.2744, 0.3371, 0.2597, 0.4381, 0.0209])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0677, 0.2683, 0.0653, 0.1526, 0.4665, 0.4460, 0.0500]) \n",
      "Test Loss tensor([0.0725, 0.2610, 0.0704, 0.1562, 0.4693, 0.4451, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0356, 0.0468, 0.0412, 0.0357, 0.0768, 0.0811, 0.4536, 0.0070]) \n",
      "Test Loss tensor([0.0368, 0.0446, 0.0399, 0.0361, 0.0832, 0.0752, 0.4490, 0.0060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5415, 0.4994, 0.0998, 0.1543, 0.4986, 0.2919])\n",
      "Valid Idx 3 | Loss tensor([0.9086, 0.8447, 0.7023, 0.5082, 0.7742])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 632 in 4.0414345264434814 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1484, 0.2523, 0.1944, 0.2549]) \n",
      "Test Loss tensor([0.1441, 0.2517, 0.1834, 0.2539])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4338, 0.5756, 0.0207, 0.2989, 0.6412]) \n",
      "Test Loss tensor([0.4362, 0.5825, 0.0225, 0.2940, 0.6318])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1309, 0.0342, 0.2651, 0.3265, 0.2668, 0.4265, 0.0211]) \n",
      "Test Loss tensor([0.1363, 0.0343, 0.2723, 0.3368, 0.2552, 0.4243, 0.0198])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0672, 0.2561, 0.0711, 0.1470, 0.4714, 0.4399, 0.0461]) \n",
      "Test Loss tensor([0.0713, 0.2608, 0.0677, 0.1559, 0.4661, 0.4475, 0.0505])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0364, 0.0450, 0.0406, 0.0348, 0.0800, 0.0739, 0.4469, 0.0059]) \n",
      "Test Loss tensor([0.0382, 0.0453, 0.0406, 0.0371, 0.0842, 0.0748, 0.4493, 0.0061])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5381, 0.5041, 0.0970, 0.1560, 0.4937, 0.2861])\n",
      "Valid Idx 3 | Loss tensor([0.9084, 0.8364, 0.7093, 0.5184, 0.7778])\n",
      "\n",
      "************** Batch 636 in 4.0704662799835205 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1393, 0.2511, 0.1789, 0.2575]) \n",
      "Test Loss tensor([0.1409, 0.2495, 0.1823, 0.2495])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4461, 0.5830, 0.0225, 0.2937, 0.6345]) \n",
      "Test Loss tensor([0.4358, 0.5781, 0.0220, 0.2900, 0.6344])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1424, 0.0335, 0.2727, 0.3326, 0.2585, 0.4319, 0.0219]) \n",
      "Test Loss tensor([0.1375, 0.0339, 0.2719, 0.3361, 0.2481, 0.4144, 0.0208])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0671, 0.2674, 0.0685, 0.1476, 0.4695, 0.4438, 0.0502]) \n",
      "Test Loss tensor([0.0687, 0.2587, 0.0677, 0.1558, 0.4658, 0.4425, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0403, 0.0441, 0.0411, 0.0363, 0.0831, 0.0767, 0.4421, 0.0061]) \n",
      "Test Loss tensor([0.0373, 0.0456, 0.0411, 0.0357, 0.0826, 0.0768, 0.4443, 0.0060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5373, 0.5162, 0.1017, 0.1537, 0.4957, 0.2831])\n",
      "Valid Idx 3 | Loss tensor([0.9020, 0.8182, 0.7085, 0.5144, 0.7764])\n",
      "\n",
      "************** Batch 640 in 4.0519068241119385 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1485, 0.2438, 0.1866, 0.2567]) \n",
      "Test Loss tensor([0.1387, 0.2486, 0.1840, 0.2515])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4221, 0.5678, 0.0218, 0.2993, 0.6477]) \n",
      "Test Loss tensor([0.4417, 0.5820, 0.0217, 0.2921, 0.6328])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1313, 0.0342, 0.2652, 0.3283, 0.2478, 0.4128, 0.0197]) \n",
      "Test Loss tensor([0.1327, 0.0352, 0.2757, 0.3359, 0.2503, 0.3985, 0.0195])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0689, 0.2741, 0.0648, 0.1540, 0.4571, 0.4476, 0.0496]) \n",
      "Test Loss tensor([0.0727, 0.2632, 0.0688, 0.1583, 0.4695, 0.4287, 0.0531])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0378, 0.0463, 0.0406, 0.0347, 0.0841, 0.0775, 0.4589, 0.0060]) \n",
      "Test Loss tensor([0.0370, 0.0451, 0.0400, 0.0340, 0.0896, 0.0717, 0.4446, 0.0058])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5469, 0.5019, 0.1016, 0.1575, 0.4864, 0.2824])\n",
      "Valid Idx 3 | Loss tensor([0.9225, 0.8307, 0.7419, 0.5409, 0.7864])\n",
      "Gradients: Input 0.09842605143785477 | Message 0.07658612728118896 | Update 0.10641654580831528 | Output 0.0841769129037857\n",
      "\n",
      "************** Batch 644 in 4.072394371032715 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1392, 0.2405, 0.1751, 0.2580]) \n",
      "Test Loss tensor([0.1428, 0.2420, 0.1837, 0.2521])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4562, 0.5978, 0.0236, 0.2861, 0.6159]) \n",
      "Test Loss tensor([0.4427, 0.5773, 0.0225, 0.2912, 0.6330])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1327, 0.0358, 0.2787, 0.3298, 0.2398, 0.4022, 0.0208]) \n",
      "Test Loss tensor([0.1362, 0.0337, 0.2728, 0.3380, 0.2430, 0.3948, 0.0198])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0700, 0.2726, 0.0690, 0.1606, 0.4763, 0.4373, 0.0520]) \n",
      "Test Loss tensor([0.0679, 0.2658, 0.0672, 0.1540, 0.4680, 0.4406, 0.0516])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0386, 0.0469, 0.0388, 0.0367, 0.0877, 0.0725, 0.4358, 0.0059]) \n",
      "Test Loss tensor([0.0372, 0.0469, 0.0411, 0.0345, 0.0814, 0.0761, 0.4406, 0.0060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5379, 0.5243, 0.1022, 0.1558, 0.4937, 0.2707])\n",
      "Valid Idx 3 | Loss tensor([0.9004, 0.7978, 0.7219, 0.5231, 0.7752])\n",
      "\n",
      "************** Batch 648 in 4.177636623382568 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1455, 0.2320, 0.1880, 0.2535]) \n",
      "Test Loss tensor([0.1426, 0.2354, 0.1854, 0.2519])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4446, 0.5781, 0.0233, 0.2944, 0.6268]) \n",
      "Test Loss tensor([0.4403, 0.5775, 0.0228, 0.2890, 0.6359])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1411, 0.0338, 0.2639, 0.3311, 0.2343, 0.3931, 0.0207]) \n",
      "Test Loss tensor([0.1414, 0.0328, 0.2732, 0.3349, 0.2408, 0.3862, 0.0202])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0686, 0.2644, 0.0664, 0.1566, 0.4465, 0.4347, 0.0532]) \n",
      "Test Loss tensor([0.0646, 0.2731, 0.0648, 0.1501, 0.4631, 0.4369, 0.0511])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0372, 0.0493, 0.0371, 0.0355, 0.0823, 0.0756, 0.4279, 0.0065]) \n",
      "Test Loss tensor([0.0368, 0.0466, 0.0397, 0.0333, 0.0769, 0.0783, 0.4349, 0.0058])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5289, 0.5443, 0.1028, 0.1514, 0.4927, 0.2673])\n",
      "Valid Idx 3 | Loss tensor([0.8894, 0.7722, 0.7093, 0.5198, 0.7797])\n",
      "\n",
      "************** Batch 652 in 4.144663333892822 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1474, 0.2369, 0.1877, 0.2556]) \n",
      "Test Loss tensor([0.1424, 0.2340, 0.1902, 0.2557])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4457, 0.5759, 0.0227, 0.2917, 0.6188]) \n",
      "Test Loss tensor([0.4485, 0.5815, 0.0210, 0.2927, 0.6321])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1396, 0.0336, 0.2690, 0.3311, 0.2403, 0.3798, 0.0211]) \n",
      "Test Loss tensor([0.1316, 0.0367, 0.2781, 0.3346, 0.2454, 0.3660, 0.0182])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0616, 0.2709, 0.0705, 0.1534, 0.4783, 0.4511, 0.0575]) \n",
      "Test Loss tensor([0.0716, 0.2612, 0.0639, 0.1583, 0.4726, 0.4174, 0.0498])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0405, 0.0437, 0.0450, 0.0319, 0.0779, 0.0755, 0.4299, 0.0061]) \n",
      "Test Loss tensor([0.0356, 0.0423, 0.0396, 0.0333, 0.0947, 0.0672, 0.4350, 0.0054])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5351, 0.5080, 0.1012, 0.1596, 0.4850, 0.2726])\n",
      "Valid Idx 3 | Loss tensor([0.9362, 0.8175, 0.7829, 0.5687, 0.7916])\n",
      "\n",
      "************** Batch 656 in 4.0760862827301025 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1453, 0.2375, 0.2003, 0.2568]) \n",
      "Test Loss tensor([0.1439, 0.2310, 0.1894, 0.2521])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4462, 0.5732, 0.0249, 0.2948, 0.6389]) \n",
      "Test Loss tensor([0.4430, 0.5746, 0.0225, 0.2890, 0.6392])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1326, 0.0364, 0.2721, 0.3322, 0.2458, 0.3676, 0.0194]) \n",
      "Test Loss tensor([0.1402, 0.0328, 0.2689, 0.3367, 0.2333, 0.3639, 0.0199])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0684, 0.2643, 0.0629, 0.1608, 0.4607, 0.4179, 0.0500]) \n",
      "Test Loss tensor([0.0647, 0.2692, 0.0642, 0.1503, 0.4653, 0.4280, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0332, 0.0438, 0.0420, 0.0321, 0.0925, 0.0686, 0.4301, 0.0051]) \n",
      "Test Loss tensor([0.0364, 0.0456, 0.0377, 0.0327, 0.0772, 0.0768, 0.4308, 0.0054])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5294, 0.5487, 0.1001, 0.1539, 0.4894, 0.2592])\n",
      "Valid Idx 3 | Loss tensor([0.8930, 0.7520, 0.7263, 0.5343, 0.7836])\n",
      "\n",
      "************** Batch 660 in 4.124849081039429 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1453, 0.2311, 0.1899, 0.2535]) \n",
      "Test Loss tensor([0.1411, 0.2275, 0.1897, 0.2514])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4437, 0.5701, 0.0225, 0.2911, 0.6443]) \n",
      "Test Loss tensor([0.4456, 0.5760, 0.0223, 0.2843, 0.6418])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1355, 0.0314, 0.2697, 0.3404, 0.2247, 0.3609, 0.0203]) \n",
      "Test Loss tensor([0.1398, 0.0331, 0.2728, 0.3388, 0.2317, 0.3555, 0.0200])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0616, 0.2733, 0.0623, 0.1502, 0.4751, 0.4226, 0.0444]) \n",
      "Test Loss tensor([0.0634, 0.2682, 0.0621, 0.1482, 0.4620, 0.4303, 0.0512])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0390, 0.0456, 0.0379, 0.0321, 0.0794, 0.0782, 0.4243, 0.0060]) \n",
      "Test Loss tensor([0.0359, 0.0453, 0.0378, 0.0318, 0.0788, 0.0733, 0.4263, 0.0054])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5279, 0.5437, 0.1014, 0.1540, 0.4872, 0.2551])\n",
      "Valid Idx 3 | Loss tensor([0.8981, 0.7512, 0.7422, 0.5485, 0.7897])\n",
      "Gradients: Input 0.1858537495136261 | Message 0.14857187867164612 | Update 0.23116403818130493 | Output 0.10295812785625458\n",
      "\n",
      "************** Batch 664 in 4.168380975723267 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1432, 0.2160, 0.1966, 0.2570]) \n",
      "Test Loss tensor([0.1399, 0.2285, 0.1906, 0.2517])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4416, 0.5784, 0.0229, 0.2810, 0.6283]) \n",
      "Test Loss tensor([0.4474, 0.5783, 0.0218, 0.2908, 0.6367])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1261, 0.0319, 0.2683, 0.3195, 0.2359, 0.3510, 0.0174]) \n",
      "Test Loss tensor([0.1287, 0.0351, 0.2758, 0.3384, 0.2337, 0.3422, 0.0186])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0625, 0.2648, 0.0653, 0.1516, 0.4689, 0.4221, 0.0553]) \n",
      "Test Loss tensor([0.0672, 0.2634, 0.0628, 0.1534, 0.4629, 0.4127, 0.0507])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0385, 0.0464, 0.0375, 0.0334, 0.0789, 0.0748, 0.4387, 0.0053]) \n",
      "Test Loss tensor([0.0350, 0.0412, 0.0392, 0.0322, 0.0868, 0.0662, 0.4224, 0.0049])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5312, 0.5292, 0.0995, 0.1552, 0.4828, 0.2606])\n",
      "Valid Idx 3 | Loss tensor([0.9253, 0.7768, 0.7823, 0.5736, 0.7941])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 668 in 4.081152439117432 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1415, 0.2329, 0.1908, 0.2553]) \n",
      "Test Loss tensor([0.1409, 0.2275, 0.1887, 0.2508])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4448, 0.5802, 0.0213, 0.2934, 0.6390]) \n",
      "Test Loss tensor([0.4470, 0.5808, 0.0226, 0.2922, 0.6356])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1413, 0.0370, 0.2818, 0.3471, 0.2331, 0.3465, 0.0207]) \n",
      "Test Loss tensor([0.1369, 0.0319, 0.2704, 0.3354, 0.2305, 0.3372, 0.0188])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0661, 0.2598, 0.0643, 0.1526, 0.4752, 0.4278, 0.0491]) \n",
      "Test Loss tensor([0.0601, 0.2661, 0.0608, 0.1425, 0.4651, 0.4247, 0.0498])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0329, 0.0403, 0.0388, 0.0332, 0.0872, 0.0658, 0.4336, 0.0047]) \n",
      "Test Loss tensor([0.0368, 0.0434, 0.0383, 0.0308, 0.0750, 0.0730, 0.4222, 0.0049])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5334, 0.5514, 0.1010, 0.1535, 0.4851, 0.2508])\n",
      "Valid Idx 3 | Loss tensor([0.8901, 0.7312, 0.7425, 0.5533, 0.7881])\n",
      "\n",
      "************** Batch 672 in 4.1367316246032715 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1440, 0.2251, 0.1949, 0.2603]) \n",
      "Test Loss tensor([0.1439, 0.2270, 0.1913, 0.2526])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4541, 0.5921, 0.0217, 0.2970, 0.6167]) \n",
      "Test Loss tensor([0.4451, 0.5788, 0.0219, 0.2887, 0.6373])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1309, 0.0315, 0.2705, 0.3349, 0.2233, 0.3520, 0.0196]) \n",
      "Test Loss tensor([0.1352, 0.0324, 0.2713, 0.3387, 0.2270, 0.3283, 0.0186])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0596, 0.2756, 0.0538, 0.1436, 0.4626, 0.4171, 0.0526]) \n",
      "Test Loss tensor([0.0601, 0.2632, 0.0610, 0.1471, 0.4619, 0.4229, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0376, 0.0434, 0.0364, 0.0323, 0.0738, 0.0765, 0.4064, 0.0056]) \n",
      "Test Loss tensor([0.0361, 0.0418, 0.0387, 0.0306, 0.0760, 0.0712, 0.4140, 0.0048])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5309, 0.5513, 0.1008, 0.1565, 0.4853, 0.2543])\n",
      "Valid Idx 3 | Loss tensor([0.8947, 0.7312, 0.7535, 0.5561, 0.7922])\n",
      "\n",
      "************** Batch 676 in 4.0758020877838135 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1355, 0.2444, 0.1873, 0.2461]) \n",
      "Test Loss tensor([0.1412, 0.2254, 0.1929, 0.2514])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4389, 0.5816, 0.0220, 0.2966, 0.6310]) \n",
      "Test Loss tensor([0.4526, 0.5778, 0.0207, 0.2831, 0.6441])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1324, 0.0323, 0.2698, 0.3391, 0.2211, 0.3216, 0.0175]) \n",
      "Test Loss tensor([0.1268, 0.0342, 0.2732, 0.3391, 0.2265, 0.3139, 0.0186])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0632, 0.2756, 0.0639, 0.1483, 0.4598, 0.4229, 0.0513]) \n",
      "Test Loss tensor([0.0637, 0.2564, 0.0598, 0.1508, 0.4659, 0.4107, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0353, 0.0381, 0.0385, 0.0306, 0.0763, 0.0702, 0.4272, 0.0045]) \n",
      "Test Loss tensor([0.0340, 0.0402, 0.0355, 0.0307, 0.0825, 0.0646, 0.4103, 0.0046])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5332, 0.5286, 0.1018, 0.1576, 0.4749, 0.2564])\n",
      "Valid Idx 3 | Loss tensor([0.9180, 0.7578, 0.7884, 0.5818, 0.7995])\n",
      "\n",
      "************** Batch 680 in 4.596431493759155 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1412, 0.2162, 0.1888, 0.2505]) \n",
      "Test Loss tensor([0.1400, 0.2243, 0.1928, 0.2472])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4394, 0.5729, 0.0188, 0.2957, 0.6467]) \n",
      "Test Loss tensor([0.4463, 0.5807, 0.0224, 0.2865, 0.6403])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1280, 0.0347, 0.2785, 0.3384, 0.2274, 0.3189, 0.0170]) \n",
      "Test Loss tensor([0.1434, 0.0297, 0.2682, 0.3393, 0.2238, 0.3231, 0.0198])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0631, 0.2612, 0.0618, 0.1487, 0.4468, 0.4102, 0.0533]) \n",
      "Test Loss tensor([0.0545, 0.2736, 0.0604, 0.1389, 0.4595, 0.4315, 0.0500])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0375, 0.0391, 0.0411, 0.0323, 0.0832, 0.0631, 0.4194, 0.0044]) \n",
      "Test Loss tensor([0.0363, 0.0431, 0.0372, 0.0305, 0.0642, 0.0785, 0.4035, 0.0048])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5212, 0.5822, 0.1042, 0.1510, 0.4906, 0.2421])\n",
      "Valid Idx 3 | Loss tensor([0.8482, 0.6720, 0.7065, 0.5320, 0.7845])\n",
      "Gradients: Input 0.2936549186706543 | Message 0.28978231549263 | Update 0.45718613266944885 | Output 0.21964985132217407\n",
      "\n",
      "************** Batch 684 in 5.106637477874756 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1432, 0.2222, 0.1889, 0.2559]) \n",
      "Test Loss tensor([0.1409, 0.2261, 0.1973, 0.2543])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4388, 0.5704, 0.0213, 0.2855, 0.6493]) \n",
      "Test Loss tensor([0.4470, 0.5809, 0.0201, 0.2900, 0.6414])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1443, 0.0292, 0.2659, 0.3320, 0.2309, 0.3235, 0.0169]) \n",
      "Test Loss tensor([0.1248, 0.0356, 0.2810, 0.3409, 0.2283, 0.2997, 0.0172])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0588, 0.2740, 0.0551, 0.1329, 0.4645, 0.4218, 0.0500]) \n",
      "Test Loss tensor([0.0644, 0.2495, 0.0578, 0.1520, 0.4621, 0.4068, 0.0477])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0323, 0.0446, 0.0382, 0.0283, 0.0640, 0.0793, 0.4108, 0.0048]) \n",
      "Test Loss tensor([0.0335, 0.0368, 0.0372, 0.0306, 0.0883, 0.0587, 0.3998, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5332, 0.5168, 0.0985, 0.1595, 0.4734, 0.2581])\n",
      "Valid Idx 3 | Loss tensor([0.9277, 0.7697, 0.8069, 0.6034, 0.8059])\n",
      "\n",
      "************** Batch 688 in 4.893693923950195 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1373, 0.2119, 0.1958, 0.2590]) \n",
      "Test Loss tensor([0.1428, 0.2252, 0.1888, 0.2522])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4445, 0.5731, 0.0190, 0.2789, 0.6504]) \n",
      "Test Loss tensor([0.4490, 0.5815, 0.0220, 0.2894, 0.6421])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1194, 0.0359, 0.2721, 0.3310, 0.2218, 0.3026, 0.0191]) \n",
      "Test Loss tensor([0.1373, 0.0303, 0.2715, 0.3417, 0.2240, 0.3084, 0.0192])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0656, 0.2594, 0.0603, 0.1568, 0.4659, 0.4178, 0.0573]) \n",
      "Test Loss tensor([0.0584, 0.2604, 0.0566, 0.1410, 0.4581, 0.4190, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0336, 0.0357, 0.0387, 0.0308, 0.0862, 0.0608, 0.4003, 0.0046]) \n",
      "Test Loss tensor([0.0346, 0.0401, 0.0366, 0.0293, 0.0678, 0.0735, 0.3972, 0.0045])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5305, 0.5675, 0.1015, 0.1523, 0.4836, 0.2469])\n",
      "Valid Idx 3 | Loss tensor([0.8620, 0.6896, 0.7291, 0.5489, 0.7942])\n",
      "\n",
      "************** Batch 692 in 4.89972996711731 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1418, 0.2275, 0.1982, 0.2530]) \n",
      "Test Loss tensor([0.1415, 0.2272, 0.1903, 0.2436])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4369, 0.5767, 0.0248, 0.2783, 0.6381]) \n",
      "Test Loss tensor([0.4483, 0.5785, 0.0214, 0.2815, 0.6430])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1391, 0.0307, 0.2686, 0.3355, 0.2221, 0.3118, 0.0183]) \n",
      "Test Loss tensor([0.1354, 0.0295, 0.2680, 0.3362, 0.2213, 0.3001, 0.0181])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0570, 0.2618, 0.0530, 0.1413, 0.4568, 0.4126, 0.0512]) \n",
      "Test Loss tensor([0.0576, 0.2575, 0.0578, 0.1394, 0.4535, 0.4173, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0332, 0.0392, 0.0363, 0.0278, 0.0688, 0.0740, 0.4018, 0.0045]) \n",
      "Test Loss tensor([0.0352, 0.0385, 0.0372, 0.0301, 0.0661, 0.0731, 0.3924, 0.0045])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5290, 0.5705, 0.1005, 0.1553, 0.4868, 0.2475])\n",
      "Valid Idx 3 | Loss tensor([0.8532, 0.6800, 0.7229, 0.5446, 0.7932])\n",
      "\n",
      "************** Batch 696 in 4.822651147842407 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1410, 0.2297, 0.1862, 0.2500]) \n",
      "Test Loss tensor([0.1420, 0.2279, 0.1941, 0.2556])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4462, 0.5828, 0.0217, 0.2883, 0.6477]) \n",
      "Test Loss tensor([0.4479, 0.5792, 0.0200, 0.2894, 0.6386])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1344, 0.0296, 0.2714, 0.3442, 0.2229, 0.3042, 0.0166]) \n",
      "Test Loss tensor([0.1239, 0.0338, 0.2774, 0.3409, 0.2262, 0.2867, 0.0165])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0558, 0.2618, 0.0581, 0.1468, 0.4555, 0.4156, 0.0584]) \n",
      "Test Loss tensor([0.0667, 0.2471, 0.0578, 0.1502, 0.4642, 0.3955, 0.0467])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0347, 0.0379, 0.0364, 0.0301, 0.0648, 0.0741, 0.4033, 0.0047]) \n",
      "Test Loss tensor([0.0323, 0.0360, 0.0373, 0.0307, 0.0851, 0.0579, 0.3829, 0.0042])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5329, 0.5141, 0.1015, 0.1628, 0.4707, 0.2653])\n",
      "Valid Idx 3 | Loss tensor([0.9161, 0.7568, 0.7975, 0.6021, 0.8091])\n",
      "\n",
      "************** Batch 700 in 5.055138826370239 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1421, 0.2311, 0.1918, 0.2517]) \n",
      "Test Loss tensor([0.1432, 0.2255, 0.1928, 0.2562])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4550, 0.5798, 0.0222, 0.2850, 0.6382]) \n",
      "Test Loss tensor([0.4375, 0.5772, 0.0215, 0.2835, 0.6434])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1331, 0.0347, 0.2900, 0.3448, 0.2266, 0.2925, 0.0177]) \n",
      "Test Loss tensor([0.1330, 0.0296, 0.2694, 0.3369, 0.2207, 0.2896, 0.0183])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0692, 0.2527, 0.0585, 0.1501, 0.4556, 0.4009, 0.0495]) \n",
      "Test Loss tensor([0.0592, 0.2504, 0.0580, 0.1418, 0.4568, 0.4087, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0279, 0.0314, 0.0391, 0.0294, 0.0845, 0.0593, 0.3816, 0.0041]) \n",
      "Test Loss tensor([0.0338, 0.0370, 0.0368, 0.0296, 0.0696, 0.0690, 0.3771, 0.0043])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5296, 0.5518, 0.1003, 0.1571, 0.4780, 0.2601])\n",
      "Valid Idx 3 | Loss tensor([0.8591, 0.6934, 0.7309, 0.5573, 0.8024])\n",
      "Gradients: Input 0.46050825715065 | Message 0.4617123305797577 | Update 0.722915530204773 | Output 0.32681000232696533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 704 in 4.843064546585083 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1393, 0.2268, 0.1901, 0.2515]) \n",
      "Test Loss tensor([0.1465, 0.2259, 0.1904, 0.2549])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4450, 0.5818, 0.0220, 0.2844, 0.6361]) \n",
      "Test Loss tensor([0.4415, 0.5791, 0.0222, 0.2817, 0.6404])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1298, 0.0297, 0.2692, 0.3434, 0.2110, 0.2817, 0.0172]) \n",
      "Test Loss tensor([0.1383, 0.0280, 0.2669, 0.3330, 0.2193, 0.2981, 0.0196])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0610, 0.2399, 0.0560, 0.1422, 0.4665, 0.4097, 0.0496]) \n",
      "Test Loss tensor([0.0554, 0.2514, 0.0581, 0.1336, 0.4528, 0.4180, 0.0498])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0375, 0.0362, 0.0396, 0.0293, 0.0709, 0.0721, 0.3736, 0.0046]) \n",
      "Test Loss tensor([0.0350, 0.0383, 0.0370, 0.0302, 0.0629, 0.0766, 0.3736, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5332, 0.5708, 0.1039, 0.1571, 0.4805, 0.2554])\n",
      "Valid Idx 3 | Loss tensor([0.8169, 0.6487, 0.6812, 0.5273, 0.7952])\n",
      "\n",
      "************** Batch 708 in 4.124887228012085 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1441, 0.2255, 0.1899, 0.2570]) \n",
      "Test Loss tensor([0.1392, 0.2237, 0.1936, 0.2517])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4348, 0.5913, 0.0221, 0.2920, 0.6306]) \n",
      "Test Loss tensor([0.4487, 0.5830, 0.0206, 0.2807, 0.6345])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1358, 0.0283, 0.2680, 0.3390, 0.2239, 0.3083, 0.0216]) \n",
      "Test Loss tensor([0.1250, 0.0309, 0.2703, 0.3402, 0.2227, 0.2760, 0.0180])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0580, 0.2707, 0.0541, 0.1399, 0.4431, 0.4104, 0.0497]) \n",
      "Test Loss tensor([0.0654, 0.2449, 0.0589, 0.1499, 0.4563, 0.4013, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0325, 0.0379, 0.0368, 0.0290, 0.0633, 0.0749, 0.3764, 0.0048]) \n",
      "Test Loss tensor([0.0325, 0.0347, 0.0378, 0.0292, 0.0811, 0.0615, 0.3645, 0.0042])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5398, 0.5247, 0.0989, 0.1643, 0.4680, 0.2718])\n",
      "Valid Idx 3 | Loss tensor([0.8882, 0.7280, 0.7631, 0.5801, 0.8085])\n",
      "\n",
      "************** Batch 712 in 4.461249589920044 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1369, 0.2248, 0.1844, 0.2408]) \n",
      "Test Loss tensor([0.1403, 0.2274, 0.1911, 0.2500])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4403, 0.5734, 0.0208, 0.2889, 0.6515]) \n",
      "Test Loss tensor([0.4403, 0.5830, 0.0207, 0.2836, 0.6375])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1186, 0.0313, 0.2752, 0.3450, 0.2257, 0.2804, 0.0177]) \n",
      "Test Loss tensor([0.1259, 0.0294, 0.2704, 0.3358, 0.2150, 0.2765, 0.0186])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0621, 0.2323, 0.0596, 0.1530, 0.4554, 0.4018, 0.0510]) \n",
      "Test Loss tensor([0.0643, 0.2457, 0.0577, 0.1423, 0.4541, 0.3997, 0.0495])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0322, 0.0340, 0.0384, 0.0320, 0.0795, 0.0624, 0.3644, 0.0039]) \n",
      "Test Loss tensor([0.0333, 0.0334, 0.0369, 0.0310, 0.0762, 0.0642, 0.3593, 0.0041])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5434, 0.5321, 0.1003, 0.1619, 0.4688, 0.2697])\n",
      "Valid Idx 3 | Loss tensor([0.8740, 0.7141, 0.7428, 0.5645, 0.8092])\n",
      "\n",
      "************** Batch 716 in 4.907953500747681 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1378, 0.2301, 0.1966, 0.2657]) \n",
      "Test Loss tensor([0.1430, 0.2245, 0.1911, 0.2494])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4427, 0.5911, 0.0197, 0.2859, 0.6325]) \n",
      "Test Loss tensor([0.4400, 0.5824, 0.0232, 0.2788, 0.6308])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1274, 0.0303, 0.2818, 0.3588, 0.2284, 0.2806, 0.0167]) \n",
      "Test Loss tensor([0.1334, 0.0269, 0.2660, 0.3349, 0.2166, 0.2844, 0.0193])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0620, 0.2456, 0.0595, 0.1477, 0.4533, 0.4119, 0.0444]) \n",
      "Test Loss tensor([0.0576, 0.2554, 0.0566, 0.1359, 0.4522, 0.4137, 0.0484])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0313, 0.0378, 0.0380, 0.0294, 0.0769, 0.0645, 0.3544, 0.0040]) \n",
      "Test Loss tensor([0.0340, 0.0364, 0.0361, 0.0296, 0.0650, 0.0755, 0.3626, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5345, 0.5702, 0.1040, 0.1580, 0.4776, 0.2648])\n",
      "Valid Idx 3 | Loss tensor([0.8157, 0.6482, 0.6764, 0.5235, 0.8026])\n",
      "\n",
      "************** Batch 720 in 4.227126121520996 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1384, 0.2206, 0.1991, 0.2479]) \n",
      "Test Loss tensor([0.1424, 0.2240, 0.1929, 0.2545])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4314, 0.5722, 0.0238, 0.2699, 0.6409]) \n",
      "Test Loss tensor([0.4420, 0.5783, 0.0221, 0.2788, 0.6358])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1376, 0.0266, 0.2670, 0.3350, 0.2155, 0.2842, 0.0206]) \n",
      "Test Loss tensor([0.1293, 0.0278, 0.2686, 0.3338, 0.2143, 0.2778, 0.0185])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0595, 0.2514, 0.0590, 0.1342, 0.4466, 0.4143, 0.0487]) \n",
      "Test Loss tensor([0.0621, 0.2464, 0.0562, 0.1362, 0.4536, 0.4038, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0341, 0.0343, 0.0370, 0.0273, 0.0632, 0.0714, 0.3535, 0.0044]) \n",
      "Test Loss tensor([0.0348, 0.0346, 0.0377, 0.0290, 0.0719, 0.0683, 0.3542, 0.0045])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5379, 0.5545, 0.0998, 0.1591, 0.4713, 0.2674])\n",
      "Valid Idx 3 | Loss tensor([0.8445, 0.6808, 0.7100, 0.5440, 0.8037])\n",
      "Gradients: Input 0.3342021703720093 | Message 0.3212559223175049 | Update 0.4758640229701996 | Output 0.14018172025680542\n",
      "\n",
      "************** Batch 724 in 5.760663747787476 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1484, 0.2285, 0.1943, 0.2638]) \n",
      "Test Loss tensor([0.1400, 0.2242, 0.1942, 0.2561])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4328, 0.5568, 0.0223, 0.2670, 0.6402]) \n",
      "Test Loss tensor([0.4444, 0.5771, 0.0213, 0.2755, 0.6351])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1282, 0.0280, 0.2597, 0.3286, 0.2158, 0.2763, 0.0221]) \n",
      "Test Loss tensor([0.1274, 0.0282, 0.2670, 0.3336, 0.2141, 0.2663, 0.0178])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0547, 0.2435, 0.0517, 0.1337, 0.4602, 0.4110, 0.0477]) \n",
      "Test Loss tensor([0.0630, 0.2480, 0.0583, 0.1420, 0.4531, 0.4017, 0.0497])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0280, 0.0343, 0.0326, 0.0286, 0.0715, 0.0691, 0.3556, 0.0045]) \n",
      "Test Loss tensor([0.0333, 0.0336, 0.0364, 0.0294, 0.0751, 0.0649, 0.3538, 0.0042])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5407, 0.5460, 0.1014, 0.1646, 0.4672, 0.2703])\n",
      "Valid Idx 3 | Loss tensor([0.8506, 0.6877, 0.7184, 0.5577, 0.8072])\n",
      "\n",
      "************** Batch 728 in 5.006871223449707 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1417, 0.2327, 0.1941, 0.2500]) \n",
      "Test Loss tensor([0.1377, 0.2244, 0.1952, 0.2513])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4373, 0.5743, 0.0219, 0.2684, 0.6391]) \n",
      "Test Loss tensor([0.4382, 0.5773, 0.0229, 0.2757, 0.6307])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1293, 0.0282, 0.2667, 0.3483, 0.2173, 0.2620, 0.0164]) \n",
      "Test Loss tensor([0.1289, 0.0267, 0.2670, 0.3367, 0.2111, 0.2720, 0.0190])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0664, 0.2592, 0.0542, 0.1366, 0.4635, 0.4045, 0.0473]) \n",
      "Test Loss tensor([0.0594, 0.2481, 0.0572, 0.1350, 0.4442, 0.4074, 0.0504])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0336, 0.0358, 0.0393, 0.0295, 0.0750, 0.0643, 0.3369, 0.0043]) \n",
      "Test Loss tensor([0.0326, 0.0347, 0.0357, 0.0297, 0.0701, 0.0709, 0.3506, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5385, 0.5630, 0.1043, 0.1618, 0.4751, 0.2678])\n",
      "Valid Idx 3 | Loss tensor([0.8276, 0.6571, 0.6916, 0.5419, 0.7994])\n",
      "\n",
      "************** Batch 732 in 4.9958672523498535 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1404, 0.2191, 0.2008, 0.2595]) \n",
      "Test Loss tensor([0.1407, 0.2254, 0.1925, 0.2564])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4357, 0.5655, 0.0219, 0.2751, 0.6343]) \n",
      "Test Loss tensor([0.4330, 0.5749, 0.0232, 0.2758, 0.6338])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1323, 0.0271, 0.2728, 0.3486, 0.2120, 0.2691, 0.0193]) \n",
      "Test Loss tensor([0.1295, 0.0259, 0.2699, 0.3335, 0.2129, 0.2685, 0.0194])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0600, 0.2463, 0.0604, 0.1394, 0.4535, 0.3916, 0.0548]) \n",
      "Test Loss tensor([0.0602, 0.2448, 0.0549, 0.1302, 0.4460, 0.4069, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0319, 0.0345, 0.0352, 0.0319, 0.0677, 0.0737, 0.3517, 0.0042]) \n",
      "Test Loss tensor([0.0331, 0.0349, 0.0355, 0.0282, 0.0692, 0.0720, 0.3480, 0.0047])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5382, 0.5714, 0.1022, 0.1587, 0.4698, 0.2697])\n",
      "Valid Idx 3 | Loss tensor([0.8088, 0.6314, 0.6720, 0.5377, 0.7994])\n",
      "\n",
      "************** Batch 736 in 4.981519937515259 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1338, 0.2136, 0.1913, 0.2488]) \n",
      "Test Loss tensor([0.1368, 0.2259, 0.1939, 0.2494])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4370, 0.5663, 0.0244, 0.2803, 0.6201]) \n",
      "Test Loss tensor([0.4359, 0.5739, 0.0227, 0.2783, 0.6301])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1323, 0.0264, 0.2659, 0.3354, 0.2120, 0.2760, 0.0235]) \n",
      "Test Loss tensor([0.1239, 0.0270, 0.2664, 0.3342, 0.2104, 0.2601, 0.0181])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0577, 0.2497, 0.0488, 0.1248, 0.4129, 0.4118, 0.0467]) \n",
      "Test Loss tensor([0.0609, 0.2441, 0.0548, 0.1349, 0.4436, 0.3929, 0.0472])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0342, 0.0328, 0.0394, 0.0269, 0.0691, 0.0717, 0.3593, 0.0045]) \n",
      "Test Loss tensor([0.0331, 0.0339, 0.0372, 0.0284, 0.0784, 0.0654, 0.3402, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5467, 0.5531, 0.0999, 0.1627, 0.4684, 0.2724])\n",
      "Valid Idx 3 | Loss tensor([0.8408, 0.6638, 0.7082, 0.5632, 0.8032])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 740 in 4.930327892303467 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1359, 0.2320, 0.1950, 0.2413]) \n",
      "Test Loss tensor([0.1379, 0.2205, 0.1916, 0.2491])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4155, 0.5510, 0.0224, 0.2630, 0.6441]) \n",
      "Test Loss tensor([0.4298, 0.5737, 0.0240, 0.2755, 0.6323])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1246, 0.0281, 0.2720, 0.3365, 0.2026, 0.2535, 0.0190]) \n",
      "Test Loss tensor([0.1267, 0.0255, 0.2675, 0.3316, 0.2078, 0.2588, 0.0202])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0647, 0.2538, 0.0580, 0.1378, 0.4437, 0.3950, 0.0492]) \n",
      "Test Loss tensor([0.0578, 0.2509, 0.0549, 0.1300, 0.4444, 0.3996, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0361, 0.0359, 0.0260, 0.0783, 0.0657, 0.3352, 0.0045]) \n",
      "Test Loss tensor([0.0339, 0.0348, 0.0365, 0.0285, 0.0716, 0.0730, 0.3394, 0.0047])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5419, 0.5763, 0.1027, 0.1607, 0.4736, 0.2699])\n",
      "Valid Idx 3 | Loss tensor([0.7960, 0.6027, 0.6620, 0.5415, 0.8011])\n",
      "Gradients: Input 0.2628191113471985 | Message 0.2549867033958435 | Update 0.38751929998397827 | Output 0.1627635359764099\n",
      "\n",
      "************** Batch 744 in 4.894816875457764 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1397, 0.2171, 0.1972, 0.2561]) \n",
      "Test Loss tensor([0.1366, 0.2237, 0.1911, 0.2464])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4206, 0.5692, 0.0269, 0.2803, 0.6291]) \n",
      "Test Loss tensor([0.4273, 0.5760, 0.0241, 0.2718, 0.6290])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1260, 0.0263, 0.2666, 0.3247, 0.2046, 0.2519, 0.0196]) \n",
      "Test Loss tensor([0.1253, 0.0250, 0.2630, 0.3295, 0.2067, 0.2555, 0.0194])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0561, 0.2558, 0.0526, 0.1240, 0.4431, 0.4138, 0.0396]) \n",
      "Test Loss tensor([0.0587, 0.2501, 0.0559, 0.1277, 0.4374, 0.3937, 0.0500])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0292, 0.0334, 0.0395, 0.0277, 0.0680, 0.0738, 0.3201, 0.0043]) \n",
      "Test Loss tensor([0.0330, 0.0355, 0.0353, 0.0273, 0.0722, 0.0707, 0.3337, 0.0046])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5420, 0.5732, 0.1037, 0.1604, 0.4766, 0.2719])\n",
      "Valid Idx 3 | Loss tensor([0.7882, 0.5871, 0.6593, 0.5454, 0.8005])\n",
      "\n",
      "************** Batch 748 in 4.845484018325806 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1412, 0.2352, 0.1973, 0.2427]) \n",
      "Test Loss tensor([0.1368, 0.2247, 0.1962, 0.2552])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4261, 0.5724, 0.0266, 0.2607, 0.6364]) \n",
      "Test Loss tensor([0.4323, 0.5755, 0.0242, 0.2728, 0.6315])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1267, 0.0245, 0.2741, 0.3330, 0.2040, 0.2526, 0.0183]) \n",
      "Test Loss tensor([0.1241, 0.0251, 0.2685, 0.3325, 0.2045, 0.2535, 0.0183])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0581, 0.2495, 0.0510, 0.1338, 0.4473, 0.3931, 0.0448]) \n",
      "Test Loss tensor([0.0591, 0.2481, 0.0547, 0.1293, 0.4400, 0.3925, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0310, 0.0372, 0.0381, 0.0273, 0.0730, 0.0671, 0.3286, 0.0043]) \n",
      "Test Loss tensor([0.0323, 0.0341, 0.0359, 0.0286, 0.0757, 0.0653, 0.3301, 0.0045])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5458, 0.5647, 0.1017, 0.1607, 0.4672, 0.2716])\n",
      "Valid Idx 3 | Loss tensor([0.8031, 0.5959, 0.6739, 0.5591, 0.8063])\n",
      "\n",
      "************** Batch 752 in 4.898623466491699 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1418, 0.2206, 0.1933, 0.2675]) \n",
      "Test Loss tensor([0.1386, 0.2251, 0.1902, 0.2460])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4460, 0.5941, 0.0244, 0.2839, 0.6270]) \n",
      "Test Loss tensor([0.4309, 0.5725, 0.0251, 0.2706, 0.6340])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1248, 0.0255, 0.2703, 0.3354, 0.2004, 0.2465, 0.0169]) \n",
      "Test Loss tensor([0.1275, 0.0243, 0.2639, 0.3275, 0.2039, 0.2527, 0.0196])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0607, 0.2392, 0.0520, 0.1294, 0.4383, 0.3949, 0.0475]) \n",
      "Test Loss tensor([0.0557, 0.2510, 0.0533, 0.1243, 0.4392, 0.3973, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0321, 0.0352, 0.0342, 0.0273, 0.0758, 0.0638, 0.3369, 0.0043]) \n",
      "Test Loss tensor([0.0335, 0.0351, 0.0371, 0.0281, 0.0694, 0.0720, 0.3271, 0.0046])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5438, 0.5828, 0.1032, 0.1602, 0.4741, 0.2714])\n",
      "Valid Idx 3 | Loss tensor([0.7590, 0.5399, 0.6275, 0.5421, 0.8008])\n",
      "\n",
      "************** Batch 756 in 4.858455181121826 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1335, 0.2170, 0.1907, 0.2449]) \n",
      "Test Loss tensor([0.1361, 0.2240, 0.1930, 0.2517])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4459, 0.5807, 0.0242, 0.2803, 0.6226]) \n",
      "Test Loss tensor([0.4273, 0.5765, 0.0257, 0.2750, 0.6272])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1294, 0.0237, 0.2695, 0.3321, 0.2126, 0.2556, 0.0189]) \n",
      "Test Loss tensor([0.1263, 0.0241, 0.2653, 0.3327, 0.2030, 0.2480, 0.0194])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0589, 0.2566, 0.0504, 0.1279, 0.4422, 0.3904, 0.0418]) \n",
      "Test Loss tensor([0.0563, 0.2508, 0.0554, 0.1240, 0.4364, 0.3909, 0.0502])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0315, 0.0346, 0.0342, 0.0262, 0.0707, 0.0750, 0.3268, 0.0043]) \n",
      "Test Loss tensor([0.0330, 0.0351, 0.0357, 0.0283, 0.0716, 0.0700, 0.3209, 0.0046])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5478, 0.5784, 0.1052, 0.1620, 0.4684, 0.2749])\n",
      "Valid Idx 3 | Loss tensor([0.7646, 0.5419, 0.6362, 0.5477, 0.8070])\n",
      "\n",
      "************** Batch 760 in 4.851516485214233 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1370, 0.2124, 0.1899, 0.2464]) \n",
      "Test Loss tensor([0.1357, 0.2223, 0.1912, 0.2494])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4209, 0.5586, 0.0266, 0.2756, 0.6382]) \n",
      "Test Loss tensor([0.4289, 0.5729, 0.0249, 0.2711, 0.6311])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1247, 0.0252, 0.2764, 0.3294, 0.2040, 0.2480, 0.0187]) \n",
      "Test Loss tensor([0.1212, 0.0243, 0.2648, 0.3298, 0.2029, 0.2436, 0.0185])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0561, 0.2520, 0.0554, 0.1234, 0.4449, 0.4033, 0.0547]) \n",
      "Test Loss tensor([0.0559, 0.2464, 0.0542, 0.1223, 0.4359, 0.3922, 0.0501])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0346, 0.0331, 0.0401, 0.0272, 0.0714, 0.0700, 0.3135, 0.0045]) \n",
      "Test Loss tensor([0.0320, 0.0342, 0.0363, 0.0273, 0.0752, 0.0678, 0.3184, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5491, 0.5756, 0.1042, 0.1612, 0.4678, 0.2746])\n",
      "Valid Idx 3 | Loss tensor([0.7745, 0.5476, 0.6491, 0.5635, 0.8058])\n",
      "Gradients: Input 0.06327099353075027 | Message 0.030903080478310585 | Update 0.02328861877322197 | Output 0.030327055603265762\n",
      "\n",
      "************** Batch 764 in 4.7892186641693115 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1322, 0.2250, 0.1866, 0.2439]) \n",
      "Test Loss tensor([0.1360, 0.2248, 0.1932, 0.2514])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4471, 0.5736, 0.0277, 0.2640, 0.6321]) \n",
      "Test Loss tensor([0.4283, 0.5768, 0.0249, 0.2660, 0.6248])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1144, 0.0242, 0.2608, 0.3312, 0.2078, 0.2475, 0.0185]) \n",
      "Test Loss tensor([0.1191, 0.0243, 0.2627, 0.3327, 0.2010, 0.2406, 0.0187])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0515, 0.2443, 0.0535, 0.1190, 0.4430, 0.3841, 0.0411]) \n",
      "Test Loss tensor([0.0571, 0.2465, 0.0540, 0.1239, 0.4377, 0.3904, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0302, 0.0378, 0.0355, 0.0280, 0.0719, 0.0671, 0.3084, 0.0045]) \n",
      "Test Loss tensor([0.0330, 0.0336, 0.0379, 0.0270, 0.0757, 0.0641, 0.3160, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5559, 0.5824, 0.1039, 0.1614, 0.4636, 0.2768])\n",
      "Valid Idx 3 | Loss tensor([0.7788, 0.5502, 0.6518, 0.5640, 0.8094])\n",
      "\n",
      "************** Batch 768 in 5.307822227478027 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1312, 0.2290, 0.2019, 0.2318]) \n",
      "Test Loss tensor([0.1364, 0.2227, 0.1937, 0.2485])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4123, 0.5657, 0.0265, 0.2697, 0.6469]) \n",
      "Test Loss tensor([0.4221, 0.5766, 0.0266, 0.2663, 0.6300])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1224, 0.0237, 0.2702, 0.3383, 0.1974, 0.2356, 0.0165]) \n",
      "Test Loss tensor([0.1214, 0.0234, 0.2600, 0.3286, 0.1984, 0.2388, 0.0192])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0526, 0.2464, 0.0482, 0.1160, 0.4461, 0.3902, 0.0504]) \n",
      "Test Loss tensor([0.0544, 0.2488, 0.0539, 0.1191, 0.4341, 0.3965, 0.0497])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0296, 0.0317, 0.0382, 0.0240, 0.0752, 0.0660, 0.3231, 0.0041]) \n",
      "Test Loss tensor([0.0332, 0.0347, 0.0364, 0.0275, 0.0686, 0.0691, 0.3108, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5499, 0.5969, 0.1070, 0.1583, 0.4685, 0.2732])\n",
      "Valid Idx 3 | Loss tensor([0.7420, 0.5040, 0.6103, 0.5455, 0.8029])\n",
      "\n",
      "************** Batch 772 in 5.148367404937744 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1418, 0.2139, 0.1877, 0.2603]) \n",
      "Test Loss tensor([0.1340, 0.2223, 0.1918, 0.2525])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4399, 0.5842, 0.0238, 0.2645, 0.6262]) \n",
      "Test Loss tensor([0.4266, 0.5730, 0.0243, 0.2640, 0.6324])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1346, 0.0237, 0.2669, 0.3273, 0.1962, 0.2512, 0.0197]) \n",
      "Test Loss tensor([0.1206, 0.0242, 0.2692, 0.3322, 0.1977, 0.2370, 0.0187])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0536, 0.2516, 0.0506, 0.1204, 0.4398, 0.4028, 0.0488]) \n",
      "Test Loss tensor([0.0561, 0.2440, 0.0548, 0.1238, 0.4359, 0.3939, 0.0492])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0323, 0.0323, 0.0355, 0.0268, 0.0678, 0.0700, 0.3047, 0.0042]) \n",
      "Test Loss tensor([0.0321, 0.0331, 0.0377, 0.0273, 0.0761, 0.0617, 0.3087, 0.0043])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5555, 0.5847, 0.1052, 0.1609, 0.4658, 0.2773])\n",
      "Valid Idx 3 | Loss tensor([0.7846, 0.5397, 0.6540, 0.5673, 0.8116])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 776 in 5.113231658935547 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1418, 0.2240, 0.1965, 0.2626]) \n",
      "Test Loss tensor([0.1321, 0.2192, 0.1938, 0.2497])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4198, 0.5807, 0.0255, 0.2687, 0.6300]) \n",
      "Test Loss tensor([0.4263, 0.5781, 0.0261, 0.2666, 0.6283])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1164, 0.0245, 0.2721, 0.3266, 0.1977, 0.2414, 0.0201]) \n",
      "Test Loss tensor([0.1155, 0.0240, 0.2669, 0.3326, 0.1949, 0.2318, 0.0184])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0559, 0.2551, 0.0500, 0.1265, 0.4294, 0.3848, 0.0473]) \n",
      "Test Loss tensor([0.0576, 0.2438, 0.0524, 0.1248, 0.4350, 0.3834, 0.0473])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0340, 0.0308, 0.0402, 0.0252, 0.0747, 0.0608, 0.2998, 0.0044]) \n",
      "Test Loss tensor([0.0317, 0.0323, 0.0367, 0.0279, 0.0760, 0.0605, 0.3027, 0.0043])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5526, 0.5879, 0.1061, 0.1641, 0.4607, 0.2848])\n",
      "Valid Idx 3 | Loss tensor([0.7854, 0.5301, 0.6546, 0.5734, 0.8085])\n",
      "\n",
      "************** Batch 780 in 5.078237771987915 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1227, 0.2182, 0.1988, 0.2395]) \n",
      "Test Loss tensor([0.1353, 0.2197, 0.1870, 0.2470])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4145, 0.5593, 0.0268, 0.2520, 0.6404]) \n",
      "Test Loss tensor([0.4186, 0.5729, 0.0276, 0.2625, 0.6292])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1172, 0.0229, 0.2642, 0.3273, 0.2031, 0.2417, 0.0178]) \n",
      "Test Loss tensor([0.1217, 0.0229, 0.2625, 0.3273, 0.2004, 0.2338, 0.0196])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0584, 0.2392, 0.0491, 0.1280, 0.4369, 0.3848, 0.0497]) \n",
      "Test Loss tensor([0.0522, 0.2464, 0.0493, 0.1161, 0.4326, 0.3951, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0361, 0.0308, 0.0401, 0.0267, 0.0743, 0.0597, 0.3131, 0.0042]) \n",
      "Test Loss tensor([0.0330, 0.0349, 0.0364, 0.0264, 0.0663, 0.0661, 0.3035, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5510, 0.6113, 0.1063, 0.1578, 0.4662, 0.2740])\n",
      "Valid Idx 3 | Loss tensor([0.7217, 0.4507, 0.5889, 0.5491, 0.8102])\n",
      "Gradients: Input 0.18690073490142822 | Message 0.16484135389328003 | Update 0.2434583306312561 | Output 0.0895378589630127\n",
      "\n",
      "************** Batch 784 in 5.069563150405884 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1346, 0.2219, 0.1869, 0.2459]) \n",
      "Test Loss tensor([0.1346, 0.2181, 0.1895, 0.2503])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4226, 0.5630, 0.0264, 0.2626, 0.6356]) \n",
      "Test Loss tensor([0.4200, 0.5727, 0.0269, 0.2628, 0.6298])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1193, 0.0230, 0.2709, 0.3333, 0.1981, 0.2357, 0.0182]) \n",
      "Test Loss tensor([0.1196, 0.0233, 0.2656, 0.3248, 0.1966, 0.2325, 0.0191])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0546, 0.2511, 0.0551, 0.1216, 0.4341, 0.4001, 0.0482]) \n",
      "Test Loss tensor([0.0538, 0.2443, 0.0526, 0.1192, 0.4310, 0.3907, 0.0492])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0281, 0.0362, 0.0385, 0.0263, 0.0650, 0.0681, 0.2880, 0.0049]) \n",
      "Test Loss tensor([0.0328, 0.0336, 0.0372, 0.0267, 0.0686, 0.0608, 0.2982, 0.0043])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5536, 0.5933, 0.1068, 0.1598, 0.4646, 0.2743])\n",
      "Valid Idx 3 | Loss tensor([0.7349, 0.4468, 0.6025, 0.5591, 0.8066])\n",
      "\n",
      "************** Batch 788 in 5.051250219345093 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1454, 0.2274, 0.1825, 0.2565]) \n",
      "Test Loss tensor([0.1315, 0.2192, 0.1907, 0.2515])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4230, 0.5720, 0.0251, 0.2731, 0.6182]) \n",
      "Test Loss tensor([0.4227, 0.5780, 0.0262, 0.2659, 0.6273])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1104, 0.0238, 0.2563, 0.3226, 0.1965, 0.2269, 0.0208]) \n",
      "Test Loss tensor([0.1157, 0.0246, 0.2680, 0.3256, 0.1965, 0.2274, 0.0185])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0544, 0.2412, 0.0486, 0.1104, 0.4371, 0.3957, 0.0515]) \n",
      "Test Loss tensor([0.0597, 0.2412, 0.0527, 0.1253, 0.4385, 0.3729, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0355, 0.0331, 0.0370, 0.0246, 0.0672, 0.0610, 0.2968, 0.0045]) \n",
      "Test Loss tensor([0.0303, 0.0320, 0.0371, 0.0271, 0.0807, 0.0516, 0.2972, 0.0042])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5638, 0.5629, 0.1064, 0.1706, 0.4526, 0.2877])\n",
      "Valid Idx 3 | Loss tensor([0.7918, 0.4872, 0.6657, 0.6024, 0.8176])\n",
      "\n",
      "************** Batch 792 in 5.046786069869995 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1339, 0.2209, 0.1902, 0.2515]) \n",
      "Test Loss tensor([0.1345, 0.2172, 0.1886, 0.2440])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4123, 0.5636, 0.0261, 0.2701, 0.6364]) \n",
      "Test Loss tensor([0.4153, 0.5745, 0.0305, 0.2623, 0.6329])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1172, 0.0241, 0.2617, 0.3250, 0.1963, 0.2370, 0.0172]) \n",
      "Test Loss tensor([0.1223, 0.0214, 0.2666, 0.3270, 0.1988, 0.2316, 0.0211])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0545, 0.2434, 0.0593, 0.1233, 0.4337, 0.3758, 0.0470]) \n",
      "Test Loss tensor([0.0506, 0.2468, 0.0504, 0.1129, 0.4257, 0.3896, 0.0483])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0331, 0.0320, 0.0395, 0.0304, 0.0817, 0.0545, 0.2853, 0.0039]) \n",
      "Test Loss tensor([0.0328, 0.0336, 0.0363, 0.0269, 0.0623, 0.0637, 0.2943, 0.0041])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5498, 0.6018, 0.1100, 0.1587, 0.4712, 0.2741])\n",
      "Valid Idx 3 | Loss tensor([0.6695, 0.3502, 0.5395, 0.5459, 0.8080])\n",
      "\n",
      "************** Batch 796 in 5.014268398284912 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1345, 0.2145, 0.1854, 0.2555]) \n",
      "Test Loss tensor([0.1313, 0.2164, 0.1883, 0.2479])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4129, 0.5647, 0.0312, 0.2578, 0.6378]) \n",
      "Test Loss tensor([0.4207, 0.5750, 0.0297, 0.2587, 0.6300])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1211, 0.0212, 0.2785, 0.3320, 0.1965, 0.2273, 0.0204]) \n",
      "Test Loss tensor([0.1194, 0.0218, 0.2658, 0.3275, 0.1981, 0.2282, 0.0204])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0504, 0.2496, 0.0513, 0.1179, 0.4305, 0.3934, 0.0544]) \n",
      "Test Loss tensor([0.0533, 0.2446, 0.0511, 0.1201, 0.4299, 0.3801, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0335, 0.0417, 0.0258, 0.0627, 0.0610, 0.2931, 0.0042]) \n",
      "Test Loss tensor([0.0304, 0.0322, 0.0360, 0.0260, 0.0692, 0.0566, 0.2914, 0.0041])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5568, 0.5804, 0.1092, 0.1645, 0.4565, 0.2830])\n",
      "Valid Idx 3 | Loss tensor([0.7165, 0.3747, 0.5847, 0.5750, 0.8160])\n",
      "\n",
      "************** Batch 800 in 5.040105104446411 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1336, 0.2113, 0.1976, 0.2502]) \n",
      "Test Loss tensor([0.1315, 0.2175, 0.1903, 0.2522])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4224, 0.5614, 0.0274, 0.2472, 0.6339]) \n",
      "Test Loss tensor([0.4237, 0.5785, 0.0276, 0.2659, 0.6277])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1166, 0.0220, 0.2712, 0.3369, 0.1981, 0.2271, 0.0197]) \n",
      "Test Loss tensor([0.1140, 0.0229, 0.2693, 0.3249, 0.1944, 0.2247, 0.0184])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0554, 0.2321, 0.0470, 0.1265, 0.4326, 0.3843, 0.0421]) \n",
      "Test Loss tensor([0.0573, 0.2383, 0.0503, 0.1236, 0.4324, 0.3743, 0.0482])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0322, 0.0383, 0.0259, 0.0687, 0.0571, 0.2841, 0.0041]) \n",
      "Test Loss tensor([0.0308, 0.0322, 0.0399, 0.0277, 0.0767, 0.0509, 0.2914, 0.0039])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5660, 0.5581, 0.1088, 0.1733, 0.4506, 0.2847])\n",
      "Valid Idx 3 | Loss tensor([0.7620, 0.3974, 0.6256, 0.6021, 0.8188])\n",
      "Gradients: Input 0.060448046773672104 | Message 0.030995886772871017 | Update 0.02796659991145134 | Output 0.022085849195718765\n",
      "\n",
      "************** Batch 804 in 5.0142457485198975 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1368, 0.2103, 0.1913, 0.2653]) \n",
      "Test Loss tensor([0.1330, 0.2153, 0.1841, 0.2436])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4195, 0.5700, 0.0261, 0.2650, 0.6432]) \n",
      "Test Loss tensor([0.4168, 0.5805, 0.0330, 0.2546, 0.6259])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1124, 0.0229, 0.2717, 0.3219, 0.1947, 0.2180, 0.0193]) \n",
      "Test Loss tensor([0.1215, 0.0209, 0.2667, 0.3219, 0.2005, 0.2294, 0.0206])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0589, 0.2379, 0.0502, 0.1311, 0.4263, 0.3761, 0.0438]) \n",
      "Test Loss tensor([0.0487, 0.2439, 0.0499, 0.1136, 0.4265, 0.3859, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0299, 0.0295, 0.0389, 0.0259, 0.0775, 0.0535, 0.2844, 0.0045]) \n",
      "Test Loss tensor([0.0317, 0.0342, 0.0365, 0.0257, 0.0607, 0.0617, 0.2866, 0.0041])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5576, 0.5994, 0.1138, 0.1572, 0.4657, 0.2725])\n",
      "Valid Idx 3 | Loss tensor([0.6475, 0.2778, 0.5037, 0.5478, 0.8082])\n",
      "\n",
      "************** Batch 808 in 5.081195116043091 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1350, 0.2212, 0.1933, 0.2373]) \n",
      "Test Loss tensor([0.1273, 0.2128, 0.1881, 0.2451])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4187, 0.5763, 0.0298, 0.2583, 0.6317]) \n",
      "Test Loss tensor([0.4192, 0.5728, 0.0294, 0.2588, 0.6306])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1234, 0.0221, 0.2782, 0.3239, 0.1972, 0.2389, 0.0195]) \n",
      "Test Loss tensor([0.1145, 0.0222, 0.2675, 0.3226, 0.1952, 0.2227, 0.0200])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0464, 0.2446, 0.0467, 0.1121, 0.4240, 0.3742, 0.0449]) \n",
      "Test Loss tensor([0.0541, 0.2395, 0.0523, 0.1184, 0.4284, 0.3749, 0.0482])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0293, 0.0335, 0.0391, 0.0247, 0.0610, 0.0628, 0.2844, 0.0048]) \n",
      "Test Loss tensor([0.0299, 0.0311, 0.0368, 0.0259, 0.0711, 0.0507, 0.2855, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5642, 0.5693, 0.1104, 0.1668, 0.4549, 0.2840])\n",
      "Valid Idx 3 | Loss tensor([0.7479, 0.3509, 0.5957, 0.5940, 0.8169])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 812 in 5.11441707611084 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1321, 0.2136, 0.1859, 0.2542]) \n",
      "Test Loss tensor([0.1283, 0.2126, 0.1879, 0.2523])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4147, 0.5723, 0.0289, 0.2473, 0.6375]) \n",
      "Test Loss tensor([0.4156, 0.5730, 0.0292, 0.2591, 0.6317])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1163, 0.0223, 0.2852, 0.3338, 0.1941, 0.2211, 0.0217]) \n",
      "Test Loss tensor([0.1127, 0.0225, 0.2715, 0.3221, 0.1938, 0.2207, 0.0189])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0546, 0.2330, 0.0540, 0.1250, 0.4329, 0.3699, 0.0499]) \n",
      "Test Loss tensor([0.0556, 0.2362, 0.0501, 0.1199, 0.4260, 0.3711, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0314, 0.0322, 0.0381, 0.0276, 0.0690, 0.0543, 0.2847, 0.0041]) \n",
      "Test Loss tensor([0.0302, 0.0315, 0.0388, 0.0268, 0.0726, 0.0497, 0.2793, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5642, 0.5657, 0.1136, 0.1679, 0.4547, 0.2876])\n",
      "Valid Idx 3 | Loss tensor([0.7625, 0.3501, 0.6038, 0.6013, 0.8201])\n",
      "\n",
      "************** Batch 816 in 5.030736923217773 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1422, 0.2087, 0.1957, 0.2668]) \n",
      "Test Loss tensor([0.1296, 0.2118, 0.1845, 0.2437])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4134, 0.5671, 0.0330, 0.2572, 0.6257]) \n",
      "Test Loss tensor([0.4134, 0.5735, 0.0353, 0.2554, 0.6295])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1205, 0.0241, 0.2687, 0.3193, 0.1965, 0.2229, 0.0188]) \n",
      "Test Loss tensor([0.1179, 0.0207, 0.2655, 0.3203, 0.1976, 0.2218, 0.0214])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0599, 0.2446, 0.0517, 0.1262, 0.4294, 0.3716, 0.0501]) \n",
      "Test Loss tensor([0.0481, 0.2416, 0.0482, 0.1121, 0.4240, 0.3814, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0298, 0.0329, 0.0386, 0.0276, 0.0722, 0.0510, 0.2837, 0.0038]) \n",
      "Test Loss tensor([0.0306, 0.0321, 0.0368, 0.0256, 0.0594, 0.0620, 0.2798, 0.0040])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5595, 0.6067, 0.1183, 0.1600, 0.4619, 0.2773])\n",
      "Valid Idx 3 | Loss tensor([0.6640, 0.2567, 0.4921, 0.5462, 0.8113])\n",
      "\n",
      "************** Batch 820 in 4.988154411315918 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1264, 0.2112, 0.1893, 0.2347]) \n",
      "Test Loss tensor([0.1260, 0.2115, 0.1863, 0.2461])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4256, 0.5800, 0.0346, 0.2518, 0.6168]) \n",
      "Test Loss tensor([0.4179, 0.5725, 0.0312, 0.2552, 0.6312])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1098, 0.0212, 0.2737, 0.3132, 0.1961, 0.2189, 0.0231]) \n",
      "Test Loss tensor([0.1090, 0.0220, 0.2670, 0.3181, 0.1937, 0.2166, 0.0195])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0454, 0.2368, 0.0461, 0.1096, 0.4145, 0.3897, 0.0509]) \n",
      "Test Loss tensor([0.0527, 0.2353, 0.0498, 0.1155, 0.4206, 0.3744, 0.0484])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0306, 0.0340, 0.0385, 0.0254, 0.0564, 0.0622, 0.2810, 0.0040]) \n",
      "Test Loss tensor([0.0308, 0.0300, 0.0374, 0.0274, 0.0683, 0.0529, 0.2725, 0.0036])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5615, 0.5858, 0.1156, 0.1692, 0.4556, 0.2878])\n",
      "Valid Idx 3 | Loss tensor([0.7418, 0.3112, 0.5674, 0.5873, 0.8188])\n",
      "Gradients: Input 0.44317498803138733 | Message 0.4378577470779419 | Update 0.6178759336471558 | Output 0.21535107493400574\n",
      "\n",
      "************** Batch 824 in 4.9904656410217285 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1327, 0.2111, 0.1849, 0.2531]) \n",
      "Test Loss tensor([0.1250, 0.2118, 0.1858, 0.2477])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4201, 0.5799, 0.0332, 0.2481, 0.6116]) \n",
      "Test Loss tensor([0.4107, 0.5727, 0.0309, 0.2547, 0.6325])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1114, 0.0228, 0.2633, 0.3079, 0.1879, 0.2117, 0.0211]) \n",
      "Test Loss tensor([0.1120, 0.0221, 0.2690, 0.3193, 0.1927, 0.2135, 0.0190])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0485, 0.2544, 0.0521, 0.1127, 0.4213, 0.3798, 0.0465]) \n",
      "Test Loss tensor([0.0514, 0.2385, 0.0495, 0.1165, 0.4240, 0.3713, 0.0479])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0320, 0.0357, 0.0242, 0.0671, 0.0501, 0.2861, 0.0037]) \n",
      "Test Loss tensor([0.0300, 0.0307, 0.0391, 0.0259, 0.0706, 0.0517, 0.2729, 0.0036])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5639, 0.5844, 0.1163, 0.1682, 0.4522, 0.2893])\n",
      "Valid Idx 3 | Loss tensor([0.7369, 0.3047, 0.5616, 0.5938, 0.8195])\n",
      "\n",
      "************** Batch 828 in 4.988632917404175 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1150, 0.2187, 0.1811, 0.2384]) \n",
      "Test Loss tensor([0.1235, 0.2129, 0.1890, 0.2471])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4165, 0.5800, 0.0310, 0.2515, 0.6316]) \n",
      "Test Loss tensor([0.4090, 0.5706, 0.0348, 0.2495, 0.6319])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1072, 0.0224, 0.2674, 0.3250, 0.2029, 0.2146, 0.0204]) \n",
      "Test Loss tensor([0.1166, 0.0209, 0.2684, 0.3222, 0.1945, 0.2136, 0.0204])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0486, 0.2278, 0.0506, 0.1220, 0.4265, 0.3693, 0.0485]) \n",
      "Test Loss tensor([0.0478, 0.2406, 0.0486, 0.1090, 0.4188, 0.3769, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0299, 0.0287, 0.0353, 0.0248, 0.0688, 0.0527, 0.2663, 0.0037]) \n",
      "Test Loss tensor([0.0312, 0.0313, 0.0372, 0.0263, 0.0628, 0.0555, 0.2753, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5663, 0.6038, 0.1182, 0.1624, 0.4563, 0.2864])\n",
      "Valid Idx 3 | Loss tensor([0.6723, 0.2504, 0.4910, 0.5632, 0.8189])\n",
      "\n",
      "************** Batch 832 in 5.0299646854400635 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1265, 0.2123, 0.1749, 0.2463]) \n",
      "Test Loss tensor([0.1244, 0.2101, 0.1858, 0.2469])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3999, 0.5790, 0.0326, 0.2466, 0.6313]) \n",
      "Test Loss tensor([0.4105, 0.5714, 0.0334, 0.2493, 0.6316])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1102, 0.0208, 0.2583, 0.3089, 0.1874, 0.2057, 0.0216]) \n",
      "Test Loss tensor([0.1145, 0.0207, 0.2652, 0.3146, 0.1916, 0.2086, 0.0187])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0437, 0.2440, 0.0484, 0.1066, 0.4151, 0.3761, 0.0466]) \n",
      "Test Loss tensor([0.0462, 0.2390, 0.0485, 0.1087, 0.4182, 0.3717, 0.0472])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0320, 0.0394, 0.0261, 0.0645, 0.0578, 0.2717, 0.0040]) \n",
      "Test Loss tensor([0.0298, 0.0306, 0.0387, 0.0260, 0.0637, 0.0539, 0.2683, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5640, 0.5998, 0.1180, 0.1615, 0.4613, 0.2871])\n",
      "Valid Idx 3 | Loss tensor([0.6669, 0.2499, 0.4904, 0.5693, 0.8219])\n",
      "\n",
      "************** Batch 836 in 5.065158367156982 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1272, 0.2129, 0.1812, 0.2348]) \n",
      "Test Loss tensor([0.1225, 0.2123, 0.1881, 0.2482])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4248, 0.5862, 0.0320, 0.2621, 0.6252]) \n",
      "Test Loss tensor([0.4139, 0.5767, 0.0293, 0.2548, 0.6240])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1121, 0.0214, 0.2635, 0.3027, 0.1975, 0.2132, 0.0188]) \n",
      "Test Loss tensor([0.1124, 0.0216, 0.2684, 0.3152, 0.1901, 0.2069, 0.0180])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0481, 0.2397, 0.0514, 0.1072, 0.4212, 0.3697, 0.0523]) \n",
      "Test Loss tensor([0.0494, 0.2379, 0.0488, 0.1135, 0.4216, 0.3643, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0289, 0.0289, 0.0384, 0.0282, 0.0645, 0.0568, 0.2766, 0.0036]) \n",
      "Test Loss tensor([0.0290, 0.0292, 0.0367, 0.0271, 0.0716, 0.0476, 0.2709, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5710, 0.5793, 0.1147, 0.1715, 0.4521, 0.3016])\n",
      "Valid Idx 3 | Loss tensor([0.7198, 0.2878, 0.5430, 0.6049, 0.8276])\n",
      "\n",
      "************** Batch 840 in 5.032640695571899 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1223, 0.2077, 0.1877, 0.2455]) \n",
      "Test Loss tensor([0.1259, 0.2125, 0.1833, 0.2404])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4038, 0.5656, 0.0276, 0.2528, 0.6347]) \n",
      "Test Loss tensor([0.4031, 0.5757, 0.0354, 0.2472, 0.6285])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1152, 0.0218, 0.2693, 0.3173, 0.1884, 0.2086, 0.0184]) \n",
      "Test Loss tensor([0.1189, 0.0194, 0.2671, 0.3133, 0.1966, 0.2112, 0.0194])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0504, 0.2356, 0.0509, 0.1136, 0.4128, 0.3677, 0.0469]) \n",
      "Test Loss tensor([0.0416, 0.2413, 0.0496, 0.1062, 0.4157, 0.3776, 0.0482])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0284, 0.0258, 0.0347, 0.0279, 0.0718, 0.0470, 0.2621, 0.0033]) \n",
      "Test Loss tensor([0.0301, 0.0305, 0.0376, 0.0259, 0.0587, 0.0575, 0.2694, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5635, 0.6217, 0.1208, 0.1600, 0.4616, 0.2864])\n",
      "Valid Idx 3 | Loss tensor([0.5894, 0.2042, 0.4111, 0.5378, 0.8201])\n",
      "Gradients: Input 0.42722535133361816 | Message 0.4533282518386841 | Update 0.6354212760925293 | Output 0.20809432864189148\n",
      "\n",
      "************** Batch 844 in 5.035891532897949 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1264, 0.2023, 0.1779, 0.2360]) \n",
      "Test Loss tensor([0.1234, 0.2120, 0.1848, 0.2479])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4027, 0.5714, 0.0369, 0.2532, 0.6294]) \n",
      "Test Loss tensor([0.4091, 0.5746, 0.0309, 0.2549, 0.6276])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1163, 0.0184, 0.2595, 0.3083, 0.1935, 0.2117, 0.0186]) \n",
      "Test Loss tensor([0.1147, 0.0204, 0.2703, 0.3173, 0.1912, 0.2078, 0.0179])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0470, 0.2410, 0.0467, 0.1106, 0.4213, 0.3915, 0.0533]) \n",
      "Test Loss tensor([0.0459, 0.2400, 0.0501, 0.1090, 0.4142, 0.3697, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0293, 0.0323, 0.0371, 0.0253, 0.0607, 0.0581, 0.2591, 0.0035]) \n",
      "Test Loss tensor([0.0292, 0.0275, 0.0367, 0.0256, 0.0668, 0.0475, 0.2646, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5676, 0.6004, 0.1160, 0.1645, 0.4582, 0.3011])\n",
      "Valid Idx 3 | Loss tensor([0.6591, 0.2504, 0.4771, 0.5738, 0.8263])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 848 in 5.064104795455933 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1250, 0.2109, 0.1919, 0.2470]) \n",
      "Test Loss tensor([0.1239, 0.2120, 0.1845, 0.2487])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4118, 0.5672, 0.0293, 0.2470, 0.6377]) \n",
      "Test Loss tensor([0.4074, 0.5763, 0.0276, 0.2520, 0.6265])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1138, 0.0203, 0.2732, 0.3141, 0.1875, 0.2108, 0.0193]) \n",
      "Test Loss tensor([0.1115, 0.0214, 0.2688, 0.3183, 0.1906, 0.2044, 0.0163])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0461, 0.2486, 0.0446, 0.1085, 0.4220, 0.3623, 0.0500]) \n",
      "Test Loss tensor([0.0477, 0.2343, 0.0499, 0.1111, 0.4165, 0.3671, 0.0474])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0299, 0.0302, 0.0372, 0.0229, 0.0651, 0.0470, 0.2666, 0.0032]) \n",
      "Test Loss tensor([0.0290, 0.0278, 0.0370, 0.0261, 0.0727, 0.0437, 0.2644, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5672, 0.5895, 0.1142, 0.1716, 0.4561, 0.3132])\n",
      "Valid Idx 3 | Loss tensor([0.6965, 0.2862, 0.5134, 0.5939, 0.8265])\n",
      "\n",
      "************** Batch 852 in 5.060074329376221 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1212, 0.1981, 0.1869, 0.2610]) \n",
      "Test Loss tensor([0.1248, 0.2138, 0.1779, 0.2386])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4150, 0.5688, 0.0260, 0.2487, 0.6382]) \n",
      "Test Loss tensor([0.4010, 0.5677, 0.0356, 0.2478, 0.6346])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1059, 0.0202, 0.2668, 0.3213, 0.1920, 0.1962, 0.0158]) \n",
      "Test Loss tensor([0.1228, 0.0193, 0.2704, 0.3098, 0.1961, 0.2120, 0.0195])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0512, 0.2438, 0.0490, 0.1141, 0.4039, 0.3691, 0.0481]) \n",
      "Test Loss tensor([0.0400, 0.2410, 0.0483, 0.1035, 0.4096, 0.3775, 0.0493])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0320, 0.0274, 0.0402, 0.0285, 0.0705, 0.0429, 0.2582, 0.0031]) \n",
      "Test Loss tensor([0.0312, 0.0285, 0.0374, 0.0252, 0.0561, 0.0571, 0.2658, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5658, 0.6422, 0.1217, 0.1580, 0.4623, 0.3015])\n",
      "Valid Idx 3 | Loss tensor([0.5385, 0.1876, 0.3615, 0.5113, 0.8163])\n",
      "\n",
      "************** Batch 856 in 5.191731929779053 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1283, 0.2108, 0.1768, 0.2495]) \n",
      "Test Loss tensor([0.1203, 0.2114, 0.1828, 0.2474])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4074, 0.5715, 0.0340, 0.2533, 0.6210]) \n",
      "Test Loss tensor([0.4083, 0.5740, 0.0274, 0.2506, 0.6218])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1227, 0.0199, 0.2681, 0.3083, 0.1945, 0.2038, 0.0193]) \n",
      "Test Loss tensor([0.1106, 0.0214, 0.2690, 0.3150, 0.1881, 0.1966, 0.0174])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0402, 0.2521, 0.0528, 0.1033, 0.4078, 0.3712, 0.0493]) \n",
      "Test Loss tensor([0.0486, 0.2395, 0.0497, 0.1108, 0.4130, 0.3650, 0.0475])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0293, 0.0295, 0.0357, 0.0222, 0.0572, 0.0565, 0.2602, 0.0030]) \n",
      "Test Loss tensor([0.0290, 0.0270, 0.0390, 0.0263, 0.0726, 0.0407, 0.2625, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5684, 0.5939, 0.1142, 0.1739, 0.4537, 0.3250])\n",
      "Valid Idx 3 | Loss tensor([0.6919, 0.2902, 0.5013, 0.5834, 0.8392])\n",
      "\n",
      "************** Batch 860 in 5.023477554321289 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1172, 0.2013, 0.1832, 0.2575]) \n",
      "Test Loss tensor([0.1224, 0.2086, 0.1799, 0.2456])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4061, 0.5656, 0.0307, 0.2361, 0.6288]) \n",
      "Test Loss tensor([0.3979, 0.5761, 0.0305, 0.2469, 0.6241])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1092, 0.0224, 0.2670, 0.3123, 0.1849, 0.1937, 0.0163]) \n",
      "Test Loss tensor([0.1135, 0.0201, 0.2681, 0.3125, 0.1923, 0.2015, 0.0180])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0464, 0.2481, 0.0492, 0.1072, 0.4084, 0.3635, 0.0434]) \n",
      "Test Loss tensor([0.0432, 0.2432, 0.0488, 0.1074, 0.4139, 0.3690, 0.0474])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0281, 0.0286, 0.0376, 0.0269, 0.0730, 0.0380, 0.2662, 0.0031]) \n",
      "Test Loss tensor([0.0296, 0.0269, 0.0364, 0.0251, 0.0639, 0.0444, 0.2608, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5644, 0.6174, 0.1160, 0.1702, 0.4612, 0.3215])\n",
      "Valid Idx 3 | Loss tensor([0.6176, 0.2333, 0.4222, 0.5530, 0.8390])\n",
      "Gradients: Input 0.5258208513259888 | Message 0.5169768333435059 | Update 0.730780839920044 | Output 0.22181209921836853\n",
      "\n",
      "************** Batch 864 in 5.041886329650879 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1206, 0.2062, 0.1736, 0.2473]) \n",
      "Test Loss tensor([0.1231, 0.2106, 0.1764, 0.2423])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4047, 0.5653, 0.0309, 0.2467, 0.6269]) \n",
      "Test Loss tensor([0.3990, 0.5758, 0.0326, 0.2473, 0.6236])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1120, 0.0199, 0.2686, 0.3138, 0.1992, 0.2076, 0.0175]) \n",
      "Test Loss tensor([0.1139, 0.0193, 0.2661, 0.3092, 0.1934, 0.2006, 0.0186])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0472, 0.2425, 0.0529, 0.1124, 0.4142, 0.3618, 0.0500]) \n",
      "Test Loss tensor([0.0422, 0.2427, 0.0497, 0.1047, 0.4089, 0.3704, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0258, 0.0290, 0.0339, 0.0244, 0.0645, 0.0457, 0.2609, 0.0028]) \n",
      "Test Loss tensor([0.0291, 0.0286, 0.0375, 0.0254, 0.0617, 0.0466, 0.2560, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5677, 0.6230, 0.1146, 0.1677, 0.4614, 0.3188])\n",
      "Valid Idx 3 | Loss tensor([0.5817, 0.2052, 0.3835, 0.5335, 0.8313])\n",
      "\n",
      "************** Batch 868 in 5.045146703720093 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1295, 0.2052, 0.1815, 0.2521]) \n",
      "Test Loss tensor([0.1214, 0.2106, 0.1774, 0.2441])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3955, 0.5753, 0.0324, 0.2387, 0.6224]) \n",
      "Test Loss tensor([0.3992, 0.5726, 0.0303, 0.2507, 0.6301])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1082, 0.0190, 0.2703, 0.3078, 0.1876, 0.1993, 0.0184]) \n",
      "Test Loss tensor([0.1082, 0.0209, 0.2695, 0.3132, 0.1890, 0.1949, 0.0180])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0421, 0.2435, 0.0460, 0.0983, 0.4022, 0.3692, 0.0430]) \n",
      "Test Loss tensor([0.0469, 0.2372, 0.0497, 0.1092, 0.4140, 0.3612, 0.0472])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0315, 0.0283, 0.0366, 0.0237, 0.0604, 0.0458, 0.2648, 0.0028]) \n",
      "Test Loss tensor([0.0285, 0.0274, 0.0374, 0.0260, 0.0694, 0.0402, 0.2583, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5756, 0.5947, 0.1136, 0.1706, 0.4523, 0.3317])\n",
      "Valid Idx 3 | Loss tensor([0.6457, 0.2356, 0.4392, 0.5703, 0.8412])\n",
      "\n",
      "************** Batch 872 in 5.1176512241363525 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1293, 0.2099, 0.1792, 0.2478]) \n",
      "Test Loss tensor([0.1240, 0.2090, 0.1747, 0.2413])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3970, 0.5663, 0.0307, 0.2474, 0.6410]) \n",
      "Test Loss tensor([0.3950, 0.5752, 0.0362, 0.2481, 0.6219])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1076, 0.0204, 0.2674, 0.3012, 0.1912, 0.1995, 0.0179]) \n",
      "Test Loss tensor([0.1138, 0.0191, 0.2652, 0.3096, 0.1939, 0.1969, 0.0201])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0468, 0.2314, 0.0371, 0.1086, 0.4043, 0.3598, 0.0384]) \n",
      "Test Loss tensor([0.0432, 0.2415, 0.0488, 0.1031, 0.4026, 0.3666, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0245, 0.0305, 0.0331, 0.0281, 0.0699, 0.0421, 0.2623, 0.0037]) \n",
      "Test Loss tensor([0.0297, 0.0295, 0.0370, 0.0253, 0.0612, 0.0465, 0.2559, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5663, 0.6160, 0.1192, 0.1668, 0.4547, 0.3295])\n",
      "Valid Idx 3 | Loss tensor([0.5472, 0.1771, 0.3513, 0.5262, 0.8327])\n",
      "\n",
      "************** Batch 876 in 4.974053621292114 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0905, 0.1607, 0.1346, 0.1790]) \n",
      "Test Loss tensor([0.1213, 0.2107, 0.1711, 0.2426])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3061, 0.4407, 0.0266, 0.1920, 0.4608]) \n",
      "Test Loss tensor([0.4005, 0.5739, 0.0347, 0.2465, 0.6242])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0864, 0.0140, 0.2023, 0.2282, 0.1539, 0.1470, 0.0142]) \n",
      "Test Loss tensor([0.1110, 0.0195, 0.2656, 0.3030, 0.1945, 0.1935, 0.0185])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0325, 0.1784, 0.0377, 0.0775, 0.2995, 0.2784, 0.0355]) \n",
      "Test Loss tensor([0.0458, 0.2327, 0.0485, 0.1071, 0.4086, 0.3575, 0.0471])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0194, 0.0214, 0.0272, 0.0190, 0.0463, 0.0337, 0.1896, 0.0022]) \n",
      "Test Loss tensor([0.0290, 0.0293, 0.0382, 0.0259, 0.0644, 0.0439, 0.2560, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5744, 0.5992, 0.1188, 0.1726, 0.4524, 0.3361])\n",
      "Valid Idx 3 | Loss tensor([0.5701, 0.1802, 0.3652, 0.5378, 0.8319])\n",
      "\n",
      "************** Batch 0 in 4.995099067687988 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1205, 0.2058, 0.1732, 0.2502]) \n",
      "Test Loss tensor([0.1196, 0.2081, 0.1736, 0.2409])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3911, 0.5717, 0.0328, 0.2481, 0.6351]) \n",
      "Test Loss tensor([0.4011, 0.5716, 0.0296, 0.2490, 0.6271])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1159, 0.0192, 0.2770, 0.3085, 0.1930, 0.1922, 0.0198]) \n",
      "Test Loss tensor([0.1098, 0.0209, 0.2715, 0.3132, 0.1917, 0.1948, 0.0176])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0448, 0.2532, 0.0456, 0.1075, 0.3982, 0.3624, 0.0475]) \n",
      "Test Loss tensor([0.0497, 0.2364, 0.0488, 0.1120, 0.4114, 0.3541, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0306, 0.0273, 0.0426, 0.0294, 0.0671, 0.0451, 0.2524, 0.0031]) \n",
      "Test Loss tensor([0.0285, 0.0269, 0.0389, 0.0255, 0.0761, 0.0368, 0.2584, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5727, 0.5703, 0.1137, 0.1804, 0.4443, 0.3460])\n",
      "Valid Idx 3 | Loss tensor([0.6579, 0.2247, 0.4400, 0.5820, 0.8380])\n",
      "Gradients: Input 0.19955292344093323 | Message 0.15320855379104614 | Update 0.21065974235534668 | Output 0.04706232249736786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 4 in 4.937122821807861 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1187, 0.1983, 0.1795, 0.2473]) \n",
      "Test Loss tensor([0.1223, 0.2135, 0.1686, 0.2386])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4057, 0.5791, 0.0305, 0.2515, 0.6191]) \n",
      "Test Loss tensor([0.3985, 0.5722, 0.0415, 0.2449, 0.6254])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1067, 0.0209, 0.2615, 0.2999, 0.1914, 0.1953, 0.0207]) \n",
      "Test Loss tensor([0.1154, 0.0180, 0.2657, 0.3023, 0.2004, 0.1943, 0.0207])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0507, 0.2429, 0.0495, 0.1163, 0.4101, 0.3503, 0.0567]) \n",
      "Test Loss tensor([0.0417, 0.2477, 0.0487, 0.1009, 0.3982, 0.3631, 0.0497])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0279, 0.0393, 0.0249, 0.0758, 0.0354, 0.2547, 0.0026]) \n",
      "Test Loss tensor([0.0303, 0.0281, 0.0365, 0.0256, 0.0558, 0.0514, 0.2568, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5682, 0.6263, 0.1212, 0.1661, 0.4567, 0.3329])\n",
      "Valid Idx 3 | Loss tensor([0.4654, 0.1328, 0.2799, 0.4879, 0.8275])\n",
      "\n",
      "************** Batch 8 in 4.903590202331543 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1126, 0.2111, 0.1608, 0.2362]) \n",
      "Test Loss tensor([0.1188, 0.2085, 0.1711, 0.2367])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3911, 0.5647, 0.0423, 0.2383, 0.6223]) \n",
      "Test Loss tensor([0.3982, 0.5722, 0.0304, 0.2468, 0.6240])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1078, 0.0192, 0.2638, 0.2952, 0.1968, 0.1896, 0.0202]) \n",
      "Test Loss tensor([0.1080, 0.0200, 0.2686, 0.3093, 0.1933, 0.1891, 0.0181])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0399, 0.2461, 0.0500, 0.1004, 0.4058, 0.3738, 0.0431]) \n",
      "Test Loss tensor([0.0455, 0.2385, 0.0494, 0.1070, 0.4056, 0.3585, 0.0473])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0289, 0.0296, 0.0356, 0.0237, 0.0554, 0.0513, 0.2528, 0.0035]) \n",
      "Test Loss tensor([0.0287, 0.0272, 0.0376, 0.0248, 0.0694, 0.0387, 0.2481, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5734, 0.5838, 0.1149, 0.1777, 0.4483, 0.3529])\n",
      "Valid Idx 3 | Loss tensor([0.6120, 0.2015, 0.3849, 0.5520, 0.8403])\n",
      "\n",
      "************** Batch 12 in 4.9385786056518555 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1248, 0.2119, 0.1755, 0.2353]) \n",
      "Test Loss tensor([0.1196, 0.2108, 0.1719, 0.2394])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4049, 0.5626, 0.0290, 0.2496, 0.6174]) \n",
      "Test Loss tensor([0.3977, 0.5738, 0.0301, 0.2481, 0.6207])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1015, 0.0228, 0.2662, 0.3005, 0.1955, 0.1998, 0.0189]) \n",
      "Test Loss tensor([0.1074, 0.0203, 0.2697, 0.3121, 0.1914, 0.1883, 0.0185])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0462, 0.2227, 0.0432, 0.0976, 0.4066, 0.3496, 0.0432]) \n",
      "Test Loss tensor([0.0453, 0.2356, 0.0482, 0.1068, 0.4024, 0.3543, 0.0466])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0317, 0.0281, 0.0378, 0.0236, 0.0685, 0.0385, 0.2342, 0.0032]) \n",
      "Test Loss tensor([0.0287, 0.0261, 0.0368, 0.0251, 0.0671, 0.0395, 0.2463, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5709, 0.5888, 0.1135, 0.1788, 0.4461, 0.3558])\n",
      "Valid Idx 3 | Loss tensor([0.5993, 0.1970, 0.3708, 0.5405, 0.8412])\n",
      "\n",
      "************** Batch 16 in 4.96224045753479 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1225, 0.2085, 0.1755, 0.2448]) \n",
      "Test Loss tensor([0.1214, 0.2132, 0.1683, 0.2358])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.4094, 0.5903, 0.0336, 0.2471, 0.6138]) \n",
      "Test Loss tensor([0.3945, 0.5704, 0.0397, 0.2448, 0.6248])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1035, 0.0198, 0.2585, 0.3043, 0.2081, 0.1974, 0.0171]) \n",
      "Test Loss tensor([0.1131, 0.0180, 0.2638, 0.3011, 0.1998, 0.1926, 0.0203])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0468, 0.2406, 0.0509, 0.1140, 0.4066, 0.3611, 0.0490]) \n",
      "Test Loss tensor([0.0398, 0.2419, 0.0481, 0.1014, 0.3997, 0.3676, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0305, 0.0239, 0.0401, 0.0266, 0.0658, 0.0402, 0.2462, 0.0029]) \n",
      "Test Loss tensor([0.0307, 0.0279, 0.0361, 0.0251, 0.0539, 0.0501, 0.2497, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5635, 0.6296, 0.1208, 0.1641, 0.4541, 0.3369])\n",
      "Valid Idx 3 | Loss tensor([0.4380, 0.1259, 0.2600, 0.4768, 0.8253])\n",
      "\n",
      "************** Batch 20 in 5.030944108963013 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1239, 0.2079, 0.1627, 0.2343]) \n",
      "Test Loss tensor([0.1194, 0.2081, 0.1715, 0.2378])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3896, 0.5744, 0.0376, 0.2453, 0.6193]) \n",
      "Test Loss tensor([0.3971, 0.5748, 0.0295, 0.2427, 0.6188])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1091, 0.0185, 0.2518, 0.2915, 0.1984, 0.1923, 0.0234]) \n",
      "Test Loss tensor([0.1066, 0.0197, 0.2686, 0.3075, 0.1920, 0.1861, 0.0174])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0399, 0.2494, 0.0484, 0.1038, 0.3934, 0.3707, 0.0453]) \n",
      "Test Loss tensor([0.0460, 0.2355, 0.0500, 0.1087, 0.4008, 0.3547, 0.0455])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0265, 0.0251, 0.0381, 0.0249, 0.0518, 0.0490, 0.2449, 0.0032]) \n",
      "Test Loss tensor([0.0276, 0.0268, 0.0374, 0.0248, 0.0672, 0.0396, 0.2427, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5786, 0.5843, 0.1125, 0.1794, 0.4421, 0.3626])\n",
      "Valid Idx 3 | Loss tensor([0.5707, 0.1807, 0.3470, 0.5422, 0.8411])\n",
      "Gradients: Input 0.7090027332305908 | Message 0.7158712148666382 | Update 0.9825534820556641 | Output 0.2508294880390167\n",
      "\n",
      "************** Batch 24 in 4.988500118255615 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1218, 0.2106, 0.1773, 0.2306]) \n",
      "Test Loss tensor([0.1210, 0.2104, 0.1699, 0.2378])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3885, 0.5815, 0.0300, 0.2391, 0.6133]) \n",
      "Test Loss tensor([0.3915, 0.5726, 0.0301, 0.2439, 0.6223])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1071, 0.0201, 0.2706, 0.3049, 0.2009, 0.1872, 0.0180]) \n",
      "Test Loss tensor([0.1064, 0.0204, 0.2678, 0.3053, 0.1904, 0.1814, 0.0179])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0388, 0.2395, 0.0503, 0.1150, 0.4071, 0.3611, 0.0491]) \n",
      "Test Loss tensor([0.0456, 0.2365, 0.0494, 0.1070, 0.4042, 0.3523, 0.0469])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0285, 0.0284, 0.0364, 0.0255, 0.0654, 0.0362, 0.2505, 0.0030]) \n",
      "Test Loss tensor([0.0279, 0.0266, 0.0374, 0.0255, 0.0676, 0.0389, 0.2494, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5800, 0.5804, 0.1126, 0.1791, 0.4473, 0.3632])\n",
      "Valid Idx 3 | Loss tensor([0.5621, 0.1767, 0.3379, 0.5403, 0.8392])\n",
      "\n",
      "************** Batch 28 in 4.922780752182007 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1183, 0.2037, 0.1742, 0.2527]) \n",
      "Test Loss tensor([0.1240, 0.2130, 0.1659, 0.2318])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3933, 0.5723, 0.0309, 0.2391, 0.6134]) \n",
      "Test Loss tensor([0.3853, 0.5760, 0.0430, 0.2443, 0.6190])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1077, 0.0198, 0.2668, 0.3021, 0.1956, 0.1799, 0.0183]) \n",
      "Test Loss tensor([0.1163, 0.0179, 0.2679, 0.2970, 0.2028, 0.1897, 0.0208])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0418, 0.2287, 0.0477, 0.1004, 0.3982, 0.3694, 0.0457]) \n",
      "Test Loss tensor([0.0397, 0.2431, 0.0475, 0.0969, 0.3953, 0.3644, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0261, 0.0256, 0.0370, 0.0260, 0.0698, 0.0373, 0.2441, 0.0029]) \n",
      "Test Loss tensor([0.0307, 0.0292, 0.0362, 0.0248, 0.0526, 0.0518, 0.2441, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5654, 0.6267, 0.1223, 0.1650, 0.4523, 0.3468])\n",
      "Valid Idx 3 | Loss tensor([0.3669, 0.1071, 0.2261, 0.4646, 0.8235])\n",
      "\n",
      "************** Batch 32 in 5.013702630996704 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1252, 0.2180, 0.1683, 0.2409]) \n",
      "Test Loss tensor([0.1189, 0.2080, 0.1670, 0.2358])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3909, 0.5821, 0.0442, 0.2358, 0.6061]) \n",
      "Test Loss tensor([0.3841, 0.5729, 0.0324, 0.2423, 0.6202])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1122, 0.0181, 0.2565, 0.2929, 0.1993, 0.1850, 0.0203]) \n",
      "Test Loss tensor([0.1082, 0.0203, 0.2696, 0.3015, 0.1949, 0.1778, 0.0178])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0354, 0.2351, 0.0497, 0.1057, 0.3872, 0.3657, 0.0501]) \n",
      "Test Loss tensor([0.0470, 0.2392, 0.0503, 0.1065, 0.3974, 0.3509, 0.0463])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0312, 0.0297, 0.0369, 0.0247, 0.0551, 0.0520, 0.2474, 0.0029]) \n",
      "Test Loss tensor([0.0282, 0.0262, 0.0376, 0.0255, 0.0686, 0.0386, 0.2436, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5771, 0.5817, 0.1116, 0.1834, 0.4388, 0.3693])\n",
      "Valid Idx 3 | Loss tensor([0.5265, 0.1625, 0.3124, 0.5322, 0.8405])\n",
      "\n",
      "************** Batch 36 in 4.954645156860352 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1186, 0.1979, 0.1693, 0.2365]) \n",
      "Test Loss tensor([0.1196, 0.2095, 0.1687, 0.2390])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3948, 0.5693, 0.0308, 0.2364, 0.6213]) \n",
      "Test Loss tensor([0.3864, 0.5747, 0.0313, 0.2466, 0.6173])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1034, 0.0209, 0.2683, 0.2944, 0.1956, 0.1884, 0.0183]) \n",
      "Test Loss tensor([0.1077, 0.0216, 0.2683, 0.3009, 0.1921, 0.1808, 0.0177])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0454, 0.2378, 0.0489, 0.1015, 0.3985, 0.3309, 0.0420]) \n",
      "Test Loss tensor([0.0497, 0.2358, 0.0488, 0.1098, 0.3989, 0.3421, 0.0473])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0292, 0.0280, 0.0349, 0.0275, 0.0708, 0.0394, 0.2340, 0.0036]) \n",
      "Test Loss tensor([0.0280, 0.0262, 0.0368, 0.0245, 0.0735, 0.0359, 0.2415, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5814, 0.5696, 0.1116, 0.1876, 0.4388, 0.3813])\n",
      "Valid Idx 3 | Loss tensor([0.5529, 0.1746, 0.3311, 0.5439, 0.8416])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 40 in 4.950738906860352 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1220, 0.2014, 0.1716, 0.2447]) \n",
      "Test Loss tensor([0.1247, 0.2125, 0.1630, 0.2307])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3873, 0.5695, 0.0323, 0.2564, 0.6192]) \n",
      "Test Loss tensor([0.3819, 0.5724, 0.0401, 0.2372, 0.6210])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1116, 0.0220, 0.2757, 0.3019, 0.1972, 0.1789, 0.0184]) \n",
      "Test Loss tensor([0.1144, 0.0188, 0.2667, 0.2930, 0.2017, 0.1791, 0.0203])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0450, 0.2260, 0.0507, 0.1039, 0.3981, 0.3373, 0.0498]) \n",
      "Test Loss tensor([0.0425, 0.2397, 0.0476, 0.1025, 0.3947, 0.3586, 0.0481])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0284, 0.0257, 0.0344, 0.0223, 0.0769, 0.0379, 0.2408, 0.0035]) \n",
      "Test Loss tensor([0.0296, 0.0281, 0.0356, 0.0248, 0.0563, 0.0458, 0.2425, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5713, 0.6139, 0.1198, 0.1751, 0.4483, 0.3653])\n",
      "Valid Idx 3 | Loss tensor([0.3862, 0.1156, 0.2346, 0.4783, 0.8307])\n",
      "Gradients: Input 0.5092521905899048 | Message 0.5431710481643677 | Update 0.7303245067596436 | Output 0.21692226827144623\n",
      "\n",
      "************** Batch 44 in 4.92942476272583 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1183, 0.2115, 0.1568, 0.2172]) \n",
      "Test Loss tensor([0.1185, 0.2078, 0.1685, 0.2329])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3866, 0.5707, 0.0372, 0.2405, 0.6195]) \n",
      "Test Loss tensor([0.3803, 0.5770, 0.0341, 0.2407, 0.6154])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1064, 0.0185, 0.2566, 0.2880, 0.2028, 0.1819, 0.0227]) \n",
      "Test Loss tensor([0.1085, 0.0206, 0.2680, 0.3022, 0.1954, 0.1753, 0.0189])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0437, 0.2436, 0.0502, 0.1058, 0.4062, 0.3675, 0.0385]) \n",
      "Test Loss tensor([0.0467, 0.2347, 0.0492, 0.1050, 0.3943, 0.3507, 0.0477])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0266, 0.0306, 0.0359, 0.0263, 0.0573, 0.0490, 0.2326, 0.0035]) \n",
      "Test Loss tensor([0.0288, 0.0271, 0.0371, 0.0244, 0.0670, 0.0371, 0.2404, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5736, 0.5880, 0.1111, 0.1837, 0.4377, 0.3792])\n",
      "Valid Idx 3 | Loss tensor([0.4902, 0.1508, 0.2895, 0.5175, 0.8400])\n",
      "\n",
      "************** Batch 48 in 4.967194080352783 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1125, 0.2114, 0.1643, 0.2414]) \n",
      "Test Loss tensor([0.1213, 0.2091, 0.1664, 0.2338])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3979, 0.5703, 0.0331, 0.2355, 0.6225]) \n",
      "Test Loss tensor([0.3866, 0.5731, 0.0322, 0.2355, 0.6215])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1104, 0.0195, 0.2704, 0.3063, 0.1988, 0.1757, 0.0177]) \n",
      "Test Loss tensor([0.1082, 0.0205, 0.2728, 0.3015, 0.1980, 0.1782, 0.0180])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0433, 0.2230, 0.0491, 0.1017, 0.3975, 0.3457, 0.0523]) \n",
      "Test Loss tensor([0.0479, 0.2349, 0.0494, 0.1061, 0.3925, 0.3470, 0.0470])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0332, 0.0273, 0.0379, 0.0229, 0.0663, 0.0376, 0.2401, 0.0031]) \n",
      "Test Loss tensor([0.0284, 0.0264, 0.0370, 0.0254, 0.0669, 0.0360, 0.2369, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5792, 0.5803, 0.1080, 0.1846, 0.4383, 0.3848])\n",
      "Valid Idx 3 | Loss tensor([0.5026, 0.1537, 0.2935, 0.5225, 0.8426])\n",
      "\n",
      "************** Batch 52 in 5.044301748275757 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1208, 0.2009, 0.1723, 0.2377]) \n",
      "Test Loss tensor([0.1190, 0.2086, 0.1624, 0.2294])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3788, 0.5647, 0.0281, 0.2329, 0.6325]) \n",
      "Test Loss tensor([0.3773, 0.5698, 0.0400, 0.2381, 0.6232])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0969, 0.0195, 0.2843, 0.3071, 0.1929, 0.1735, 0.0182]) \n",
      "Test Loss tensor([0.1132, 0.0183, 0.2634, 0.2917, 0.2002, 0.1744, 0.0204])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0450, 0.2397, 0.0458, 0.1011, 0.3876, 0.3385, 0.0465]) \n",
      "Test Loss tensor([0.0423, 0.2412, 0.0478, 0.1032, 0.3943, 0.3551, 0.0460])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0273, 0.0423, 0.0229, 0.0662, 0.0370, 0.2305, 0.0030]) \n",
      "Test Loss tensor([0.0300, 0.0280, 0.0356, 0.0253, 0.0555, 0.0433, 0.2407, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5716, 0.6158, 0.1170, 0.1782, 0.4469, 0.3763])\n",
      "Valid Idx 3 | Loss tensor([0.3703, 0.1102, 0.2282, 0.4776, 0.8333])\n",
      "\n",
      "************** Batch 56 in 5.149134159088135 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1306, 0.2066, 0.1739, 0.2296]) \n",
      "Test Loss tensor([0.1169, 0.2080, 0.1671, 0.2301])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3895, 0.5787, 0.0382, 0.2353, 0.6152]) \n",
      "Test Loss tensor([0.3836, 0.5789, 0.0342, 0.2376, 0.6155])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1111, 0.0198, 0.2686, 0.2805, 0.1957, 0.1771, 0.0199]) \n",
      "Test Loss tensor([0.1107, 0.0198, 0.2669, 0.2970, 0.1970, 0.1729, 0.0192])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0425, 0.2297, 0.0484, 0.0996, 0.3881, 0.3530, 0.0472]) \n",
      "Test Loss tensor([0.0448, 0.2335, 0.0467, 0.1042, 0.3924, 0.3441, 0.0468])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0325, 0.0282, 0.0401, 0.0271, 0.0538, 0.0423, 0.2369, 0.0035]) \n",
      "Test Loss tensor([0.0278, 0.0259, 0.0357, 0.0249, 0.0636, 0.0368, 0.2359, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5779, 0.5847, 0.1122, 0.1839, 0.4386, 0.3831])\n",
      "Valid Idx 3 | Loss tensor([0.4525, 0.1308, 0.2655, 0.5164, 0.8400])\n",
      "\n",
      "************** Batch 60 in 5.009220600128174 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1260, 0.2101, 0.1578, 0.2397]) \n",
      "Test Loss tensor([0.1166, 0.2109, 0.1690, 0.2314])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3858, 0.5895, 0.0343, 0.2411, 0.5957]) \n",
      "Test Loss tensor([0.3852, 0.5809, 0.0311, 0.2391, 0.6120])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1021, 0.0191, 0.2717, 0.3043, 0.1952, 0.1748, 0.0180]) \n",
      "Test Loss tensor([0.1075, 0.0197, 0.2673, 0.2967, 0.1960, 0.1758, 0.0192])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0478, 0.2322, 0.0438, 0.1006, 0.3825, 0.3509, 0.0422]) \n",
      "Test Loss tensor([0.0462, 0.2340, 0.0496, 0.1069, 0.3913, 0.3445, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0281, 0.0281, 0.0363, 0.0237, 0.0652, 0.0361, 0.2230, 0.0029]) \n",
      "Test Loss tensor([0.0287, 0.0270, 0.0367, 0.0242, 0.0655, 0.0346, 0.2320, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5791, 0.5688, 0.1129, 0.1883, 0.4349, 0.3841])\n",
      "Valid Idx 3 | Loss tensor([0.4688, 0.1330, 0.2777, 0.5299, 0.8443])\n",
      "Gradients: Input 0.17281533777713776 | Message 0.16129463911056519 | Update 0.21477746963500977 | Output 0.05651596933603287\n",
      "\n",
      "************** Batch 64 in 4.933993816375732 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1210, 0.2082, 0.1655, 0.2418]) \n",
      "Test Loss tensor([0.1193, 0.2116, 0.1615, 0.2241])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3899, 0.5760, 0.0324, 0.2280, 0.6180]) \n",
      "Test Loss tensor([0.3825, 0.5802, 0.0409, 0.2346, 0.6163])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1059, 0.0191, 0.2606, 0.2918, 0.1916, 0.1723, 0.0191]) \n",
      "Test Loss tensor([0.1151, 0.0171, 0.2650, 0.2927, 0.2042, 0.1768, 0.0210])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0485, 0.2488, 0.0479, 0.1062, 0.3916, 0.3460, 0.0483]) \n",
      "Test Loss tensor([0.0388, 0.2364, 0.0462, 0.0994, 0.3894, 0.3504, 0.0465])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0217, 0.0305, 0.0348, 0.0243, 0.0653, 0.0347, 0.2297, 0.0033]) \n",
      "Test Loss tensor([0.0299, 0.0283, 0.0361, 0.0242, 0.0522, 0.0430, 0.2412, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5748, 0.6087, 0.1159, 0.1757, 0.4406, 0.3644])\n",
      "Valid Idx 3 | Loss tensor([0.3132, 0.0910, 0.2081, 0.4729, 0.8304])\n",
      "\n",
      "************** Batch 68 in 4.983290672302246 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1134, 0.2092, 0.1631, 0.2248]) \n",
      "Test Loss tensor([0.1186, 0.2100, 0.1680, 0.2336])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3870, 0.5824, 0.0430, 0.2371, 0.6141]) \n",
      "Test Loss tensor([0.3819, 0.5825, 0.0334, 0.2354, 0.6110])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1122, 0.0173, 0.2645, 0.2768, 0.2088, 0.1766, 0.0197]) \n",
      "Test Loss tensor([0.1078, 0.0190, 0.2697, 0.2963, 0.1948, 0.1731, 0.0186])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0381, 0.2370, 0.0433, 0.0976, 0.3853, 0.3530, 0.0454]) \n",
      "Test Loss tensor([0.0455, 0.2362, 0.0482, 0.1057, 0.3908, 0.3419, 0.0457])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0288, 0.0273, 0.0373, 0.0227, 0.0513, 0.0449, 0.2360, 0.0029]) \n",
      "Test Loss tensor([0.0283, 0.0258, 0.0364, 0.0251, 0.0633, 0.0334, 0.2391, 0.0029])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5811, 0.5648, 0.1130, 0.1877, 0.4344, 0.3857])\n",
      "Valid Idx 3 | Loss tensor([0.4398, 0.1203, 0.2599, 0.5304, 0.8388])\n",
      "\n",
      "************** Batch 72 in 5.0376341342926025 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1182, 0.2121, 0.1694, 0.2334]) \n",
      "Test Loss tensor([0.1195, 0.2082, 0.1678, 0.2332])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3739, 0.5745, 0.0347, 0.2357, 0.6210]) \n",
      "Test Loss tensor([0.3789, 0.5741, 0.0314, 0.2289, 0.6243])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1096, 0.0185, 0.2631, 0.3001, 0.2045, 0.1800, 0.0179]) \n",
      "Test Loss tensor([0.1071, 0.0187, 0.2690, 0.2953, 0.1957, 0.1723, 0.0192])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0456, 0.2394, 0.0460, 0.1068, 0.3951, 0.3435, 0.0466]) \n",
      "Test Loss tensor([0.0432, 0.2358, 0.0483, 0.1053, 0.3865, 0.3405, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0269, 0.0244, 0.0383, 0.0242, 0.0622, 0.0332, 0.2378, 0.0033]) \n",
      "Test Loss tensor([0.0288, 0.0262, 0.0351, 0.0248, 0.0597, 0.0349, 0.2338, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5779, 0.5725, 0.1130, 0.1889, 0.4313, 0.3798])\n",
      "Valid Idx 3 | Loss tensor([0.4097, 0.1105, 0.2504, 0.5164, 0.8400])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 76 in 5.034010648727417 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1160, 0.2081, 0.1637, 0.2196]) \n",
      "Test Loss tensor([0.1211, 0.2086, 0.1604, 0.2259])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3818, 0.5675, 0.0336, 0.2395, 0.6205]) \n",
      "Test Loss tensor([0.3810, 0.5780, 0.0411, 0.2297, 0.6215])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1075, 0.0189, 0.2775, 0.3085, 0.1942, 0.1718, 0.0169]) \n",
      "Test Loss tensor([0.1150, 0.0161, 0.2632, 0.2910, 0.2059, 0.1753, 0.0220])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0438, 0.2320, 0.0464, 0.1039, 0.3854, 0.3334, 0.0416]) \n",
      "Test Loss tensor([0.0377, 0.2427, 0.0467, 0.0983, 0.3866, 0.3521, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0296, 0.0284, 0.0320, 0.0207, 0.0609, 0.0339, 0.2206, 0.0026]) \n",
      "Test Loss tensor([0.0302, 0.0273, 0.0349, 0.0238, 0.0488, 0.0456, 0.2345, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5741, 0.6166, 0.1160, 0.1729, 0.4414, 0.3639])\n",
      "Valid Idx 3 | Loss tensor([0.2712, 0.0833, 0.1933, 0.4600, 0.8238])\n",
      "\n",
      "************** Batch 80 in 5.10309910774231 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1216, 0.2082, 0.1691, 0.2319]) \n",
      "Test Loss tensor([0.1164, 0.2066, 0.1684, 0.2288])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3739, 0.5827, 0.0381, 0.2369, 0.6109]) \n",
      "Test Loss tensor([0.3800, 0.5776, 0.0309, 0.2320, 0.6168])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1150, 0.0176, 0.2689, 0.2852, 0.2116, 0.1728, 0.0224]) \n",
      "Test Loss tensor([0.1059, 0.0187, 0.2669, 0.2950, 0.1961, 0.1687, 0.0175])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0362, 0.2455, 0.0409, 0.0980, 0.3956, 0.3463, 0.0473]) \n",
      "Test Loss tensor([0.0427, 0.2341, 0.0470, 0.1020, 0.3879, 0.3383, 0.0471])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0277, 0.0371, 0.0234, 0.0482, 0.0431, 0.2396, 0.0035]) \n",
      "Test Loss tensor([0.0285, 0.0273, 0.0361, 0.0243, 0.0604, 0.0337, 0.2309, 0.0029])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5796, 0.5709, 0.1107, 0.1894, 0.4320, 0.3900])\n",
      "Valid Idx 3 | Loss tensor([0.4241, 0.1182, 0.2530, 0.5151, 0.8431])\n",
      "Gradients: Input 0.8569642305374146 | Message 0.8674663305282593 | Update 1.1600509881973267 | Output 0.3206648826599121\n",
      "\n",
      "************** Batch 84 in 5.089149713516235 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1172, 0.2139, 0.1636, 0.2389]) \n",
      "Test Loss tensor([0.1154, 0.2074, 0.1653, 0.2255])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3768, 0.5660, 0.0333, 0.2305, 0.6242]) \n",
      "Test Loss tensor([0.3782, 0.5766, 0.0334, 0.2302, 0.6162])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1126, 0.0184, 0.2689, 0.2934, 0.2042, 0.1686, 0.0199]) \n",
      "Test Loss tensor([0.1100, 0.0184, 0.2679, 0.2917, 0.1970, 0.1692, 0.0190])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0475, 0.2307, 0.0474, 0.1073, 0.3867, 0.3379, 0.0509]) \n",
      "Test Loss tensor([0.0412, 0.2364, 0.0477, 0.1016, 0.3816, 0.3395, 0.0473])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0263, 0.0250, 0.0403, 0.0239, 0.0599, 0.0346, 0.2335, 0.0033]) \n",
      "Test Loss tensor([0.0282, 0.0273, 0.0363, 0.0236, 0.0599, 0.0355, 0.2285, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5794, 0.5780, 0.1110, 0.1853, 0.4339, 0.3912])\n",
      "Valid Idx 3 | Loss tensor([0.4026, 0.1149, 0.2448, 0.5048, 0.8417])\n",
      "\n",
      "************** Batch 88 in 5.103711366653442 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1190, 0.2020, 0.1618, 0.2317]) \n",
      "Test Loss tensor([0.1193, 0.2094, 0.1587, 0.2201])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3798, 0.5625, 0.0321, 0.2265, 0.6385]) \n",
      "Test Loss tensor([0.3742, 0.5735, 0.0397, 0.2320, 0.6188])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1032, 0.0176, 0.2709, 0.3026, 0.1999, 0.1722, 0.0187]) \n",
      "Test Loss tensor([0.1139, 0.0169, 0.2716, 0.2875, 0.2043, 0.1698, 0.0219])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0395, 0.2443, 0.0441, 0.0973, 0.3806, 0.3480, 0.0465]) \n",
      "Test Loss tensor([0.0385, 0.2347, 0.0467, 0.0981, 0.3860, 0.3498, 0.0448])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0273, 0.0263, 0.0434, 0.0255, 0.0602, 0.0362, 0.2225, 0.0036]) \n",
      "Test Loss tensor([0.0309, 0.0277, 0.0356, 0.0248, 0.0514, 0.0426, 0.2329, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5740, 0.6114, 0.1144, 0.1775, 0.4384, 0.3796])\n",
      "Valid Idx 3 | Loss tensor([0.2983, 0.0905, 0.2000, 0.4686, 0.8329])\n",
      "\n",
      "************** Batch 92 in 5.0474467277526855 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1216, 0.2118, 0.1586, 0.2097]) \n",
      "Test Loss tensor([0.1160, 0.2096, 0.1643, 0.2246])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3791, 0.5699, 0.0430, 0.2296, 0.6114]) \n",
      "Test Loss tensor([0.3734, 0.5745, 0.0336, 0.2311, 0.6184])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1085, 0.0165, 0.2625, 0.2857, 0.1994, 0.1620, 0.0239]) \n",
      "Test Loss tensor([0.1090, 0.0186, 0.2709, 0.2920, 0.1962, 0.1641, 0.0198])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0395, 0.2435, 0.0480, 0.0942, 0.3662, 0.3474, 0.0491]) \n",
      "Test Loss tensor([0.0448, 0.2374, 0.0480, 0.1025, 0.3836, 0.3364, 0.0460])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0323, 0.0229, 0.0340, 0.0241, 0.0506, 0.0434, 0.2273, 0.0032]) \n",
      "Test Loss tensor([0.0290, 0.0270, 0.0369, 0.0248, 0.0606, 0.0348, 0.2270, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5803, 0.5790, 0.1129, 0.1898, 0.4302, 0.3992])\n",
      "Valid Idx 3 | Loss tensor([0.4059, 0.1157, 0.2418, 0.4995, 0.8419])\n",
      "\n",
      "************** Batch 96 in 5.089653015136719 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1135, 0.2077, 0.1635, 0.2224]) \n",
      "Test Loss tensor([0.1163, 0.2079, 0.1609, 0.2222])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3738, 0.5712, 0.0338, 0.2306, 0.6285]) \n",
      "Test Loss tensor([0.3712, 0.5792, 0.0347, 0.2321, 0.6129])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1038, 0.0185, 0.2761, 0.2994, 0.2045, 0.1668, 0.0211]) \n",
      "Test Loss tensor([0.1097, 0.0190, 0.2717, 0.2902, 0.1969, 0.1626, 0.0200])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0434, 0.2432, 0.0435, 0.1075, 0.3886, 0.3476, 0.0496]) \n",
      "Test Loss tensor([0.0436, 0.2356, 0.0490, 0.1027, 0.3804, 0.3386, 0.0471])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0275, 0.0265, 0.0347, 0.0220, 0.0641, 0.0360, 0.2225, 0.0030]) \n",
      "Test Loss tensor([0.0283, 0.0261, 0.0365, 0.0236, 0.0614, 0.0348, 0.2230, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5777, 0.5802, 0.1118, 0.1871, 0.4345, 0.3979])\n",
      "Valid Idx 3 | Loss tensor([0.4034, 0.1125, 0.2409, 0.4996, 0.8424])\n",
      "\n",
      "************** Batch 100 in 5.1879518032073975 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1242, 0.2101, 0.1549, 0.2339]) \n",
      "Test Loss tensor([0.1185, 0.2090, 0.1590, 0.2193])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3913, 0.5816, 0.0340, 0.2402, 0.6181]) \n",
      "Test Loss tensor([0.3700, 0.5709, 0.0419, 0.2301, 0.6215])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1050, 0.0199, 0.2761, 0.2938, 0.1997, 0.1660, 0.0213]) \n",
      "Test Loss tensor([0.1133, 0.0174, 0.2657, 0.2808, 0.2030, 0.1631, 0.0230])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0405, 0.2357, 0.0441, 0.1053, 0.3955, 0.3488, 0.0462]) \n",
      "Test Loss tensor([0.0406, 0.2340, 0.0483, 0.0983, 0.3796, 0.3420, 0.0494])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0296, 0.0227, 0.0384, 0.0229, 0.0586, 0.0353, 0.2286, 0.0034]) \n",
      "Test Loss tensor([0.0293, 0.0278, 0.0346, 0.0245, 0.0527, 0.0402, 0.2258, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5766, 0.6099, 0.1144, 0.1802, 0.4333, 0.3929])\n",
      "Valid Idx 3 | Loss tensor([0.3108, 0.0933, 0.2029, 0.4641, 0.8330])\n",
      "Gradients: Input 0.2929568290710449 | Message 0.3185362219810486 | Update 0.41922640800476074 | Output 0.1151096448302269\n",
      "\n",
      "************** Batch 104 in 5.090633153915405 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1194, 0.2192, 0.1560, 0.2236]) \n",
      "Test Loss tensor([0.1176, 0.2068, 0.1618, 0.2270])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3618, 0.5745, 0.0385, 0.2231, 0.6154]) \n",
      "Test Loss tensor([0.3691, 0.5773, 0.0339, 0.2341, 0.6131])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1145, 0.0177, 0.2748, 0.2888, 0.2082, 0.1710, 0.0229]) \n",
      "Test Loss tensor([0.1089, 0.0187, 0.2710, 0.2887, 0.1955, 0.1596, 0.0197])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0389, 0.2437, 0.0440, 0.1051, 0.3919, 0.3329, 0.0450]) \n",
      "Test Loss tensor([0.0444, 0.2296, 0.0494, 0.1026, 0.3756, 0.3329, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0309, 0.0220, 0.0339, 0.0258, 0.0542, 0.0400, 0.2290, 0.0029]) \n",
      "Test Loss tensor([0.0289, 0.0263, 0.0365, 0.0250, 0.0617, 0.0338, 0.2228, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5799, 0.5808, 0.1100, 0.1907, 0.4289, 0.4087])\n",
      "Valid Idx 3 | Loss tensor([0.4092, 0.1139, 0.2371, 0.4951, 0.8435])\n",
      "\n",
      "************** Batch 108 in 5.130597829818726 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1151, 0.2013, 0.1589, 0.2125]) \n",
      "Test Loss tensor([0.1173, 0.2072, 0.1611, 0.2249])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3605, 0.5672, 0.0306, 0.2250, 0.6246]) \n",
      "Test Loss tensor([0.3692, 0.5754, 0.0355, 0.2326, 0.6126])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1041, 0.0201, 0.2773, 0.2980, 0.1959, 0.1635, 0.0219]) \n",
      "Test Loss tensor([0.1061, 0.0188, 0.2672, 0.2881, 0.1970, 0.1597, 0.0210])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0473, 0.2333, 0.0468, 0.1035, 0.3706, 0.3281, 0.0488]) \n",
      "Test Loss tensor([0.0468, 0.2333, 0.0489, 0.1038, 0.3787, 0.3275, 0.0477])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0318, 0.0279, 0.0402, 0.0273, 0.0621, 0.0346, 0.2275, 0.0031]) \n",
      "Test Loss tensor([0.0295, 0.0261, 0.0362, 0.0247, 0.0609, 0.0333, 0.2232, 0.0029])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5788, 0.5797, 0.1122, 0.1912, 0.4289, 0.4147])\n",
      "Valid Idx 3 | Loss tensor([0.4073, 0.1136, 0.2326, 0.4907, 0.8455])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 112 in 5.105815887451172 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1150, 0.2130, 0.1482, 0.2159]) \n",
      "Test Loss tensor([0.1170, 0.2088, 0.1587, 0.2189])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3757, 0.5710, 0.0350, 0.2338, 0.6124]) \n",
      "Test Loss tensor([0.3651, 0.5753, 0.0405, 0.2292, 0.6163])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1004, 0.0191, 0.2607, 0.2799, 0.2001, 0.1653, 0.0197]) \n",
      "Test Loss tensor([0.1120, 0.0176, 0.2687, 0.2848, 0.2004, 0.1602, 0.0231])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0404, 0.2399, 0.0528, 0.1069, 0.3838, 0.3372, 0.0526]) \n",
      "Test Loss tensor([0.0416, 0.2333, 0.0490, 0.0994, 0.3750, 0.3331, 0.0483])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0291, 0.0260, 0.0412, 0.0280, 0.0626, 0.0340, 0.2285, 0.0030]) \n",
      "Test Loss tensor([0.0297, 0.0274, 0.0369, 0.0239, 0.0551, 0.0370, 0.2262, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5730, 0.6099, 0.1138, 0.1831, 0.4350, 0.4070])\n",
      "Valid Idx 3 | Loss tensor([0.3372, 0.0953, 0.2043, 0.4617, 0.8397])\n",
      "\n",
      "************** Batch 116 in 5.02906346321106 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1161, 0.2107, 0.1571, 0.2259]) \n",
      "Test Loss tensor([0.1145, 0.2075, 0.1600, 0.2169])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3728, 0.5817, 0.0411, 0.2285, 0.6073]) \n",
      "Test Loss tensor([0.3668, 0.5792, 0.0352, 0.2289, 0.6121])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1094, 0.0183, 0.2687, 0.2950, 0.2097, 0.1586, 0.0212]) \n",
      "Test Loss tensor([0.1070, 0.0179, 0.2693, 0.2893, 0.1977, 0.1583, 0.0199])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0441, 0.2495, 0.0521, 0.1042, 0.3726, 0.3467, 0.0509]) \n",
      "Test Loss tensor([0.0446, 0.2341, 0.0486, 0.1037, 0.3776, 0.3343, 0.0471])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0266, 0.0359, 0.0225, 0.0519, 0.0376, 0.2204, 0.0029]) \n",
      "Test Loss tensor([0.0284, 0.0267, 0.0363, 0.0244, 0.0573, 0.0338, 0.2244, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5795, 0.5916, 0.1101, 0.1867, 0.4277, 0.4146])\n",
      "Valid Idx 3 | Loss tensor([0.3919, 0.1061, 0.2225, 0.4801, 0.8406])\n",
      "\n",
      "************** Batch 120 in 4.9877119064331055 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1115, 0.2065, 0.1529, 0.2161]) \n",
      "Test Loss tensor([0.1150, 0.2082, 0.1615, 0.2198])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3601, 0.5756, 0.0352, 0.2347, 0.6219]) \n",
      "Test Loss tensor([0.3690, 0.5801, 0.0323, 0.2282, 0.6116])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1043, 0.0200, 0.2687, 0.2773, 0.1994, 0.1610, 0.0205]) \n",
      "Test Loss tensor([0.1073, 0.0183, 0.2663, 0.2881, 0.1985, 0.1568, 0.0210])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0452, 0.2347, 0.0446, 0.0974, 0.3667, 0.3385, 0.0498]) \n",
      "Test Loss tensor([0.0435, 0.2315, 0.0482, 0.1016, 0.3764, 0.3335, 0.0467])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0288, 0.0232, 0.0366, 0.0224, 0.0564, 0.0319, 0.2128, 0.0036]) \n",
      "Test Loss tensor([0.0290, 0.0256, 0.0379, 0.0241, 0.0601, 0.0319, 0.2231, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5788, 0.5864, 0.1133, 0.1912, 0.4313, 0.4217])\n",
      "Valid Idx 3 | Loss tensor([0.4122, 0.1116, 0.2311, 0.4882, 0.8478])\n",
      "Gradients: Input 0.1104595959186554 | Message 0.09416952729225159 | Update 0.12432681024074554 | Output 0.03238234668970108\n",
      "\n",
      "************** Batch 124 in 4.920106410980225 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1153, 0.2027, 0.1606, 0.2276]) \n",
      "Test Loss tensor([0.1150, 0.2106, 0.1541, 0.2132])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3794, 0.5841, 0.0333, 0.2256, 0.6009]) \n",
      "Test Loss tensor([0.3668, 0.5797, 0.0398, 0.2297, 0.6117])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1052, 0.0193, 0.2782, 0.2925, 0.1977, 0.1516, 0.0206]) \n",
      "Test Loss tensor([0.1095, 0.0165, 0.2676, 0.2835, 0.2033, 0.1595, 0.0222])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0450, 0.2284, 0.0508, 0.1030, 0.3717, 0.3415, 0.0437]) \n",
      "Test Loss tensor([0.0402, 0.2405, 0.0476, 0.0970, 0.3700, 0.3346, 0.0471])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0251, 0.0259, 0.0410, 0.0221, 0.0606, 0.0331, 0.2296, 0.0028]) \n",
      "Test Loss tensor([0.0294, 0.0267, 0.0360, 0.0241, 0.0522, 0.0371, 0.2226, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5715, 0.6089, 0.1143, 0.1825, 0.4317, 0.4046])\n",
      "Valid Idx 3 | Loss tensor([0.3143, 0.0875, 0.1932, 0.4642, 0.8409])\n",
      "\n",
      "************** Batch 128 in 4.957425832748413 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1198, 0.2129, 0.1642, 0.2160]) \n",
      "Test Loss tensor([0.1164, 0.2071, 0.1599, 0.2181])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3607, 0.5778, 0.0408, 0.2339, 0.6031]) \n",
      "Test Loss tensor([0.3662, 0.5787, 0.0350, 0.2299, 0.6130])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1142, 0.0171, 0.2664, 0.2865, 0.1966, 0.1599, 0.0200]) \n",
      "Test Loss tensor([0.1081, 0.0174, 0.2683, 0.2850, 0.1999, 0.1575, 0.0210])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0382, 0.2390, 0.0525, 0.1023, 0.3691, 0.3478, 0.0530]) \n",
      "Test Loss tensor([0.0428, 0.2341, 0.0470, 0.0994, 0.3746, 0.3279, 0.0455])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0253, 0.0269, 0.0329, 0.0194, 0.0511, 0.0374, 0.2111, 0.0031]) \n",
      "Test Loss tensor([0.0286, 0.0277, 0.0361, 0.0242, 0.0561, 0.0328, 0.2240, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5801, 0.5876, 0.1132, 0.1908, 0.4269, 0.4202])\n",
      "Valid Idx 3 | Loss tensor([0.3556, 0.0956, 0.2069, 0.4823, 0.8392])\n",
      "\n",
      "************** Batch 132 in 5.219665288925171 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1129, 0.2124, 0.1579, 0.2147]) \n",
      "Test Loss tensor([0.1136, 0.2065, 0.1597, 0.2119])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3647, 0.5677, 0.0336, 0.2347, 0.6377]) \n",
      "Test Loss tensor([0.3696, 0.5757, 0.0330, 0.2296, 0.6149])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1096, 0.0182, 0.2857, 0.2996, 0.1968, 0.1609, 0.0203]) \n",
      "Test Loss tensor([0.1067, 0.0175, 0.2684, 0.2856, 0.1979, 0.1588, 0.0197])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0424, 0.2394, 0.0531, 0.1080, 0.3768, 0.3304, 0.0479]) \n",
      "Test Loss tensor([0.0441, 0.2332, 0.0475, 0.1013, 0.3695, 0.3259, 0.0482])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0290, 0.0268, 0.0327, 0.0222, 0.0578, 0.0340, 0.2200, 0.0035]) \n",
      "Test Loss tensor([0.0279, 0.0262, 0.0364, 0.0239, 0.0594, 0.0310, 0.2217, 0.0029])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5741, 0.5802, 0.1123, 0.1953, 0.4244, 0.4256])\n",
      "Valid Idx 3 | Loss tensor([0.3763, 0.0992, 0.2180, 0.5019, 0.8463])\n",
      "\n",
      "************** Batch 136 in 5.098576307296753 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1047, 0.2093, 0.1587, 0.2163]) \n",
      "Test Loss tensor([0.1129, 0.2076, 0.1536, 0.2113])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3680, 0.5819, 0.0300, 0.2280, 0.6193]) \n",
      "Test Loss tensor([0.3651, 0.5804, 0.0375, 0.2258, 0.6137])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1070, 0.0182, 0.2679, 0.2804, 0.1979, 0.1611, 0.0194]) \n",
      "Test Loss tensor([0.1089, 0.0166, 0.2679, 0.2833, 0.1983, 0.1583, 0.0212])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0422, 0.2429, 0.0457, 0.0988, 0.3764, 0.3421, 0.0466]) \n",
      "Test Loss tensor([0.0411, 0.2343, 0.0478, 0.0987, 0.3682, 0.3231, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0253, 0.0244, 0.0364, 0.0237, 0.0619, 0.0329, 0.2083, 0.0026]) \n",
      "Test Loss tensor([0.0293, 0.0267, 0.0346, 0.0236, 0.0542, 0.0334, 0.2202, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5761, 0.5897, 0.1151, 0.1892, 0.4300, 0.4216])\n",
      "Valid Idx 3 | Loss tensor([0.3126, 0.0869, 0.1954, 0.4831, 0.8418])\n",
      "\n",
      "************** Batch 140 in 4.990917205810547 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1216, 0.2023, 0.1520, 0.2173]) \n",
      "Test Loss tensor([0.1128, 0.2061, 0.1566, 0.2111])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3593, 0.5952, 0.0378, 0.2277, 0.6047]) \n",
      "Test Loss tensor([0.3635, 0.5763, 0.0367, 0.2264, 0.6194])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1119, 0.0180, 0.2784, 0.2887, 0.2017, 0.1619, 0.0205]) \n",
      "Test Loss tensor([0.1107, 0.0168, 0.2664, 0.2822, 0.2001, 0.1544, 0.0214])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0391, 0.2359, 0.0486, 0.0962, 0.3696, 0.3198, 0.0475]) \n",
      "Test Loss tensor([0.0425, 0.2370, 0.0475, 0.0994, 0.3675, 0.3237, 0.0465])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0276, 0.0258, 0.0351, 0.0215, 0.0502, 0.0343, 0.2114, 0.0032]) \n",
      "Test Loss tensor([0.0286, 0.0262, 0.0340, 0.0246, 0.0551, 0.0328, 0.2191, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5774, 0.5874, 0.1125, 0.1918, 0.4254, 0.4265])\n",
      "Valid Idx 3 | Loss tensor([0.3109, 0.0858, 0.1937, 0.4835, 0.8422])\n",
      "Gradients: Input 0.2317870557308197 | Message 0.21602940559387207 | Update 0.2764074206352234 | Output 0.06545057147741318\n",
      "\n",
      "************** Batch 144 in 5.114857912063599 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1125, 0.2092, 0.1549, 0.2082]) \n",
      "Test Loss tensor([0.1135, 0.2060, 0.1548, 0.2115])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3744, 0.5881, 0.0365, 0.2279, 0.6033]) \n",
      "Test Loss tensor([0.3599, 0.5846, 0.0342, 0.2279, 0.6114])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0993, 0.0163, 0.2670, 0.2851, 0.2035, 0.1546, 0.0223]) \n",
      "Test Loss tensor([0.1099, 0.0172, 0.2681, 0.2827, 0.2015, 0.1566, 0.0200])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0412, 0.2308, 0.0464, 0.0982, 0.3587, 0.3266, 0.0421]) \n",
      "Test Loss tensor([0.0422, 0.2316, 0.0462, 0.0995, 0.3653, 0.3217, 0.0460])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0279, 0.0273, 0.0331, 0.0217, 0.0572, 0.0336, 0.2214, 0.0031]) \n",
      "Test Loss tensor([0.0278, 0.0261, 0.0364, 0.0241, 0.0580, 0.0312, 0.2211, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5820, 0.5860, 0.1116, 0.1949, 0.4209, 0.4382])\n",
      "Valid Idx 3 | Loss tensor([0.3313, 0.0917, 0.1966, 0.4902, 0.8496])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 148 in 4.964693546295166 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1143, 0.1968, 0.1607, 0.2242]) \n",
      "Test Loss tensor([0.1137, 0.2041, 0.1577, 0.2131])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3649, 0.5799, 0.0335, 0.2342, 0.6154]) \n",
      "Test Loss tensor([0.3649, 0.5750, 0.0337, 0.2254, 0.6169])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1072, 0.0174, 0.2695, 0.2775, 0.2058, 0.1648, 0.0231]) \n",
      "Test Loss tensor([0.1068, 0.0179, 0.2676, 0.2870, 0.2008, 0.1542, 0.0199])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0435, 0.2290, 0.0450, 0.0997, 0.3754, 0.3118, 0.0552]) \n",
      "Test Loss tensor([0.0444, 0.2325, 0.0473, 0.1026, 0.3675, 0.3228, 0.0460])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0253, 0.0271, 0.0314, 0.0235, 0.0565, 0.0303, 0.2146, 0.0026]) \n",
      "Test Loss tensor([0.0287, 0.0262, 0.0364, 0.0242, 0.0568, 0.0304, 0.2197, 0.0028])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5790, 0.5880, 0.1106, 0.1954, 0.4241, 0.4414])\n",
      "Valid Idx 3 | Loss tensor([0.3328, 0.0927, 0.1972, 0.4833, 0.8424])\n",
      "\n",
      "************** Batch 152 in 5.154014587402344 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1148, 0.1954, 0.1657, 0.2112]) \n",
      "Test Loss tensor([0.1149, 0.2091, 0.1515, 0.2085])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3516, 0.5805, 0.0325, 0.2318, 0.6174]) \n",
      "Test Loss tensor([0.3572, 0.5808, 0.0391, 0.2246, 0.6126])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1064, 0.0184, 0.2540, 0.2757, 0.2005, 0.1492, 0.0212]) \n",
      "Test Loss tensor([0.1109, 0.0165, 0.2695, 0.2801, 0.2057, 0.1533, 0.0230])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0466, 0.2325, 0.0442, 0.1003, 0.3644, 0.3191, 0.0468]) \n",
      "Test Loss tensor([0.0405, 0.2341, 0.0473, 0.0979, 0.3642, 0.3279, 0.0464])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0287, 0.0374, 0.0227, 0.0552, 0.0296, 0.2089, 0.0029]) \n",
      "Test Loss tensor([0.0297, 0.0274, 0.0356, 0.0245, 0.0508, 0.0335, 0.2228, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5687, 0.6184, 0.1114, 0.1924, 0.4285, 0.4373])\n",
      "Valid Idx 3 | Loss tensor([0.2635, 0.0819, 0.1756, 0.4594, 0.8410])\n",
      "\n",
      "************** Batch 156 in 5.076651334762573 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1167, 0.2023, 0.1526, 0.2039]) \n",
      "Test Loss tensor([0.1144, 0.2048, 0.1609, 0.2124])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3554, 0.5788, 0.0350, 0.2239, 0.6109]) \n",
      "Test Loss tensor([0.3591, 0.5779, 0.0325, 0.2232, 0.6133])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1099, 0.0180, 0.2760, 0.2808, 0.2020, 0.1592, 0.0251]) \n",
      "Test Loss tensor([0.1068, 0.0182, 0.2708, 0.2880, 0.1987, 0.1509, 0.0200])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0404, 0.2434, 0.0513, 0.0946, 0.3812, 0.3341, 0.0496]) \n",
      "Test Loss tensor([0.0451, 0.2319, 0.0476, 0.1032, 0.3635, 0.3203, 0.0469])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0316, 0.0290, 0.0399, 0.0219, 0.0527, 0.0336, 0.2234, 0.0029]) \n",
      "Test Loss tensor([0.0284, 0.0266, 0.0364, 0.0234, 0.0577, 0.0283, 0.2174, 0.0029])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5798, 0.5899, 0.1127, 0.2029, 0.4190, 0.4549])\n",
      "Valid Idx 3 | Loss tensor([0.3466, 0.0984, 0.2008, 0.4861, 0.8469])\n",
      "\n",
      "************** Batch 160 in 5.070084810256958 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1158, 0.1977, 0.1620, 0.2134]) \n",
      "Test Loss tensor([0.1124, 0.2038, 0.1571, 0.2121])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3653, 0.5698, 0.0291, 0.2206, 0.6049]) \n",
      "Test Loss tensor([0.3544, 0.5758, 0.0326, 0.2237, 0.6157])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1035, 0.0174, 0.2635, 0.2857, 0.1965, 0.1620, 0.0179]) \n",
      "Test Loss tensor([0.1080, 0.0179, 0.2686, 0.2856, 0.1989, 0.1511, 0.0201])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0453, 0.2402, 0.0488, 0.1040, 0.3627, 0.3203, 0.0414]) \n",
      "Test Loss tensor([0.0440, 0.2283, 0.0457, 0.1001, 0.3666, 0.3189, 0.0461])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0253, 0.0246, 0.0354, 0.0215, 0.0582, 0.0268, 0.2204, 0.0032]) \n",
      "Test Loss tensor([0.0287, 0.0258, 0.0356, 0.0238, 0.0571, 0.0287, 0.2173, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5819, 0.5924, 0.1103, 0.2018, 0.4199, 0.4559])\n",
      "Valid Idx 3 | Loss tensor([0.3380, 0.0976, 0.1953, 0.4850, 0.8498])\n",
      "Gradients: Input 0.32987484335899353 | Message 0.3328148126602173 | Update 0.4380751848220825 | Output 0.11476414650678635\n",
      "\n",
      "************** Batch 164 in 4.98614764213562 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1071, 0.2057, 0.1550, 0.1956]) \n",
      "Test Loss tensor([0.1177, 0.2063, 0.1540, 0.2072])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3623, 0.5900, 0.0332, 0.2214, 0.6080]) \n",
      "Test Loss tensor([0.3493, 0.5797, 0.0413, 0.2230, 0.6154])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1042, 0.0182, 0.2784, 0.2794, 0.1971, 0.1475, 0.0211]) \n",
      "Test Loss tensor([0.1150, 0.0169, 0.2678, 0.2728, 0.2058, 0.1500, 0.0247])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0452, 0.2301, 0.0554, 0.1072, 0.3621, 0.3232, 0.0491]) \n",
      "Test Loss tensor([0.0404, 0.2356, 0.0473, 0.0983, 0.3612, 0.3235, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0262, 0.0268, 0.0357, 0.0207, 0.0558, 0.0286, 0.2176, 0.0027]) \n",
      "Test Loss tensor([0.0303, 0.0266, 0.0361, 0.0236, 0.0504, 0.0335, 0.2172, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5706, 0.6284, 0.1135, 0.1952, 0.4216, 0.4473])\n",
      "Valid Idx 3 | Loss tensor([0.2473, 0.0797, 0.1694, 0.4597, 0.8379])\n",
      "\n",
      "************** Batch 168 in 5.04112982749939 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1115, 0.2060, 0.1510, 0.2123]) \n",
      "Test Loss tensor([0.1126, 0.2070, 0.1622, 0.2067])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3556, 0.5904, 0.0377, 0.2265, 0.6101]) \n",
      "Test Loss tensor([0.3522, 0.5766, 0.0318, 0.2240, 0.6140])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1143, 0.0173, 0.2711, 0.2744, 0.2048, 0.1527, 0.0250]) \n",
      "Test Loss tensor([0.1092, 0.0189, 0.2721, 0.2840, 0.1958, 0.1506, 0.0201])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0388, 0.2304, 0.0413, 0.0941, 0.3536, 0.3200, 0.0469]) \n",
      "Test Loss tensor([0.0458, 0.2343, 0.0482, 0.1015, 0.3624, 0.3136, 0.0459])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0269, 0.0358, 0.0233, 0.0477, 0.0324, 0.2198, 0.0030]) \n",
      "Test Loss tensor([0.0278, 0.0256, 0.0372, 0.0238, 0.0616, 0.0264, 0.2195, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5819, 0.5867, 0.1090, 0.2093, 0.4157, 0.4687])\n",
      "Valid Idx 3 | Loss tensor([0.3679, 0.1019, 0.2054, 0.5095, 0.8555])\n",
      "\n",
      "************** Batch 172 in 4.934652090072632 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1166, 0.2018, 0.1581, 0.2214]) \n",
      "Test Loss tensor([0.1147, 0.2073, 0.1595, 0.2128])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3441, 0.5631, 0.0334, 0.2284, 0.6213]) \n",
      "Test Loss tensor([0.3499, 0.5754, 0.0340, 0.2225, 0.6189])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1013, 0.0186, 0.2725, 0.2935, 0.2038, 0.1481, 0.0208]) \n",
      "Test Loss tensor([0.1071, 0.0181, 0.2665, 0.2807, 0.1993, 0.1467, 0.0216])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0484, 0.2394, 0.0489, 0.1041, 0.3650, 0.3035, 0.0422]) \n",
      "Test Loss tensor([0.0430, 0.2317, 0.0466, 0.0998, 0.3611, 0.3194, 0.0465])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0285, 0.0375, 0.0266, 0.0632, 0.0267, 0.2190, 0.0029]) \n",
      "Test Loss tensor([0.0295, 0.0262, 0.0362, 0.0242, 0.0571, 0.0289, 0.2161, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5770, 0.6022, 0.1106, 0.2051, 0.4206, 0.4565])\n",
      "Valid Idx 3 | Loss tensor([0.3279, 0.0932, 0.1925, 0.4933, 0.8498])\n",
      "\n",
      "************** Batch 176 in 5.231606721878052 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1139, 0.1942, 0.1623, 0.2164]) \n",
      "Test Loss tensor([0.1134, 0.2079, 0.1508, 0.2021])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3568, 0.5737, 0.0311, 0.2312, 0.6114]) \n",
      "Test Loss tensor([0.3439, 0.5822, 0.0424, 0.2249, 0.6093])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1101, 0.0193, 0.2702, 0.2853, 0.2119, 0.1467, 0.0208]) \n",
      "Test Loss tensor([0.1155, 0.0166, 0.2656, 0.2755, 0.2065, 0.1460, 0.0242])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0425, 0.2373, 0.0447, 0.1015, 0.3606, 0.3234, 0.0503]) \n",
      "Test Loss tensor([0.0390, 0.2344, 0.0471, 0.0949, 0.3615, 0.3259, 0.0492])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0267, 0.0242, 0.0385, 0.0223, 0.0571, 0.0277, 0.2190, 0.0033]) \n",
      "Test Loss tensor([0.0305, 0.0270, 0.0345, 0.0225, 0.0485, 0.0349, 0.2176, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5741, 0.6378, 0.1165, 0.1928, 0.4254, 0.4469])\n",
      "Valid Idx 3 | Loss tensor([0.2413, 0.0765, 0.1633, 0.4586, 0.8394])\n",
      "\n",
      "************** Batch 180 in 5.043589353561401 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1117, 0.2097, 0.1469, 0.2077]) \n",
      "Test Loss tensor([0.1118, 0.2025, 0.1597, 0.2065])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3507, 0.5785, 0.0386, 0.2248, 0.6081]) \n",
      "Test Loss tensor([0.3509, 0.5798, 0.0306, 0.2256, 0.6127])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1099, 0.0162, 0.2698, 0.2772, 0.2044, 0.1484, 0.0231]) \n",
      "Test Loss tensor([0.1041, 0.0191, 0.2685, 0.2887, 0.1945, 0.1493, 0.0193])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0414, 0.2361, 0.0514, 0.0937, 0.3543, 0.3295, 0.0523]) \n",
      "Test Loss tensor([0.0444, 0.2296, 0.0481, 0.1001, 0.3563, 0.3142, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0317, 0.0271, 0.0335, 0.0230, 0.0485, 0.0349, 0.2198, 0.0029]) \n",
      "Test Loss tensor([0.0278, 0.0259, 0.0368, 0.0241, 0.0607, 0.0268, 0.2173, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5768, 0.5914, 0.1107, 0.2087, 0.4178, 0.4642])\n",
      "Valid Idx 3 | Loss tensor([0.3909, 0.1058, 0.2040, 0.5143, 0.8551])\n",
      "Gradients: Input 0.7912216186523438 | Message 0.8237383365631104 | Update 1.0692923069000244 | Output 0.24618668854236603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 184 in 4.968436002731323 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1086, 0.1984, 0.1619, 0.2094]) \n",
      "Test Loss tensor([0.1130, 0.2067, 0.1531, 0.2077])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3593, 0.5654, 0.0286, 0.2280, 0.6234]) \n",
      "Test Loss tensor([0.3490, 0.5817, 0.0355, 0.2239, 0.6100])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0973, 0.0194, 0.2660, 0.2914, 0.1993, 0.1502, 0.0180]) \n",
      "Test Loss tensor([0.1090, 0.0167, 0.2675, 0.2785, 0.2006, 0.1462, 0.0212])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0432, 0.2433, 0.0468, 0.1049, 0.3605, 0.3095, 0.0464]) \n",
      "Test Loss tensor([0.0399, 0.2327, 0.0461, 0.0956, 0.3620, 0.3220, 0.0463])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0306, 0.0249, 0.0388, 0.0222, 0.0574, 0.0249, 0.2156, 0.0031]) \n",
      "Test Loss tensor([0.0293, 0.0276, 0.0356, 0.0217, 0.0505, 0.0312, 0.2143, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5728, 0.6285, 0.1145, 0.1960, 0.4237, 0.4508])\n",
      "Valid Idx 3 | Loss tensor([0.2942, 0.0854, 0.1757, 0.4781, 0.8476])\n",
      "\n",
      "************** Batch 188 in 4.977659702301025 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1084, 0.2007, 0.1487, 0.2000]) \n",
      "Test Loss tensor([0.1123, 0.2081, 0.1527, 0.2004])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3544, 0.5810, 0.0386, 0.2181, 0.6167]) \n",
      "Test Loss tensor([0.3487, 0.5774, 0.0412, 0.2233, 0.6145])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1063, 0.0164, 0.2647, 0.2777, 0.1996, 0.1453, 0.0234]) \n",
      "Test Loss tensor([0.1126, 0.0158, 0.2693, 0.2746, 0.2070, 0.1450, 0.0239])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0346, 0.2280, 0.0467, 0.0980, 0.3523, 0.3307, 0.0467]) \n",
      "Test Loss tensor([0.0364, 0.2329, 0.0460, 0.0924, 0.3572, 0.3212, 0.0459])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0302, 0.0230, 0.0404, 0.0231, 0.0523, 0.0318, 0.2113, 0.0031]) \n",
      "Test Loss tensor([0.0302, 0.0273, 0.0340, 0.0229, 0.0448, 0.0363, 0.2170, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5698, 0.6576, 0.1160, 0.1887, 0.4293, 0.4424])\n",
      "Valid Idx 3 | Loss tensor([0.2371, 0.0743, 0.1585, 0.4552, 0.8418])\n",
      "\n",
      "************** Batch 192 in 5.013265132904053 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1170, 0.2051, 0.1503, 0.2023]) \n",
      "Test Loss tensor([0.1105, 0.2042, 0.1603, 0.2098])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3410, 0.5565, 0.0399, 0.2237, 0.6208]) \n",
      "Test Loss tensor([0.3596, 0.5797, 0.0272, 0.2244, 0.6107])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1112, 0.0154, 0.2682, 0.2763, 0.2059, 0.1463, 0.0243]) \n",
      "Test Loss tensor([0.1055, 0.0190, 0.2730, 0.2902, 0.1950, 0.1502, 0.0178])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0330, 0.2245, 0.0451, 0.0950, 0.3528, 0.3272, 0.0406]) \n",
      "Test Loss tensor([0.0436, 0.2290, 0.0488, 0.1000, 0.3497, 0.3097, 0.0480])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0312, 0.0256, 0.0330, 0.0216, 0.0450, 0.0373, 0.2094, 0.0036]) \n",
      "Test Loss tensor([0.0274, 0.0262, 0.0373, 0.0237, 0.0593, 0.0248, 0.2136, 0.0029])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5833, 0.5941, 0.1107, 0.2083, 0.4141, 0.4723])\n",
      "Valid Idx 3 | Loss tensor([0.4277, 0.1111, 0.2131, 0.5261, 0.8581])\n",
      "\n",
      "************** Batch 196 in 5.21682596206665 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1068, 0.2041, 0.1550, 0.1981]) \n",
      "Test Loss tensor([0.1104, 0.2048, 0.1542, 0.2031])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3506, 0.5691, 0.0254, 0.2183, 0.6239]) \n",
      "Test Loss tensor([0.3504, 0.5787, 0.0325, 0.2223, 0.6124])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0976, 0.0188, 0.2764, 0.2956, 0.1961, 0.1532, 0.0181]) \n",
      "Test Loss tensor([0.1037, 0.0167, 0.2665, 0.2841, 0.1976, 0.1444, 0.0199])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0449, 0.2334, 0.0450, 0.1011, 0.3629, 0.3171, 0.0522]) \n",
      "Test Loss tensor([0.0412, 0.2286, 0.0464, 0.0976, 0.3537, 0.3117, 0.0461])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0287, 0.0296, 0.0369, 0.0222, 0.0595, 0.0239, 0.2077, 0.0038]) \n",
      "Test Loss tensor([0.0290, 0.0258, 0.0358, 0.0231, 0.0514, 0.0287, 0.2132, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5789, 0.6250, 0.1131, 0.1999, 0.4193, 0.4574])\n",
      "Valid Idx 3 | Loss tensor([0.3320, 0.0886, 0.1826, 0.4867, 0.8503])\n",
      "\n",
      "************** Batch 200 in 5.215023517608643 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1085, 0.2014, 0.1570, 0.2065]) \n",
      "Test Loss tensor([0.1138, 0.2080, 0.1485, 0.1989])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3432, 0.5706, 0.0342, 0.2173, 0.6261]) \n",
      "Test Loss tensor([0.3442, 0.5838, 0.0425, 0.2273, 0.6125])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1108, 0.0170, 0.2844, 0.2953, 0.2052, 0.1514, 0.0206]) \n",
      "Test Loss tensor([0.1161, 0.0156, 0.2681, 0.2709, 0.2088, 0.1460, 0.0249])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0423, 0.2177, 0.0525, 0.0977, 0.3434, 0.3165, 0.0498]) \n",
      "Test Loss tensor([0.0362, 0.2373, 0.0457, 0.0955, 0.3547, 0.3200, 0.0475])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0255, 0.0229, 0.0342, 0.0216, 0.0506, 0.0285, 0.2121, 0.0028]) \n",
      "Test Loss tensor([0.0314, 0.0279, 0.0352, 0.0229, 0.0429, 0.0349, 0.2153, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5658, 0.6618, 0.1191, 0.1885, 0.4242, 0.4398])\n",
      "Valid Idx 3 | Loss tensor([0.2241, 0.0716, 0.1549, 0.4539, 0.8373])\n",
      "Gradients: Input 0.15809427201747894 | Message 0.13783210515975952 | Update 0.18151350319385529 | Output 0.04126254841685295\n",
      "\n",
      "************** Batch 204 in 5.196461200714111 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1044, 0.2039, 0.1440, 0.1920]) \n",
      "Test Loss tensor([0.1085, 0.2033, 0.1561, 0.2037])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3365, 0.5756, 0.0415, 0.2209, 0.6080]) \n",
      "Test Loss tensor([0.3501, 0.5778, 0.0314, 0.2195, 0.6128])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1091, 0.0164, 0.2753, 0.2758, 0.2029, 0.1446, 0.0245]) \n",
      "Test Loss tensor([0.1040, 0.0178, 0.2686, 0.2834, 0.1978, 0.1425, 0.0206])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0372, 0.2401, 0.0487, 0.0955, 0.3521, 0.3275, 0.0530]) \n",
      "Test Loss tensor([0.0416, 0.2295, 0.0459, 0.0988, 0.3528, 0.3055, 0.0471])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0321, 0.0244, 0.0362, 0.0245, 0.0445, 0.0348, 0.2187, 0.0032]) \n",
      "Test Loss tensor([0.0273, 0.0259, 0.0360, 0.0233, 0.0556, 0.0259, 0.2142, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5759, 0.6072, 0.1134, 0.2071, 0.4207, 0.4653])\n",
      "Valid Idx 3 | Loss tensor([0.3693, 0.0975, 0.1905, 0.5115, 0.8549])\n",
      "\n",
      "************** Batch 208 in 5.024410963058472 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1047, 0.2032, 0.1566, 0.2047]) \n",
      "Test Loss tensor([0.1096, 0.2049, 0.1535, 0.2043])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3518, 0.5852, 0.0333, 0.2209, 0.6060]) \n",
      "Test Loss tensor([0.3387, 0.5767, 0.0350, 0.2208, 0.6126])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1110, 0.0179, 0.2667, 0.2684, 0.1960, 0.1495, 0.0211]) \n",
      "Test Loss tensor([0.1060, 0.0168, 0.2712, 0.2773, 0.1996, 0.1429, 0.0214])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0429, 0.2300, 0.0467, 0.1014, 0.3567, 0.3229, 0.0448]) \n",
      "Test Loss tensor([0.0422, 0.2337, 0.0462, 0.0965, 0.3526, 0.3096, 0.0474])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0316, 0.0278, 0.0393, 0.0226, 0.0582, 0.0258, 0.2068, 0.0034]) \n",
      "Test Loss tensor([0.0287, 0.0266, 0.0364, 0.0224, 0.0533, 0.0273, 0.2114, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5777, 0.6237, 0.1117, 0.2017, 0.4194, 0.4681])\n",
      "Valid Idx 3 | Loss tensor([0.3297, 0.0890, 0.1827, 0.4933, 0.8521])\n",
      "\n",
      "************** Batch 212 in 4.9733593463897705 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1058, 0.1982, 0.1559, 0.2065]) \n",
      "Test Loss tensor([0.1113, 0.2051, 0.1493, 0.1990])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3501, 0.5688, 0.0317, 0.2186, 0.6231]) \n",
      "Test Loss tensor([0.3405, 0.5876, 0.0400, 0.2285, 0.6083])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1040, 0.0171, 0.2528, 0.2840, 0.1995, 0.1417, 0.0242]) \n",
      "Test Loss tensor([0.1096, 0.0162, 0.2666, 0.2698, 0.2059, 0.1406, 0.0241])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0414, 0.2328, 0.0414, 0.0937, 0.3592, 0.3061, 0.0417]) \n",
      "Test Loss tensor([0.0390, 0.2342, 0.0459, 0.0960, 0.3453, 0.3138, 0.0463])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0293, 0.0277, 0.0371, 0.0218, 0.0521, 0.0285, 0.2076, 0.0033]) \n",
      "Test Loss tensor([0.0301, 0.0268, 0.0363, 0.0230, 0.0490, 0.0310, 0.2093, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5790, 0.6461, 0.1122, 0.1932, 0.4223, 0.4595])\n",
      "Valid Idx 3 | Loss tensor([0.2655, 0.0782, 0.1631, 0.4656, 0.8426])\n",
      "\n",
      "************** Batch 216 in 5.190091848373413 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1146, 0.2110, 0.1494, 0.1985]) \n",
      "Test Loss tensor([0.1079, 0.2032, 0.1535, 0.2015])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3419, 0.5685, 0.0394, 0.2132, 0.6148]) \n",
      "Test Loss tensor([0.3404, 0.5774, 0.0333, 0.2234, 0.6130])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1035, 0.0170, 0.2746, 0.2759, 0.2076, 0.1386, 0.0257]) \n",
      "Test Loss tensor([0.1059, 0.0173, 0.2744, 0.2770, 0.2007, 0.1413, 0.0209])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0379, 0.2480, 0.0443, 0.0989, 0.3576, 0.3171, 0.0425]) \n",
      "Test Loss tensor([0.0414, 0.2292, 0.0480, 0.0999, 0.3499, 0.3092, 0.0481])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0277, 0.0311, 0.0213, 0.0467, 0.0297, 0.1984, 0.0038]) \n",
      "Test Loss tensor([0.0290, 0.0262, 0.0362, 0.0235, 0.0536, 0.0268, 0.2051, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5787, 0.6275, 0.1122, 0.2050, 0.4164, 0.4700])\n",
      "Valid Idx 3 | Loss tensor([0.3295, 0.0913, 0.1766, 0.4941, 0.8569])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 220 in 5.114219665527344 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1087, 0.1934, 0.1505, 0.2010]) \n",
      "Test Loss tensor([0.1090, 0.2046, 0.1507, 0.2011])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3481, 0.5762, 0.0338, 0.2206, 0.6065]) \n",
      "Test Loss tensor([0.3380, 0.5801, 0.0354, 0.2254, 0.6143])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0975, 0.0172, 0.2677, 0.2662, 0.2045, 0.1392, 0.0199]) \n",
      "Test Loss tensor([0.1072, 0.0167, 0.2714, 0.2720, 0.2008, 0.1399, 0.0227])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0396, 0.2207, 0.0433, 0.0979, 0.3432, 0.3026, 0.0508]) \n",
      "Test Loss tensor([0.0408, 0.2292, 0.0473, 0.0986, 0.3487, 0.3089, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0302, 0.0272, 0.0392, 0.0289, 0.0533, 0.0270, 0.2090, 0.0031]) \n",
      "Test Loss tensor([0.0287, 0.0263, 0.0363, 0.0234, 0.0525, 0.0277, 0.2067, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5783, 0.6402, 0.1086, 0.2008, 0.4213, 0.4713])\n",
      "Valid Idx 3 | Loss tensor([0.3123, 0.0868, 0.1750, 0.4805, 0.8509])\n",
      "Gradients: Input 0.27923983335494995 | Message 0.3241243362426758 | Update 0.4020843505859375 | Output 0.08785594999790192\n",
      "\n",
      "************** Batch 224 in 5.119393587112427 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1153, 0.2034, 0.1475, 0.2094]) \n",
      "Test Loss tensor([0.1091, 0.2050, 0.1499, 0.2011])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3443, 0.5853, 0.0378, 0.2305, 0.6016]) \n",
      "Test Loss tensor([0.3380, 0.5821, 0.0359, 0.2250, 0.6089])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1051, 0.0163, 0.2654, 0.2611, 0.1967, 0.1380, 0.0210]) \n",
      "Test Loss tensor([0.1082, 0.0168, 0.2718, 0.2719, 0.2033, 0.1385, 0.0228])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0401, 0.2340, 0.0473, 0.0951, 0.3438, 0.2947, 0.0473]) \n",
      "Test Loss tensor([0.0413, 0.2319, 0.0469, 0.0977, 0.3488, 0.3082, 0.0471])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0322, 0.0287, 0.0431, 0.0266, 0.0530, 0.0288, 0.2117, 0.0028]) \n",
      "Test Loss tensor([0.0297, 0.0267, 0.0373, 0.0228, 0.0518, 0.0277, 0.2046, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5798, 0.6456, 0.1096, 0.2016, 0.4160, 0.4775])\n",
      "Valid Idx 3 | Loss tensor([0.3038, 0.0859, 0.1679, 0.4822, 0.8513])\n",
      "\n",
      "************** Batch 228 in 4.972924470901489 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1092, 0.2084, 0.1476, 0.1994]) \n",
      "Test Loss tensor([0.1083, 0.2048, 0.1544, 0.2010])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3227, 0.5814, 0.0364, 0.2264, 0.5990]) \n",
      "Test Loss tensor([0.3370, 0.5767, 0.0328, 0.2214, 0.6165])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0995, 0.0169, 0.2496, 0.2666, 0.1974, 0.1326, 0.0218]) \n",
      "Test Loss tensor([0.1083, 0.0174, 0.2701, 0.2717, 0.1987, 0.1384, 0.0213])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0355, 0.2437, 0.0519, 0.0961, 0.3455, 0.3178, 0.0497]) \n",
      "Test Loss tensor([0.0439, 0.2297, 0.0473, 0.0982, 0.3467, 0.3062, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0335, 0.0274, 0.0389, 0.0228, 0.0475, 0.0279, 0.2015, 0.0032]) \n",
      "Test Loss tensor([0.0283, 0.0257, 0.0378, 0.0224, 0.0565, 0.0260, 0.2030, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5820, 0.6302, 0.1101, 0.2090, 0.4161, 0.4823])\n",
      "Valid Idx 3 | Loss tensor([0.3482, 0.0919, 0.1770, 0.5014, 0.8586])\n",
      "\n",
      "************** Batch 232 in 5.01176643371582 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1117, 0.1962, 0.1571, 0.2073]) \n",
      "Test Loss tensor([0.1092, 0.2074, 0.1525, 0.2013])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3389, 0.5755, 0.0308, 0.2169, 0.6266]) \n",
      "Test Loss tensor([0.3319, 0.5831, 0.0373, 0.2239, 0.6067])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1061, 0.0187, 0.2791, 0.2698, 0.1890, 0.1309, 0.0191]) \n",
      "Test Loss tensor([0.1109, 0.0164, 0.2735, 0.2687, 0.2033, 0.1363, 0.0237])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0391, 0.2333, 0.0450, 0.0930, 0.3396, 0.3146, 0.0433]) \n",
      "Test Loss tensor([0.0411, 0.2290, 0.0455, 0.0954, 0.3448, 0.3095, 0.0483])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0267, 0.0252, 0.0356, 0.0225, 0.0550, 0.0276, 0.2032, 0.0035]) \n",
      "Test Loss tensor([0.0295, 0.0263, 0.0363, 0.0234, 0.0506, 0.0282, 0.2050, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5810, 0.6507, 0.1111, 0.2036, 0.4153, 0.4785])\n",
      "Valid Idx 3 | Loss tensor([0.2987, 0.0812, 0.1643, 0.4797, 0.8489])\n",
      "\n",
      "************** Batch 236 in 5.114200115203857 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1159, 0.1989, 0.1500, 0.2052]) \n",
      "Test Loss tensor([0.1082, 0.2040, 0.1522, 0.2007])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3460, 0.5829, 0.0366, 0.2127, 0.6180]) \n",
      "Test Loss tensor([0.3302, 0.5827, 0.0361, 0.2196, 0.6063])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1001, 0.0163, 0.2617, 0.2612, 0.2070, 0.1358, 0.0214]) \n",
      "Test Loss tensor([0.1064, 0.0165, 0.2729, 0.2659, 0.2003, 0.1357, 0.0231])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0390, 0.2415, 0.0465, 0.0854, 0.3412, 0.3074, 0.0434]) \n",
      "Test Loss tensor([0.0422, 0.2306, 0.0471, 0.0956, 0.3454, 0.3108, 0.0455])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0253, 0.0273, 0.0346, 0.0219, 0.0512, 0.0288, 0.2035, 0.0029]) \n",
      "Test Loss tensor([0.0292, 0.0253, 0.0368, 0.0224, 0.0527, 0.0271, 0.2007, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5816, 0.6463, 0.1114, 0.2069, 0.4164, 0.4783])\n",
      "Valid Idx 3 | Loss tensor([0.3174, 0.0838, 0.1666, 0.4890, 0.8482])\n",
      "\n",
      "************** Batch 240 in 5.293206214904785 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1103, 0.2023, 0.1596, 0.2057]) \n",
      "Test Loss tensor([0.1062, 0.2043, 0.1572, 0.2001])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3426, 0.5803, 0.0374, 0.2103, 0.5997]) \n",
      "Test Loss tensor([0.3346, 0.5783, 0.0337, 0.2183, 0.6139])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1079, 0.0171, 0.2766, 0.2654, 0.2135, 0.1307, 0.0216]) \n",
      "Test Loss tensor([0.1049, 0.0177, 0.2696, 0.2705, 0.1980, 0.1346, 0.0205])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0406, 0.2247, 0.0457, 0.0899, 0.3412, 0.2981, 0.0418]) \n",
      "Test Loss tensor([0.0447, 0.2306, 0.0472, 0.0985, 0.3431, 0.2986, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0275, 0.0256, 0.0341, 0.0214, 0.0517, 0.0265, 0.1981, 0.0031]) \n",
      "Test Loss tensor([0.0293, 0.0270, 0.0369, 0.0233, 0.0571, 0.0243, 0.2019, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5882, 0.6285, 0.1091, 0.2142, 0.4144, 0.4859])\n",
      "Valid Idx 3 | Loss tensor([0.3706, 0.0904, 0.1787, 0.5129, 0.8535])\n",
      "Gradients: Input 0.1510048806667328 | Message 0.14733734726905823 | Update 0.18064731359481812 | Output 0.04329157620668411\n",
      "\n",
      "************** Batch 244 in 5.184872627258301 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1066, 0.1938, 0.1650, 0.2005]) \n",
      "Test Loss tensor([0.1047, 0.2029, 0.1508, 0.1935])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3317, 0.5744, 0.0324, 0.2154, 0.6326]) \n",
      "Test Loss tensor([0.3236, 0.5784, 0.0394, 0.2224, 0.6098])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1035, 0.0184, 0.2624, 0.2726, 0.2019, 0.1350, 0.0198]) \n",
      "Test Loss tensor([0.1113, 0.0163, 0.2702, 0.2639, 0.2044, 0.1329, 0.0242])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0435, 0.2387, 0.0432, 0.0946, 0.3499, 0.3027, 0.0516]) \n",
      "Test Loss tensor([0.0419, 0.2336, 0.0481, 0.0975, 0.3431, 0.3046, 0.0482])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0284, 0.0254, 0.0371, 0.0249, 0.0556, 0.0248, 0.1906, 0.0028]) \n",
      "Test Loss tensor([0.0301, 0.0264, 0.0374, 0.0233, 0.0507, 0.0279, 0.1995, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5813, 0.6489, 0.1157, 0.2061, 0.4160, 0.4786])\n",
      "Valid Idx 3 | Loss tensor([0.2947, 0.0779, 0.1580, 0.4894, 0.8498])\n",
      "\n",
      "************** Batch 248 in 5.100908517837524 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1137, 0.2010, 0.1566, 0.2080]) \n",
      "Test Loss tensor([0.1090, 0.2042, 0.1545, 0.1973])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3349, 0.5695, 0.0379, 0.2164, 0.6092]) \n",
      "Test Loss tensor([0.3267, 0.5788, 0.0356, 0.2164, 0.6154])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1090, 0.0157, 0.2672, 0.2578, 0.2000, 0.1364, 0.0297]) \n",
      "Test Loss tensor([0.1055, 0.0168, 0.2651, 0.2667, 0.2007, 0.1338, 0.0224])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0361, 0.2302, 0.0416, 0.0911, 0.3445, 0.3024, 0.0423]) \n",
      "Test Loss tensor([0.0419, 0.2289, 0.0452, 0.0962, 0.3439, 0.3034, 0.0465])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0318, 0.0261, 0.0345, 0.0232, 0.0503, 0.0284, 0.1948, 0.0030]) \n",
      "Test Loss tensor([0.0283, 0.0250, 0.0380, 0.0228, 0.0528, 0.0252, 0.1983, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5872, 0.6466, 0.1099, 0.2085, 0.4145, 0.4820])\n",
      "Valid Idx 3 | Loss tensor([0.3311, 0.0824, 0.1674, 0.5083, 0.8510])\n",
      "\n",
      "************** Batch 252 in 5.23827600479126 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1096, 0.1998, 0.1551, 0.2075]) \n",
      "Test Loss tensor([0.1069, 0.2033, 0.1534, 0.1958])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3252, 0.5795, 0.0362, 0.2183, 0.6112]) \n",
      "Test Loss tensor([0.3255, 0.5766, 0.0351, 0.2189, 0.6138])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1036, 0.0159, 0.2616, 0.2454, 0.2040, 0.1301, 0.0248]) \n",
      "Test Loss tensor([0.1062, 0.0166, 0.2646, 0.2684, 0.2003, 0.1334, 0.0213])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0423, 0.2364, 0.0473, 0.0979, 0.3458, 0.3043, 0.0507]) \n",
      "Test Loss tensor([0.0420, 0.2282, 0.0470, 0.0950, 0.3427, 0.3042, 0.0471])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0270, 0.0237, 0.0359, 0.0263, 0.0522, 0.0249, 0.1983, 0.0030]) \n",
      "Test Loss tensor([0.0289, 0.0254, 0.0370, 0.0219, 0.0522, 0.0251, 0.1963, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5785, 0.6494, 0.1100, 0.2093, 0.4138, 0.4792])\n",
      "Valid Idx 3 | Loss tensor([0.3333, 0.0827, 0.1661, 0.5051, 0.8529])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 256 in 5.1447060108184814 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1099, 0.1963, 0.1585, 0.2001]) \n",
      "Test Loss tensor([0.1066, 0.2065, 0.1504, 0.1951])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3338, 0.5758, 0.0342, 0.2148, 0.6181]) \n",
      "Test Loss tensor([0.3175, 0.5796, 0.0385, 0.2202, 0.6097])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1075, 0.0170, 0.2532, 0.2609, 0.2106, 0.1313, 0.0226]) \n",
      "Test Loss tensor([0.1106, 0.0159, 0.2706, 0.2685, 0.2051, 0.1335, 0.0233])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0486, 0.2356, 0.0459, 0.0953, 0.3266, 0.2980, 0.0521]) \n",
      "Test Loss tensor([0.0397, 0.2323, 0.0459, 0.0944, 0.3396, 0.3039, 0.0467])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0335, 0.0256, 0.0386, 0.0227, 0.0513, 0.0244, 0.1944, 0.0032]) \n",
      "Test Loss tensor([0.0298, 0.0264, 0.0363, 0.0223, 0.0495, 0.0270, 0.1941, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5815, 0.6682, 0.1131, 0.2017, 0.4177, 0.4758])\n",
      "Valid Idx 3 | Loss tensor([0.2854, 0.0762, 0.1560, 0.4942, 0.8532])\n",
      "\n",
      "************** Batch 260 in 5.279122352600098 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1122, 0.2043, 0.1598, 0.2017]) \n",
      "Test Loss tensor([0.1051, 0.2021, 0.1548, 0.1975])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3331, 0.5784, 0.0358, 0.2199, 0.6042]) \n",
      "Test Loss tensor([0.3268, 0.5883, 0.0342, 0.2219, 0.6054])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1156, 0.0163, 0.2654, 0.2482, 0.2041, 0.1379, 0.0241]) \n",
      "Test Loss tensor([0.1067, 0.0171, 0.2722, 0.2731, 0.1985, 0.1342, 0.0212])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0431, 0.2382, 0.0469, 0.1017, 0.3392, 0.3008, 0.0464]) \n",
      "Test Loss tensor([0.0427, 0.2312, 0.0473, 0.0957, 0.3384, 0.3027, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0312, 0.0248, 0.0365, 0.0194, 0.0476, 0.0278, 0.2004, 0.0038]) \n",
      "Test Loss tensor([0.0285, 0.0257, 0.0371, 0.0226, 0.0542, 0.0239, 0.1926, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5804, 0.6463, 0.1091, 0.2102, 0.4113, 0.4876])\n",
      "Valid Idx 3 | Loss tensor([0.3597, 0.0889, 0.1775, 0.5240, 0.8598])\n",
      "Gradients: Input 0.3899354338645935 | Message 0.43211793899536133 | Update 0.5307357311248779 | Output 0.1307426393032074\n",
      "\n",
      "************** Batch 264 in 5.10811710357666 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1120, 0.2108, 0.1631, 0.2055]) \n",
      "Test Loss tensor([0.1037, 0.2027, 0.1502, 0.1913])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3196, 0.5751, 0.0303, 0.2090, 0.6211]) \n",
      "Test Loss tensor([0.3185, 0.5804, 0.0365, 0.2218, 0.6140])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1042, 0.0172, 0.2627, 0.2674, 0.1992, 0.1337, 0.0198]) \n",
      "Test Loss tensor([0.1086, 0.0163, 0.2694, 0.2647, 0.1994, 0.1360, 0.0223])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0393, 0.2326, 0.0500, 0.0928, 0.3335, 0.2976, 0.0493]) \n",
      "Test Loss tensor([0.0407, 0.2302, 0.0464, 0.0953, 0.3441, 0.3026, 0.0466])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0279, 0.0242, 0.0328, 0.0199, 0.0559, 0.0230, 0.1899, 0.0037]) \n",
      "Test Loss tensor([0.0293, 0.0255, 0.0363, 0.0217, 0.0515, 0.0258, 0.1915, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5797, 0.6615, 0.1106, 0.2073, 0.4144, 0.4840])\n",
      "Valid Idx 3 | Loss tensor([0.3157, 0.0811, 0.1649, 0.5125, 0.8534])\n",
      "\n",
      "************** Batch 268 in 5.122842788696289 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1057, 0.1910, 0.1470, 0.1926]) \n",
      "Test Loss tensor([0.1051, 0.2034, 0.1504, 0.1954])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3229, 0.5862, 0.0366, 0.2183, 0.6047]) \n",
      "Test Loss tensor([0.3180, 0.5779, 0.0382, 0.2216, 0.6136])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1000, 0.0165, 0.2814, 0.2713, 0.2026, 0.1333, 0.0237]) \n",
      "Test Loss tensor([0.1095, 0.0161, 0.2697, 0.2638, 0.2005, 0.1317, 0.0230])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0384, 0.2351, 0.0417, 0.0908, 0.3369, 0.2922, 0.0461]) \n",
      "Test Loss tensor([0.0394, 0.2329, 0.0456, 0.0942, 0.3391, 0.3001, 0.0471])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0291, 0.0246, 0.0375, 0.0206, 0.0524, 0.0249, 0.1841, 0.0033]) \n",
      "Test Loss tensor([0.0285, 0.0263, 0.0365, 0.0220, 0.0510, 0.0265, 0.1892, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5774, 0.6633, 0.1137, 0.2054, 0.4158, 0.4830])\n",
      "Valid Idx 3 | Loss tensor([0.2951, 0.0762, 0.1630, 0.5041, 0.8531])\n",
      "\n",
      "************** Batch 272 in 5.140934944152832 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1120, 0.1979, 0.1544, 0.1933]) \n",
      "Test Loss tensor([0.1046, 0.2032, 0.1547, 0.1950])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3270, 0.5877, 0.0385, 0.2157, 0.6064]) \n",
      "Test Loss tensor([0.3202, 0.5789, 0.0348, 0.2219, 0.6100])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1118, 0.0152, 0.2665, 0.2632, 0.1947, 0.1342, 0.0244]) \n",
      "Test Loss tensor([0.1078, 0.0174, 0.2727, 0.2685, 0.1976, 0.1342, 0.0204])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0412, 0.2220, 0.0454, 0.0920, 0.3474, 0.3007, 0.0459]) \n",
      "Test Loss tensor([0.0430, 0.2320, 0.0463, 0.0945, 0.3395, 0.2929, 0.0460])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0308, 0.0313, 0.0383, 0.0217, 0.0499, 0.0246, 0.1862, 0.0036]) \n",
      "Test Loss tensor([0.0293, 0.0268, 0.0372, 0.0234, 0.0561, 0.0238, 0.1872, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5808, 0.6525, 0.1078, 0.2138, 0.4090, 0.4980])\n",
      "Valid Idx 3 | Loss tensor([0.3633, 0.0865, 0.1757, 0.5347, 0.8612])\n",
      "\n",
      "************** Batch 276 in 5.200620412826538 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1049, 0.2016, 0.1476, 0.1997]) \n",
      "Test Loss tensor([0.1049, 0.2024, 0.1464, 0.1937])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3187, 0.5582, 0.0306, 0.2273, 0.6348]) \n",
      "Test Loss tensor([0.3165, 0.5857, 0.0390, 0.2270, 0.6072])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1071, 0.0175, 0.2663, 0.2631, 0.2071, 0.1325, 0.0202]) \n",
      "Test Loss tensor([0.1095, 0.0164, 0.2719, 0.2648, 0.2000, 0.1315, 0.0231])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0420, 0.2301, 0.0467, 0.0928, 0.3336, 0.2893, 0.0462]) \n",
      "Test Loss tensor([0.0404, 0.2314, 0.0450, 0.0929, 0.3340, 0.3007, 0.0462])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0219, 0.0385, 0.0245, 0.0596, 0.0242, 0.1853, 0.0029]) \n",
      "Test Loss tensor([0.0301, 0.0278, 0.0362, 0.0224, 0.0510, 0.0264, 0.1852, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5766, 0.6725, 0.1147, 0.2062, 0.4122, 0.4928])\n",
      "Valid Idx 3 | Loss tensor([0.3053, 0.0770, 0.1646, 0.5126, 0.8600])\n",
      "\n",
      "************** Batch 280 in 5.212312698364258 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1036, 0.2030, 0.1458, 0.1994]) \n",
      "Test Loss tensor([0.1035, 0.2027, 0.1478, 0.1964])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3118, 0.5800, 0.0378, 0.2316, 0.6088]) \n",
      "Test Loss tensor([0.3183, 0.5802, 0.0363, 0.2235, 0.6147])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1029, 0.0166, 0.2793, 0.2757, 0.2082, 0.1270, 0.0238]) \n",
      "Test Loss tensor([0.1097, 0.0170, 0.2736, 0.2639, 0.1978, 0.1327, 0.0210])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0410, 0.2295, 0.0466, 0.0917, 0.3375, 0.2976, 0.0436]) \n",
      "Test Loss tensor([0.0437, 0.2288, 0.0451, 0.0942, 0.3376, 0.2979, 0.0473])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0298, 0.0240, 0.0328, 0.0239, 0.0501, 0.0263, 0.1744, 0.0035]) \n",
      "Test Loss tensor([0.0286, 0.0266, 0.0369, 0.0223, 0.0542, 0.0248, 0.1840, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5858, 0.6630, 0.1090, 0.2098, 0.4099, 0.5027])\n",
      "Valid Idx 3 | Loss tensor([0.3524, 0.0816, 0.1710, 0.5311, 0.8611])\n",
      "Gradients: Input 0.3358325958251953 | Message 0.34638434648513794 | Update 0.4184216856956482 | Output 0.08040770888328552\n",
      "\n",
      "************** Batch 284 in 5.21696925163269 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1071, 0.2034, 0.1532, 0.1995]) \n",
      "Test Loss tensor([0.1026, 0.2048, 0.1493, 0.1934])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3178, 0.5761, 0.0342, 0.2179, 0.6124]) \n",
      "Test Loss tensor([0.3099, 0.5829, 0.0349, 0.2244, 0.6071])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1047, 0.0178, 0.2874, 0.2740, 0.2015, 0.1282, 0.0220]) \n",
      "Test Loss tensor([0.1063, 0.0179, 0.2702, 0.2641, 0.1972, 0.1302, 0.0220])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0419, 0.2368, 0.0464, 0.0990, 0.3388, 0.3066, 0.0488]) \n",
      "Test Loss tensor([0.0428, 0.2294, 0.0451, 0.0927, 0.3334, 0.2973, 0.0459])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0278, 0.0263, 0.0361, 0.0213, 0.0542, 0.0254, 0.1809, 0.0040]) \n",
      "Test Loss tensor([0.0287, 0.0265, 0.0384, 0.0222, 0.0547, 0.0244, 0.1801, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5872, 0.6652, 0.1103, 0.2124, 0.4086, 0.5069])\n",
      "Valid Idx 3 | Loss tensor([0.3622, 0.0813, 0.1736, 0.5342, 0.8565])\n",
      "\n",
      "************** Batch 288 in 5.150402545928955 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1060, 0.1924, 0.1462, 0.1806]) \n",
      "Test Loss tensor([0.1044, 0.2051, 0.1464, 0.1893])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3163, 0.5860, 0.0389, 0.2238, 0.6062]) \n",
      "Test Loss tensor([0.3058, 0.5855, 0.0421, 0.2238, 0.6099])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1061, 0.0185, 0.2955, 0.2710, 0.2019, 0.1351, 0.0194]) \n",
      "Test Loss tensor([0.1072, 0.0161, 0.2698, 0.2585, 0.2037, 0.1274, 0.0229])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0377, 0.2392, 0.0468, 0.0991, 0.3302, 0.2910, 0.0480]) \n",
      "Test Loss tensor([0.0407, 0.2341, 0.0466, 0.0920, 0.3339, 0.2993, 0.0480])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0317, 0.0283, 0.0401, 0.0260, 0.0527, 0.0247, 0.1779, 0.0029]) \n",
      "Test Loss tensor([0.0301, 0.0263, 0.0373, 0.0219, 0.0485, 0.0272, 0.1828, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5841, 0.6870, 0.1142, 0.2065, 0.4183, 0.4937])\n",
      "Valid Idx 3 | Loss tensor([0.3014, 0.0710, 0.1580, 0.5145, 0.8517])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 292 in 5.159897565841675 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1027, 0.2034, 0.1387, 0.1849]) \n",
      "Test Loss tensor([0.1012, 0.2036, 0.1493, 0.1875])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3151, 0.5845, 0.0386, 0.2257, 0.6114]) \n",
      "Test Loss tensor([0.3107, 0.5817, 0.0353, 0.2225, 0.6104])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1069, 0.0163, 0.2612, 0.2561, 0.1991, 0.1300, 0.0220]) \n",
      "Test Loss tensor([0.1065, 0.0172, 0.2684, 0.2635, 0.1988, 0.1278, 0.0214])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0428, 0.2412, 0.0525, 0.0961, 0.3348, 0.3016, 0.0511]) \n",
      "Test Loss tensor([0.0434, 0.2301, 0.0457, 0.0964, 0.3311, 0.2950, 0.0458])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0318, 0.0253, 0.0382, 0.0201, 0.0460, 0.0275, 0.1869, 0.0031]) \n",
      "Test Loss tensor([0.0289, 0.0269, 0.0382, 0.0225, 0.0526, 0.0233, 0.1809, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5833, 0.6655, 0.1098, 0.2132, 0.4097, 0.4989])\n",
      "Valid Idx 3 | Loss tensor([0.3627, 0.0774, 0.1692, 0.5545, 0.8600])\n",
      "\n",
      "************** Batch 296 in 5.229125499725342 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0973, 0.1957, 0.1533, 0.1890]) \n",
      "Test Loss tensor([0.1001, 0.2038, 0.1484, 0.1903])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3178, 0.5756, 0.0339, 0.2139, 0.6123]) \n",
      "Test Loss tensor([0.3058, 0.5838, 0.0363, 0.2215, 0.6090])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1035, 0.0165, 0.2636, 0.2631, 0.1982, 0.1358, 0.0214]) \n",
      "Test Loss tensor([0.1048, 0.0164, 0.2704, 0.2670, 0.2004, 0.1302, 0.0210])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0441, 0.2351, 0.0490, 0.0961, 0.3297, 0.2926, 0.0421]) \n",
      "Test Loss tensor([0.0421, 0.2307, 0.0460, 0.0946, 0.3296, 0.2962, 0.0482])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0293, 0.0265, 0.0431, 0.0205, 0.0534, 0.0238, 0.2001, 0.0033]) \n",
      "Test Loss tensor([0.0296, 0.0267, 0.0381, 0.0229, 0.0509, 0.0235, 0.1783, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5797, 0.6716, 0.1117, 0.2117, 0.4081, 0.4953])\n",
      "Valid Idx 3 | Loss tensor([0.3417, 0.0723, 0.1638, 0.5534, 0.8591])\n",
      "\n",
      "************** Batch 300 in 5.264644622802734 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1047, 0.2147, 0.1540, 0.1944]) \n",
      "Test Loss tensor([0.1067, 0.2051, 0.1501, 0.1905])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3063, 0.5879, 0.0337, 0.2128, 0.6125]) \n",
      "Test Loss tensor([0.2998, 0.5858, 0.0429, 0.2228, 0.6098])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0922, 0.0159, 0.2792, 0.2669, 0.1980, 0.1231, 0.0198]) \n",
      "Test Loss tensor([0.1102, 0.0152, 0.2720, 0.2587, 0.2036, 0.1282, 0.0246])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0416, 0.2335, 0.0446, 0.0935, 0.3414, 0.2936, 0.0468]) \n",
      "Test Loss tensor([0.0385, 0.2335, 0.0462, 0.0938, 0.3361, 0.3002, 0.0480])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0274, 0.0234, 0.0336, 0.0211, 0.0500, 0.0246, 0.1747, 0.0029]) \n",
      "Test Loss tensor([0.0312, 0.0269, 0.0356, 0.0219, 0.0434, 0.0268, 0.1801, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5785, 0.6959, 0.1141, 0.2034, 0.4178, 0.4748])\n",
      "Valid Idx 3 | Loss tensor([0.2665, 0.0639, 0.1473, 0.5303, 0.8528])\n",
      "Gradients: Input 0.24137815833091736 | Message 0.2432892918586731 | Update 0.31334954500198364 | Output 0.05364720895886421\n",
      "\n",
      "************** Batch 304 in 5.391831398010254 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1116, 0.1982, 0.1535, 0.2000]) \n",
      "Test Loss tensor([0.1009, 0.2019, 0.1559, 0.1911])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3000, 0.5833, 0.0421, 0.2252, 0.6124]) \n",
      "Test Loss tensor([0.3095, 0.5798, 0.0297, 0.2218, 0.6144])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1045, 0.0152, 0.2766, 0.2627, 0.2167, 0.1318, 0.0222]) \n",
      "Test Loss tensor([0.1016, 0.0190, 0.2753, 0.2724, 0.1980, 0.1351, 0.0177])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0393, 0.2186, 0.0458, 0.0875, 0.3444, 0.2959, 0.0455]) \n",
      "Test Loss tensor([0.0448, 0.2269, 0.0453, 0.0991, 0.3354, 0.2903, 0.0465])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0324, 0.0250, 0.0398, 0.0216, 0.0443, 0.0276, 0.1824, 0.0036]) \n",
      "Test Loss tensor([0.0277, 0.0252, 0.0383, 0.0231, 0.0575, 0.0191, 0.1811, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5850, 0.6446, 0.1068, 0.2258, 0.4074, 0.5080])\n",
      "Valid Idx 3 | Loss tensor([0.4395, 0.0871, 0.1903, 0.6259, 0.8672])\n",
      "\n",
      "************** Batch 308 in 5.115848541259766 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0940, 0.1982, 0.1583, 0.1886]) \n",
      "Test Loss tensor([0.1055, 0.2036, 0.1461, 0.1862])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3030, 0.5788, 0.0289, 0.2140, 0.6260]) \n",
      "Test Loss tensor([0.2945, 0.5871, 0.0403, 0.2237, 0.6091])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1076, 0.0192, 0.2732, 0.2693, 0.2007, 0.1390, 0.0166]) \n",
      "Test Loss tensor([0.1123, 0.0152, 0.2731, 0.2625, 0.2081, 0.1277, 0.0228])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0460, 0.2205, 0.0438, 0.0994, 0.3420, 0.2874, 0.0428]) \n",
      "Test Loss tensor([0.0368, 0.2323, 0.0453, 0.0917, 0.3360, 0.2985, 0.0482])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0290, 0.0408, 0.0208, 0.0585, 0.0196, 0.1855, 0.0034]) \n",
      "Test Loss tensor([0.0311, 0.0264, 0.0369, 0.0224, 0.0430, 0.0252, 0.1782, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5774, 0.7041, 0.1138, 0.2030, 0.4150, 0.4728])\n",
      "Valid Idx 3 | Loss tensor([0.2645, 0.0674, 0.1500, 0.5379, 0.8583])\n",
      "\n",
      "************** Batch 312 in 5.077541828155518 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1152, 0.2106, 0.1506, 0.1971]) \n",
      "Test Loss tensor([0.1006, 0.2028, 0.1465, 0.1862])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3032, 0.5867, 0.0416, 0.2295, 0.6018]) \n",
      "Test Loss tensor([0.2958, 0.5876, 0.0358, 0.2243, 0.6075])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1139, 0.0159, 0.2576, 0.2632, 0.2117, 0.1257, 0.0216]) \n",
      "Test Loss tensor([0.1085, 0.0158, 0.2679, 0.2619, 0.2032, 0.1259, 0.0213])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0370, 0.2301, 0.0449, 0.0878, 0.3408, 0.2968, 0.0427]) \n",
      "Test Loss tensor([0.0396, 0.2317, 0.0457, 0.0923, 0.3318, 0.2981, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0265, 0.0242, 0.0358, 0.0175, 0.0448, 0.0261, 0.1681, 0.0033]) \n",
      "Test Loss tensor([0.0307, 0.0268, 0.0379, 0.0219, 0.0454, 0.0236, 0.1765, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5848, 0.6948, 0.1142, 0.2047, 0.4173, 0.4887])\n",
      "Valid Idx 3 | Loss tensor([0.3203, 0.0733, 0.1588, 0.5629, 0.8573])\n",
      "\n",
      "************** Batch 316 in 5.107700824737549 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1072, 0.2040, 0.1439, 0.1905]) \n",
      "Test Loss tensor([0.0975, 0.2008, 0.1517, 0.1915])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3047, 0.5749, 0.0367, 0.2091, 0.6093]) \n",
      "Test Loss tensor([0.3028, 0.5803, 0.0294, 0.2257, 0.6156])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1061, 0.0165, 0.2805, 0.2576, 0.2022, 0.1319, 0.0199]) \n",
      "Test Loss tensor([0.1014, 0.0184, 0.2711, 0.2716, 0.1950, 0.1299, 0.0183])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0395, 0.2354, 0.0450, 0.0960, 0.3245, 0.2837, 0.0440]) \n",
      "Test Loss tensor([0.0441, 0.2247, 0.0473, 0.0948, 0.3310, 0.2935, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0325, 0.0268, 0.0356, 0.0235, 0.0452, 0.0230, 0.1684, 0.0030]) \n",
      "Test Loss tensor([0.0281, 0.0250, 0.0392, 0.0233, 0.0542, 0.0192, 0.1745, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5897, 0.6611, 0.1046, 0.2219, 0.4010, 0.5071])\n",
      "Valid Idx 3 | Loss tensor([0.4356, 0.0873, 0.1867, 0.6143, 0.8658])\n",
      "\n",
      "************** Batch 320 in 5.116376161575317 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0924, 0.1994, 0.1407, 0.1898]) \n",
      "Test Loss tensor([0.1023, 0.2036, 0.1412, 0.1834])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.3147, 0.5851, 0.0271, 0.2262, 0.6234]) \n",
      "Test Loss tensor([0.2914, 0.5850, 0.0430, 0.2277, 0.6121])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1068, 0.0191, 0.2883, 0.2710, 0.1970, 0.1300, 0.0162]) \n",
      "Test Loss tensor([0.1137, 0.0154, 0.2678, 0.2578, 0.2088, 0.1246, 0.0259])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0399, 0.2235, 0.0459, 0.0982, 0.3348, 0.2908, 0.0510]) \n",
      "Test Loss tensor([0.0381, 0.2297, 0.0449, 0.0920, 0.3268, 0.2963, 0.0460])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0267, 0.0240, 0.0400, 0.0237, 0.0543, 0.0195, 0.1745, 0.0031]) \n",
      "Test Loss tensor([0.0322, 0.0268, 0.0364, 0.0210, 0.0426, 0.0264, 0.1744, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5784, 0.7098, 0.1145, 0.2015, 0.4170, 0.4796])\n",
      "Valid Idx 3 | Loss tensor([0.2746, 0.0662, 0.1492, 0.5341, 0.8497])\n",
      "Gradients: Input 0.9984943866729736 | Message 1.0729261636734009 | Update 1.3384225368499756 | Output 0.28869691491127014\n",
      "\n",
      "************** Batch 324 in 5.026423215866089 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1086, 0.2059, 0.1447, 0.1923]) \n",
      "Test Loss tensor([0.0960, 0.2014, 0.1462, 0.1877])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2911, 0.5858, 0.0458, 0.2244, 0.6181]) \n",
      "Test Loss tensor([0.2921, 0.5790, 0.0356, 0.2260, 0.6164])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1124, 0.0151, 0.2812, 0.2565, 0.2174, 0.1274, 0.0276]) \n",
      "Test Loss tensor([0.1044, 0.0176, 0.2692, 0.2631, 0.1979, 0.1246, 0.0218])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0403, 0.2364, 0.0495, 0.0925, 0.3275, 0.2918, 0.0430]) \n",
      "Test Loss tensor([0.0430, 0.2269, 0.0450, 0.0936, 0.3309, 0.2902, 0.0463])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0321, 0.0301, 0.0413, 0.0223, 0.0439, 0.0264, 0.1668, 0.0034]) \n",
      "Test Loss tensor([0.0292, 0.0272, 0.0370, 0.0224, 0.0501, 0.0218, 0.1698, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5894, 0.6740, 0.1094, 0.2131, 0.4106, 0.5002])\n",
      "Valid Idx 3 | Loss tensor([0.3907, 0.0793, 0.1750, 0.5974, 0.8593])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 328 in 5.042371988296509 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0917, 0.1994, 0.1402, 0.1818]) \n",
      "Test Loss tensor([0.0945, 0.2019, 0.1484, 0.1874])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2963, 0.5966, 0.0334, 0.2267, 0.6002]) \n",
      "Test Loss tensor([0.2907, 0.5804, 0.0331, 0.2276, 0.6126])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1039, 0.0175, 0.2692, 0.2588, 0.1982, 0.1227, 0.0236]) \n",
      "Test Loss tensor([0.1037, 0.0188, 0.2710, 0.2660, 0.1974, 0.1276, 0.0205])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0452, 0.2340, 0.0467, 0.1049, 0.3316, 0.2980, 0.0432]) \n",
      "Test Loss tensor([0.0459, 0.2290, 0.0451, 0.0959, 0.3304, 0.2862, 0.0482])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0275, 0.0230, 0.0392, 0.0211, 0.0542, 0.0230, 0.1607, 0.0033]) \n",
      "Test Loss tensor([0.0283, 0.0268, 0.0400, 0.0223, 0.0524, 0.0203, 0.1697, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5877, 0.6642, 0.1120, 0.2175, 0.4082, 0.5027])\n",
      "Valid Idx 3 | Loss tensor([0.4267, 0.0856, 0.1831, 0.6219, 0.8638])\n",
      "\n",
      "************** Batch 332 in 5.167400360107422 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0970, 0.2044, 0.1449, 0.1789]) \n",
      "Test Loss tensor([0.1047, 0.2071, 0.1416, 0.1832])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2922, 0.5746, 0.0330, 0.2279, 0.6185]) \n",
      "Test Loss tensor([0.2843, 0.5859, 0.0475, 0.2305, 0.6066])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1034, 0.0186, 0.2842, 0.2738, 0.1935, 0.1292, 0.0220]) \n",
      "Test Loss tensor([0.1144, 0.0148, 0.2659, 0.2562, 0.2097, 0.1221, 0.0273])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0450, 0.2335, 0.0474, 0.0944, 0.3274, 0.2870, 0.0506]) \n",
      "Test Loss tensor([0.0388, 0.2321, 0.0448, 0.0905, 0.3308, 0.2942, 0.0472])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0270, 0.0249, 0.0406, 0.0228, 0.0537, 0.0208, 0.1724, 0.0033]) \n",
      "Test Loss tensor([0.0338, 0.0277, 0.0372, 0.0222, 0.0402, 0.0281, 0.1690, 0.0037])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5795, 0.7213, 0.1140, 0.1954, 0.4155, 0.4715])\n",
      "Valid Idx 3 | Loss tensor([0.2617, 0.0636, 0.1414, 0.5427, 0.8510])\n",
      "\n",
      "************** Batch 336 in 5.1375648975372314 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1029, 0.2064, 0.1395, 0.1831]) \n",
      "Test Loss tensor([0.0947, 0.1995, 0.1506, 0.1906])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2894, 0.5716, 0.0474, 0.2325, 0.6262]) \n",
      "Test Loss tensor([0.2948, 0.5764, 0.0303, 0.2251, 0.6156])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1182, 0.0152, 0.2701, 0.2578, 0.2105, 0.1197, 0.0287]) \n",
      "Test Loss tensor([0.1029, 0.0184, 0.2736, 0.2723, 0.1961, 0.1279, 0.0191])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0377, 0.2364, 0.0442, 0.0927, 0.3265, 0.2901, 0.0469]) \n",
      "Test Loss tensor([0.0451, 0.2242, 0.0461, 0.0965, 0.3315, 0.2863, 0.0473])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0344, 0.0251, 0.0368, 0.0235, 0.0396, 0.0287, 0.1737, 0.0033]) \n",
      "Test Loss tensor([0.0295, 0.0257, 0.0385, 0.0222, 0.0509, 0.0198, 0.1701, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5902, 0.6684, 0.1079, 0.2145, 0.4034, 0.5018])\n",
      "Valid Idx 3 | Loss tensor([0.4444, 0.0870, 0.1847, 0.6294, 0.8704])\n",
      "\n",
      "************** Batch 340 in 5.092486143112183 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0957, 0.2011, 0.1491, 0.1911]) \n",
      "Test Loss tensor([0.0972, 0.1974, 0.1482, 0.1862])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2947, 0.5891, 0.0311, 0.2281, 0.6033]) \n",
      "Test Loss tensor([0.2876, 0.5824, 0.0316, 0.2300, 0.6125])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1060, 0.0193, 0.2665, 0.2600, 0.1938, 0.1244, 0.0221]) \n",
      "Test Loss tensor([0.1027, 0.0171, 0.2758, 0.2679, 0.1978, 0.1240, 0.0195])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0463, 0.2350, 0.0464, 0.0978, 0.3248, 0.2873, 0.0468]) \n",
      "Test Loss tensor([0.0428, 0.2278, 0.0473, 0.0935, 0.3300, 0.2903, 0.0495])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0330, 0.0268, 0.0418, 0.0223, 0.0471, 0.0197, 0.1710, 0.0029]) \n",
      "Test Loss tensor([0.0298, 0.0257, 0.0387, 0.0215, 0.0457, 0.0214, 0.1660, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5942, 0.6916, 0.1093, 0.2074, 0.4104, 0.4939])\n",
      "Valid Idx 3 | Loss tensor([0.4037, 0.0821, 0.1728, 0.6019, 0.8606])\n",
      "Gradients: Input 0.8203022480010986 | Message 0.9581931233406067 | Update 1.1567628383636475 | Output 0.2548709511756897\n",
      "\n",
      "************** Batch 344 in 5.155695199966431 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0976, 0.1964, 0.1546, 0.1853]) \n",
      "Test Loss tensor([0.1022, 0.2057, 0.1414, 0.1805])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2872, 0.5865, 0.0327, 0.2188, 0.6111]) \n",
      "Test Loss tensor([0.2732, 0.5824, 0.0424, 0.2303, 0.6119])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1011, 0.0176, 0.2756, 0.2717, 0.1945, 0.1292, 0.0207]) \n",
      "Test Loss tensor([0.1133, 0.0146, 0.2713, 0.2562, 0.2084, 0.1233, 0.0252])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0404, 0.2254, 0.0439, 0.0937, 0.3373, 0.2998, 0.0502]) \n",
      "Test Loss tensor([0.0364, 0.2329, 0.0433, 0.0883, 0.3315, 0.3003, 0.0454])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0281, 0.0260, 0.0379, 0.0201, 0.0448, 0.0217, 0.1674, 0.0032]) \n",
      "Test Loss tensor([0.0355, 0.0281, 0.0363, 0.0221, 0.0370, 0.0282, 0.1721, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5792, 0.7470, 0.1136, 0.1907, 0.4229, 0.4644])\n",
      "Valid Idx 3 | Loss tensor([0.2668, 0.0659, 0.1458, 0.5290, 0.8527])\n",
      "\n",
      "************** Batch 348 in 5.108811855316162 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1043, 0.2061, 0.1421, 0.1884]) \n",
      "Test Loss tensor([0.0950, 0.2012, 0.1532, 0.1889])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2806, 0.5873, 0.0440, 0.2188, 0.6068]) \n",
      "Test Loss tensor([0.2919, 0.5845, 0.0283, 0.2298, 0.6123])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1028, 0.0147, 0.2685, 0.2601, 0.2047, 0.1205, 0.0223]) \n",
      "Test Loss tensor([0.1009, 0.0189, 0.2748, 0.2783, 0.1950, 0.1274, 0.0165])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0358, 0.2305, 0.0422, 0.0848, 0.3242, 0.3004, 0.0418]) \n",
      "Test Loss tensor([0.0447, 0.2294, 0.0454, 0.0952, 0.3291, 0.2852, 0.0484])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0329, 0.0279, 0.0365, 0.0220, 0.0371, 0.0299, 0.1701, 0.0032]) \n",
      "Test Loss tensor([0.0288, 0.0250, 0.0396, 0.0230, 0.0519, 0.0197, 0.1612, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5929, 0.6796, 0.1077, 0.2184, 0.4046, 0.5064])\n",
      "Valid Idx 3 | Loss tensor([0.4971, 0.0954, 0.1918, 0.6488, 0.8716])\n",
      "\n",
      "************** Batch 352 in 5.157098054885864 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0941, 0.1915, 0.1530, 0.1935]) \n",
      "Test Loss tensor([0.0949, 0.2028, 0.1475, 0.1872])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2939, 0.5716, 0.0275, 0.2196, 0.6060]) \n",
      "Test Loss tensor([0.2814, 0.5761, 0.0340, 0.2249, 0.6174])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0955, 0.0185, 0.2748, 0.2795, 0.1967, 0.1255, 0.0195]) \n",
      "Test Loss tensor([0.1063, 0.0168, 0.2763, 0.2691, 0.2002, 0.1254, 0.0201])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0497, 0.2241, 0.0452, 0.1001, 0.3316, 0.2919, 0.0448]) \n",
      "Test Loss tensor([0.0413, 0.2326, 0.0449, 0.0925, 0.3258, 0.2907, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0224, 0.0423, 0.0232, 0.0522, 0.0183, 0.1626, 0.0033]) \n",
      "Test Loss tensor([0.0312, 0.0271, 0.0394, 0.0225, 0.0456, 0.0227, 0.1620, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5852, 0.7048, 0.1078, 0.2042, 0.4128, 0.4839])\n",
      "Valid Idx 3 | Loss tensor([0.4042, 0.0771, 0.1682, 0.6109, 0.8636])\n",
      "\n",
      "************** Batch 356 in 4.5731916427612305 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0960, 0.2011, 0.1445, 0.1878]) \n",
      "Test Loss tensor([0.0998, 0.2071, 0.1402, 0.1742])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2792, 0.5746, 0.0380, 0.2140, 0.6257]) \n",
      "Test Loss tensor([0.2711, 0.5828, 0.0478, 0.2263, 0.6116])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0998, 0.0173, 0.2625, 0.2748, 0.1980, 0.1258, 0.0187]) \n",
      "Test Loss tensor([0.1165, 0.0150, 0.2724, 0.2541, 0.2096, 0.1226, 0.0277])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0425, 0.2382, 0.0485, 0.0964, 0.3267, 0.2900, 0.0491]) \n",
      "Test Loss tensor([0.0363, 0.2307, 0.0428, 0.0879, 0.3282, 0.2924, 0.0455])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0283, 0.0257, 0.0359, 0.0236, 0.0465, 0.0224, 0.1599, 0.0030]) \n",
      "Test Loss tensor([0.0347, 0.0283, 0.0370, 0.0213, 0.0391, 0.0293, 0.1621, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5781, 0.7383, 0.1144, 0.1932, 0.4183, 0.4582])\n",
      "Valid Idx 3 | Loss tensor([0.2887, 0.0630, 0.1454, 0.5583, 0.8495])\n",
      "\n",
      "************** Batch 360 in 4.792149782180786 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1035, 0.1990, 0.1391, 0.1844]) \n",
      "Test Loss tensor([0.0913, 0.2002, 0.1491, 0.1816])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2903, 0.5974, 0.0471, 0.2394, 0.5964]) \n",
      "Test Loss tensor([0.2868, 0.5820, 0.0323, 0.2282, 0.6082])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1106, 0.0147, 0.2629, 0.2560, 0.2071, 0.1229, 0.0249]) \n",
      "Test Loss tensor([0.1037, 0.0188, 0.2773, 0.2693, 0.1971, 0.1282, 0.0203])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0335, 0.2255, 0.0455, 0.0888, 0.3252, 0.2930, 0.0552]) \n",
      "Test Loss tensor([0.0443, 0.2280, 0.0461, 0.0933, 0.3226, 0.2856, 0.0470])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0324, 0.0283, 0.0370, 0.0217, 0.0394, 0.0315, 0.1579, 0.0040]) \n",
      "Test Loss tensor([0.0292, 0.0265, 0.0395, 0.0215, 0.0521, 0.0211, 0.1588, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5959, 0.6808, 0.1071, 0.2151, 0.4073, 0.4935])\n",
      "Valid Idx 3 | Loss tensor([0.4789, 0.0852, 0.1873, 0.6660, 0.8656])\n",
      "Gradients: Input 1.22959566116333 | Message 1.4113154411315918 | Update 1.7266950607299805 | Output 0.3762633800506592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 364 in 5.805169343948364 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0908, 0.1951, 0.1517, 0.1827]) \n",
      "Test Loss tensor([0.0915, 0.2031, 0.1443, 0.1797])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2927, 0.5808, 0.0291, 0.2276, 0.6081]) \n",
      "Test Loss tensor([0.2769, 0.5809, 0.0354, 0.2264, 0.6112])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1032, 0.0183, 0.2825, 0.2724, 0.2001, 0.1286, 0.0208]) \n",
      "Test Loss tensor([0.1029, 0.0176, 0.2733, 0.2624, 0.1970, 0.1213, 0.0215])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0418, 0.2247, 0.0447, 0.0930, 0.3255, 0.2809, 0.0411]) \n",
      "Test Loss tensor([0.0420, 0.2273, 0.0455, 0.0929, 0.3216, 0.2902, 0.0472])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0287, 0.0285, 0.0376, 0.0242, 0.0534, 0.0205, 0.1585, 0.0034]) \n",
      "Test Loss tensor([0.0308, 0.0269, 0.0394, 0.0214, 0.0474, 0.0230, 0.1582, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5870, 0.6963, 0.1089, 0.2093, 0.4096, 0.4828])\n",
      "Valid Idx 3 | Loss tensor([0.4198, 0.0764, 0.1666, 0.6346, 0.8655])\n",
      "\n",
      "************** Batch 368 in 5.796479940414429 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0871, 0.1992, 0.1527, 0.1800]) \n",
      "Test Loss tensor([0.0984, 0.2039, 0.1369, 0.1749])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2805, 0.5824, 0.0380, 0.2221, 0.5976]) \n",
      "Test Loss tensor([0.2702, 0.5860, 0.0454, 0.2322, 0.6086])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1067, 0.0174, 0.2850, 0.2579, 0.1983, 0.1201, 0.0223]) \n",
      "Test Loss tensor([0.1125, 0.0146, 0.2720, 0.2574, 0.2082, 0.1201, 0.0270])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0403, 0.2367, 0.0394, 0.0886, 0.3263, 0.2884, 0.0411]) \n",
      "Test Loss tensor([0.0382, 0.2330, 0.0442, 0.0893, 0.3263, 0.2954, 0.0482])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0260, 0.0277, 0.0380, 0.0196, 0.0489, 0.0229, 0.1540, 0.0031]) \n",
      "Test Loss tensor([0.0344, 0.0280, 0.0392, 0.0226, 0.0392, 0.0282, 0.1607, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5805, 0.7346, 0.1174, 0.1951, 0.4185, 0.4625])\n",
      "Valid Idx 3 | Loss tensor([0.2959, 0.0636, 0.1491, 0.5627, 0.8522])\n",
      "\n",
      "************** Batch 372 in 5.4038612842559814 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0981, 0.1990, 0.1385, 0.1850]) \n",
      "Test Loss tensor([0.0915, 0.2009, 0.1449, 0.1839])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2618, 0.5819, 0.0451, 0.2284, 0.6066]) \n",
      "Test Loss tensor([0.2796, 0.5831, 0.0319, 0.2247, 0.6094])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1059, 0.0139, 0.2719, 0.2570, 0.2024, 0.1131, 0.0269]) \n",
      "Test Loss tensor([0.1007, 0.0176, 0.2727, 0.2701, 0.1990, 0.1216, 0.0186])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0369, 0.2379, 0.0412, 0.0840, 0.3229, 0.2844, 0.0420]) \n",
      "Test Loss tensor([0.0441, 0.2271, 0.0449, 0.0946, 0.3250, 0.2795, 0.0474])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0366, 0.0271, 0.0363, 0.0188, 0.0393, 0.0285, 0.1533, 0.0037]) \n",
      "Test Loss tensor([0.0297, 0.0265, 0.0400, 0.0232, 0.0489, 0.0198, 0.1588, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5946, 0.6813, 0.1080, 0.2133, 0.4073, 0.4921])\n",
      "Valid Idx 3 | Loss tensor([0.4728, 0.0839, 0.1778, 0.6514, 0.8635])\n",
      "\n",
      "************** Batch 376 in 5.448711156845093 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0870, 0.1998, 0.1367, 0.1795]) \n",
      "Test Loss tensor([0.0916, 0.2000, 0.1423, 0.1806])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2870, 0.5852, 0.0305, 0.2298, 0.6122]) \n",
      "Test Loss tensor([0.2749, 0.5819, 0.0327, 0.2209, 0.6125])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1029, 0.0177, 0.2749, 0.2675, 0.1948, 0.1237, 0.0178]) \n",
      "Test Loss tensor([0.1021, 0.0160, 0.2746, 0.2659, 0.2015, 0.1207, 0.0199])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0429, 0.2360, 0.0463, 0.0901, 0.3218, 0.2884, 0.0436]) \n",
      "Test Loss tensor([0.0426, 0.2274, 0.0442, 0.0930, 0.3230, 0.2880, 0.0481])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0314, 0.0228, 0.0421, 0.0243, 0.0488, 0.0200, 0.1628, 0.0029]) \n",
      "Test Loss tensor([0.0293, 0.0265, 0.0403, 0.0218, 0.0445, 0.0213, 0.1565, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5913, 0.6984, 0.1090, 0.2087, 0.4085, 0.4891])\n",
      "Valid Idx 3 | Loss tensor([0.4147, 0.0772, 0.1650, 0.6264, 0.8639])\n",
      "\n",
      "************** Batch 380 in 5.335860967636108 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0956, 0.2040, 0.1462, 0.1818]) \n",
      "Test Loss tensor([0.0966, 0.2025, 0.1376, 0.1795])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2754, 0.5994, 0.0321, 0.2209, 0.6053]) \n",
      "Test Loss tensor([0.2664, 0.5855, 0.0403, 0.2255, 0.6098])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1030, 0.0163, 0.2660, 0.2638, 0.2043, 0.1273, 0.0194]) \n",
      "Test Loss tensor([0.1104, 0.0140, 0.2673, 0.2554, 0.2086, 0.1197, 0.0237])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0422, 0.2305, 0.0462, 0.0878, 0.3174, 0.2808, 0.0429]) \n",
      "Test Loss tensor([0.0364, 0.2307, 0.0448, 0.0890, 0.3257, 0.2894, 0.0473])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0293, 0.0268, 0.0410, 0.0210, 0.0444, 0.0221, 0.1624, 0.0030]) \n",
      "Test Loss tensor([0.0344, 0.0288, 0.0386, 0.0212, 0.0368, 0.0262, 0.1605, 0.0036])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5900, 0.7356, 0.1119, 0.1917, 0.4129, 0.4658])\n",
      "Valid Idx 3 | Loss tensor([0.2899, 0.0638, 0.1418, 0.5585, 0.8538])\n",
      "Gradients: Input 0.3560897409915924 | Message 0.36759650707244873 | Update 0.45947766304016113 | Output 0.0866028368473053\n",
      "\n",
      "************** Batch 384 in 5.286238431930542 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.1001, 0.2017, 0.1347, 0.1823]) \n",
      "Test Loss tensor([0.0903, 0.2022, 0.1416, 0.1815])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2823, 0.5853, 0.0401, 0.2311, 0.5957]) \n",
      "Test Loss tensor([0.2790, 0.5862, 0.0313, 0.2284, 0.6094])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1135, 0.0152, 0.2716, 0.2555, 0.2038, 0.1204, 0.0264]) \n",
      "Test Loss tensor([0.1029, 0.0168, 0.2769, 0.2675, 0.2006, 0.1196, 0.0187])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0390, 0.2243, 0.0400, 0.0843, 0.3188, 0.2854, 0.0413]) \n",
      "Test Loss tensor([0.0433, 0.2286, 0.0448, 0.0936, 0.3212, 0.2868, 0.0467])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0317, 0.0248, 0.0399, 0.0246, 0.0375, 0.0272, 0.1570, 0.0036]) \n",
      "Test Loss tensor([0.0296, 0.0264, 0.0393, 0.0222, 0.0440, 0.0198, 0.1532, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5946, 0.6944, 0.1082, 0.2093, 0.4086, 0.4961])\n",
      "Valid Idx 3 | Loss tensor([0.4283, 0.0782, 0.1663, 0.6326, 0.8635])\n",
      "\n",
      "************** Batch 388 in 5.287663459777832 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0892, 0.1962, 0.1465, 0.1757]) \n",
      "Test Loss tensor([0.0907, 0.2006, 0.1433, 0.1825])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2773, 0.5854, 0.0294, 0.2310, 0.6165]) \n",
      "Test Loss tensor([0.2732, 0.5784, 0.0314, 0.2230, 0.6117])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0950, 0.0162, 0.2679, 0.2708, 0.1964, 0.1184, 0.0193]) \n",
      "Test Loss tensor([0.1000, 0.0164, 0.2735, 0.2657, 0.1972, 0.1198, 0.0194])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0403, 0.2151, 0.0471, 0.1021, 0.3207, 0.2868, 0.0553]) \n",
      "Test Loss tensor([0.0426, 0.2289, 0.0453, 0.0944, 0.3220, 0.2876, 0.0470])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0332, 0.0286, 0.0441, 0.0249, 0.0450, 0.0188, 0.1570, 0.0036]) \n",
      "Test Loss tensor([0.0302, 0.0266, 0.0405, 0.0219, 0.0438, 0.0190, 0.1526, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5940, 0.6958, 0.1044, 0.2092, 0.4072, 0.4904])\n",
      "Valid Idx 3 | Loss tensor([0.4278, 0.0804, 0.1678, 0.6423, 0.8659])\n",
      "\n",
      "************** Batch 392 in 5.329598426818848 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0918, 0.2044, 0.1434, 0.1795]) \n",
      "Test Loss tensor([0.0925, 0.2009, 0.1332, 0.1747])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2744, 0.5894, 0.0300, 0.2203, 0.6144]) \n",
      "Test Loss tensor([0.2654, 0.5806, 0.0413, 0.2267, 0.6132])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1008, 0.0155, 0.2634, 0.2655, 0.1971, 0.1159, 0.0197]) \n",
      "Test Loss tensor([0.1107, 0.0146, 0.2679, 0.2570, 0.2050, 0.1168, 0.0250])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0434, 0.2247, 0.0454, 0.0895, 0.3163, 0.2858, 0.0489]) \n",
      "Test Loss tensor([0.0387, 0.2308, 0.0437, 0.0919, 0.3241, 0.2901, 0.0477])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0298, 0.0251, 0.0461, 0.0232, 0.0458, 0.0195, 0.1523, 0.0033]) \n",
      "Test Loss tensor([0.0337, 0.0284, 0.0386, 0.0217, 0.0382, 0.0240, 0.1552, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5884, 0.7229, 0.1077, 0.1971, 0.4113, 0.4729])\n",
      "Valid Idx 3 | Loss tensor([0.3089, 0.0642, 0.1464, 0.5836, 0.8586])\n",
      "\n",
      "************** Batch 396 in 5.466248035430908 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0942, 0.2007, 0.1322, 0.1813]) \n",
      "Test Loss tensor([0.0920, 0.2024, 0.1357, 0.1763])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2619, 0.5709, 0.0397, 0.2235, 0.6233]) \n",
      "Test Loss tensor([0.2671, 0.5809, 0.0365, 0.2275, 0.6086])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1115, 0.0146, 0.2685, 0.2677, 0.2098, 0.1134, 0.0273]) \n",
      "Test Loss tensor([0.1033, 0.0162, 0.2735, 0.2608, 0.1977, 0.1191, 0.0223])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0350, 0.2300, 0.0428, 0.0898, 0.3199, 0.2972, 0.0436]) \n",
      "Test Loss tensor([0.0427, 0.2270, 0.0445, 0.0950, 0.3211, 0.2837, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0321, 0.0266, 0.0345, 0.0217, 0.0383, 0.0260, 0.1576, 0.0034]) \n",
      "Test Loss tensor([0.0313, 0.0278, 0.0402, 0.0220, 0.0442, 0.0201, 0.1535, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5907, 0.6894, 0.1099, 0.2066, 0.4060, 0.4919])\n",
      "Valid Idx 3 | Loss tensor([0.3919, 0.0727, 0.1643, 0.6408, 0.8635])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 400 in 5.38567328453064 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0875, 0.2022, 0.1400, 0.1730]) \n",
      "Test Loss tensor([0.0868, 0.2013, 0.1353, 0.1774])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2758, 0.5646, 0.0350, 0.2339, 0.6287]) \n",
      "Test Loss tensor([0.2671, 0.5792, 0.0352, 0.2333, 0.6107])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1027, 0.0159, 0.2699, 0.2601, 0.1964, 0.1202, 0.0218]) \n",
      "Test Loss tensor([0.1021, 0.0166, 0.2793, 0.2614, 0.1971, 0.1211, 0.0227])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0410, 0.2230, 0.0435, 0.0950, 0.3343, 0.2771, 0.0425]) \n",
      "Test Loss tensor([0.0440, 0.2327, 0.0450, 0.0937, 0.3217, 0.2825, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0342, 0.0283, 0.0411, 0.0250, 0.0428, 0.0203, 0.1558, 0.0031]) \n",
      "Test Loss tensor([0.0295, 0.0262, 0.0409, 0.0219, 0.0465, 0.0194, 0.1521, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5898, 0.6807, 0.1081, 0.2135, 0.4053, 0.4926])\n",
      "Valid Idx 3 | Loss tensor([0.4162, 0.0783, 0.1708, 0.6586, 0.8638])\n",
      "Gradients: Input 0.30019158124923706 | Message 0.3336939215660095 | Update 0.3841397166252136 | Output 0.08866594731807709\n",
      "\n",
      "************** Batch 404 in 5.394172191619873 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0880, 0.2031, 0.1321, 0.1787]) \n",
      "Test Loss tensor([0.0924, 0.2038, 0.1323, 0.1766])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2709, 0.5840, 0.0381, 0.2269, 0.6128]) \n",
      "Test Loss tensor([0.2674, 0.5872, 0.0402, 0.2322, 0.6022])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0951, 0.0178, 0.2708, 0.2560, 0.1916, 0.1217, 0.0208]) \n",
      "Test Loss tensor([0.1058, 0.0152, 0.2720, 0.2556, 0.2000, 0.1173, 0.0254])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0471, 0.2233, 0.0438, 0.0990, 0.3131, 0.2864, 0.0448]) \n",
      "Test Loss tensor([0.0414, 0.2297, 0.0445, 0.0910, 0.3156, 0.2816, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0310, 0.0285, 0.0403, 0.0185, 0.0452, 0.0180, 0.1469, 0.0036]) \n",
      "Test Loss tensor([0.0319, 0.0281, 0.0391, 0.0214, 0.0408, 0.0214, 0.1516, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5912, 0.7077, 0.1108, 0.2011, 0.4046, 0.4830])\n",
      "Valid Idx 3 | Loss tensor([0.3432, 0.0691, 0.1569, 0.6146, 0.8604])\n",
      "\n",
      "************** Batch 408 in 5.376538515090942 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0871, 0.2102, 0.1263, 0.1679]) \n",
      "Test Loss tensor([0.0880, 0.2002, 0.1323, 0.1779])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2616, 0.5783, 0.0400, 0.2258, 0.6124]) \n",
      "Test Loss tensor([0.2649, 0.5827, 0.0357, 0.2283, 0.6117])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1137, 0.0148, 0.2941, 0.2626, 0.1911, 0.1177, 0.0274]) \n",
      "Test Loss tensor([0.1043, 0.0157, 0.2722, 0.2605, 0.1998, 0.1178, 0.0218])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0375, 0.2305, 0.0440, 0.0960, 0.3170, 0.2814, 0.0461]) \n",
      "Test Loss tensor([0.0416, 0.2309, 0.0458, 0.0923, 0.3201, 0.2827, 0.0492])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0337, 0.0295, 0.0413, 0.0251, 0.0412, 0.0239, 0.1478, 0.0036]) \n",
      "Test Loss tensor([0.0316, 0.0271, 0.0393, 0.0218, 0.0417, 0.0198, 0.1507, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5944, 0.6996, 0.1088, 0.2057, 0.4061, 0.4877])\n",
      "Valid Idx 3 | Loss tensor([0.3825, 0.0753, 0.1629, 0.6331, 0.8620])\n",
      "\n",
      "************** Batch 412 in 5.428342342376709 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0901, 0.2046, 0.1332, 0.1736]) \n",
      "Test Loss tensor([0.0872, 0.2009, 0.1329, 0.1752])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2738, 0.5894, 0.0357, 0.2288, 0.5943]) \n",
      "Test Loss tensor([0.2618, 0.5872, 0.0338, 0.2281, 0.6028])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1050, 0.0153, 0.2900, 0.2662, 0.1997, 0.1237, 0.0228]) \n",
      "Test Loss tensor([0.1027, 0.0151, 0.2750, 0.2627, 0.1977, 0.1157, 0.0204])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0421, 0.2269, 0.0411, 0.0917, 0.3148, 0.2855, 0.0469]) \n",
      "Test Loss tensor([0.0418, 0.2289, 0.0442, 0.0913, 0.3162, 0.2872, 0.0460])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0338, 0.0260, 0.0378, 0.0212, 0.0426, 0.0193, 0.1503, 0.0032]) \n",
      "Test Loss tensor([0.0316, 0.0266, 0.0395, 0.0216, 0.0406, 0.0199, 0.1508, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5912, 0.7058, 0.1051, 0.2033, 0.4115, 0.4824])\n",
      "Valid Idx 3 | Loss tensor([0.3801, 0.0734, 0.1547, 0.6308, 0.8649])\n",
      "\n",
      "************** Batch 416 in 5.319188356399536 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0884, 0.2070, 0.1366, 0.1872]) \n",
      "Test Loss tensor([0.0908, 0.2023, 0.1359, 0.1786])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2697, 0.5700, 0.0334, 0.2323, 0.6125]) \n",
      "Test Loss tensor([0.2624, 0.5859, 0.0354, 0.2277, 0.6054])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1053, 0.0155, 0.2851, 0.2619, 0.1940, 0.1177, 0.0198]) \n",
      "Test Loss tensor([0.1020, 0.0140, 0.2741, 0.2610, 0.2002, 0.1170, 0.0217])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0423, 0.2249, 0.0483, 0.0909, 0.3001, 0.2723, 0.0508]) \n",
      "Test Loss tensor([0.0399, 0.2283, 0.0457, 0.0917, 0.3202, 0.2886, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0342, 0.0255, 0.0438, 0.0225, 0.0392, 0.0192, 0.1482, 0.0040]) \n",
      "Test Loss tensor([0.0318, 0.0266, 0.0384, 0.0226, 0.0376, 0.0209, 0.1480, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5883, 0.7147, 0.1083, 0.2003, 0.4069, 0.4709])\n",
      "Valid Idx 3 | Loss tensor([0.3513, 0.0709, 0.1525, 0.6103, 0.8623])\n",
      "\n",
      "************** Batch 420 in 5.24932599067688 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0873, 0.2037, 0.1377, 0.1844]) \n",
      "Test Loss tensor([0.0874, 0.2002, 0.1312, 0.1774])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2530, 0.5816, 0.0346, 0.2298, 0.6082]) \n",
      "Test Loss tensor([0.2622, 0.5881, 0.0329, 0.2263, 0.6070])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0974, 0.0142, 0.2662, 0.2596, 0.1997, 0.1204, 0.0227]) \n",
      "Test Loss tensor([0.1015, 0.0150, 0.2756, 0.2654, 0.2019, 0.1175, 0.0193])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0388, 0.2288, 0.0460, 0.0943, 0.3259, 0.2793, 0.0458]) \n",
      "Test Loss tensor([0.0421, 0.2298, 0.0455, 0.0918, 0.3173, 0.2857, 0.0481])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0281, 0.0401, 0.0200, 0.0366, 0.0204, 0.1481, 0.0042]) \n",
      "Test Loss tensor([0.0301, 0.0269, 0.0398, 0.0210, 0.0413, 0.0191, 0.1476, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5984, 0.7009, 0.1058, 0.2061, 0.4054, 0.4843])\n",
      "Valid Idx 3 | Loss tensor([0.3952, 0.0753, 0.1574, 0.6393, 0.8680])\n",
      "Gradients: Input 0.3069537281990051 | Message 0.32334059476852417 | Update 0.3781762421131134 | Output 0.0772065818309784\n",
      "\n",
      "************** Batch 424 in 5.392962455749512 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0877, 0.2001, 0.1379, 0.1749]) \n",
      "Test Loss tensor([0.0889, 0.2009, 0.1307, 0.1773])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2605, 0.5798, 0.0308, 0.2185, 0.6247]) \n",
      "Test Loss tensor([0.2599, 0.5888, 0.0367, 0.2286, 0.6004])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1034, 0.0140, 0.2815, 0.2618, 0.2077, 0.1216, 0.0199]) \n",
      "Test Loss tensor([0.1040, 0.0144, 0.2719, 0.2556, 0.2008, 0.1165, 0.0223])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0468, 0.2201, 0.0492, 0.0941, 0.3263, 0.2894, 0.0454]) \n",
      "Test Loss tensor([0.0403, 0.2267, 0.0441, 0.0911, 0.3189, 0.2844, 0.0470])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0279, 0.0265, 0.0392, 0.0206, 0.0416, 0.0191, 0.1468, 0.0038]) \n",
      "Test Loss tensor([0.0310, 0.0268, 0.0387, 0.0214, 0.0383, 0.0203, 0.1472, 0.0037])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5968, 0.7056, 0.1073, 0.2019, 0.4082, 0.4741])\n",
      "Valid Idx 3 | Loss tensor([0.3583, 0.0714, 0.1490, 0.6270, 0.8637])\n",
      "\n",
      "************** Batch 428 in 5.357581615447998 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0869, 0.2037, 0.1273, 0.1771]) \n",
      "Test Loss tensor([0.0866, 0.2024, 0.1292, 0.1757])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2613, 0.5838, 0.0368, 0.2178, 0.6151]) \n",
      "Test Loss tensor([0.2575, 0.5860, 0.0389, 0.2304, 0.6070])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1042, 0.0151, 0.2879, 0.2578, 0.2057, 0.1222, 0.0220]) \n",
      "Test Loss tensor([0.1028, 0.0143, 0.2747, 0.2592, 0.2027, 0.1174, 0.0238])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0353, 0.2196, 0.0472, 0.0994, 0.3154, 0.2888, 0.0496]) \n",
      "Test Loss tensor([0.0405, 0.2275, 0.0447, 0.0898, 0.3146, 0.2789, 0.0483])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0258, 0.0290, 0.0381, 0.0218, 0.0377, 0.0212, 0.1429, 0.0036]) \n",
      "Test Loss tensor([0.0311, 0.0272, 0.0386, 0.0212, 0.0395, 0.0206, 0.1460, 0.0037])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5908, 0.7004, 0.1085, 0.2063, 0.4070, 0.4752])\n",
      "Valid Idx 3 | Loss tensor([0.3582, 0.0688, 0.1482, 0.6310, 0.8590])\n",
      "\n",
      "************** Batch 432 in 5.457543611526489 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0823, 0.1964, 0.1250, 0.1754]) \n",
      "Test Loss tensor([0.0875, 0.2020, 0.1281, 0.1724])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2521, 0.5716, 0.0424, 0.2169, 0.6060]) \n",
      "Test Loss tensor([0.2568, 0.5852, 0.0383, 0.2301, 0.6026])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0978, 0.0136, 0.2735, 0.2538, 0.1986, 0.1158, 0.0265]) \n",
      "Test Loss tensor([0.1042, 0.0146, 0.2746, 0.2567, 0.2001, 0.1191, 0.0246])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0426, 0.2291, 0.0430, 0.0899, 0.3153, 0.2884, 0.0512]) \n",
      "Test Loss tensor([0.0407, 0.2287, 0.0450, 0.0927, 0.3148, 0.2819, 0.0492])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0352, 0.0263, 0.0419, 0.0186, 0.0408, 0.0213, 0.1453, 0.0035]) \n",
      "Test Loss tensor([0.0306, 0.0261, 0.0375, 0.0219, 0.0402, 0.0205, 0.1438, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5885, 0.6992, 0.1088, 0.2060, 0.4076, 0.4742])\n",
      "Valid Idx 3 | Loss tensor([0.3700, 0.0688, 0.1502, 0.6426, 0.8610])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 436 in 5.307115316390991 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0849, 0.1983, 0.1277, 0.1809]) \n",
      "Test Loss tensor([0.0849, 0.2004, 0.1323, 0.1764])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2488, 0.5773, 0.0424, 0.2216, 0.6208]) \n",
      "Test Loss tensor([0.2608, 0.5808, 0.0339, 0.2307, 0.6071])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0997, 0.0137, 0.2752, 0.2577, 0.2007, 0.1150, 0.0232]) \n",
      "Test Loss tensor([0.1006, 0.0157, 0.2767, 0.2630, 0.2002, 0.1212, 0.0211])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0421, 0.2399, 0.0450, 0.0844, 0.3168, 0.2817, 0.0401]) \n",
      "Test Loss tensor([0.0452, 0.2273, 0.0437, 0.0923, 0.3210, 0.2816, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0314, 0.0270, 0.0367, 0.0238, 0.0406, 0.0210, 0.1394, 0.0037]) \n",
      "Test Loss tensor([0.0292, 0.0262, 0.0402, 0.0220, 0.0443, 0.0186, 0.1477, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5918, 0.6783, 0.1050, 0.2140, 0.3988, 0.4870])\n",
      "Valid Idx 3 | Loss tensor([0.4456, 0.0798, 0.1617, 0.6778, 0.8672])\n",
      "\n",
      "************** Batch 440 in 5.342949390411377 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0825, 0.1977, 0.1317, 0.1764]) \n",
      "Test Loss tensor([0.0884, 0.2006, 0.1261, 0.1747])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2713, 0.5781, 0.0322, 0.2315, 0.6029]) \n",
      "Test Loss tensor([0.2566, 0.5844, 0.0399, 0.2275, 0.6039])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1004, 0.0155, 0.2842, 0.2616, 0.1938, 0.1178, 0.0232]) \n",
      "Test Loss tensor([0.1047, 0.0137, 0.2743, 0.2540, 0.2059, 0.1143, 0.0243])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0413, 0.2384, 0.0431, 0.0850, 0.3182, 0.2768, 0.0473]) \n",
      "Test Loss tensor([0.0387, 0.2310, 0.0456, 0.0893, 0.3133, 0.2904, 0.0475])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0282, 0.0263, 0.0417, 0.0233, 0.0442, 0.0176, 0.1462, 0.0032]) \n",
      "Test Loss tensor([0.0329, 0.0268, 0.0386, 0.0200, 0.0360, 0.0225, 0.1473, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5874, 0.7192, 0.1091, 0.1961, 0.4124, 0.4614])\n",
      "Valid Idx 3 | Loss tensor([0.3343, 0.0656, 0.1401, 0.6084, 0.8574])\n",
      "Gradients: Input 0.7478036284446716 | Message 0.8488912582397461 | Update 1.010936975479126 | Output 0.21491003036499023\n",
      "\n",
      "************** Batch 444 in 5.332688331604004 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0897, 0.1946, 0.1232, 0.1837]) \n",
      "Test Loss tensor([0.0864, 0.2008, 0.1259, 0.1750])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2503, 0.5834, 0.0415, 0.2295, 0.6122]) \n",
      "Test Loss tensor([0.2560, 0.5847, 0.0355, 0.2258, 0.6053])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0985, 0.0129, 0.2721, 0.2453, 0.1898, 0.1094, 0.0244]) \n",
      "Test Loss tensor([0.1019, 0.0138, 0.2748, 0.2603, 0.2018, 0.1164, 0.0206])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0381, 0.2341, 0.0452, 0.0888, 0.3149, 0.2891, 0.0473]) \n",
      "Test Loss tensor([0.0406, 0.2247, 0.0447, 0.0899, 0.3131, 0.2826, 0.0466])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0323, 0.0320, 0.0384, 0.0213, 0.0357, 0.0232, 0.1462, 0.0034]) \n",
      "Test Loss tensor([0.0312, 0.0273, 0.0385, 0.0216, 0.0382, 0.0203, 0.1449, 0.0036])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5925, 0.7093, 0.1060, 0.1989, 0.4073, 0.4731])\n",
      "Valid Idx 3 | Loss tensor([0.3859, 0.0741, 0.1486, 0.6273, 0.8636])\n",
      "\n",
      "************** Batch 448 in 5.236149072647095 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0868, 0.2031, 0.1260, 0.1813]) \n",
      "Test Loss tensor([0.0848, 0.1999, 0.1265, 0.1774])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2612, 0.5738, 0.0299, 0.2185, 0.6109]) \n",
      "Test Loss tensor([0.2618, 0.5804, 0.0304, 0.2251, 0.6093])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0999, 0.0151, 0.2808, 0.2622, 0.2062, 0.1150, 0.0203]) \n",
      "Test Loss tensor([0.0989, 0.0146, 0.2711, 0.2652, 0.2014, 0.1162, 0.0182])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0393, 0.2273, 0.0433, 0.0890, 0.3176, 0.2911, 0.0444]) \n",
      "Test Loss tensor([0.0438, 0.2290, 0.0454, 0.0927, 0.3133, 0.2818, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0299, 0.0265, 0.0390, 0.0216, 0.0381, 0.0198, 0.1497, 0.0032]) \n",
      "Test Loss tensor([0.0288, 0.0253, 0.0382, 0.0210, 0.0401, 0.0181, 0.1438, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5956, 0.6911, 0.1085, 0.2074, 0.4038, 0.4881])\n",
      "Valid Idx 3 | Loss tensor([0.4547, 0.0836, 0.1569, 0.6548, 0.8705])\n",
      "\n",
      "************** Batch 452 in 5.32236123085022 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0853, 0.2116, 0.1291, 0.1688]) \n",
      "Test Loss tensor([0.0849, 0.2031, 0.1219, 0.1696])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2638, 0.5897, 0.0296, 0.2340, 0.6030]) \n",
      "Test Loss tensor([0.2541, 0.5858, 0.0378, 0.2288, 0.6023])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1004, 0.0159, 0.2859, 0.2625, 0.2053, 0.1086, 0.0220]) \n",
      "Test Loss tensor([0.1025, 0.0135, 0.2699, 0.2539, 0.2047, 0.1123, 0.0237])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0478, 0.2410, 0.0443, 0.0949, 0.3226, 0.2767, 0.0509]) \n",
      "Test Loss tensor([0.0396, 0.2316, 0.0430, 0.0893, 0.3149, 0.2820, 0.0471])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0308, 0.0232, 0.0414, 0.0211, 0.0429, 0.0195, 0.1495, 0.0037]) \n",
      "Test Loss tensor([0.0319, 0.0270, 0.0383, 0.0211, 0.0356, 0.0205, 0.1429, 0.0037])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5838, 0.7077, 0.1103, 0.1979, 0.4123, 0.4657])\n",
      "Valid Idx 3 | Loss tensor([0.3620, 0.0688, 0.1418, 0.6137, 0.8599])\n",
      "\n",
      "************** Batch 456 in 5.272276878356934 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0878, 0.1975, 0.1248, 0.1730]) \n",
      "Test Loss tensor([0.0845, 0.2012, 0.1203, 0.1725])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2635, 0.5740, 0.0366, 0.2181, 0.6136]) \n",
      "Test Loss tensor([0.2570, 0.5904, 0.0390, 0.2318, 0.6022])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0939, 0.0140, 0.2731, 0.2539, 0.2018, 0.1116, 0.0215]) \n",
      "Test Loss tensor([0.1030, 0.0137, 0.2754, 0.2549, 0.2061, 0.1153, 0.0241])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0444, 0.2381, 0.0450, 0.0952, 0.3106, 0.2811, 0.0428]) \n",
      "Test Loss tensor([0.0408, 0.2281, 0.0422, 0.0896, 0.3138, 0.2811, 0.0473])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0261, 0.0378, 0.0223, 0.0365, 0.0216, 0.1447, 0.0035]) \n",
      "Test Loss tensor([0.0309, 0.0268, 0.0386, 0.0216, 0.0367, 0.0202, 0.1457, 0.0040])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5897, 0.6981, 0.1119, 0.1968, 0.4086, 0.4682])\n",
      "Valid Idx 3 | Loss tensor([0.3789, 0.0710, 0.1447, 0.6279, 0.8608])\n",
      "\n",
      "************** Batch 460 in 5.205079793930054 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0879, 0.1925, 0.1231, 0.1830]) \n",
      "Test Loss tensor([0.0808, 0.2001, 0.1204, 0.1715])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2575, 0.5843, 0.0367, 0.2232, 0.6005]) \n",
      "Test Loss tensor([0.2581, 0.5860, 0.0342, 0.2307, 0.6046])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0942, 0.0142, 0.2813, 0.2539, 0.2045, 0.1133, 0.0269]) \n",
      "Test Loss tensor([0.0985, 0.0147, 0.2770, 0.2598, 0.1980, 0.1166, 0.0226])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0429, 0.2309, 0.0377, 0.0990, 0.3125, 0.2796, 0.0452]) \n",
      "Test Loss tensor([0.0435, 0.2292, 0.0457, 0.0925, 0.3114, 0.2791, 0.0501])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0291, 0.0259, 0.0413, 0.0223, 0.0385, 0.0197, 0.1413, 0.0041]) \n",
      "Test Loss tensor([0.0299, 0.0268, 0.0401, 0.0216, 0.0410, 0.0180, 0.1483, 0.0036])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5901, 0.6771, 0.1083, 0.2031, 0.4007, 0.4726])\n",
      "Valid Idx 3 | Loss tensor([0.4472, 0.0787, 0.1551, 0.6680, 0.8657])\n",
      "Gradients: Input 0.22163288295269012 | Message 0.19424454867839813 | Update 0.23949652910232544 | Output 0.04811485484242439\n",
      "\n",
      "************** Batch 464 in 5.221528768539429 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0808, 0.2065, 0.1193, 0.1664]) \n",
      "Test Loss tensor([0.0821, 0.2013, 0.1171, 0.1672])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2576, 0.5870, 0.0301, 0.2345, 0.6094]) \n",
      "Test Loss tensor([0.2531, 0.5886, 0.0391, 0.2341, 0.5978])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1000, 0.0147, 0.2745, 0.2452, 0.2045, 0.1173, 0.0217]) \n",
      "Test Loss tensor([0.1028, 0.0130, 0.2720, 0.2559, 0.2056, 0.1117, 0.0248])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0452, 0.2330, 0.0408, 0.1029, 0.3085, 0.2804, 0.0527]) \n",
      "Test Loss tensor([0.0366, 0.2265, 0.0439, 0.0901, 0.3121, 0.2839, 0.0470])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0259, 0.0263, 0.0393, 0.0218, 0.0410, 0.0189, 0.1509, 0.0043]) \n",
      "Test Loss tensor([0.0316, 0.0270, 0.0361, 0.0209, 0.0350, 0.0209, 0.1414, 0.0040])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5892, 0.7011, 0.1101, 0.1909, 0.4086, 0.4511])\n",
      "Valid Idx 3 | Loss tensor([0.3615, 0.0679, 0.1405, 0.6263, 0.8609])\n",
      "\n",
      "************** Batch 468 in 5.1722023487091064 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0855, 0.1925, 0.1183, 0.1731]) \n",
      "Test Loss tensor([0.0819, 0.2021, 0.1155, 0.1703])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2499, 0.5742, 0.0361, 0.2243, 0.6025]) \n",
      "Test Loss tensor([0.2552, 0.5866, 0.0343, 0.2330, 0.6004])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1044, 0.0130, 0.2878, 0.2539, 0.2031, 0.1120, 0.0238]) \n",
      "Test Loss tensor([0.1004, 0.0139, 0.2809, 0.2656, 0.2031, 0.1133, 0.0214])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0363, 0.2189, 0.0437, 0.0823, 0.3045, 0.2904, 0.0473]) \n",
      "Test Loss tensor([0.0396, 0.2294, 0.0444, 0.0923, 0.3128, 0.2809, 0.0487])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0326, 0.0257, 0.0378, 0.0216, 0.0339, 0.0198, 0.1378, 0.0036]) \n",
      "Test Loss tensor([0.0311, 0.0274, 0.0400, 0.0210, 0.0375, 0.0192, 0.1451, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5855, 0.6827, 0.1118, 0.1997, 0.4069, 0.4565])\n",
      "Valid Idx 3 | Loss tensor([0.4200, 0.0752, 0.1481, 0.6489, 0.8680])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 472 in 5.2954161167144775 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0833, 0.2010, 0.1171, 0.1771]) \n",
      "Test Loss tensor([0.0818, 0.2018, 0.1169, 0.1706])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2513, 0.5938, 0.0362, 0.2295, 0.5960]) \n",
      "Test Loss tensor([0.2559, 0.5870, 0.0343, 0.2310, 0.6024])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0985, 0.0136, 0.2828, 0.2631, 0.2060, 0.1120, 0.0231]) \n",
      "Test Loss tensor([0.1007, 0.0137, 0.2769, 0.2619, 0.2024, 0.1113, 0.0224])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0372, 0.2197, 0.0420, 0.0997, 0.3171, 0.2882, 0.0453]) \n",
      "Test Loss tensor([0.0384, 0.2283, 0.0440, 0.0902, 0.3140, 0.2803, 0.0483])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0300, 0.0271, 0.0380, 0.0192, 0.0388, 0.0188, 0.1340, 0.0044]) \n",
      "Test Loss tensor([0.0301, 0.0265, 0.0402, 0.0213, 0.0375, 0.0188, 0.1446, 0.0039])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5886, 0.6881, 0.1096, 0.1937, 0.4056, 0.4551])\n",
      "Valid Idx 3 | Loss tensor([0.4251, 0.0776, 0.1496, 0.6474, 0.8659])\n",
      "\n",
      "************** Batch 476 in 5.221050024032593 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0791, 0.1890, 0.1200, 0.1682]) \n",
      "Test Loss tensor([0.0813, 0.2000, 0.1137, 0.1686])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2539, 0.5735, 0.0347, 0.2390, 0.6136]) \n",
      "Test Loss tensor([0.2537, 0.5860, 0.0368, 0.2330, 0.6017])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0977, 0.0132, 0.2824, 0.2474, 0.2071, 0.1106, 0.0204]) \n",
      "Test Loss tensor([0.1010, 0.0131, 0.2783, 0.2557, 0.2039, 0.1111, 0.0245])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0421, 0.2286, 0.0387, 0.0909, 0.3019, 0.2675, 0.0463]) \n",
      "Test Loss tensor([0.0370, 0.2278, 0.0438, 0.0892, 0.3093, 0.2817, 0.0465])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0271, 0.0242, 0.0387, 0.0188, 0.0385, 0.0199, 0.1418, 0.0035]) \n",
      "Test Loss tensor([0.0312, 0.0267, 0.0380, 0.0214, 0.0353, 0.0198, 0.1431, 0.0039])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5919, 0.6991, 0.1094, 0.1893, 0.4040, 0.4476])\n",
      "Valid Idx 3 | Loss tensor([0.3891, 0.0728, 0.1451, 0.6324, 0.8621])\n",
      "\n",
      "************** Batch 480 in 5.203973293304443 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0756, 0.1941, 0.1133, 0.1660]) \n",
      "Test Loss tensor([0.0819, 0.2018, 0.1131, 0.1699])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2634, 0.5779, 0.0349, 0.2278, 0.6033]) \n",
      "Test Loss tensor([0.2539, 0.5810, 0.0345, 0.2280, 0.6050])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1029, 0.0127, 0.2789, 0.2506, 0.2024, 0.1124, 0.0258]) \n",
      "Test Loss tensor([0.1009, 0.0138, 0.2773, 0.2581, 0.2034, 0.1124, 0.0226])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0388, 0.2261, 0.0428, 0.0830, 0.3028, 0.2822, 0.0444]) \n",
      "Test Loss tensor([0.0389, 0.2297, 0.0445, 0.0889, 0.3122, 0.2821, 0.0484])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0346, 0.0289, 0.0378, 0.0222, 0.0351, 0.0201, 0.1420, 0.0036]) \n",
      "Test Loss tensor([0.0300, 0.0265, 0.0367, 0.0209, 0.0371, 0.0187, 0.1402, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5900, 0.6841, 0.1111, 0.1952, 0.4009, 0.4580])\n",
      "Valid Idx 3 | Loss tensor([0.4261, 0.0772, 0.1471, 0.6536, 0.8622])\n",
      "Gradients: Input 0.28985679149627686 | Message 0.32356536388397217 | Update 0.3751140832901001 | Output 0.07475683093070984\n",
      "\n",
      "************** Batch 484 in 5.237010955810547 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0850, 0.1918, 0.1179, 0.1833]) \n",
      "Test Loss tensor([0.0807, 0.1964, 0.1130, 0.1691])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2662, 0.5959, 0.0337, 0.2353, 0.5863]) \n",
      "Test Loss tensor([0.2515, 0.5835, 0.0348, 0.2307, 0.6040])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1057, 0.0125, 0.2812, 0.2518, 0.1983, 0.1092, 0.0210]) \n",
      "Test Loss tensor([0.1012, 0.0134, 0.2713, 0.2566, 0.2068, 0.1121, 0.0223])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0369, 0.2279, 0.0432, 0.0905, 0.3075, 0.2691, 0.0537]) \n",
      "Test Loss tensor([0.0395, 0.2277, 0.0438, 0.0912, 0.3117, 0.2819, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0288, 0.0284, 0.0371, 0.0227, 0.0374, 0.0187, 0.1437, 0.0040]) \n",
      "Test Loss tensor([0.0308, 0.0261, 0.0386, 0.0213, 0.0366, 0.0189, 0.1406, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5928, 0.6925, 0.1074, 0.1915, 0.4042, 0.4536])\n",
      "Valid Idx 3 | Loss tensor([0.4104, 0.0760, 0.1472, 0.6504, 0.8629])\n",
      "\n",
      "************** Batch 488 in 5.271770715713501 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0813, 0.2041, 0.1161, 0.1693]) \n",
      "Test Loss tensor([0.0837, 0.2024, 0.1114, 0.1668])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2567, 0.5977, 0.0360, 0.2441, 0.6004]) \n",
      "Test Loss tensor([0.2502, 0.5883, 0.0382, 0.2283, 0.5985])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1016, 0.0141, 0.2874, 0.2568, 0.1943, 0.1122, 0.0256]) \n",
      "Test Loss tensor([0.1017, 0.0125, 0.2724, 0.2556, 0.2075, 0.1079, 0.0238])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0393, 0.2299, 0.0465, 0.0834, 0.3028, 0.2765, 0.0517]) \n",
      "Test Loss tensor([0.0373, 0.2266, 0.0424, 0.0899, 0.3120, 0.2780, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0321, 0.0286, 0.0374, 0.0227, 0.0372, 0.0187, 0.1485, 0.0040]) \n",
      "Test Loss tensor([0.0314, 0.0266, 0.0376, 0.0212, 0.0343, 0.0196, 0.1441, 0.0040])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5870, 0.7026, 0.1094, 0.1873, 0.4109, 0.4459])\n",
      "Valid Idx 3 | Loss tensor([0.3635, 0.0691, 0.1374, 0.6390, 0.8581])\n",
      "\n",
      "************** Batch 492 in 5.308547258377075 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0833, 0.2034, 0.1146, 0.1697]) \n",
      "Test Loss tensor([0.0792, 0.2014, 0.1128, 0.1686])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2434, 0.5765, 0.0366, 0.2288, 0.6074]) \n",
      "Test Loss tensor([0.2493, 0.5830, 0.0326, 0.2270, 0.6059])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1020, 0.0127, 0.2684, 0.2462, 0.2037, 0.1125, 0.0236]) \n",
      "Test Loss tensor([0.0992, 0.0139, 0.2783, 0.2617, 0.2027, 0.1098, 0.0208])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0326, 0.2278, 0.0446, 0.0903, 0.3142, 0.2856, 0.0481]) \n",
      "Test Loss tensor([0.0402, 0.2273, 0.0447, 0.0903, 0.3073, 0.2788, 0.0482])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0300, 0.0273, 0.0324, 0.0173, 0.0325, 0.0209, 0.1360, 0.0037]) \n",
      "Test Loss tensor([0.0285, 0.0259, 0.0386, 0.0216, 0.0380, 0.0170, 0.1426, 0.0036])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5915, 0.6819, 0.1066, 0.1982, 0.4055, 0.4646])\n",
      "Valid Idx 3 | Loss tensor([0.4485, 0.0820, 0.1474, 0.6840, 0.8676])\n",
      "\n",
      "************** Batch 496 in 5.300762414932251 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0805, 0.1859, 0.1123, 0.1764]) \n",
      "Test Loss tensor([0.0790, 0.2006, 0.1101, 0.1673])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2599, 0.5898, 0.0284, 0.2395, 0.5925]) \n",
      "Test Loss tensor([0.2451, 0.5861, 0.0369, 0.2293, 0.6035])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0946, 0.0135, 0.2739, 0.2595, 0.1980, 0.1100, 0.0193]) \n",
      "Test Loss tensor([0.1004, 0.0134, 0.2767, 0.2570, 0.2063, 0.1096, 0.0226])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0350, 0.2375, 0.0477, 0.0807, 0.3165, 0.2724, 0.0446]) \n",
      "Test Loss tensor([0.0378, 0.2306, 0.0457, 0.0906, 0.3113, 0.2800, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0288, 0.0233, 0.0391, 0.0215, 0.0394, 0.0169, 0.1406, 0.0040]) \n",
      "Test Loss tensor([0.0299, 0.0262, 0.0372, 0.0211, 0.0356, 0.0181, 0.1410, 0.0040])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5899, 0.7018, 0.1081, 0.1892, 0.4056, 0.4602])\n",
      "Valid Idx 3 | Loss tensor([0.4018, 0.0765, 0.1404, 0.6666, 0.8640])\n",
      "\n",
      "************** Batch 500 in 5.351708889007568 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0785, 0.2006, 0.1185, 0.1682]) \n",
      "Test Loss tensor([0.0794, 0.1981, 0.1081, 0.1657])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2449, 0.5888, 0.0346, 0.2317, 0.5975]) \n",
      "Test Loss tensor([0.2416, 0.5874, 0.0387, 0.2288, 0.5983])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1017, 0.0134, 0.2795, 0.2602, 0.2065, 0.1132, 0.0224]) \n",
      "Test Loss tensor([0.1012, 0.0130, 0.2744, 0.2556, 0.2081, 0.1102, 0.0236])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0410, 0.2318, 0.0396, 0.0941, 0.3091, 0.2842, 0.0473]) \n",
      "Test Loss tensor([0.0383, 0.2303, 0.0451, 0.0873, 0.3074, 0.2804, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0304, 0.0292, 0.0438, 0.0193, 0.0352, 0.0189, 0.1462, 0.0047]) \n",
      "Test Loss tensor([0.0296, 0.0264, 0.0381, 0.0216, 0.0351, 0.0187, 0.1403, 0.0041])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5868, 0.7090, 0.1099, 0.1922, 0.4067, 0.4587])\n",
      "Valid Idx 3 | Loss tensor([0.3852, 0.0740, 0.1383, 0.6543, 0.8624])\n",
      "Gradients: Input 0.09561507403850555 | Message 0.05320555716753006 | Update 0.04891667142510414 | Output 0.011057250201702118\n",
      "\n",
      "************** Batch 504 in 5.426754713058472 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0816, 0.2030, 0.1107, 0.1706]) \n",
      "Test Loss tensor([0.0779, 0.1993, 0.1085, 0.1660])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2420, 0.5694, 0.0393, 0.2321, 0.6021]) \n",
      "Test Loss tensor([0.2479, 0.5830, 0.0316, 0.2302, 0.6004])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0958, 0.0130, 0.2746, 0.2511, 0.1985, 0.1066, 0.0236]) \n",
      "Test Loss tensor([0.0974, 0.0147, 0.2773, 0.2600, 0.1992, 0.1102, 0.0210])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0345, 0.2286, 0.0423, 0.0876, 0.3118, 0.2799, 0.0490]) \n",
      "Test Loss tensor([0.0427, 0.2231, 0.0453, 0.0901, 0.3081, 0.2782, 0.0465])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0292, 0.0280, 0.0368, 0.0209, 0.0343, 0.0175, 0.1463, 0.0044]) \n",
      "Test Loss tensor([0.0281, 0.0259, 0.0393, 0.0210, 0.0399, 0.0164, 0.1421, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5955, 0.6847, 0.1062, 0.2011, 0.4021, 0.4796])\n",
      "Valid Idx 3 | Loss tensor([0.4786, 0.0907, 0.1522, 0.7049, 0.8690])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 508 in 5.340841293334961 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0834, 0.2041, 0.1093, 0.1698]) \n",
      "Test Loss tensor([0.0809, 0.2042, 0.1061, 0.1664])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2496, 0.5947, 0.0322, 0.2156, 0.6036]) \n",
      "Test Loss tensor([0.2393, 0.5840, 0.0387, 0.2328, 0.5974])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0968, 0.0149, 0.2789, 0.2576, 0.2068, 0.1161, 0.0214]) \n",
      "Test Loss tensor([0.1015, 0.0133, 0.2778, 0.2529, 0.2049, 0.1075, 0.0241])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0414, 0.2261, 0.0417, 0.0880, 0.2958, 0.2757, 0.0424]) \n",
      "Test Loss tensor([0.0404, 0.2275, 0.0454, 0.0880, 0.3071, 0.2794, 0.0480])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0271, 0.0231, 0.0392, 0.0208, 0.0400, 0.0167, 0.1409, 0.0039]) \n",
      "Test Loss tensor([0.0312, 0.0260, 0.0389, 0.0219, 0.0353, 0.0189, 0.1430, 0.0043])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5879, 0.7102, 0.1098, 0.1896, 0.4062, 0.4564])\n",
      "Valid Idx 3 | Loss tensor([0.3933, 0.0743, 0.1386, 0.6575, 0.8618])\n",
      "\n",
      "************** Batch 512 in 5.293951511383057 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0842, 0.1984, 0.1080, 0.1631]) \n",
      "Test Loss tensor([0.0788, 0.2019, 0.1073, 0.1646])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2433, 0.5868, 0.0419, 0.2315, 0.5829]) \n",
      "Test Loss tensor([0.2429, 0.5805, 0.0344, 0.2277, 0.5997])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1024, 0.0130, 0.2736, 0.2485, 0.2122, 0.1136, 0.0272]) \n",
      "Test Loss tensor([0.1002, 0.0136, 0.2729, 0.2521, 0.2059, 0.1078, 0.0218])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0367, 0.2408, 0.0378, 0.0793, 0.3023, 0.2805, 0.0509]) \n",
      "Test Loss tensor([0.0399, 0.2238, 0.0439, 0.0901, 0.3071, 0.2775, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0346, 0.0240, 0.0418, 0.0200, 0.0353, 0.0193, 0.1453, 0.0044]) \n",
      "Test Loss tensor([0.0295, 0.0251, 0.0385, 0.0213, 0.0360, 0.0177, 0.1396, 0.0042])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5930, 0.7015, 0.1089, 0.1902, 0.4065, 0.4573])\n",
      "Valid Idx 3 | Loss tensor([0.4259, 0.0802, 0.1423, 0.6809, 0.8637])\n",
      "\n",
      "************** Batch 516 in 5.6268298625946045 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0755, 0.1954, 0.1058, 0.1669]) \n",
      "Test Loss tensor([0.0771, 0.1991, 0.1080, 0.1662])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2462, 0.5850, 0.0343, 0.2255, 0.6100]) \n",
      "Test Loss tensor([0.2454, 0.5856, 0.0326, 0.2267, 0.5971])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1033, 0.0136, 0.2891, 0.2595, 0.2066, 0.1102, 0.0196]) \n",
      "Test Loss tensor([0.0983, 0.0141, 0.2758, 0.2613, 0.2061, 0.1094, 0.0202])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0367, 0.2263, 0.0379, 0.0850, 0.3007, 0.2806, 0.0488]) \n",
      "Test Loss tensor([0.0388, 0.2294, 0.0450, 0.0883, 0.3089, 0.2756, 0.0494])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0327, 0.0248, 0.0411, 0.0243, 0.0349, 0.0175, 0.1400, 0.0051]) \n",
      "Test Loss tensor([0.0287, 0.0255, 0.0393, 0.0219, 0.0362, 0.0165, 0.1405, 0.0042])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5910, 0.6951, 0.1072, 0.1941, 0.4016, 0.4600])\n",
      "Valid Idx 3 | Loss tensor([0.4563, 0.0872, 0.1466, 0.7049, 0.8754])\n",
      "\n",
      "************** Batch 520 in 5.021936655044556 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0767, 0.1930, 0.1043, 0.1674]) \n",
      "Test Loss tensor([0.0821, 0.2017, 0.1044, 0.1630])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2430, 0.5899, 0.0355, 0.2361, 0.5914]) \n",
      "Test Loss tensor([0.2425, 0.5856, 0.0416, 0.2295, 0.5931])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1024, 0.0155, 0.2796, 0.2630, 0.2078, 0.1117, 0.0211]) \n",
      "Test Loss tensor([0.1038, 0.0117, 0.2756, 0.2558, 0.2131, 0.1091, 0.0242])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0446, 0.2303, 0.0387, 0.0967, 0.2984, 0.2781, 0.0448]) \n",
      "Test Loss tensor([0.0356, 0.2270, 0.0431, 0.0881, 0.3108, 0.2832, 0.0479])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0286, 0.0262, 0.0387, 0.0207, 0.0358, 0.0163, 0.1362, 0.0044]) \n",
      "Test Loss tensor([0.0320, 0.0265, 0.0372, 0.0204, 0.0314, 0.0199, 0.1388, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5866, 0.7219, 0.1088, 0.1811, 0.4094, 0.4297])\n",
      "Valid Idx 3 | Loss tensor([0.3434, 0.0688, 0.1345, 0.6484, 0.8560])\n",
      "Gradients: Input 0.6124885678291321 | Message 0.6843273639678955 | Update 0.8244112730026245 | Output 0.14827309548854828\n",
      "\n",
      "************** Batch 524 in 5.0269389152526855 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0843, 0.2016, 0.1127, 0.1687]) \n",
      "Test Loss tensor([0.0777, 0.1985, 0.1056, 0.1646])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2520, 0.5861, 0.0389, 0.2303, 0.5835]) \n",
      "Test Loss tensor([0.2427, 0.5844, 0.0361, 0.2259, 0.5953])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1070, 0.0127, 0.2816, 0.2549, 0.2090, 0.1085, 0.0251]) \n",
      "Test Loss tensor([0.0967, 0.0126, 0.2721, 0.2593, 0.2075, 0.1104, 0.0211])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0364, 0.2346, 0.0364, 0.0855, 0.2945, 0.2865, 0.0433]) \n",
      "Test Loss tensor([0.0381, 0.2254, 0.0447, 0.0887, 0.3105, 0.2786, 0.0480])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0325, 0.0280, 0.0380, 0.0196, 0.0322, 0.0199, 0.1425, 0.0048]) \n",
      "Test Loss tensor([0.0284, 0.0262, 0.0389, 0.0206, 0.0353, 0.0168, 0.1401, 0.0042])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5915, 0.6885, 0.1107, 0.1880, 0.4021, 0.4427])\n",
      "Valid Idx 3 | Loss tensor([0.4303, 0.0800, 0.1426, 0.7026, 0.8637])\n",
      "\n",
      "************** Batch 528 in 4.997853994369507 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0804, 0.1821, 0.1049, 0.1749]) \n",
      "Test Loss tensor([0.0756, 0.2012, 0.1033, 0.1635])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2460, 0.5856, 0.0340, 0.2320, 0.6036]) \n",
      "Test Loss tensor([0.2467, 0.5862, 0.0376, 0.2274, 0.5948])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1001, 0.0131, 0.2882, 0.2573, 0.2076, 0.1090, 0.0190]) \n",
      "Test Loss tensor([0.0972, 0.0128, 0.2764, 0.2588, 0.2091, 0.1102, 0.0230])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0372, 0.2299, 0.0405, 0.0887, 0.3201, 0.2775, 0.0490]) \n",
      "Test Loss tensor([0.0397, 0.2307, 0.0439, 0.0886, 0.3078, 0.2738, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0260, 0.0278, 0.0392, 0.0205, 0.0344, 0.0162, 0.1360, 0.0043]) \n",
      "Test Loss tensor([0.0295, 0.0246, 0.0382, 0.0205, 0.0345, 0.0169, 0.1389, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5866, 0.6817, 0.1112, 0.1899, 0.4000, 0.4395])\n",
      "Valid Idx 3 | Loss tensor([0.4342, 0.0796, 0.1411, 0.7053, 0.8609])\n",
      "\n",
      "************** Batch 532 in 4.717278003692627 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0797, 0.2096, 0.1008, 0.1607]) \n",
      "Test Loss tensor([0.0772, 0.1998, 0.1008, 0.1598])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2393, 0.5957, 0.0365, 0.2343, 0.5925]) \n",
      "Test Loss tensor([0.2464, 0.5857, 0.0397, 0.2252, 0.5944])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0968, 0.0127, 0.2742, 0.2562, 0.2066, 0.1094, 0.0255]) \n",
      "Test Loss tensor([0.0998, 0.0121, 0.2762, 0.2554, 0.2129, 0.1098, 0.0255])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0396, 0.2265, 0.0464, 0.0887, 0.3130, 0.2725, 0.0520]) \n",
      "Test Loss tensor([0.0383, 0.2269, 0.0437, 0.0875, 0.3083, 0.2758, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0275, 0.0256, 0.0454, 0.0183, 0.0345, 0.0183, 0.1349, 0.0046]) \n",
      "Test Loss tensor([0.0301, 0.0263, 0.0375, 0.0206, 0.0312, 0.0179, 0.1370, 0.0046])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5826, 0.6940, 0.1104, 0.1830, 0.4034, 0.4314])\n",
      "Valid Idx 3 | Loss tensor([0.3897, 0.0723, 0.1343, 0.6734, 0.8575])\n",
      "\n",
      "************** Batch 536 in 4.681361436843872 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0807, 0.2020, 0.1017, 0.1659]) \n",
      "Test Loss tensor([0.0766, 0.2003, 0.0983, 0.1604])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2490, 0.5946, 0.0379, 0.2288, 0.6040]) \n",
      "Test Loss tensor([0.2443, 0.5869, 0.0389, 0.2309, 0.5920])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1038, 0.0127, 0.2871, 0.2597, 0.2149, 0.1125, 0.0234]) \n",
      "Test Loss tensor([0.0987, 0.0120, 0.2774, 0.2551, 0.2114, 0.1077, 0.0246])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0359, 0.2254, 0.0375, 0.0861, 0.3058, 0.2608, 0.0433]) \n",
      "Test Loss tensor([0.0369, 0.2285, 0.0423, 0.0866, 0.3041, 0.2781, 0.0480])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0329, 0.0260, 0.0434, 0.0209, 0.0312, 0.0182, 0.1423, 0.0047]) \n",
      "Test Loss tensor([0.0303, 0.0261, 0.0398, 0.0206, 0.0316, 0.0182, 0.1373, 0.0046])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5856, 0.7067, 0.1111, 0.1825, 0.4033, 0.4402])\n",
      "Valid Idx 3 | Loss tensor([0.3921, 0.0778, 0.1380, 0.6615, 0.8555])\n",
      "\n",
      "************** Batch 540 in 4.664879322052002 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0770, 0.1918, 0.1096, 0.1711]) \n",
      "Test Loss tensor([0.0766, 0.1984, 0.0995, 0.1638])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2350, 0.5718, 0.0389, 0.2204, 0.6011]) \n",
      "Test Loss tensor([0.2462, 0.5814, 0.0329, 0.2253, 0.5994])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0945, 0.0125, 0.2767, 0.2475, 0.2015, 0.1112, 0.0225]) \n",
      "Test Loss tensor([0.0995, 0.0130, 0.2770, 0.2567, 0.2049, 0.1076, 0.0205])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0358, 0.2247, 0.0437, 0.0999, 0.3136, 0.2824, 0.0480]) \n",
      "Test Loss tensor([0.0406, 0.2311, 0.0441, 0.0872, 0.3080, 0.2774, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0308, 0.0234, 0.0419, 0.0222, 0.0314, 0.0184, 0.1378, 0.0051]) \n",
      "Test Loss tensor([0.0285, 0.0257, 0.0410, 0.0213, 0.0336, 0.0160, 0.1389, 0.0043])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5894, 0.6967, 0.1075, 0.1847, 0.4043, 0.4530])\n",
      "Valid Idx 3 | Loss tensor([0.4585, 0.0939, 0.1428, 0.6864, 0.8699])\n",
      "Gradients: Input 0.2728407084941864 | Message 0.31150439381599426 | Update 0.34835562109947205 | Output 0.06947460770606995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 544 in 4.822541236877441 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0788, 0.2038, 0.0989, 0.1626]) \n",
      "Test Loss tensor([0.0771, 0.1989, 0.0977, 0.1612])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2594, 0.5894, 0.0321, 0.2383, 0.5836]) \n",
      "Test Loss tensor([0.2391, 0.5832, 0.0378, 0.2277, 0.5938])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0953, 0.0125, 0.2794, 0.2588, 0.2030, 0.1011, 0.0192]) \n",
      "Test Loss tensor([0.0995, 0.0120, 0.2756, 0.2567, 0.2103, 0.1043, 0.0229])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0400, 0.2308, 0.0450, 0.0906, 0.3025, 0.2644, 0.0486]) \n",
      "Test Loss tensor([0.0360, 0.2273, 0.0434, 0.0850, 0.3073, 0.2806, 0.0490])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0272, 0.0281, 0.0420, 0.0210, 0.0338, 0.0182, 0.1386, 0.0049]) \n",
      "Test Loss tensor([0.0311, 0.0260, 0.0373, 0.0215, 0.0304, 0.0179, 0.1368, 0.0047])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5828, 0.7257, 0.1054, 0.1773, 0.4047, 0.4447])\n",
      "Valid Idx 3 | Loss tensor([0.3741, 0.0803, 0.1351, 0.6483, 0.8652])\n",
      "\n",
      "************** Batch 548 in 4.967143297195435 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0838, 0.2019, 0.0987, 0.1673]) \n",
      "Test Loss tensor([0.0774, 0.2027, 0.0981, 0.1603])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2461, 0.5698, 0.0344, 0.2329, 0.6048]) \n",
      "Test Loss tensor([0.2405, 0.5911, 0.0371, 0.2324, 0.5844])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0941, 0.0119, 0.2674, 0.2442, 0.2092, 0.1082, 0.0234]) \n",
      "Test Loss tensor([0.1006, 0.0122, 0.2770, 0.2547, 0.2077, 0.1072, 0.0228])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0352, 0.2343, 0.0521, 0.0859, 0.3025, 0.2720, 0.0464]) \n",
      "Test Loss tensor([0.0375, 0.2278, 0.0455, 0.0894, 0.3106, 0.2780, 0.0497])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0330, 0.0266, 0.0416, 0.0234, 0.0291, 0.0178, 0.1440, 0.0046]) \n",
      "Test Loss tensor([0.0301, 0.0269, 0.0390, 0.0212, 0.0314, 0.0172, 0.1365, 0.0046])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5853, 0.7132, 0.1084, 0.1828, 0.4039, 0.4491])\n",
      "Valid Idx 3 | Loss tensor([0.3891, 0.0826, 0.1363, 0.6624, 0.8629])\n",
      "\n",
      "************** Batch 552 in 4.807066917419434 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0729, 0.1961, 0.0935, 0.1579]) \n",
      "Test Loss tensor([0.0767, 0.1992, 0.0986, 0.1579])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2340, 0.6021, 0.0355, 0.2281, 0.5761]) \n",
      "Test Loss tensor([0.2411, 0.5866, 0.0365, 0.2287, 0.5896])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1019, 0.0121, 0.2770, 0.2526, 0.2081, 0.1112, 0.0216]) \n",
      "Test Loss tensor([0.1004, 0.0131, 0.2792, 0.2577, 0.2046, 0.1067, 0.0223])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0370, 0.2312, 0.0450, 0.0891, 0.3054, 0.2634, 0.0504]) \n",
      "Test Loss tensor([0.0385, 0.2258, 0.0428, 0.0864, 0.3074, 0.2735, 0.0483])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0244, 0.0393, 0.0183, 0.0326, 0.0167, 0.1407, 0.0043]) \n",
      "Test Loss tensor([0.0301, 0.0248, 0.0389, 0.0209, 0.0338, 0.0165, 0.1401, 0.0046])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5862, 0.6950, 0.1080, 0.1864, 0.3988, 0.4570])\n",
      "Valid Idx 3 | Loss tensor([0.4302, 0.0898, 0.1411, 0.6932, 0.8685])\n",
      "\n",
      "************** Batch 556 in 4.653581380844116 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0721, 0.1927, 0.1016, 0.1569]) \n",
      "Test Loss tensor([0.0801, 0.2033, 0.0942, 0.1576])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2475, 0.5757, 0.0372, 0.2281, 0.5964]) \n",
      "Test Loss tensor([0.2350, 0.5843, 0.0437, 0.2296, 0.5896])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0880, 0.0133, 0.2713, 0.2490, 0.2163, 0.1069, 0.0214]) \n",
      "Test Loss tensor([0.1053, 0.0119, 0.2731, 0.2560, 0.2122, 0.1066, 0.0271])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0379, 0.2327, 0.0421, 0.0865, 0.3081, 0.2742, 0.0471]) \n",
      "Test Loss tensor([0.0356, 0.2297, 0.0440, 0.0862, 0.3085, 0.2741, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0278, 0.0293, 0.0404, 0.0203, 0.0352, 0.0163, 0.1431, 0.0047]) \n",
      "Test Loss tensor([0.0318, 0.0278, 0.0380, 0.0199, 0.0312, 0.0189, 0.1380, 0.0052])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5828, 0.7268, 0.1093, 0.1767, 0.4035, 0.4386])\n",
      "Valid Idx 3 | Loss tensor([0.3399, 0.0759, 0.1318, 0.6505, 0.8508])\n",
      "\n",
      "************** Batch 560 in 4.751721143722534 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0766, 0.1905, 0.1014, 0.1551]) \n",
      "Test Loss tensor([0.0770, 0.1970, 0.1006, 0.1612])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2262, 0.5850, 0.0451, 0.2308, 0.5943]) \n",
      "Test Loss tensor([0.2427, 0.5822, 0.0304, 0.2253, 0.5969])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1038, 0.0123, 0.2661, 0.2564, 0.1967, 0.1046, 0.0269]) \n",
      "Test Loss tensor([0.0951, 0.0140, 0.2779, 0.2640, 0.2041, 0.1115, 0.0192])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0313, 0.2411, 0.0375, 0.0804, 0.3003, 0.2817, 0.0471]) \n",
      "Test Loss tensor([0.0418, 0.2265, 0.0449, 0.0893, 0.3050, 0.2703, 0.0493])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0372, 0.0288, 0.0453, 0.0212, 0.0309, 0.0191, 0.1423, 0.0047]) \n",
      "Test Loss tensor([0.0284, 0.0254, 0.0415, 0.0230, 0.0380, 0.0146, 0.1437, 0.0045])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5917, 0.6833, 0.1053, 0.1969, 0.3904, 0.4749])\n",
      "Valid Idx 3 | Loss tensor([0.5045, 0.1087, 0.1498, 0.7333, 0.8739])\n",
      "Gradients: Input 0.9754149317741394 | Message 1.0700833797454834 | Update 1.2866158485412598 | Output 0.2132457196712494\n",
      "\n",
      "************** Batch 564 in 4.622424125671387 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0703, 0.1884, 0.0984, 0.1540]) \n",
      "Test Loss tensor([0.0773, 0.2008, 0.0962, 0.1581])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2535, 0.5804, 0.0311, 0.2312, 0.5925]) \n",
      "Test Loss tensor([0.2380, 0.5856, 0.0409, 0.2251, 0.5900])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1008, 0.0154, 0.2703, 0.2617, 0.2027, 0.1111, 0.0221]) \n",
      "Test Loss tensor([0.1050, 0.0115, 0.2714, 0.2500, 0.2141, 0.1043, 0.0242])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0446, 0.2303, 0.0507, 0.0892, 0.2981, 0.2820, 0.0496]) \n",
      "Test Loss tensor([0.0341, 0.2265, 0.0431, 0.0844, 0.3067, 0.2797, 0.0474])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0260, 0.0271, 0.0376, 0.0192, 0.0372, 0.0144, 0.1382, 0.0050]) \n",
      "Test Loss tensor([0.0321, 0.0251, 0.0372, 0.0211, 0.0294, 0.0192, 0.1368, 0.0049])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5792, 0.7377, 0.1099, 0.1749, 0.4098, 0.4331])\n",
      "Valid Idx 3 | Loss tensor([0.3361, 0.0768, 0.1283, 0.6425, 0.8542])\n",
      "\n",
      "************** Batch 568 in 4.640221118927002 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0812, 0.2015, 0.0999, 0.1547]) \n",
      "Test Loss tensor([0.0751, 0.1997, 0.0979, 0.1574])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2343, 0.5897, 0.0395, 0.2274, 0.5878]) \n",
      "Test Loss tensor([0.2393, 0.5874, 0.0352, 0.2250, 0.5866])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1007, 0.0103, 0.2797, 0.2459, 0.2142, 0.1101, 0.0251]) \n",
      "Test Loss tensor([0.0989, 0.0124, 0.2765, 0.2571, 0.2066, 0.1026, 0.0201])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0340, 0.2200, 0.0436, 0.0870, 0.3139, 0.2802, 0.0505]) \n",
      "Test Loss tensor([0.0369, 0.2312, 0.0441, 0.0859, 0.3071, 0.2769, 0.0497])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0254, 0.0274, 0.0333, 0.0208, 0.0297, 0.0192, 0.1335, 0.0054]) \n",
      "Test Loss tensor([0.0303, 0.0253, 0.0374, 0.0210, 0.0312, 0.0167, 0.1346, 0.0048])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5855, 0.7157, 0.1102, 0.1817, 0.4052, 0.4524])\n",
      "Valid Idx 3 | Loss tensor([0.4184, 0.0915, 0.1370, 0.6734, 0.8659])\n",
      "\n",
      "************** Batch 572 in 4.63962197303772 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0782, 0.1989, 0.0964, 0.1593]) \n",
      "Test Loss tensor([0.0759, 0.2022, 0.0968, 0.1561])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2372, 0.5672, 0.0337, 0.2173, 0.5986]) \n",
      "Test Loss tensor([0.2391, 0.5838, 0.0337, 0.2208, 0.5904])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0985, 0.0129, 0.2719, 0.2576, 0.2029, 0.1036, 0.0214]) \n",
      "Test Loss tensor([0.0965, 0.0125, 0.2772, 0.2592, 0.2049, 0.1041, 0.0208])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0405, 0.2223, 0.0431, 0.0905, 0.3007, 0.2790, 0.0445]) \n",
      "Test Loss tensor([0.0355, 0.2251, 0.0452, 0.0869, 0.3069, 0.2737, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0242, 0.0262, 0.0375, 0.0209, 0.0336, 0.0166, 0.1332, 0.0045]) \n",
      "Test Loss tensor([0.0288, 0.0263, 0.0372, 0.0211, 0.0325, 0.0161, 0.1347, 0.0049])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5858, 0.7122, 0.1078, 0.1847, 0.4030, 0.4556])\n",
      "Valid Idx 3 | Loss tensor([0.4570, 0.0992, 0.1360, 0.6875, 0.8663])\n",
      "\n",
      "************** Batch 576 in 4.722173452377319 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0779, 0.2029, 0.1004, 0.1604]) \n",
      "Test Loss tensor([0.0764, 0.2022, 0.0952, 0.1553])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2312, 0.5931, 0.0333, 0.2180, 0.5809]) \n",
      "Test Loss tensor([0.2347, 0.5851, 0.0443, 0.2254, 0.5816])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0924, 0.0120, 0.2828, 0.2581, 0.2090, 0.1099, 0.0210]) \n",
      "Test Loss tensor([0.0974, 0.0116, 0.2756, 0.2492, 0.2107, 0.1020, 0.0261])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0336, 0.2264, 0.0484, 0.0854, 0.3061, 0.2603, 0.0510]) \n",
      "Test Loss tensor([0.0346, 0.2307, 0.0426, 0.0833, 0.3063, 0.2752, 0.0493])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0291, 0.0241, 0.0436, 0.0211, 0.0325, 0.0155, 0.1451, 0.0045]) \n",
      "Test Loss tensor([0.0309, 0.0253, 0.0385, 0.0200, 0.0299, 0.0189, 0.1364, 0.0055])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5817, 0.7310, 0.1085, 0.1767, 0.4039, 0.4341])\n",
      "Valid Idx 3 | Loss tensor([0.3776, 0.0788, 0.1301, 0.6487, 0.8556])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 580 in 4.659484386444092 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0783, 0.2004, 0.0886, 0.1449]) \n",
      "Test Loss tensor([0.0744, 0.2013, 0.0959, 0.1542])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2359, 0.5721, 0.0452, 0.2159, 0.5776]) \n",
      "Test Loss tensor([0.2386, 0.5864, 0.0437, 0.2271, 0.5755])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0970, 0.0114, 0.2682, 0.2418, 0.2099, 0.1068, 0.0289]) \n",
      "Test Loss tensor([0.0967, 0.0125, 0.2758, 0.2530, 0.2072, 0.1039, 0.0275])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0350, 0.2208, 0.0397, 0.0873, 0.3045, 0.2886, 0.0497]) \n",
      "Test Loss tensor([0.0369, 0.2257, 0.0445, 0.0854, 0.3032, 0.2763, 0.0479])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0263, 0.0355, 0.0253, 0.0314, 0.0185, 0.1440, 0.0056]) \n",
      "Test Loss tensor([0.0295, 0.0241, 0.0400, 0.0213, 0.0325, 0.0184, 0.1373, 0.0057])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5864, 0.7117, 0.1105, 0.1845, 0.3974, 0.4500])\n",
      "Valid Idx 3 | Loss tensor([0.4365, 0.0876, 0.1367, 0.6807, 0.8568])\n",
      "Gradients: Input 0.4716203808784485 | Message 0.5770756006240845 | Update 0.6712455749511719 | Output 0.14326581358909607\n",
      "\n",
      "************** Batch 584 in 4.812218189239502 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0732, 0.2037, 0.0928, 0.1455]) \n",
      "Test Loss tensor([0.0729, 0.2029, 0.0947, 0.1539])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2333, 0.5869, 0.0445, 0.2173, 0.5801]) \n",
      "Test Loss tensor([0.2407, 0.5810, 0.0379, 0.2253, 0.5817])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0962, 0.0126, 0.2779, 0.2561, 0.2039, 0.0999, 0.0296]) \n",
      "Test Loss tensor([0.0938, 0.0133, 0.2782, 0.2559, 0.2057, 0.1073, 0.0249])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0354, 0.2281, 0.0421, 0.0923, 0.2908, 0.2926, 0.0465]) \n",
      "Test Loss tensor([0.0416, 0.2258, 0.0444, 0.0865, 0.3000, 0.2733, 0.0484])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0245, 0.0400, 0.0210, 0.0332, 0.0183, 0.1268, 0.0056]) \n",
      "Test Loss tensor([0.0282, 0.0249, 0.0406, 0.0219, 0.0347, 0.0167, 0.1369, 0.0056])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5871, 0.6999, 0.1077, 0.1868, 0.3923, 0.4579])\n",
      "Valid Idx 3 | Loss tensor([0.4927, 0.1012, 0.1407, 0.7012, 0.8589])\n",
      "\n",
      "************** Batch 588 in 4.825472593307495 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0769, 0.2007, 0.0994, 0.1575]) \n",
      "Test Loss tensor([0.0753, 0.2035, 0.0908, 0.1504])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2480, 0.5905, 0.0412, 0.2329, 0.5732]) \n",
      "Test Loss tensor([0.2339, 0.5843, 0.0474, 0.2293, 0.5679])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1030, 0.0127, 0.2882, 0.2612, 0.2074, 0.1095, 0.0247]) \n",
      "Test Loss tensor([0.1000, 0.0116, 0.2722, 0.2486, 0.2130, 0.1036, 0.0303])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0419, 0.2212, 0.0436, 0.0864, 0.2981, 0.2755, 0.0507]) \n",
      "Test Loss tensor([0.0360, 0.2241, 0.0438, 0.0856, 0.3063, 0.2762, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0282, 0.0255, 0.0388, 0.0242, 0.0335, 0.0159, 0.1382, 0.0053]) \n",
      "Test Loss tensor([0.0317, 0.0264, 0.0395, 0.0205, 0.0300, 0.0199, 0.1370, 0.0062])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5799, 0.7381, 0.1108, 0.1748, 0.4034, 0.4428])\n",
      "Valid Idx 3 | Loss tensor([0.3699, 0.0832, 0.1285, 0.6400, 0.8501])\n",
      "\n",
      "************** Batch 592 in 4.8441002368927 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0710, 0.1924, 0.0972, 0.1485]) \n",
      "Test Loss tensor([0.0750, 0.2031, 0.0937, 0.1525])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2423, 0.5824, 0.0437, 0.2197, 0.5607]) \n",
      "Test Loss tensor([0.2372, 0.5859, 0.0364, 0.2264, 0.5763])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1016, 0.0125, 0.2722, 0.2574, 0.2170, 0.1076, 0.0289]) \n",
      "Test Loss tensor([0.0973, 0.0127, 0.2779, 0.2549, 0.2087, 0.1052, 0.0238])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0407, 0.2365, 0.0427, 0.0873, 0.3125, 0.2778, 0.0541]) \n",
      "Test Loss tensor([0.0365, 0.2271, 0.0427, 0.0843, 0.3068, 0.2722, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0289, 0.0269, 0.0355, 0.0238, 0.0280, 0.0196, 0.1331, 0.0064]) \n",
      "Test Loss tensor([0.0299, 0.0251, 0.0402, 0.0204, 0.0305, 0.0177, 0.1352, 0.0053])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5826, 0.7201, 0.1087, 0.1792, 0.4045, 0.4505])\n",
      "Valid Idx 3 | Loss tensor([0.4315, 0.1022, 0.1346, 0.6645, 0.8656])\n",
      "\n",
      "************** Batch 596 in 4.757252216339111 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0821, 0.1899, 0.0934, 0.1603]) \n",
      "Test Loss tensor([0.0745, 0.1988, 0.0945, 0.1521])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2353, 0.5743, 0.0376, 0.2196, 0.5896]) \n",
      "Test Loss tensor([0.2392, 0.5855, 0.0345, 0.2263, 0.5779])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0931, 0.0121, 0.2851, 0.2653, 0.2084, 0.0992, 0.0247]) \n",
      "Test Loss tensor([0.0953, 0.0126, 0.2758, 0.2606, 0.2086, 0.1030, 0.0217])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0348, 0.2214, 0.0435, 0.0839, 0.2992, 0.2768, 0.0536]) \n",
      "Test Loss tensor([0.0360, 0.2268, 0.0442, 0.0852, 0.3048, 0.2741, 0.0484])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0279, 0.0235, 0.0339, 0.0202, 0.0309, 0.0172, 0.1347, 0.0054]) \n",
      "Test Loss tensor([0.0294, 0.0246, 0.0394, 0.0219, 0.0315, 0.0168, 0.1358, 0.0052])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5864, 0.7135, 0.1065, 0.1814, 0.4019, 0.4549])\n",
      "Valid Idx 3 | Loss tensor([0.4554, 0.1122, 0.1382, 0.6771, 0.8667])\n",
      "\n",
      "************** Batch 600 in 4.748358249664307 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0677, 0.2139, 0.0972, 0.1483]) \n",
      "Test Loss tensor([0.0748, 0.2021, 0.0923, 0.1489])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2404, 0.5824, 0.0332, 0.2263, 0.5767]) \n",
      "Test Loss tensor([0.2315, 0.5806, 0.0438, 0.2247, 0.5715])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0987, 0.0121, 0.2783, 0.2607, 0.2143, 0.1039, 0.0199]) \n",
      "Test Loss tensor([0.1015, 0.0114, 0.2761, 0.2517, 0.2165, 0.1032, 0.0275])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0379, 0.2252, 0.0451, 0.0896, 0.3079, 0.2830, 0.0487]) \n",
      "Test Loss tensor([0.0328, 0.2325, 0.0447, 0.0843, 0.3047, 0.2786, 0.0513])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0312, 0.0231, 0.0385, 0.0244, 0.0324, 0.0165, 0.1350, 0.0063]) \n",
      "Test Loss tensor([0.0331, 0.0246, 0.0374, 0.0217, 0.0280, 0.0203, 0.1374, 0.0060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5801, 0.7417, 0.1098, 0.1734, 0.4085, 0.4335])\n",
      "Valid Idx 3 | Loss tensor([0.3485, 0.0891, 0.1290, 0.6319, 0.8511])\n",
      "Gradients: Input 0.5660135746002197 | Message 0.6687192916870117 | Update 0.7943743467330933 | Output 0.12808433175086975\n",
      "\n",
      "************** Batch 604 in 4.828106641769409 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0765, 0.1920, 0.0954, 0.1526]) \n",
      "Test Loss tensor([0.0729, 0.2024, 0.0936, 0.1511])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2332, 0.5994, 0.0414, 0.2336, 0.5612]) \n",
      "Test Loss tensor([0.2345, 0.5841, 0.0373, 0.2250, 0.5697])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0965, 0.0113, 0.2722, 0.2545, 0.2111, 0.1031, 0.0301]) \n",
      "Test Loss tensor([0.0958, 0.0128, 0.2740, 0.2538, 0.2078, 0.1058, 0.0251])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0353, 0.2300, 0.0423, 0.0863, 0.3065, 0.2836, 0.0489]) \n",
      "Test Loss tensor([0.0380, 0.2231, 0.0433, 0.0857, 0.3004, 0.2740, 0.0492])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0296, 0.0259, 0.0397, 0.0189, 0.0280, 0.0211, 0.1367, 0.0062]) \n",
      "Test Loss tensor([0.0288, 0.0252, 0.0394, 0.0212, 0.0325, 0.0173, 0.1379, 0.0058])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5904, 0.7081, 0.1088, 0.1844, 0.3953, 0.4555])\n",
      "Valid Idx 3 | Loss tensor([0.4466, 0.1079, 0.1392, 0.6878, 0.8575])\n",
      "\n",
      "************** Batch 608 in 4.90480637550354 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0790, 0.1988, 0.0927, 0.1544]) \n",
      "Test Loss tensor([0.0733, 0.2024, 0.0930, 0.1485])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2301, 0.5820, 0.0395, 0.2285, 0.5686]) \n",
      "Test Loss tensor([0.2317, 0.5794, 0.0398, 0.2278, 0.5710])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1007, 0.0150, 0.2914, 0.2534, 0.2095, 0.1099, 0.0257]) \n",
      "Test Loss tensor([0.0975, 0.0137, 0.2801, 0.2539, 0.2091, 0.1059, 0.0254])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0385, 0.2363, 0.0475, 0.0876, 0.2997, 0.2727, 0.0534]) \n",
      "Test Loss tensor([0.0407, 0.2277, 0.0446, 0.0877, 0.2982, 0.2698, 0.0495])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0286, 0.0225, 0.0373, 0.0222, 0.0304, 0.0167, 0.1299, 0.0059]) \n",
      "Test Loss tensor([0.0294, 0.0246, 0.0400, 0.0206, 0.0334, 0.0170, 0.1371, 0.0060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5820, 0.7043, 0.1071, 0.1850, 0.3971, 0.4587])\n",
      "Valid Idx 3 | Loss tensor([0.4505, 0.1078, 0.1399, 0.7000, 0.8643])\n",
      "\n",
      "************** Batch 612 in 4.9315505027771 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0718, 0.1877, 0.0943, 0.1542]) \n",
      "Test Loss tensor([0.0772, 0.2056, 0.0888, 0.1461])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2314, 0.5637, 0.0391, 0.2164, 0.5851]) \n",
      "Test Loss tensor([0.2273, 0.5871, 0.0574, 0.2243, 0.5481])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1003, 0.0127, 0.2711, 0.2448, 0.2125, 0.1079, 0.0292]) \n",
      "Test Loss tensor([0.1038, 0.0116, 0.2744, 0.2478, 0.2168, 0.1013, 0.0362])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0429, 0.2315, 0.0436, 0.0916, 0.3032, 0.2759, 0.0539]) \n",
      "Test Loss tensor([0.0327, 0.2255, 0.0420, 0.0840, 0.3061, 0.2800, 0.0495])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0270, 0.0285, 0.0404, 0.0185, 0.0362, 0.0159, 0.1387, 0.0066]) \n",
      "Test Loss tensor([0.0340, 0.0245, 0.0369, 0.0203, 0.0269, 0.0236, 0.1384, 0.0076])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5792, 0.7430, 0.1154, 0.1692, 0.4092, 0.4142])\n",
      "Valid Idx 3 | Loss tensor([0.2976, 0.0726, 0.1261, 0.6309, 0.8333])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 616 in 4.913614988327026 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0736, 0.1901, 0.0916, 0.1474]) \n",
      "Test Loss tensor([0.0770, 0.2011, 0.0994, 0.1544])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2304, 0.5882, 0.0583, 0.2190, 0.5658]) \n",
      "Test Loss tensor([0.2446, 0.5833, 0.0302, 0.2256, 0.5702])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1101, 0.0121, 0.2668, 0.2551, 0.2165, 0.1026, 0.0331]) \n",
      "Test Loss tensor([0.0906, 0.0150, 0.2794, 0.2716, 0.2085, 0.1085, 0.0201])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0343, 0.2284, 0.0422, 0.0865, 0.3038, 0.2790, 0.0536]) \n",
      "Test Loss tensor([0.0446, 0.2244, 0.0451, 0.0893, 0.3044, 0.2670, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0320, 0.0216, 0.0368, 0.0212, 0.0266, 0.0224, 0.1367, 0.0079]) \n",
      "Test Loss tensor([0.0264, 0.0243, 0.0418, 0.0221, 0.0384, 0.0138, 0.1423, 0.0052])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5995, 0.6684, 0.1048, 0.1948, 0.3845, 0.4758])\n",
      "Valid Idx 3 | Loss tensor([0.5728, 0.1508, 0.1513, 0.7588, 0.8696])\n",
      "\n",
      "************** Batch 620 in 4.949658393859863 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0708, 0.2064, 0.1018, 0.1511]) \n",
      "Test Loss tensor([0.0730, 0.2011, 0.0908, 0.1468])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2390, 0.5782, 0.0311, 0.2299, 0.5710]) \n",
      "Test Loss tensor([0.2285, 0.5861, 0.0419, 0.2201, 0.5567])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0918, 0.0149, 0.2733, 0.2785, 0.1975, 0.1041, 0.0195]) \n",
      "Test Loss tensor([0.0966, 0.0118, 0.2730, 0.2504, 0.2164, 0.1052, 0.0269])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0432, 0.2300, 0.0414, 0.0872, 0.2889, 0.2681, 0.0554]) \n",
      "Test Loss tensor([0.0355, 0.2320, 0.0447, 0.0855, 0.3019, 0.2752, 0.0509])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0298, 0.0240, 0.0446, 0.0226, 0.0390, 0.0127, 0.1488, 0.0054]) \n",
      "Test Loss tensor([0.0304, 0.0245, 0.0377, 0.0213, 0.0278, 0.0197, 0.1386, 0.0065])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5846, 0.7332, 0.1088, 0.1689, 0.4057, 0.4244])\n",
      "Valid Idx 3 | Loss tensor([0.3654, 0.0915, 0.1297, 0.6477, 0.8480])\n",
      "Gradients: Input 1.9393982887268066 | Message 2.326260805130005 | Update 2.7168312072753906 | Output 0.4692224860191345\n",
      "\n",
      "************** Batch 624 in 4.8799920082092285 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0684, 0.2127, 0.0927, 0.1400]) \n",
      "Test Loss tensor([0.0745, 0.2012, 0.0916, 0.1430])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2267, 0.5907, 0.0462, 0.2186, 0.5511]) \n",
      "Test Loss tensor([0.2279, 0.5859, 0.0493, 0.2237, 0.5506])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0974, 0.0127, 0.2815, 0.2492, 0.2106, 0.1014, 0.0247]) \n",
      "Test Loss tensor([0.1025, 0.0107, 0.2769, 0.2520, 0.2244, 0.1047, 0.0316])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0331, 0.2295, 0.0419, 0.0851, 0.2979, 0.2904, 0.0509]) \n",
      "Test Loss tensor([0.0313, 0.2241, 0.0420, 0.0803, 0.2997, 0.2807, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0275, 0.0217, 0.0414, 0.0209, 0.0264, 0.0204, 0.1385, 0.0065]) \n",
      "Test Loss tensor([0.0339, 0.0244, 0.0368, 0.0204, 0.0256, 0.0238, 0.1448, 0.0070])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5816, 0.7492, 0.1113, 0.1629, 0.4071, 0.4074])\n",
      "Valid Idx 3 | Loss tensor([0.2988, 0.0797, 0.1259, 0.6122, 0.8362])\n",
      "\n",
      "************** Batch 628 in 4.813041925430298 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0734, 0.2005, 0.0942, 0.1431]) \n",
      "Test Loss tensor([0.0775, 0.2054, 0.0982, 0.1529])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2285, 0.5721, 0.0499, 0.2266, 0.5455]) \n",
      "Test Loss tensor([0.2495, 0.5778, 0.0255, 0.2257, 0.5718])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1014, 0.0103, 0.2752, 0.2580, 0.2189, 0.1012, 0.0294]) \n",
      "Test Loss tensor([0.0924, 0.0157, 0.2817, 0.2757, 0.2095, 0.1112, 0.0182])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0312, 0.2343, 0.0402, 0.0819, 0.3079, 0.2751, 0.0560]) \n",
      "Test Loss tensor([0.0430, 0.2247, 0.0444, 0.0898, 0.3016, 0.2719, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0338, 0.0214, 0.0376, 0.0197, 0.0259, 0.0239, 0.1352, 0.0071]) \n",
      "Test Loss tensor([0.0261, 0.0243, 0.0415, 0.0216, 0.0377, 0.0127, 0.1399, 0.0051])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5968, 0.6527, 0.1039, 0.1965, 0.3845, 0.4815])\n",
      "Valid Idx 3 | Loss tensor([0.6040, 0.1782, 0.1619, 0.7601, 0.8715])\n",
      "\n",
      "************** Batch 632 in 4.800314426422119 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0794, 0.1938, 0.0983, 0.1542]) \n",
      "Test Loss tensor([0.0721, 0.2026, 0.0895, 0.1428])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2487, 0.5695, 0.0236, 0.2229, 0.5827]) \n",
      "Test Loss tensor([0.2291, 0.5787, 0.0435, 0.2183, 0.5551])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0904, 0.0158, 0.2777, 0.2770, 0.2092, 0.1121, 0.0171]) \n",
      "Test Loss tensor([0.0948, 0.0119, 0.2792, 0.2532, 0.2156, 0.1015, 0.0312])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0475, 0.2167, 0.0429, 0.0896, 0.3064, 0.2663, 0.0486]) \n",
      "Test Loss tensor([0.0341, 0.2247, 0.0438, 0.0848, 0.2962, 0.2730, 0.0498])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0300, 0.0221, 0.0441, 0.0225, 0.0381, 0.0126, 0.1410, 0.0055]) \n",
      "Test Loss tensor([0.0303, 0.0260, 0.0375, 0.0211, 0.0283, 0.0196, 0.1366, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5807, 0.7172, 0.1116, 0.1729, 0.3987, 0.4264])\n",
      "Valid Idx 3 | Loss tensor([0.3718, 0.0993, 0.1325, 0.6521, 0.8434])\n",
      "\n",
      "************** Batch 636 in 4.797692775726318 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0700, 0.2093, 0.0857, 0.1346]) \n",
      "Test Loss tensor([0.0791, 0.2049, 0.0881, 0.1445])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2280, 0.5861, 0.0425, 0.2266, 0.5408]) \n",
      "Test Loss tensor([0.2257, 0.5856, 0.0622, 0.2300, 0.5288])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0977, 0.0119, 0.2728, 0.2632, 0.2153, 0.1059, 0.0334]) \n",
      "Test Loss tensor([0.1060, 0.0112, 0.2777, 0.2523, 0.2264, 0.1041, 0.0463])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0291, 0.2391, 0.0459, 0.0875, 0.3069, 0.2692, 0.0542]) \n",
      "Test Loss tensor([0.0310, 0.2300, 0.0427, 0.0833, 0.3004, 0.2784, 0.0520])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0262, 0.0232, 0.0326, 0.0189, 0.0274, 0.0196, 0.1299, 0.0067]) \n",
      "Test Loss tensor([0.0339, 0.0253, 0.0364, 0.0199, 0.0257, 0.0263, 0.1408, 0.0091])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5766, 0.7452, 0.1141, 0.1589, 0.4078, 0.3978])\n",
      "Valid Idx 3 | Loss tensor([0.2643, 0.0757, 0.1225, 0.6008, 0.8190])\n",
      "\n",
      "************** Batch 640 in 4.940439701080322 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0814, 0.1985, 0.0856, 0.1495]) \n",
      "Test Loss tensor([0.0750, 0.2046, 0.1024, 0.1530])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2295, 0.5886, 0.0594, 0.2270, 0.5348]) \n",
      "Test Loss tensor([0.2477, 0.5739, 0.0263, 0.2315, 0.5704])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1107, 0.0105, 0.2761, 0.2479, 0.2339, 0.1048, 0.0447]) \n",
      "Test Loss tensor([0.0911, 0.0185, 0.2822, 0.2808, 0.2056, 0.1182, 0.0193])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0313, 0.2276, 0.0418, 0.0856, 0.3036, 0.2769, 0.0481]) \n",
      "Test Loss tensor([0.0445, 0.2219, 0.0464, 0.0933, 0.3021, 0.2669, 0.0484])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0361, 0.0227, 0.0411, 0.0210, 0.0264, 0.0240, 0.1464, 0.0101]) \n",
      "Test Loss tensor([0.0248, 0.0255, 0.0421, 0.0232, 0.0434, 0.0124, 0.1469, 0.0055])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6062, 0.6324, 0.1045, 0.2013, 0.3738, 0.4994])\n",
      "Valid Idx 3 | Loss tensor([0.6445, 0.2177, 0.1727, 0.7741, 0.8731])\n",
      "Gradients: Input 2.314669370651245 | Message 2.787140369415283 | Update 3.2709906101226807 | Output 0.5768939852714539\n",
      "\n",
      "************** Batch 644 in 4.881886959075928 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0735, 0.2000, 0.1050, 0.1517]) \n",
      "Test Loss tensor([0.0700, 0.2017, 0.0911, 0.1403])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2593, 0.5765, 0.0244, 0.2423, 0.5708]) \n",
      "Test Loss tensor([0.2267, 0.5837, 0.0409, 0.2250, 0.5410])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0924, 0.0195, 0.2832, 0.2916, 0.2056, 0.1264, 0.0180]) \n",
      "Test Loss tensor([0.0957, 0.0131, 0.2832, 0.2536, 0.2113, 0.1018, 0.0305])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0471, 0.2302, 0.0463, 0.0926, 0.3046, 0.2589, 0.0475]) \n",
      "Test Loss tensor([0.0341, 0.2295, 0.0445, 0.0843, 0.2967, 0.2762, 0.0522])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0219, 0.0235, 0.0464, 0.0210, 0.0426, 0.0124, 0.1504, 0.0055]) \n",
      "Test Loss tensor([0.0301, 0.0258, 0.0382, 0.0205, 0.0301, 0.0189, 0.1375, 0.0075])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5872, 0.7084, 0.1085, 0.1755, 0.3988, 0.4434])\n",
      "Valid Idx 3 | Loss tensor([0.4099, 0.1208, 0.1350, 0.6625, 0.8540])\n",
      "\n",
      "************** Batch 648 in 5.048823118209839 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0724, 0.2115, 0.0947, 0.1459]) \n",
      "Test Loss tensor([0.0862, 0.2053, 0.0928, 0.1427])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2198, 0.5693, 0.0398, 0.2269, 0.5439]) \n",
      "Test Loss tensor([0.2272, 0.5855, 0.0746, 0.2317, 0.5082])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0963, 0.0129, 0.2814, 0.2542, 0.2150, 0.1059, 0.0268]) \n",
      "Test Loss tensor([0.1190, 0.0104, 0.2751, 0.2582, 0.2384, 0.1068, 0.0509])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0366, 0.2263, 0.0448, 0.0878, 0.3024, 0.2713, 0.0551]) \n",
      "Test Loss tensor([0.0264, 0.2285, 0.0423, 0.0806, 0.3039, 0.2845, 0.0520])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0225, 0.0426, 0.0205, 0.0307, 0.0183, 0.1302, 0.0080]) \n",
      "Test Loss tensor([0.0410, 0.0251, 0.0357, 0.0191, 0.0230, 0.0356, 0.1437, 0.0111])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5739, 0.7826, 0.1207, 0.1542, 0.4153, 0.3846])\n",
      "Valid Idx 3 | Loss tensor([0.1990, 0.0695, 0.1235, 0.5442, 0.8013])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 652 in 4.875625133514404 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0841, 0.2089, 0.0921, 0.1438]) \n",
      "Test Loss tensor([0.0795, 0.2052, 0.1020, 0.1578])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2230, 0.5679, 0.0704, 0.2272, 0.5214]) \n",
      "Test Loss tensor([0.2539, 0.5777, 0.0227, 0.2282, 0.5676])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1178, 0.0122, 0.2665, 0.2479, 0.2346, 0.1082, 0.0519]) \n",
      "Test Loss tensor([0.0931, 0.0181, 0.2839, 0.2989, 0.2099, 0.1138, 0.0167])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0243, 0.2300, 0.0428, 0.0839, 0.3003, 0.2782, 0.0528]) \n",
      "Test Loss tensor([0.0429, 0.2215, 0.0442, 0.0920, 0.3035, 0.2695, 0.0492])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0389, 0.0238, 0.0370, 0.0209, 0.0257, 0.0381, 0.1479, 0.0106]) \n",
      "Test Loss tensor([0.0248, 0.0237, 0.0421, 0.0228, 0.0420, 0.0120, 0.1430, 0.0051])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6046, 0.6434, 0.1042, 0.2029, 0.3786, 0.5054])\n",
      "Valid Idx 3 | Loss tensor([0.6662, 0.2678, 0.1768, 0.7665, 0.8797])\n",
      "\n",
      "************** Batch 656 in 4.857253074645996 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0779, 0.2017, 0.0997, 0.1550]) \n",
      "Test Loss tensor([0.0793, 0.2032, 0.0963, 0.1496])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2535, 0.5754, 0.0228, 0.2238, 0.5619]) \n",
      "Test Loss tensor([0.2487, 0.5836, 0.0242, 0.2255, 0.5603])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0890, 0.0196, 0.2853, 0.3019, 0.2107, 0.1134, 0.0144]) \n",
      "Test Loss tensor([0.0902, 0.0162, 0.2842, 0.2917, 0.2114, 0.1084, 0.0172])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0454, 0.2228, 0.0486, 0.0970, 0.2961, 0.2535, 0.0464]) \n",
      "Test Loss tensor([0.0405, 0.2229, 0.0446, 0.0892, 0.2996, 0.2684, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0231, 0.0262, 0.0396, 0.0233, 0.0425, 0.0114, 0.1472, 0.0051]) \n",
      "Test Loss tensor([0.0255, 0.0238, 0.0417, 0.0212, 0.0364, 0.0136, 0.1405, 0.0054])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5964, 0.6690, 0.1025, 0.1928, 0.3846, 0.4936])\n",
      "Valid Idx 3 | Loss tensor([0.6113, 0.2423, 0.1625, 0.7283, 0.8776])\n",
      "\n",
      "************** Batch 660 in 4.855572462081909 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0763, 0.1982, 0.0938, 0.1474]) \n",
      "Test Loss tensor([0.0806, 0.2037, 0.0954, 0.1390])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2604, 0.5779, 0.0225, 0.2212, 0.5693]) \n",
      "Test Loss tensor([0.2255, 0.5954, 0.0617, 0.2360, 0.5064])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0902, 0.0172, 0.2757, 0.2871, 0.2077, 0.1035, 0.0207]) \n",
      "Test Loss tensor([0.1094, 0.0102, 0.2779, 0.2538, 0.2414, 0.1123, 0.0418])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0415, 0.2237, 0.0498, 0.0846, 0.3021, 0.2723, 0.0472]) \n",
      "Test Loss tensor([0.0271, 0.2297, 0.0430, 0.0803, 0.3051, 0.2856, 0.0523])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0248, 0.0223, 0.0385, 0.0208, 0.0353, 0.0122, 0.1412, 0.0053]) \n",
      "Test Loss tensor([0.0398, 0.0245, 0.0350, 0.0195, 0.0221, 0.0345, 0.1471, 0.0109])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5710, 0.7953, 0.1128, 0.1512, 0.4181, 0.3843])\n",
      "Valid Idx 3 | Loss tensor([0.2151, 0.0797, 0.1205, 0.5367, 0.8041])\n",
      "Gradients: Input 2.1898481845855713 | Message 2.719810962677002 | Update 3.1805996894836426 | Output 0.5557390451431274\n",
      "\n",
      "************** Batch 664 in 4.859562873840332 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0817, 0.2033, 0.0927, 0.1390]) \n",
      "Test Loss tensor([0.0709, 0.2017, 0.0882, 0.1315])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2403, 0.5845, 0.0615, 0.2170, 0.5090]) \n",
      "Test Loss tensor([0.2248, 0.5884, 0.0471, 0.2287, 0.5171])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1099, 0.0105, 0.2818, 0.2555, 0.2443, 0.1172, 0.0466]) \n",
      "Test Loss tensor([0.0969, 0.0109, 0.2772, 0.2530, 0.2272, 0.1037, 0.0350])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0286, 0.2277, 0.0431, 0.0883, 0.3015, 0.2855, 0.0561]) \n",
      "Test Loss tensor([0.0312, 0.2286, 0.0435, 0.0828, 0.3019, 0.2761, 0.0512])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0371, 0.0229, 0.0322, 0.0183, 0.0219, 0.0360, 0.1456, 0.0101]) \n",
      "Test Loss tensor([0.0329, 0.0253, 0.0369, 0.0210, 0.0250, 0.0251, 0.1389, 0.0097])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5853, 0.7497, 0.1119, 0.1592, 0.4077, 0.4098])\n",
      "Valid Idx 3 | Loss tensor([0.3264, 0.1051, 0.1262, 0.5921, 0.8307])\n",
      "\n",
      "************** Batch 668 in 4.803919553756714 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0683, 0.1917, 0.0843, 0.1321]) \n",
      "Test Loss tensor([0.0772, 0.2042, 0.0922, 0.1395])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2325, 0.6025, 0.0509, 0.2325, 0.5070]) \n",
      "Test Loss tensor([0.2496, 0.5763, 0.0264, 0.2274, 0.5497])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0988, 0.0109, 0.2715, 0.2516, 0.2221, 0.1028, 0.0384]) \n",
      "Test Loss tensor([0.0893, 0.0170, 0.2826, 0.2864, 0.2072, 0.1093, 0.0218])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0310, 0.2386, 0.0444, 0.0879, 0.3029, 0.2776, 0.0542]) \n",
      "Test Loss tensor([0.0425, 0.2206, 0.0458, 0.0883, 0.2973, 0.2655, 0.0505])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0331, 0.0240, 0.0330, 0.0185, 0.0228, 0.0250, 0.1328, 0.0090]) \n",
      "Test Loss tensor([0.0250, 0.0243, 0.0425, 0.0232, 0.0387, 0.0139, 0.1483, 0.0068])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5969, 0.6460, 0.1053, 0.1949, 0.3778, 0.4844])\n",
      "Valid Idx 3 | Loss tensor([0.6395, 0.2314, 0.1660, 0.7347, 0.8686])\n",
      "\n",
      "************** Batch 672 in 4.839730262756348 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0808, 0.2037, 0.0894, 0.1415]) \n",
      "Test Loss tensor([0.0715, 0.1983, 0.0870, 0.1312])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2516, 0.5797, 0.0264, 0.2248, 0.5396]) \n",
      "Test Loss tensor([0.2325, 0.5780, 0.0370, 0.2251, 0.5242])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0866, 0.0187, 0.2860, 0.2825, 0.2037, 0.1083, 0.0211]) \n",
      "Test Loss tensor([0.0901, 0.0143, 0.2806, 0.2636, 0.2105, 0.1018, 0.0323])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0491, 0.2169, 0.0466, 0.0981, 0.2861, 0.2599, 0.0506]) \n",
      "Test Loss tensor([0.0390, 0.2245, 0.0435, 0.0860, 0.2949, 0.2684, 0.0516])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0269, 0.0257, 0.0453, 0.0232, 0.0383, 0.0148, 0.1574, 0.0068]) \n",
      "Test Loss tensor([0.0274, 0.0250, 0.0424, 0.0212, 0.0339, 0.0176, 0.1428, 0.0092])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5927, 0.6762, 0.1070, 0.1819, 0.3864, 0.4528])\n",
      "Valid Idx 3 | Loss tensor([0.5230, 0.1608, 0.1459, 0.6894, 0.8449])\n",
      "\n",
      "************** Batch 676 in 4.828570365905762 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0740, 0.1963, 0.0896, 0.1371]) \n",
      "Test Loss tensor([0.0776, 0.2132, 0.0850, 0.1444])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2425, 0.5739, 0.0361, 0.2144, 0.5320]) \n",
      "Test Loss tensor([0.2242, 0.5794, 0.0831, 0.2338, 0.4826])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0853, 0.0150, 0.2880, 0.2693, 0.2170, 0.1015, 0.0331]) \n",
      "Test Loss tensor([0.1046, 0.0105, 0.2759, 0.2586, 0.2391, 0.1023, 0.0700])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0353, 0.2279, 0.0462, 0.0963, 0.2886, 0.2850, 0.0492]) \n",
      "Test Loss tensor([0.0287, 0.2288, 0.0424, 0.0809, 0.3014, 0.2741, 0.0567])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0296, 0.0310, 0.0384, 0.0244, 0.0337, 0.0177, 0.1427, 0.0097]) \n",
      "Test Loss tensor([0.0366, 0.0252, 0.0372, 0.0199, 0.0243, 0.0355, 0.1402, 0.0185])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5752, 0.7624, 0.1232, 0.1563, 0.4129, 0.3849])\n",
      "Valid Idx 3 | Loss tensor([0.2448, 0.0748, 0.1232, 0.5661, 0.7736])\n",
      "\n",
      "************** Batch 680 in 5.079295873641968 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0758, 0.2127, 0.0884, 0.1452]) \n",
      "Test Loss tensor([0.0716, 0.2018, 0.0870, 0.1274])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2222, 0.5788, 0.0783, 0.2311, 0.4935]) \n",
      "Test Loss tensor([0.2374, 0.5796, 0.0341, 0.2212, 0.5231])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1088, 0.0108, 0.2855, 0.2593, 0.2453, 0.1055, 0.0736]) \n",
      "Test Loss tensor([0.0897, 0.0141, 0.2819, 0.2609, 0.2114, 0.1030, 0.0297])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0329, 0.2387, 0.0442, 0.0825, 0.3037, 0.2853, 0.0703]) \n",
      "Test Loss tensor([0.0391, 0.2215, 0.0446, 0.0877, 0.2935, 0.2673, 0.0510])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0335, 0.0251, 0.0352, 0.0193, 0.0244, 0.0336, 0.1451, 0.0185]) \n",
      "Test Loss tensor([0.0262, 0.0232, 0.0396, 0.0208, 0.0337, 0.0175, 0.1396, 0.0090])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5919, 0.6792, 0.1077, 0.1849, 0.3854, 0.4530])\n",
      "Valid Idx 3 | Loss tensor([0.5036, 0.1611, 0.1434, 0.6873, 0.8405])\n",
      "Gradients: Input 3.607335090637207 | Message 4.693624496459961 | Update 5.440816879272461 | Output 0.9074581861495972\n",
      "\n",
      "************** Batch 684 in 4.926440477371216 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0681, 0.1991, 0.0868, 0.1295]) \n",
      "Test Loss tensor([0.0765, 0.2040, 0.0920, 0.1327])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2372, 0.5857, 0.0354, 0.2227, 0.5289]) \n",
      "Test Loss tensor([0.2445, 0.5729, 0.0263, 0.2233, 0.5429])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0910, 0.0142, 0.2900, 0.2653, 0.2120, 0.1063, 0.0309]) \n",
      "Test Loss tensor([0.0905, 0.0159, 0.2833, 0.2828, 0.2133, 0.1071, 0.0221])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0372, 0.2305, 0.0452, 0.0914, 0.2959, 0.2641, 0.0498]) \n",
      "Test Loss tensor([0.0397, 0.2230, 0.0453, 0.0873, 0.2918, 0.2700, 0.0490])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0251, 0.0269, 0.0440, 0.0203, 0.0331, 0.0166, 0.1377, 0.0088]) \n",
      "Test Loss tensor([0.0255, 0.0240, 0.0401, 0.0231, 0.0364, 0.0141, 0.1403, 0.0069])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5951, 0.6630, 0.1056, 0.1930, 0.3808, 0.4763])\n",
      "Valid Idx 3 | Loss tensor([0.5814, 0.2178, 0.1529, 0.7198, 0.8633])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 688 in 5.011700868606567 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0821, 0.1914, 0.0927, 0.1378]) \n",
      "Test Loss tensor([0.0704, 0.2014, 0.0892, 0.1259])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2586, 0.5851, 0.0290, 0.2218, 0.5486]) \n",
      "Test Loss tensor([0.2256, 0.5895, 0.0398, 0.2231, 0.5132])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0888, 0.0160, 0.2897, 0.2841, 0.2183, 0.1084, 0.0228]) \n",
      "Test Loss tensor([0.0980, 0.0119, 0.2750, 0.2523, 0.2210, 0.1010, 0.0301])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0390, 0.2206, 0.0410, 0.0854, 0.2888, 0.2717, 0.0479]) \n",
      "Test Loss tensor([0.0311, 0.2231, 0.0424, 0.0823, 0.2886, 0.2763, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0222, 0.0228, 0.0398, 0.0196, 0.0375, 0.0135, 0.1324, 0.0071]) \n",
      "Test Loss tensor([0.0311, 0.0243, 0.0369, 0.0224, 0.0273, 0.0215, 0.1388, 0.0089])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5862, 0.7290, 0.1109, 0.1752, 0.4049, 0.4304])\n",
      "Valid Idx 3 | Loss tensor([0.3521, 0.1253, 0.1272, 0.6223, 0.8322])\n",
      "\n",
      "************** Batch 692 in 4.94828724861145 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0739, 0.1972, 0.0973, 0.1259]) \n",
      "Test Loss tensor([0.0772, 0.2024, 0.0937, 0.1302])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2291, 0.5865, 0.0432, 0.2138, 0.4970]) \n",
      "Test Loss tensor([0.2219, 0.5898, 0.0579, 0.2255, 0.4930])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1039, 0.0118, 0.2846, 0.2665, 0.2249, 0.1089, 0.0290]) \n",
      "Test Loss tensor([0.1131, 0.0110, 0.2720, 0.2530, 0.2310, 0.1046, 0.0421])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0296, 0.2339, 0.0428, 0.0823, 0.2914, 0.2800, 0.0474]) \n",
      "Test Loss tensor([0.0268, 0.2281, 0.0427, 0.0815, 0.2978, 0.2806, 0.0540])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0282, 0.0239, 0.0396, 0.0199, 0.0314, 0.0211, 0.1378, 0.0095]) \n",
      "Test Loss tensor([0.0360, 0.0248, 0.0351, 0.0216, 0.0247, 0.0322, 0.1437, 0.0118])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5870, 0.7680, 0.1139, 0.1603, 0.4127, 0.3996])\n",
      "Valid Idx 3 | Loss tensor([0.2313, 0.0866, 0.1180, 0.5635, 0.7943])\n",
      "\n",
      "************** Batch 696 in 4.966641187667847 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0784, 0.1973, 0.0905, 0.1349]) \n",
      "Test Loss tensor([0.0717, 0.2007, 0.0932, 0.1305])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2223, 0.5776, 0.0546, 0.2232, 0.4887]) \n",
      "Test Loss tensor([0.2280, 0.5828, 0.0327, 0.2198, 0.5168])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1058, 0.0100, 0.2675, 0.2425, 0.2334, 0.1110, 0.0456]) \n",
      "Test Loss tensor([0.0943, 0.0145, 0.2784, 0.2626, 0.2139, 0.1011, 0.0253])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0261, 0.2359, 0.0416, 0.0812, 0.2990, 0.2829, 0.0510]) \n",
      "Test Loss tensor([0.0360, 0.2246, 0.0442, 0.0863, 0.2913, 0.2702, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0340, 0.0225, 0.0375, 0.0179, 0.0246, 0.0293, 0.1432, 0.0113]) \n",
      "Test Loss tensor([0.0279, 0.0227, 0.0380, 0.0218, 0.0329, 0.0174, 0.1367, 0.0080])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5953, 0.6974, 0.1055, 0.1873, 0.3892, 0.4723])\n",
      "Valid Idx 3 | Loss tensor([0.4630, 0.1718, 0.1392, 0.6818, 0.8444])\n",
      "\n",
      "************** Batch 700 in 4.856966018676758 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0750, 0.1918, 0.0889, 0.1285]) \n",
      "Test Loss tensor([0.0745, 0.2014, 0.0963, 0.1336])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2372, 0.5596, 0.0327, 0.2089, 0.5284]) \n",
      "Test Loss tensor([0.2349, 0.5762, 0.0291, 0.2220, 0.5244])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0929, 0.0139, 0.2745, 0.2577, 0.2065, 0.1101, 0.0259]) \n",
      "Test Loss tensor([0.0908, 0.0174, 0.2794, 0.2722, 0.2114, 0.1068, 0.0235])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0350, 0.2233, 0.0410, 0.0899, 0.2958, 0.2782, 0.0539]) \n",
      "Test Loss tensor([0.0418, 0.2225, 0.0449, 0.0877, 0.2924, 0.2660, 0.0500])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0274, 0.0247, 0.0429, 0.0205, 0.0337, 0.0170, 0.1493, 0.0075]) \n",
      "Test Loss tensor([0.0251, 0.0235, 0.0401, 0.0229, 0.0400, 0.0147, 0.1431, 0.0076])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5971, 0.6603, 0.1061, 0.2035, 0.3800, 0.5046])\n",
      "Valid Idx 3 | Loss tensor([0.5575, 0.2193, 0.1523, 0.7332, 0.8611])\n",
      "Gradients: Input 0.9966806173324585 | Message 1.2086589336395264 | Update 1.4123475551605225 | Output 0.19210270047187805\n",
      "\n",
      "************** Batch 704 in 4.827477216720581 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0712, 0.1927, 0.0912, 0.1299]) \n",
      "Test Loss tensor([0.0723, 0.2026, 0.0898, 0.1261])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2393, 0.5796, 0.0290, 0.2297, 0.5371]) \n",
      "Test Loss tensor([0.2183, 0.5776, 0.0543, 0.2190, 0.4824])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0936, 0.0179, 0.2903, 0.2712, 0.2076, 0.1106, 0.0220]) \n",
      "Test Loss tensor([0.1013, 0.0129, 0.2781, 0.2481, 0.2206, 0.1012, 0.0430])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0428, 0.2279, 0.0453, 0.0883, 0.2902, 0.2651, 0.0509]) \n",
      "Test Loss tensor([0.0335, 0.2226, 0.0422, 0.0828, 0.2906, 0.2709, 0.0520])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0236, 0.0203, 0.0421, 0.0199, 0.0400, 0.0165, 0.1471, 0.0078]) \n",
      "Test Loss tensor([0.0302, 0.0241, 0.0371, 0.0208, 0.0300, 0.0243, 0.1366, 0.0124])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5863, 0.7288, 0.1115, 0.1800, 0.3885, 0.4525])\n",
      "Valid Idx 3 | Loss tensor([0.3388, 0.1157, 0.1302, 0.6324, 0.8094])\n",
      "\n",
      "************** Batch 708 in 4.8571250438690186 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0726, 0.1981, 0.0927, 0.1224]) \n",
      "Test Loss tensor([0.0718, 0.2002, 0.0866, 0.1277])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2203, 0.5588, 0.0526, 0.2113, 0.4840]) \n",
      "Test Loss tensor([0.2152, 0.5763, 0.0608, 0.2216, 0.4811])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0990, 0.0121, 0.2748, 0.2509, 0.2284, 0.1066, 0.0439]) \n",
      "Test Loss tensor([0.1043, 0.0128, 0.2754, 0.2473, 0.2211, 0.1011, 0.0487])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0289, 0.2250, 0.0462, 0.0891, 0.2874, 0.2776, 0.0464]) \n",
      "Test Loss tensor([0.0326, 0.2265, 0.0447, 0.0842, 0.2932, 0.2736, 0.0537])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0329, 0.0250, 0.0400, 0.0229, 0.0287, 0.0248, 0.1395, 0.0128]) \n",
      "Test Loss tensor([0.0307, 0.0243, 0.0350, 0.0206, 0.0291, 0.0265, 0.1389, 0.0141])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5820, 0.7333, 0.1129, 0.1780, 0.3962, 0.4459])\n",
      "Valid Idx 3 | Loss tensor([0.3189, 0.1100, 0.1274, 0.6174, 0.7996])\n",
      "\n",
      "************** Batch 712 in 5.00974440574646 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0733, 0.1926, 0.0910, 0.1275]) \n",
      "Test Loss tensor([0.0732, 0.2006, 0.0932, 0.1261])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2179, 0.5693, 0.0622, 0.2160, 0.4853]) \n",
      "Test Loss tensor([0.2271, 0.5749, 0.0347, 0.2189, 0.5074])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1002, 0.0122, 0.2761, 0.2426, 0.2200, 0.1035, 0.0509]) \n",
      "Test Loss tensor([0.0911, 0.0164, 0.2793, 0.2624, 0.2116, 0.1041, 0.0280])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0340, 0.2312, 0.0417, 0.0903, 0.2865, 0.2806, 0.0494]) \n",
      "Test Loss tensor([0.0398, 0.2208, 0.0447, 0.0886, 0.2925, 0.2647, 0.0501])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0320, 0.0264, 0.0385, 0.0210, 0.0288, 0.0269, 0.1410, 0.0126]) \n",
      "Test Loss tensor([0.0260, 0.0244, 0.0410, 0.0225, 0.0381, 0.0159, 0.1425, 0.0087])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5943, 0.6747, 0.1077, 0.2015, 0.3747, 0.5071])\n",
      "Valid Idx 3 | Loss tensor([0.5163, 0.1989, 0.1423, 0.7039, 0.8468])\n",
      "\n",
      "************** Batch 716 in 5.078541278839111 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0697, 0.2015, 0.0973, 0.1366]) \n",
      "Test Loss tensor([0.0721, 0.2044, 0.0917, 0.1257])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2226, 0.5672, 0.0324, 0.2262, 0.5222]) \n",
      "Test Loss tensor([0.2290, 0.5774, 0.0352, 0.2244, 0.5011])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0914, 0.0171, 0.2717, 0.2560, 0.2073, 0.1108, 0.0311]) \n",
      "Test Loss tensor([0.0904, 0.0152, 0.2802, 0.2622, 0.2123, 0.1026, 0.0270])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0424, 0.2221, 0.0427, 0.0859, 0.2850, 0.2763, 0.0445]) \n",
      "Test Loss tensor([0.0390, 0.2260, 0.0452, 0.0881, 0.2880, 0.2738, 0.0514])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0262, 0.0239, 0.0378, 0.0252, 0.0369, 0.0166, 0.1419, 0.0087]) \n",
      "Test Loss tensor([0.0258, 0.0231, 0.0387, 0.0224, 0.0353, 0.0166, 0.1395, 0.0084])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5949, 0.6912, 0.1060, 0.1953, 0.3813, 0.5062])\n",
      "Valid Idx 3 | Loss tensor([0.4852, 0.1906, 0.1366, 0.6856, 0.8437])\n",
      "\n",
      "************** Batch 720 in 4.991211175918579 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0748, 0.2053, 0.0953, 0.1194]) \n",
      "Test Loss tensor([0.0717, 0.2026, 0.0882, 0.1247])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2205, 0.5746, 0.0367, 0.2103, 0.5044]) \n",
      "Test Loss tensor([0.2183, 0.5893, 0.0568, 0.2237, 0.4652])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0813, 0.0157, 0.2669, 0.2506, 0.2159, 0.1077, 0.0250]) \n",
      "Test Loss tensor([0.1027, 0.0122, 0.2777, 0.2514, 0.2275, 0.1018, 0.0392])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0396, 0.2374, 0.0433, 0.0870, 0.2781, 0.2625, 0.0530]) \n",
      "Test Loss tensor([0.0301, 0.2251, 0.0432, 0.0825, 0.2906, 0.2732, 0.0508])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0254, 0.0178, 0.0361, 0.0190, 0.0348, 0.0163, 0.1375, 0.0084]) \n",
      "Test Loss tensor([0.0324, 0.0244, 0.0367, 0.0218, 0.0259, 0.0266, 0.1405, 0.0117])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5808, 0.7520, 0.1102, 0.1732, 0.4024, 0.4496])\n",
      "Valid Idx 3 | Loss tensor([0.2914, 0.1130, 0.1242, 0.5888, 0.8015])\n",
      "Gradients: Input 1.325575351715088 | Message 1.6768279075622559 | Update 1.928252935409546 | Output 0.2701762020587921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 724 in 4.905534744262695 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0751, 0.1984, 0.0921, 0.1328]) \n",
      "Test Loss tensor([0.0700, 0.2011, 0.0875, 0.1225])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2140, 0.5580, 0.0557, 0.2196, 0.4728]) \n",
      "Test Loss tensor([0.2184, 0.5784, 0.0485, 0.2233, 0.4753])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0971, 0.0134, 0.2817, 0.2544, 0.2318, 0.1019, 0.0361]) \n",
      "Test Loss tensor([0.0990, 0.0124, 0.2768, 0.2497, 0.2221, 0.1011, 0.0340])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0297, 0.2354, 0.0447, 0.0790, 0.2831, 0.2740, 0.0482]) \n",
      "Test Loss tensor([0.0320, 0.2252, 0.0439, 0.0835, 0.2866, 0.2755, 0.0509])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0276, 0.0255, 0.0420, 0.0183, 0.0262, 0.0269, 0.1366, 0.0122]) \n",
      "Test Loss tensor([0.0306, 0.0228, 0.0373, 0.0214, 0.0271, 0.0241, 0.1371, 0.0105])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5945, 0.7388, 0.1084, 0.1761, 0.3974, 0.4709])\n",
      "Valid Idx 3 | Loss tensor([0.3336, 0.1319, 0.1286, 0.6034, 0.8146])\n",
      "\n",
      "************** Batch 728 in 4.824129581451416 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0712, 0.2088, 0.0928, 0.1243]) \n",
      "Test Loss tensor([0.0712, 0.2057, 0.0891, 0.1228])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2215, 0.5678, 0.0469, 0.2251, 0.4823]) \n",
      "Test Loss tensor([0.2259, 0.5788, 0.0320, 0.2173, 0.4975])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0973, 0.0137, 0.2806, 0.2482, 0.2255, 0.0986, 0.0367]) \n",
      "Test Loss tensor([0.0891, 0.0153, 0.2778, 0.2625, 0.2095, 0.1014, 0.0249])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0349, 0.2185, 0.0484, 0.0894, 0.2866, 0.2773, 0.0506]) \n",
      "Test Loss tensor([0.0389, 0.2266, 0.0439, 0.0873, 0.2880, 0.2702, 0.0507])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0338, 0.0241, 0.0385, 0.0234, 0.0277, 0.0250, 0.1419, 0.0103]) \n",
      "Test Loss tensor([0.0260, 0.0237, 0.0389, 0.0218, 0.0326, 0.0157, 0.1381, 0.0078])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6028, 0.6890, 0.1045, 0.1952, 0.3794, 0.5161])\n",
      "Valid Idx 3 | Loss tensor([0.4941, 0.2132, 0.1382, 0.6780, 0.8505])\n",
      "\n",
      "************** Batch 732 in 4.8351640701293945 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0738, 0.2013, 0.0917, 0.1206]) \n",
      "Test Loss tensor([0.0714, 0.2014, 0.0861, 0.1215])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2342, 0.5894, 0.0351, 0.2298, 0.4900]) \n",
      "Test Loss tensor([0.2235, 0.5816, 0.0383, 0.2196, 0.4837])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0898, 0.0142, 0.2912, 0.2628, 0.2186, 0.1023, 0.0244]) \n",
      "Test Loss tensor([0.0909, 0.0145, 0.2818, 0.2586, 0.2146, 0.1015, 0.0299])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0398, 0.2342, 0.0491, 0.0859, 0.2880, 0.2807, 0.0538]) \n",
      "Test Loss tensor([0.0379, 0.2241, 0.0442, 0.0861, 0.2891, 0.2660, 0.0509])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0305, 0.0233, 0.0387, 0.0212, 0.0326, 0.0162, 0.1372, 0.0073]) \n",
      "Test Loss tensor([0.0268, 0.0241, 0.0394, 0.0216, 0.0308, 0.0175, 0.1378, 0.0091])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5940, 0.6995, 0.1079, 0.1884, 0.3848, 0.5021])\n",
      "Valid Idx 3 | Loss tensor([0.4470, 0.1801, 0.1334, 0.6605, 0.8364])\n",
      "\n",
      "************** Batch 736 in 4.9083757400512695 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0740, 0.2006, 0.0843, 0.1259]) \n",
      "Test Loss tensor([0.0709, 0.2001, 0.0861, 0.1258])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2209, 0.5753, 0.0377, 0.2294, 0.4707]) \n",
      "Test Loss tensor([0.2145, 0.5831, 0.0632, 0.2243, 0.4520])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0891, 0.0132, 0.2957, 0.2676, 0.2110, 0.1006, 0.0314]) \n",
      "Test Loss tensor([0.0990, 0.0122, 0.2737, 0.2504, 0.2275, 0.0997, 0.0468])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0368, 0.2370, 0.0524, 0.0908, 0.2899, 0.2674, 0.0528]) \n",
      "Test Loss tensor([0.0316, 0.2285, 0.0448, 0.0837, 0.2892, 0.2731, 0.0545])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0275, 0.0263, 0.0381, 0.0197, 0.0313, 0.0168, 0.1477, 0.0098]) \n",
      "Test Loss tensor([0.0312, 0.0248, 0.0363, 0.0210, 0.0255, 0.0258, 0.1432, 0.0132])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5833, 0.7364, 0.1126, 0.1696, 0.3932, 0.4580])\n",
      "Valid Idx 3 | Loss tensor([0.2907, 0.1111, 0.1196, 0.5966, 0.7834])\n",
      "\n",
      "************** Batch 740 in 4.818072080612183 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0705, 0.2035, 0.0861, 0.1267]) \n",
      "Test Loss tensor([0.0690, 0.1985, 0.0852, 0.1179])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2227, 0.5907, 0.0669, 0.2346, 0.4442]) \n",
      "Test Loss tensor([0.2226, 0.5781, 0.0392, 0.2184, 0.4740])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0995, 0.0126, 0.2708, 0.2487, 0.2277, 0.1012, 0.0465]) \n",
      "Test Loss tensor([0.0914, 0.0145, 0.2807, 0.2553, 0.2161, 0.1029, 0.0305])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0328, 0.2359, 0.0417, 0.0849, 0.2784, 0.2663, 0.0526]) \n",
      "Test Loss tensor([0.0390, 0.2262, 0.0453, 0.0851, 0.2938, 0.2635, 0.0521])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0285, 0.0237, 0.0376, 0.0197, 0.0282, 0.0266, 0.1428, 0.0144]) \n",
      "Test Loss tensor([0.0261, 0.0250, 0.0387, 0.0221, 0.0303, 0.0169, 0.1411, 0.0091])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5969, 0.6865, 0.1062, 0.1875, 0.3803, 0.4981])\n",
      "Valid Idx 3 | Loss tensor([0.4370, 0.1746, 0.1339, 0.6692, 0.8293])\n",
      "Gradients: Input 2.151644706726074 | Message 2.801280975341797 | Update 3.2105979919433594 | Output 0.5197873115539551\n",
      "\n",
      "************** Batch 744 in 4.871554613113403 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0670, 0.2122, 0.0826, 0.1189]) \n",
      "Test Loss tensor([0.0697, 0.2009, 0.0847, 0.1187])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2240, 0.5706, 0.0392, 0.2177, 0.4756]) \n",
      "Test Loss tensor([0.2249, 0.5755, 0.0363, 0.2189, 0.4819])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0987, 0.0147, 0.2962, 0.2653, 0.2095, 0.1057, 0.0318]) \n",
      "Test Loss tensor([0.0929, 0.0148, 0.2815, 0.2613, 0.2150, 0.1035, 0.0251])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0353, 0.2314, 0.0494, 0.0832, 0.2896, 0.2591, 0.0503]) \n",
      "Test Loss tensor([0.0359, 0.2250, 0.0436, 0.0831, 0.2880, 0.2675, 0.0501])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0265, 0.0253, 0.0422, 0.0258, 0.0305, 0.0183, 0.1408, 0.0088]) \n",
      "Test Loss tensor([0.0270, 0.0226, 0.0369, 0.0208, 0.0309, 0.0159, 0.1385, 0.0081])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5973, 0.6748, 0.1034, 0.1905, 0.3798, 0.5017])\n",
      "Valid Idx 3 | Loss tensor([0.4563, 0.1883, 0.1330, 0.6887, 0.8391])\n",
      "\n",
      "************** Batch 748 in 4.76279091835022 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0721, 0.1935, 0.0835, 0.1201]) \n",
      "Test Loss tensor([0.0704, 0.1997, 0.0876, 0.1198])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2248, 0.5725, 0.0390, 0.2208, 0.4927]) \n",
      "Test Loss tensor([0.2181, 0.5853, 0.0514, 0.2210, 0.4563])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0885, 0.0144, 0.2887, 0.2547, 0.2116, 0.1092, 0.0257]) \n",
      "Test Loss tensor([0.0987, 0.0123, 0.2799, 0.2508, 0.2217, 0.1003, 0.0367])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0361, 0.2310, 0.0501, 0.0931, 0.2812, 0.2651, 0.0507]) \n",
      "Test Loss tensor([0.0318, 0.2245, 0.0418, 0.0812, 0.2862, 0.2697, 0.0505])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0258, 0.0233, 0.0378, 0.0213, 0.0313, 0.0167, 0.1365, 0.0082]) \n",
      "Test Loss tensor([0.0310, 0.0240, 0.0357, 0.0212, 0.0257, 0.0231, 0.1388, 0.0104])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5823, 0.7214, 0.1084, 0.1706, 0.3972, 0.4557])\n",
      "Valid Idx 3 | Loss tensor([0.3071, 0.1198, 0.1234, 0.6226, 0.8014])\n",
      "\n",
      "************** Batch 752 in 4.906579971313477 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0756, 0.2034, 0.0875, 0.1304]) \n",
      "Test Loss tensor([0.0705, 0.1998, 0.0873, 0.1202])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2200, 0.5824, 0.0499, 0.2137, 0.4597]) \n",
      "Test Loss tensor([0.2173, 0.5775, 0.0475, 0.2191, 0.4597])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0973, 0.0122, 0.2732, 0.2491, 0.2182, 0.0991, 0.0346]) \n",
      "Test Loss tensor([0.0969, 0.0125, 0.2784, 0.2481, 0.2200, 0.1023, 0.0340])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0296, 0.2322, 0.0421, 0.0766, 0.3007, 0.2778, 0.0481]) \n",
      "Test Loss tensor([0.0322, 0.2274, 0.0427, 0.0828, 0.2886, 0.2702, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0334, 0.0268, 0.0406, 0.0222, 0.0288, 0.0234, 0.1432, 0.0107]) \n",
      "Test Loss tensor([0.0294, 0.0242, 0.0351, 0.0211, 0.0266, 0.0215, 0.1379, 0.0099])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5912, 0.7064, 0.1110, 0.1773, 0.3923, 0.4630])\n",
      "Valid Idx 3 | Loss tensor([0.3287, 0.1255, 0.1253, 0.6321, 0.8082])\n",
      "\n",
      "************** Batch 756 in 4.897956609725952 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0712, 0.1983, 0.0918, 0.1276]) \n",
      "Test Loss tensor([0.0697, 0.2014, 0.0894, 0.1191])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2157, 0.5539, 0.0444, 0.2271, 0.4652]) \n",
      "Test Loss tensor([0.2259, 0.5727, 0.0354, 0.2136, 0.4761])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1018, 0.0134, 0.2754, 0.2540, 0.2255, 0.1026, 0.0395]) \n",
      "Test Loss tensor([0.0919, 0.0146, 0.2775, 0.2555, 0.2154, 0.1035, 0.0252])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0314, 0.2348, 0.0420, 0.0842, 0.3015, 0.2662, 0.0490]) \n",
      "Test Loss tensor([0.0349, 0.2283, 0.0462, 0.0858, 0.2870, 0.2670, 0.0522])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0271, 0.0220, 0.0321, 0.0194, 0.0253, 0.0211, 0.1385, 0.0101]) \n",
      "Test Loss tensor([0.0263, 0.0229, 0.0379, 0.0207, 0.0305, 0.0155, 0.1379, 0.0079])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5972, 0.6670, 0.1051, 0.1906, 0.3807, 0.4883])\n",
      "Valid Idx 3 | Loss tensor([0.4534, 0.1834, 0.1339, 0.7001, 0.8392])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 760 in 4.8157641887664795 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0688, 0.2018, 0.0865, 0.1232]) \n",
      "Test Loss tensor([0.0690, 0.2041, 0.0857, 0.1186])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2324, 0.5614, 0.0376, 0.2176, 0.4715]) \n",
      "Test Loss tensor([0.2216, 0.5794, 0.0399, 0.2158, 0.4628])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0891, 0.0140, 0.2700, 0.2696, 0.2055, 0.1045, 0.0265]) \n",
      "Test Loss tensor([0.0908, 0.0145, 0.2806, 0.2551, 0.2159, 0.1027, 0.0291])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0358, 0.2270, 0.0454, 0.0814, 0.2765, 0.2636, 0.0470]) \n",
      "Test Loss tensor([0.0337, 0.2270, 0.0424, 0.0844, 0.2874, 0.2679, 0.0514])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0251, 0.0230, 0.0380, 0.0220, 0.0333, 0.0152, 0.1390, 0.0072]) \n",
      "Test Loss tensor([0.0262, 0.0237, 0.0372, 0.0214, 0.0305, 0.0172, 0.1383, 0.0086])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5972, 0.6760, 0.1054, 0.1842, 0.3793, 0.4812])\n",
      "Valid Idx 3 | Loss tensor([0.4313, 0.1661, 0.1291, 0.6899, 0.8220])\n",
      "Gradients: Input 1.1057591438293457 | Message 1.4187575578689575 | Update 1.6300235986709595 | Output 0.2359796166419983\n",
      "\n",
      "************** Batch 764 in 4.878949403762817 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0684, 0.2108, 0.0934, 0.1166]) \n",
      "Test Loss tensor([0.0679, 0.2013, 0.0854, 0.1218])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2221, 0.5827, 0.0376, 0.2171, 0.4577]) \n",
      "Test Loss tensor([0.2175, 0.5805, 0.0551, 0.2198, 0.4426])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0862, 0.0151, 0.2703, 0.2508, 0.2044, 0.1154, 0.0294]) \n",
      "Test Loss tensor([0.0960, 0.0123, 0.2732, 0.2502, 0.2205, 0.1018, 0.0387])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0343, 0.2351, 0.0442, 0.0873, 0.2863, 0.2790, 0.0443]) \n",
      "Test Loss tensor([0.0324, 0.2266, 0.0426, 0.0831, 0.2857, 0.2671, 0.0523])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0256, 0.0212, 0.0373, 0.0218, 0.0313, 0.0182, 0.1343, 0.0083]) \n",
      "Test Loss tensor([0.0288, 0.0231, 0.0355, 0.0210, 0.0272, 0.0223, 0.1384, 0.0116])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5917, 0.7002, 0.1122, 0.1722, 0.3884, 0.4490])\n",
      "Valid Idx 3 | Loss tensor([0.3483, 0.1273, 0.1245, 0.6477, 0.7884])\n",
      "\n",
      "************** Batch 768 in 4.945773124694824 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0670, 0.2001, 0.0832, 0.1190]) \n",
      "Test Loss tensor([0.0685, 0.1979, 0.0859, 0.1211])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2181, 0.5705, 0.0539, 0.2092, 0.4414]) \n",
      "Test Loss tensor([0.2206, 0.5803, 0.0489, 0.2204, 0.4453])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0939, 0.0129, 0.2963, 0.2556, 0.2070, 0.1044, 0.0376]) \n",
      "Test Loss tensor([0.0938, 0.0132, 0.2827, 0.2512, 0.2177, 0.1012, 0.0356])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0322, 0.2363, 0.0456, 0.0793, 0.2935, 0.2533, 0.0512]) \n",
      "Test Loss tensor([0.0338, 0.2242, 0.0433, 0.0838, 0.2878, 0.2641, 0.0528])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0272, 0.0257, 0.0345, 0.0215, 0.0259, 0.0215, 0.1431, 0.0132]) \n",
      "Test Loss tensor([0.0286, 0.0229, 0.0382, 0.0207, 0.0279, 0.0195, 0.1392, 0.0106])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5895, 0.6914, 0.1092, 0.1791, 0.3806, 0.4633])\n",
      "Valid Idx 3 | Loss tensor([0.3914, 0.1472, 0.1264, 0.6662, 0.8006])\n",
      "\n",
      "************** Batch 772 in 4.8739914894104 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0648, 0.1925, 0.0878, 0.1272]) \n",
      "Test Loss tensor([0.0689, 0.1986, 0.0857, 0.1176])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2183, 0.5847, 0.0486, 0.2182, 0.4499]) \n",
      "Test Loss tensor([0.2249, 0.5767, 0.0375, 0.2177, 0.4571])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0989, 0.0128, 0.2781, 0.2438, 0.2321, 0.1063, 0.0370]) \n",
      "Test Loss tensor([0.0900, 0.0154, 0.2829, 0.2578, 0.2140, 0.1036, 0.0273])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0334, 0.2207, 0.0379, 0.0870, 0.2854, 0.2733, 0.0494]) \n",
      "Test Loss tensor([0.0358, 0.2253, 0.0450, 0.0856, 0.2877, 0.2668, 0.0518])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0276, 0.0216, 0.0385, 0.0199, 0.0292, 0.0219, 0.1401, 0.0100]) \n",
      "Test Loss tensor([0.0264, 0.0228, 0.0381, 0.0222, 0.0319, 0.0155, 0.1392, 0.0085])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5993, 0.6611, 0.1084, 0.1883, 0.3781, 0.4945])\n",
      "Valid Idx 3 | Loss tensor([0.4809, 0.1946, 0.1337, 0.6978, 0.8270])\n",
      "\n",
      "************** Batch 776 in 4.895148754119873 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0672, 0.1952, 0.0861, 0.1170]) \n",
      "Test Loss tensor([0.0674, 0.1995, 0.0868, 0.1167])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2139, 0.5908, 0.0366, 0.2166, 0.4465]) \n",
      "Test Loss tensor([0.2210, 0.5805, 0.0427, 0.2188, 0.4481])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0870, 0.0157, 0.2768, 0.2536, 0.2068, 0.1034, 0.0252]) \n",
      "Test Loss tensor([0.0912, 0.0141, 0.2798, 0.2535, 0.2164, 0.1000, 0.0308])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0370, 0.2253, 0.0416, 0.0822, 0.2827, 0.2763, 0.0546]) \n",
      "Test Loss tensor([0.0343, 0.2261, 0.0427, 0.0852, 0.2888, 0.2673, 0.0512])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0207, 0.0254, 0.0390, 0.0207, 0.0319, 0.0163, 0.1407, 0.0091]) \n",
      "Test Loss tensor([0.0271, 0.0230, 0.0372, 0.0219, 0.0297, 0.0185, 0.1369, 0.0090])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5969, 0.6843, 0.1071, 0.1817, 0.3883, 0.4841])\n",
      "Valid Idx 3 | Loss tensor([0.4251, 0.1756, 0.1319, 0.6688, 0.8169])\n",
      "\n",
      "************** Batch 780 in 4.895448684692383 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0674, 0.1829, 0.0840, 0.1194]) \n",
      "Test Loss tensor([0.0691, 0.2008, 0.0862, 0.1213])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2275, 0.5769, 0.0395, 0.2096, 0.4497]) \n",
      "Test Loss tensor([0.2191, 0.5797, 0.0522, 0.2212, 0.4293])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0938, 0.0140, 0.2748, 0.2537, 0.2135, 0.1040, 0.0304]) \n",
      "Test Loss tensor([0.0970, 0.0128, 0.2741, 0.2461, 0.2231, 0.1005, 0.0352])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0401, 0.2301, 0.0449, 0.0879, 0.2818, 0.2763, 0.0531]) \n",
      "Test Loss tensor([0.0323, 0.2281, 0.0419, 0.0814, 0.2894, 0.2666, 0.0501])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0272, 0.0251, 0.0372, 0.0182, 0.0285, 0.0177, 0.1317, 0.0092]) \n",
      "Test Loss tensor([0.0295, 0.0228, 0.0367, 0.0210, 0.0260, 0.0229, 0.1388, 0.0108])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5930, 0.7152, 0.1092, 0.1708, 0.3952, 0.4643])\n",
      "Valid Idx 3 | Loss tensor([0.3400, 0.1390, 0.1229, 0.6232, 0.7859])\n",
      "Gradients: Input 0.3330789804458618 | Message 0.3620246648788452 | Update 0.4214555025100708 | Output 0.04468144476413727\n",
      "\n",
      "************** Batch 784 in 4.820953369140625 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0691, 0.1998, 0.0839, 0.1186]) \n",
      "Test Loss tensor([0.0689, 0.2030, 0.0857, 0.1163])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2115, 0.5740, 0.0496, 0.2129, 0.4315]) \n",
      "Test Loss tensor([0.2224, 0.5796, 0.0378, 0.2194, 0.4491])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0895, 0.0122, 0.2640, 0.2430, 0.2090, 0.0957, 0.0365]) \n",
      "Test Loss tensor([0.0919, 0.0142, 0.2826, 0.2557, 0.2160, 0.1011, 0.0270])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0296, 0.2243, 0.0426, 0.0874, 0.2885, 0.2707, 0.0429]) \n",
      "Test Loss tensor([0.0345, 0.2264, 0.0435, 0.0836, 0.2832, 0.2669, 0.0497])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0230, 0.0332, 0.0172, 0.0247, 0.0224, 0.1369, 0.0102]) \n",
      "Test Loss tensor([0.0271, 0.0243, 0.0380, 0.0215, 0.0291, 0.0173, 0.1356, 0.0078])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5912, 0.6790, 0.1086, 0.1866, 0.3833, 0.4991])\n",
      "Valid Idx 3 | Loss tensor([0.4468, 0.1960, 0.1303, 0.6744, 0.8261])\n",
      "\n",
      "************** Batch 788 in 4.885638475418091 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0668, 0.1960, 0.0852, 0.1200]) \n",
      "Test Loss tensor([0.0701, 0.2024, 0.0863, 0.1190])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2294, 0.5973, 0.0362, 0.2238, 0.4310]) \n",
      "Test Loss tensor([0.2287, 0.5825, 0.0376, 0.2153, 0.4477])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0876, 0.0148, 0.2815, 0.2440, 0.2098, 0.0992, 0.0264]) \n",
      "Test Loss tensor([0.0912, 0.0148, 0.2819, 0.2580, 0.2155, 0.1032, 0.0271])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0358, 0.2303, 0.0430, 0.0795, 0.2857, 0.2746, 0.0543]) \n",
      "Test Loss tensor([0.0356, 0.2279, 0.0434, 0.0817, 0.2841, 0.2638, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0253, 0.0215, 0.0427, 0.0195, 0.0298, 0.0169, 0.1349, 0.0079]) \n",
      "Test Loss tensor([0.0260, 0.0242, 0.0374, 0.0219, 0.0297, 0.0162, 0.1368, 0.0080])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5967, 0.6687, 0.1064, 0.1878, 0.3803, 0.5064])\n",
      "Valid Idx 3 | Loss tensor([0.4555, 0.2010, 0.1307, 0.6850, 0.8269])\n",
      "\n",
      "************** Batch 792 in 4.92533016204834 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0641, 0.2030, 0.0826, 0.1162]) \n",
      "Test Loss tensor([0.0687, 0.2001, 0.0862, 0.1189])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2280, 0.5803, 0.0376, 0.2127, 0.4448]) \n",
      "Test Loss tensor([0.2188, 0.5802, 0.0490, 0.2185, 0.4274])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0857, 0.0163, 0.2794, 0.2531, 0.2164, 0.0990, 0.0252]) \n",
      "Test Loss tensor([0.0933, 0.0130, 0.2831, 0.2542, 0.2232, 0.0994, 0.0353])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0354, 0.2242, 0.0488, 0.0914, 0.2865, 0.2653, 0.0486]) \n",
      "Test Loss tensor([0.0322, 0.2240, 0.0430, 0.0816, 0.2847, 0.2671, 0.0512])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0237, 0.0234, 0.0318, 0.0213, 0.0289, 0.0160, 0.1360, 0.0078]) \n",
      "Test Loss tensor([0.0277, 0.0217, 0.0367, 0.0211, 0.0275, 0.0217, 0.1365, 0.0101])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5888, 0.6951, 0.1096, 0.1810, 0.3890, 0.4792])\n",
      "Valid Idx 3 | Loss tensor([0.3644, 0.1503, 0.1255, 0.6430, 0.7909])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 796 in 4.955554008483887 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0709, 0.1934, 0.0811, 0.1191]) \n",
      "Test Loss tensor([0.0674, 0.2014, 0.0855, 0.1179])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2231, 0.5601, 0.0477, 0.2070, 0.4280]) \n",
      "Test Loss tensor([0.2164, 0.5742, 0.0469, 0.2162, 0.4283])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0972, 0.0142, 0.2888, 0.2536, 0.2198, 0.1080, 0.0367]) \n",
      "Test Loss tensor([0.0934, 0.0141, 0.2789, 0.2492, 0.2182, 0.0987, 0.0342])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0277, 0.2407, 0.0412, 0.0825, 0.2875, 0.2728, 0.0507]) \n",
      "Test Loss tensor([0.0342, 0.2264, 0.0452, 0.0843, 0.2853, 0.2682, 0.0542])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0278, 0.0214, 0.0377, 0.0179, 0.0270, 0.0224, 0.1322, 0.0097]) \n",
      "Test Loss tensor([0.0268, 0.0234, 0.0368, 0.0207, 0.0275, 0.0200, 0.1360, 0.0100])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5946, 0.6844, 0.1087, 0.1846, 0.3862, 0.4912])\n",
      "Valid Idx 3 | Loss tensor([0.3923, 0.1608, 0.1239, 0.6611, 0.7937])\n",
      "\n",
      "************** Batch 800 in 4.8385021686553955 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0660, 0.2075, 0.0867, 0.1138]) \n",
      "Test Loss tensor([0.0688, 0.2031, 0.0878, 0.1174])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2194, 0.5690, 0.0473, 0.2123, 0.4352]) \n",
      "Test Loss tensor([0.2203, 0.5729, 0.0385, 0.2138, 0.4415])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0887, 0.0141, 0.2718, 0.2426, 0.2111, 0.0998, 0.0331]) \n",
      "Test Loss tensor([0.0911, 0.0152, 0.2819, 0.2550, 0.2147, 0.1023, 0.0284])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0335, 0.2204, 0.0440, 0.0834, 0.2751, 0.2740, 0.0496]) \n",
      "Test Loss tensor([0.0368, 0.2278, 0.0440, 0.0822, 0.2871, 0.2688, 0.0516])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0205, 0.0378, 0.0185, 0.0277, 0.0198, 0.1387, 0.0098]) \n",
      "Test Loss tensor([0.0273, 0.0237, 0.0385, 0.0220, 0.0306, 0.0164, 0.1378, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5935, 0.6599, 0.1082, 0.1944, 0.3758, 0.5144])\n",
      "Valid Idx 3 | Loss tensor([0.4637, 0.1991, 0.1324, 0.7017, 0.8168])\n",
      "Gradients: Input 0.33232223987579346 | Message 0.46968117356300354 | Update 0.5186534523963928 | Output 0.09446588158607483\n",
      "\n",
      "************** Batch 804 in 5.015851020812988 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0678, 0.2114, 0.0832, 0.1166]) \n",
      "Test Loss tensor([0.0665, 0.2022, 0.0857, 0.1149])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2206, 0.5763, 0.0422, 0.2130, 0.4271]) \n",
      "Test Loss tensor([0.2171, 0.5773, 0.0465, 0.2156, 0.4224])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0900, 0.0144, 0.2781, 0.2538, 0.2206, 0.1042, 0.0228]) \n",
      "Test Loss tensor([0.0922, 0.0134, 0.2759, 0.2494, 0.2184, 0.0998, 0.0321])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0368, 0.2237, 0.0437, 0.0939, 0.2857, 0.2768, 0.0473]) \n",
      "Test Loss tensor([0.0339, 0.2288, 0.0426, 0.0825, 0.2837, 0.2693, 0.0516])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0220, 0.0347, 0.0230, 0.0311, 0.0170, 0.1292, 0.0085]) \n",
      "Test Loss tensor([0.0277, 0.0233, 0.0364, 0.0207, 0.0276, 0.0193, 0.1342, 0.0096])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5956, 0.6775, 0.1065, 0.1856, 0.3858, 0.4983])\n",
      "Valid Idx 3 | Loss tensor([0.3922, 0.1637, 0.1251, 0.6703, 0.7948])\n",
      "\n",
      "************** Batch 808 in 5.057358026504517 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0685, 0.2048, 0.0869, 0.1200]) \n",
      "Test Loss tensor([0.0686, 0.2023, 0.0855, 0.1175])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2224, 0.5705, 0.0460, 0.2101, 0.4324]) \n",
      "Test Loss tensor([0.2171, 0.5815, 0.0478, 0.2171, 0.4172])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0928, 0.0135, 0.2822, 0.2393, 0.2086, 0.1004, 0.0338]) \n",
      "Test Loss tensor([0.0960, 0.0132, 0.2816, 0.2525, 0.2238, 0.1007, 0.0337])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0364, 0.2299, 0.0426, 0.0816, 0.2861, 0.2770, 0.0506]) \n",
      "Test Loss tensor([0.0324, 0.2274, 0.0430, 0.0795, 0.2840, 0.2632, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0275, 0.0235, 0.0380, 0.0224, 0.0278, 0.0194, 0.1447, 0.0103]) \n",
      "Test Loss tensor([0.0274, 0.0222, 0.0350, 0.0204, 0.0263, 0.0204, 0.1343, 0.0096])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5904, 0.6877, 0.1080, 0.1810, 0.3857, 0.4893])\n",
      "Valid Idx 3 | Loss tensor([0.3584, 0.1539, 0.1207, 0.6591, 0.7868])\n",
      "\n",
      "************** Batch 812 in 4.947007417678833 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0673, 0.1900, 0.0871, 0.1230]) \n",
      "Test Loss tensor([0.0673, 0.2003, 0.0857, 0.1139])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2109, 0.5620, 0.0444, 0.2087, 0.4369]) \n",
      "Test Loss tensor([0.2197, 0.5779, 0.0399, 0.2128, 0.4306])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0922, 0.0141, 0.2745, 0.2438, 0.2295, 0.1050, 0.0343]) \n",
      "Test Loss tensor([0.0914, 0.0138, 0.2819, 0.2541, 0.2175, 0.1003, 0.0264])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0324, 0.2401, 0.0396, 0.0916, 0.2905, 0.2791, 0.0434]) \n",
      "Test Loss tensor([0.0328, 0.2266, 0.0446, 0.0833, 0.2868, 0.2664, 0.0502])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0300, 0.0215, 0.0373, 0.0218, 0.0268, 0.0212, 0.1370, 0.0104]) \n",
      "Test Loss tensor([0.0263, 0.0235, 0.0356, 0.0199, 0.0275, 0.0165, 0.1359, 0.0079])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5953, 0.6712, 0.1046, 0.1847, 0.3799, 0.5045])\n",
      "Valid Idx 3 | Loss tensor([0.4016, 0.1800, 0.1265, 0.6799, 0.8116])\n",
      "\n",
      "************** Batch 816 in 4.916054010391235 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0725, 0.2043, 0.0853, 0.1182]) \n",
      "Test Loss tensor([0.0688, 0.2018, 0.0857, 0.1138])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2260, 0.5830, 0.0382, 0.2206, 0.4241]) \n",
      "Test Loss tensor([0.2198, 0.5863, 0.0419, 0.2184, 0.4220])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0927, 0.0136, 0.2855, 0.2540, 0.2111, 0.1014, 0.0253]) \n",
      "Test Loss tensor([0.0941, 0.0134, 0.2803, 0.2574, 0.2205, 0.0997, 0.0255])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0339, 0.2229, 0.0464, 0.0931, 0.2868, 0.2667, 0.0563]) \n",
      "Test Loss tensor([0.0321, 0.2285, 0.0429, 0.0839, 0.2848, 0.2676, 0.0514])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0250, 0.0210, 0.0339, 0.0173, 0.0278, 0.0161, 0.1258, 0.0074]) \n",
      "Test Loss tensor([0.0286, 0.0229, 0.0376, 0.0217, 0.0264, 0.0175, 0.1356, 0.0079])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5932, 0.6699, 0.1074, 0.1869, 0.3727, 0.5015])\n",
      "Valid Idx 3 | Loss tensor([0.3788, 0.1745, 0.1253, 0.6688, 0.8083])\n",
      "\n",
      "************** Batch 820 in 4.975252866744995 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0663, 0.1974, 0.0842, 0.1133]) \n",
      "Test Loss tensor([0.0689, 0.2010, 0.0849, 0.1139])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2229, 0.5816, 0.0400, 0.2176, 0.4230]) \n",
      "Test Loss tensor([0.2170, 0.5855, 0.0460, 0.2181, 0.4157])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0944, 0.0144, 0.2875, 0.2417, 0.2137, 0.1016, 0.0255]) \n",
      "Test Loss tensor([0.0969, 0.0131, 0.2828, 0.2525, 0.2239, 0.1001, 0.0286])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0348, 0.2304, 0.0474, 0.0833, 0.2784, 0.2542, 0.0541]) \n",
      "Test Loss tensor([0.0309, 0.2327, 0.0439, 0.0813, 0.2846, 0.2690, 0.0500])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0273, 0.0250, 0.0358, 0.0252, 0.0270, 0.0173, 0.1334, 0.0073]) \n",
      "Test Loss tensor([0.0305, 0.0225, 0.0369, 0.0220, 0.0254, 0.0194, 0.1344, 0.0088])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5962, 0.6804, 0.1075, 0.1809, 0.3822, 0.4931])\n",
      "Valid Idx 3 | Loss tensor([0.3465, 0.1539, 0.1209, 0.6518, 0.7898])\n",
      "Gradients: Input 0.19220709800720215 | Message 0.2387540489435196 | Update 0.2573266625404358 | Output 0.01845794916152954\n",
      "\n",
      "************** Batch 824 in 4.973744869232178 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0686, 0.1953, 0.0799, 0.1084]) \n",
      "Test Loss tensor([0.0685, 0.1997, 0.0848, 0.1153])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2241, 0.5792, 0.0485, 0.2051, 0.4183]) \n",
      "Test Loss tensor([0.2190, 0.5769, 0.0405, 0.2111, 0.4224])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1020, 0.0139, 0.2727, 0.2531, 0.2168, 0.1027, 0.0316]) \n",
      "Test Loss tensor([0.0915, 0.0137, 0.2770, 0.2541, 0.2185, 0.0995, 0.0263])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0353, 0.2360, 0.0375, 0.0817, 0.2827, 0.2656, 0.0525]) \n",
      "Test Loss tensor([0.0328, 0.2261, 0.0443, 0.0830, 0.2850, 0.2652, 0.0502])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0271, 0.0225, 0.0353, 0.0199, 0.0239, 0.0194, 0.1374, 0.0084]) \n",
      "Test Loss tensor([0.0254, 0.0225, 0.0365, 0.0202, 0.0279, 0.0166, 0.1341, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5987, 0.6579, 0.1060, 0.1889, 0.3795, 0.5115])\n",
      "Valid Idx 3 | Loss tensor([0.4005, 0.1804, 0.1242, 0.6887, 0.8101])\n",
      "\n",
      "************** Batch 828 in 4.9731457233428955 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0717, 0.2022, 0.0883, 0.1172]) \n",
      "Test Loss tensor([0.0667, 0.2034, 0.0839, 0.1158])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2267, 0.6011, 0.0445, 0.2189, 0.4113]) \n",
      "Test Loss tensor([0.2185, 0.5790, 0.0449, 0.2150, 0.4136])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0837, 0.0142, 0.2688, 0.2479, 0.2144, 0.1041, 0.0200]) \n",
      "Test Loss tensor([0.0940, 0.0142, 0.2810, 0.2503, 0.2163, 0.0992, 0.0285])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0339, 0.2433, 0.0389, 0.0875, 0.2890, 0.2795, 0.0493]) \n",
      "Test Loss tensor([0.0341, 0.2256, 0.0447, 0.0810, 0.2819, 0.2677, 0.0504])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0265, 0.0262, 0.0417, 0.0225, 0.0256, 0.0167, 0.1464, 0.0079]) \n",
      "Test Loss tensor([0.0277, 0.0230, 0.0372, 0.0198, 0.0273, 0.0172, 0.1375, 0.0089])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5950, 0.6574, 0.1080, 0.1894, 0.3751, 0.5050])\n",
      "Valid Idx 3 | Loss tensor([0.3887, 0.1707, 0.1225, 0.6825, 0.7963])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 832 in 5.007543325424194 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0659, 0.2008, 0.0816, 0.1154]) \n",
      "Test Loss tensor([0.0675, 0.2029, 0.0831, 0.1168])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2004, 0.5776, 0.0440, 0.2149, 0.4196]) \n",
      "Test Loss tensor([0.2155, 0.5799, 0.0470, 0.2160, 0.4068])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0894, 0.0141, 0.2909, 0.2494, 0.2175, 0.1042, 0.0274]) \n",
      "Test Loss tensor([0.0937, 0.0133, 0.2847, 0.2511, 0.2210, 0.0998, 0.0310])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0336, 0.2300, 0.0479, 0.0833, 0.2816, 0.2586, 0.0515]) \n",
      "Test Loss tensor([0.0331, 0.2306, 0.0433, 0.0828, 0.2832, 0.2646, 0.0524])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0270, 0.0239, 0.0358, 0.0195, 0.0301, 0.0175, 0.1277, 0.0091]) \n",
      "Test Loss tensor([0.0294, 0.0225, 0.0365, 0.0206, 0.0267, 0.0188, 0.1358, 0.0099])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5982, 0.6633, 0.1131, 0.1881, 0.3759, 0.5012])\n",
      "Valid Idx 3 | Loss tensor([0.3777, 0.1591, 0.1247, 0.6756, 0.7863])\n",
      "\n",
      "************** Batch 836 in 5.03716254234314 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0699, 0.2057, 0.0816, 0.1227]) \n",
      "Test Loss tensor([0.0669, 0.2060, 0.0858, 0.1140])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2262, 0.5820, 0.0524, 0.2114, 0.4059]) \n",
      "Test Loss tensor([0.2201, 0.5745, 0.0416, 0.2133, 0.4106])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0929, 0.0142, 0.2791, 0.2502, 0.2160, 0.0988, 0.0275]) \n",
      "Test Loss tensor([0.0910, 0.0141, 0.2810, 0.2519, 0.2170, 0.1011, 0.0268])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0346, 0.2241, 0.0378, 0.0821, 0.2852, 0.2630, 0.0575]) \n",
      "Test Loss tensor([0.0337, 0.2290, 0.0425, 0.0801, 0.2814, 0.2609, 0.0498])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0281, 0.0189, 0.0339, 0.0220, 0.0268, 0.0192, 0.1399, 0.0087]) \n",
      "Test Loss tensor([0.0267, 0.0222, 0.0374, 0.0215, 0.0286, 0.0169, 0.1368, 0.0089])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6004, 0.6548, 0.1083, 0.1904, 0.3797, 0.5161])\n",
      "Valid Idx 3 | Loss tensor([0.4172, 0.1793, 0.1261, 0.6967, 0.7999])\n",
      "\n",
      "************** Batch 840 in 5.10103702545166 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0659, 0.2016, 0.0914, 0.1199]) \n",
      "Test Loss tensor([0.0665, 0.2002, 0.0848, 0.1172])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2231, 0.5720, 0.0407, 0.2071, 0.4277]) \n",
      "Test Loss tensor([0.2179, 0.5790, 0.0415, 0.2124, 0.4046])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0897, 0.0147, 0.2796, 0.2572, 0.2118, 0.0952, 0.0257]) \n",
      "Test Loss tensor([0.0947, 0.0138, 0.2841, 0.2532, 0.2174, 0.1024, 0.0268])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0343, 0.2176, 0.0496, 0.0864, 0.2817, 0.2751, 0.0542]) \n",
      "Test Loss tensor([0.0333, 0.2313, 0.0450, 0.0827, 0.2824, 0.2662, 0.0522])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0284, 0.0233, 0.0398, 0.0221, 0.0266, 0.0183, 0.1430, 0.0090]) \n",
      "Test Loss tensor([0.0280, 0.0226, 0.0353, 0.0203, 0.0276, 0.0177, 0.1324, 0.0087])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5911, 0.6623, 0.1091, 0.1877, 0.3793, 0.5088])\n",
      "Valid Idx 3 | Loss tensor([0.3918, 0.1716, 0.1246, 0.6840, 0.8006])\n",
      "Gradients: Input 0.46080148220062256 | Message 0.540238618850708 | Update 0.6186144351959229 | Output 0.07212632894515991\n",
      "\n",
      "************** Batch 844 in 4.958958148956299 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0704, 0.1980, 0.0934, 0.1197]) \n",
      "Test Loss tensor([0.0660, 0.1981, 0.0846, 0.1167])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2341, 0.5732, 0.0424, 0.2069, 0.4129]) \n",
      "Test Loss tensor([0.2165, 0.5804, 0.0464, 0.2152, 0.4017])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0923, 0.0133, 0.2956, 0.2482, 0.2133, 0.0991, 0.0267]) \n",
      "Test Loss tensor([0.0968, 0.0131, 0.2771, 0.2515, 0.2196, 0.0990, 0.0271])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0372, 0.2394, 0.0425, 0.0842, 0.2980, 0.2673, 0.0561]) \n",
      "Test Loss tensor([0.0308, 0.2299, 0.0450, 0.0803, 0.2803, 0.2647, 0.0536])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0317, 0.0224, 0.0414, 0.0160, 0.0262, 0.0178, 0.1358, 0.0087]) \n",
      "Test Loss tensor([0.0285, 0.0224, 0.0356, 0.0207, 0.0261, 0.0198, 0.1340, 0.0090])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5869, 0.6775, 0.1082, 0.1833, 0.3830, 0.5006])\n",
      "Valid Idx 3 | Loss tensor([0.3461, 0.1534, 0.1219, 0.6637, 0.7844])\n",
      "\n",
      "************** Batch 848 in 4.891747951507568 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0714, 0.1936, 0.0858, 0.1214]) \n",
      "Test Loss tensor([0.0674, 0.2006, 0.0841, 0.1143])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2177, 0.5752, 0.0457, 0.2145, 0.3942]) \n",
      "Test Loss tensor([0.2185, 0.5767, 0.0354, 0.2102, 0.4146])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0852, 0.0142, 0.2893, 0.2445, 0.2101, 0.0963, 0.0281]) \n",
      "Test Loss tensor([0.0919, 0.0137, 0.2800, 0.2580, 0.2171, 0.1021, 0.0220])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0319, 0.2258, 0.0465, 0.0890, 0.2834, 0.2662, 0.0509]) \n",
      "Test Loss tensor([0.0334, 0.2265, 0.0424, 0.0819, 0.2820, 0.2638, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0271, 0.0344, 0.0207, 0.0250, 0.0208, 0.1356, 0.0093]) \n",
      "Test Loss tensor([0.0257, 0.0223, 0.0372, 0.0214, 0.0284, 0.0159, 0.1346, 0.0073])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5957, 0.6526, 0.1059, 0.1902, 0.3816, 0.5189])\n",
      "Valid Idx 3 | Loss tensor([0.4146, 0.1940, 0.1243, 0.6907, 0.8146])\n",
      "\n",
      "************** Batch 852 in 4.940681219100952 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0680, 0.2080, 0.0817, 0.1141]) \n",
      "Test Loss tensor([0.0659, 0.2024, 0.0849, 0.1133])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2226, 0.5715, 0.0374, 0.2066, 0.4050]) \n",
      "Test Loss tensor([0.2211, 0.5810, 0.0374, 0.2158, 0.4064])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0952, 0.0146, 0.2711, 0.2530, 0.2050, 0.1003, 0.0246]) \n",
      "Test Loss tensor([0.0916, 0.0131, 0.2764, 0.2549, 0.2152, 0.0998, 0.0220])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0316, 0.2355, 0.0421, 0.0774, 0.2801, 0.2623, 0.0505]) \n",
      "Test Loss tensor([0.0341, 0.2288, 0.0431, 0.0822, 0.2826, 0.2608, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0240, 0.0255, 0.0344, 0.0194, 0.0276, 0.0166, 0.1360, 0.0064]) \n",
      "Test Loss tensor([0.0278, 0.0221, 0.0357, 0.0210, 0.0262, 0.0165, 0.1323, 0.0075])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5909, 0.6613, 0.1094, 0.1849, 0.3795, 0.5193])\n",
      "Valid Idx 3 | Loss tensor([0.3943, 0.1858, 0.1253, 0.6792, 0.8115])\n",
      "\n",
      "************** Batch 856 in 4.93570351600647 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0631, 0.2116, 0.0826, 0.1100]) \n",
      "Test Loss tensor([0.0677, 0.2010, 0.0852, 0.1181])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2208, 0.5844, 0.0361, 0.2215, 0.4037]) \n",
      "Test Loss tensor([0.2160, 0.5846, 0.0441, 0.2153, 0.3932])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0884, 0.0142, 0.2871, 0.2575, 0.2183, 0.1018, 0.0250]) \n",
      "Test Loss tensor([0.0935, 0.0128, 0.2796, 0.2500, 0.2199, 0.0993, 0.0259])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0295, 0.2331, 0.0422, 0.0830, 0.2792, 0.2529, 0.0520]) \n",
      "Test Loss tensor([0.0319, 0.2268, 0.0444, 0.0808, 0.2812, 0.2641, 0.0505])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0260, 0.0239, 0.0320, 0.0225, 0.0267, 0.0154, 0.1293, 0.0072]) \n",
      "Test Loss tensor([0.0286, 0.0220, 0.0376, 0.0211, 0.0265, 0.0185, 0.1349, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5941, 0.6723, 0.1085, 0.1843, 0.3866, 0.5100])\n",
      "Valid Idx 3 | Loss tensor([0.3608, 0.1647, 0.1214, 0.6651, 0.7890])\n",
      "\n",
      "************** Batch 860 in 4.935574531555176 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0711, 0.1965, 0.0795, 0.1126]) \n",
      "Test Loss tensor([0.0650, 0.1996, 0.0827, 0.1145])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2116, 0.5765, 0.0416, 0.2145, 0.3985]) \n",
      "Test Loss tensor([0.2176, 0.5795, 0.0397, 0.2157, 0.3969])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0981, 0.0134, 0.2719, 0.2512, 0.2059, 0.0943, 0.0262]) \n",
      "Test Loss tensor([0.0944, 0.0139, 0.2785, 0.2554, 0.2211, 0.1023, 0.0250])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0326, 0.2203, 0.0432, 0.0831, 0.2701, 0.2641, 0.0574]) \n",
      "Test Loss tensor([0.0335, 0.2253, 0.0427, 0.0811, 0.2848, 0.2649, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0257, 0.0214, 0.0384, 0.0190, 0.0252, 0.0186, 0.1169, 0.0087]) \n",
      "Test Loss tensor([0.0264, 0.0233, 0.0360, 0.0210, 0.0281, 0.0163, 0.1336, 0.0084])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5913, 0.6636, 0.1065, 0.1844, 0.3767, 0.5190])\n",
      "Valid Idx 3 | Loss tensor([0.4023, 0.1826, 0.1241, 0.6861, 0.7958])\n",
      "Gradients: Input 0.4941592514514923 | Message 0.6595796346664429 | Update 0.7349993586540222 | Output 0.11431418359279633\n",
      "\n",
      "************** Batch 864 in 4.921573162078857 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0665, 0.2090, 0.0797, 0.1186]) \n",
      "Test Loss tensor([0.0679, 0.2021, 0.0836, 0.1160])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2289, 0.5830, 0.0423, 0.2139, 0.3829]) \n",
      "Test Loss tensor([0.2177, 0.5768, 0.0384, 0.2164, 0.3981])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0889, 0.0127, 0.2670, 0.2494, 0.2197, 0.1059, 0.0240]) \n",
      "Test Loss tensor([0.0917, 0.0143, 0.2798, 0.2587, 0.2191, 0.1015, 0.0240])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0322, 0.2272, 0.0455, 0.0836, 0.2820, 0.2686, 0.0564]) \n",
      "Test Loss tensor([0.0350, 0.2299, 0.0443, 0.0827, 0.2829, 0.2628, 0.0512])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0302, 0.0241, 0.0374, 0.0199, 0.0283, 0.0164, 0.1300, 0.0094]) \n",
      "Test Loss tensor([0.0260, 0.0232, 0.0377, 0.0212, 0.0289, 0.0147, 0.1341, 0.0081])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5956, 0.6399, 0.1091, 0.1911, 0.3696, 0.5235])\n",
      "Valid Idx 3 | Loss tensor([0.4409, 0.1967, 0.1273, 0.7057, 0.8030])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 868 in 5.115009307861328 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0645, 0.1924, 0.0830, 0.1122]) \n",
      "Test Loss tensor([0.0679, 0.2015, 0.0824, 0.1175])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2141, 0.5599, 0.0355, 0.2057, 0.3981]) \n",
      "Test Loss tensor([0.2185, 0.5793, 0.0452, 0.2139, 0.3833])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0858, 0.0148, 0.2590, 0.2568, 0.2191, 0.1011, 0.0246]) \n",
      "Test Loss tensor([0.0921, 0.0129, 0.2761, 0.2523, 0.2197, 0.0975, 0.0289])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0412, 0.2227, 0.0402, 0.0850, 0.2724, 0.2625, 0.0448]) \n",
      "Test Loss tensor([0.0339, 0.2246, 0.0423, 0.0813, 0.2837, 0.2623, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0258, 0.0235, 0.0360, 0.0217, 0.0288, 0.0147, 0.1325, 0.0082]) \n",
      "Test Loss tensor([0.0276, 0.0223, 0.0380, 0.0214, 0.0276, 0.0168, 0.1339, 0.0094])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5983, 0.6535, 0.1075, 0.1847, 0.3798, 0.5079])\n",
      "Valid Idx 3 | Loss tensor([0.3985, 0.1700, 0.1247, 0.6856, 0.7787])\n",
      "\n",
      "************** Batch 872 in 5.173821210861206 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0663, 0.2059, 0.0837, 0.1155]) \n",
      "Test Loss tensor([0.0687, 0.2002, 0.0814, 0.1161])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2249, 0.5574, 0.0427, 0.2186, 0.3905]) \n",
      "Test Loss tensor([0.2215, 0.5737, 0.0420, 0.2154, 0.3859])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0935, 0.0143, 0.2709, 0.2485, 0.2174, 0.1054, 0.0282]) \n",
      "Test Loss tensor([0.0918, 0.0138, 0.2780, 0.2518, 0.2186, 0.0989, 0.0264])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0303, 0.2431, 0.0499, 0.0953, 0.2891, 0.2543, 0.0543]) \n",
      "Test Loss tensor([0.0335, 0.2291, 0.0444, 0.0815, 0.2791, 0.2645, 0.0500])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0285, 0.0231, 0.0336, 0.0243, 0.0278, 0.0170, 0.1336, 0.0103]) \n",
      "Test Loss tensor([0.0272, 0.0229, 0.0364, 0.0218, 0.0271, 0.0167, 0.1327, 0.0090])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5978, 0.6505, 0.1081, 0.1864, 0.3758, 0.5062])\n",
      "Valid Idx 3 | Loss tensor([0.4099, 0.1780, 0.1198, 0.6932, 0.7875])\n",
      "\n",
      "************** Batch 876 in 4.80593729019165 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0486, 0.1425, 0.0589, 0.0908]) \n",
      "Test Loss tensor([0.0661, 0.2029, 0.0828, 0.1147])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1604, 0.4332, 0.0293, 0.1640, 0.2906]) \n",
      "Test Loss tensor([0.2229, 0.5779, 0.0378, 0.2106, 0.3890])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0756, 0.0111, 0.2120, 0.1806, 0.1688, 0.0761, 0.0232]) \n",
      "Test Loss tensor([0.0903, 0.0132, 0.2780, 0.2588, 0.2195, 0.1011, 0.0242])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0258, 0.1744, 0.0321, 0.0572, 0.2101, 0.1930, 0.0331]) \n",
      "Test Loss tensor([0.0332, 0.2248, 0.0429, 0.0825, 0.2795, 0.2632, 0.0505])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0221, 0.0158, 0.0264, 0.0148, 0.0195, 0.0122, 0.0973, 0.0066]) \n",
      "Test Loss tensor([0.0279, 0.0223, 0.0389, 0.0222, 0.0272, 0.0156, 0.1321, 0.0081])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5992, 0.6471, 0.1083, 0.1852, 0.3749, 0.5106])\n",
      "Valid Idx 3 | Loss tensor([0.4281, 0.1906, 0.1238, 0.7047, 0.7944])\n",
      "\n",
      "************** Batch 0 in 4.90940260887146 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0687, 0.2023, 0.0847, 0.1142]) \n",
      "Test Loss tensor([0.0658, 0.2015, 0.0825, 0.1142])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2103, 0.5628, 0.0360, 0.2084, 0.3942]) \n",
      "Test Loss tensor([0.2201, 0.5722, 0.0411, 0.2120, 0.3849])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0868, 0.0131, 0.2758, 0.2604, 0.2129, 0.0942, 0.0250]) \n",
      "Test Loss tensor([0.0900, 0.0132, 0.2795, 0.2589, 0.2187, 0.1004, 0.0247])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0354, 0.2329, 0.0458, 0.0820, 0.2874, 0.2658, 0.0510]) \n",
      "Test Loss tensor([0.0324, 0.2285, 0.0445, 0.0825, 0.2802, 0.2693, 0.0513])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0228, 0.0268, 0.0339, 0.0206, 0.0273, 0.0150, 0.1290, 0.0081]) \n",
      "Test Loss tensor([0.0280, 0.0228, 0.0358, 0.0207, 0.0259, 0.0167, 0.1290, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5945, 0.6602, 0.1067, 0.1807, 0.3817, 0.4980])\n",
      "Valid Idx 3 | Loss tensor([0.4003, 0.1757, 0.1217, 0.6910, 0.7932])\n",
      "Gradients: Input 0.3702147603034973 | Message 0.4890609085559845 | Update 0.541282594203949 | Output 0.06459475308656693\n",
      "\n",
      "************** Batch 4 in 5.293725967407227 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0650, 0.2080, 0.0848, 0.1133]) \n",
      "Test Loss tensor([0.0682, 0.2027, 0.0853, 0.1150])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2180, 0.5682, 0.0377, 0.2150, 0.3788]) \n",
      "Test Loss tensor([0.2210, 0.5825, 0.0382, 0.2102, 0.3832])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0871, 0.0152, 0.2922, 0.2491, 0.2248, 0.0979, 0.0282]) \n",
      "Test Loss tensor([0.0890, 0.0136, 0.2800, 0.2523, 0.2175, 0.0987, 0.0240])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0309, 0.2216, 0.0395, 0.0765, 0.2841, 0.2706, 0.0439]) \n",
      "Test Loss tensor([0.0323, 0.2256, 0.0434, 0.0822, 0.2814, 0.2650, 0.0497])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0283, 0.0221, 0.0360, 0.0217, 0.0276, 0.0166, 0.1243, 0.0085]) \n",
      "Test Loss tensor([0.0271, 0.0225, 0.0366, 0.0207, 0.0261, 0.0164, 0.1298, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6003, 0.6548, 0.1087, 0.1814, 0.3841, 0.4984])\n",
      "Valid Idx 3 | Loss tensor([0.4148, 0.1841, 0.1267, 0.6991, 0.7944])\n",
      "\n",
      "************** Batch 8 in 4.926254749298096 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0627, 0.2002, 0.0864, 0.1137]) \n",
      "Test Loss tensor([0.0679, 0.1999, 0.0845, 0.1138])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2241, 0.5904, 0.0409, 0.2119, 0.3843]) \n",
      "Test Loss tensor([0.2202, 0.5734, 0.0367, 0.2101, 0.3854])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0869, 0.0141, 0.2927, 0.2526, 0.2144, 0.0989, 0.0267]) \n",
      "Test Loss tensor([0.0910, 0.0140, 0.2809, 0.2564, 0.2176, 0.0990, 0.0222])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0327, 0.2367, 0.0480, 0.0809, 0.2810, 0.2599, 0.0555]) \n",
      "Test Loss tensor([0.0334, 0.2282, 0.0444, 0.0841, 0.2762, 0.2654, 0.0517])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0235, 0.0279, 0.0365, 0.0199, 0.0252, 0.0155, 0.1237, 0.0077]) \n",
      "Test Loss tensor([0.0275, 0.0236, 0.0367, 0.0211, 0.0273, 0.0156, 0.1289, 0.0078])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5947, 0.6517, 0.1096, 0.1851, 0.3776, 0.4981])\n",
      "Valid Idx 3 | Loss tensor([0.4291, 0.1962, 0.1276, 0.7139, 0.8007])\n",
      "\n",
      "************** Batch 12 in 5.058772325515747 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0663, 0.1997, 0.0799, 0.1159]) \n",
      "Test Loss tensor([0.0673, 0.1978, 0.0814, 0.1144])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2224, 0.5703, 0.0312, 0.2066, 0.3961]) \n",
      "Test Loss tensor([0.2203, 0.5769, 0.0389, 0.2141, 0.3782])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0891, 0.0132, 0.2763, 0.2516, 0.2275, 0.1057, 0.0202]) \n",
      "Test Loss tensor([0.0915, 0.0138, 0.2809, 0.2539, 0.2205, 0.0976, 0.0236])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0324, 0.2281, 0.0435, 0.0837, 0.2661, 0.2635, 0.0525]) \n",
      "Test Loss tensor([0.0323, 0.2259, 0.0426, 0.0813, 0.2806, 0.2657, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0213, 0.0396, 0.0198, 0.0280, 0.0152, 0.1252, 0.0078]) \n",
      "Test Loss tensor([0.0274, 0.0234, 0.0358, 0.0211, 0.0273, 0.0165, 0.1277, 0.0081])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5926, 0.6565, 0.1097, 0.1842, 0.3803, 0.4954])\n",
      "Valid Idx 3 | Loss tensor([0.4106, 0.1861, 0.1218, 0.7110, 0.7941])\n",
      "\n",
      "************** Batch 16 in 5.037531137466431 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0648, 0.2097, 0.0786, 0.1098]) \n",
      "Test Loss tensor([0.0680, 0.2026, 0.0830, 0.1151])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2128, 0.5724, 0.0404, 0.2187, 0.3909]) \n",
      "Test Loss tensor([0.2149, 0.5753, 0.0391, 0.2082, 0.3773])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0899, 0.0137, 0.2858, 0.2535, 0.2166, 0.0874, 0.0261]) \n",
      "Test Loss tensor([0.0930, 0.0135, 0.2850, 0.2572, 0.2198, 0.0979, 0.0242])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0300, 0.2223, 0.0459, 0.0781, 0.2785, 0.2644, 0.0465]) \n",
      "Test Loss tensor([0.0330, 0.2294, 0.0431, 0.0804, 0.2811, 0.2650, 0.0502])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0245, 0.0222, 0.0362, 0.0193, 0.0265, 0.0170, 0.1252, 0.0083]) \n",
      "Test Loss tensor([0.0273, 0.0222, 0.0372, 0.0205, 0.0268, 0.0161, 0.1298, 0.0086])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5930, 0.6590, 0.1092, 0.1834, 0.3781, 0.4943])\n",
      "Valid Idx 3 | Loss tensor([0.4047, 0.1831, 0.1249, 0.7149, 0.7902])\n",
      "\n",
      "************** Batch 20 in 4.884052276611328 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0715, 0.2026, 0.0819, 0.1122]) \n",
      "Test Loss tensor([0.0684, 0.2010, 0.0825, 0.1146])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2116, 0.5780, 0.0401, 0.2068, 0.3748]) \n",
      "Test Loss tensor([0.2142, 0.5768, 0.0382, 0.2118, 0.3782])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0913, 0.0125, 0.2934, 0.2625, 0.2069, 0.0973, 0.0278]) \n",
      "Test Loss tensor([0.0940, 0.0145, 0.2832, 0.2541, 0.2161, 0.0975, 0.0235])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0344, 0.2330, 0.0505, 0.0832, 0.2764, 0.2733, 0.0514]) \n",
      "Test Loss tensor([0.0327, 0.2276, 0.0422, 0.0812, 0.2819, 0.2629, 0.0516])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0296, 0.0208, 0.0368, 0.0209, 0.0262, 0.0159, 0.1320, 0.0088]) \n",
      "Test Loss tensor([0.0275, 0.0222, 0.0384, 0.0209, 0.0278, 0.0149, 0.1296, 0.0081])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5943, 0.6521, 0.1096, 0.1880, 0.3758, 0.5020])\n",
      "Valid Idx 3 | Loss tensor([0.4206, 0.1944, 0.1236, 0.7289, 0.7915])\n",
      "Gradients: Input 0.16358394920825958 | Message 0.19482696056365967 | Update 0.20646613836288452 | Output 0.04866636171936989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 24 in 4.949463844299316 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0726, 0.2009, 0.0859, 0.1184]) \n",
      "Test Loss tensor([0.0662, 0.2023, 0.0825, 0.1121])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2210, 0.5714, 0.0399, 0.2114, 0.3684]) \n",
      "Test Loss tensor([0.2152, 0.5768, 0.0363, 0.2101, 0.3799])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0885, 0.0123, 0.2831, 0.2627, 0.2119, 0.0985, 0.0226]) \n",
      "Test Loss tensor([0.0917, 0.0145, 0.2786, 0.2571, 0.2165, 0.0991, 0.0220])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0297, 0.2242, 0.0490, 0.0779, 0.2735, 0.2588, 0.0475]) \n",
      "Test Loss tensor([0.0328, 0.2265, 0.0422, 0.0790, 0.2799, 0.2618, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0268, 0.0212, 0.0351, 0.0186, 0.0281, 0.0152, 0.1272, 0.0087]) \n",
      "Test Loss tensor([0.0272, 0.0216, 0.0387, 0.0217, 0.0290, 0.0146, 0.1312, 0.0078])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6020, 0.6479, 0.1089, 0.1825, 0.3729, 0.5130])\n",
      "Valid Idx 3 | Loss tensor([0.4343, 0.2021, 0.1246, 0.7380, 0.8014])\n",
      "\n",
      "************** Batch 28 in 5.055118560791016 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0648, 0.1995, 0.0855, 0.1147]) \n",
      "Test Loss tensor([0.0674, 0.2037, 0.0814, 0.1154])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2192, 0.5753, 0.0368, 0.2115, 0.3764]) \n",
      "Test Loss tensor([0.2116, 0.5723, 0.0437, 0.2125, 0.3666])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0927, 0.0134, 0.2810, 0.2598, 0.2168, 0.0928, 0.0234]) \n",
      "Test Loss tensor([0.0955, 0.0130, 0.2809, 0.2568, 0.2232, 0.0978, 0.0249])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0370, 0.2252, 0.0450, 0.0715, 0.2854, 0.2654, 0.0471]) \n",
      "Test Loss tensor([0.0326, 0.2270, 0.0417, 0.0788, 0.2805, 0.2632, 0.0497])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0249, 0.0197, 0.0400, 0.0227, 0.0284, 0.0155, 0.1271, 0.0073]) \n",
      "Test Loss tensor([0.0290, 0.0217, 0.0381, 0.0217, 0.0270, 0.0168, 0.1307, 0.0086])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5920, 0.6621, 0.1109, 0.1849, 0.3826, 0.4926])\n",
      "Valid Idx 3 | Loss tensor([0.3803, 0.1721, 0.1227, 0.7137, 0.7811])\n",
      "\n",
      "************** Batch 32 in 4.967576026916504 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0688, 0.1925, 0.0852, 0.1151]) \n",
      "Test Loss tensor([0.0665, 0.1989, 0.0808, 0.1142])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2115, 0.5583, 0.0436, 0.2037, 0.3751]) \n",
      "Test Loss tensor([0.2121, 0.5737, 0.0389, 0.2115, 0.3736])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0957, 0.0124, 0.2857, 0.2539, 0.2210, 0.0966, 0.0236]) \n",
      "Test Loss tensor([0.0911, 0.0146, 0.2797, 0.2536, 0.2135, 0.0950, 0.0228])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0326, 0.2267, 0.0420, 0.0926, 0.2785, 0.2680, 0.0523]) \n",
      "Test Loss tensor([0.0307, 0.2305, 0.0429, 0.0804, 0.2801, 0.2629, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0239, 0.0364, 0.0201, 0.0264, 0.0176, 0.1270, 0.0081]) \n",
      "Test Loss tensor([0.0283, 0.0230, 0.0372, 0.0214, 0.0271, 0.0150, 0.1301, 0.0077])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5977, 0.6529, 0.1102, 0.1884, 0.3745, 0.5084])\n",
      "Valid Idx 3 | Loss tensor([0.4145, 0.1933, 0.1252, 0.7284, 0.7992])\n",
      "\n",
      "************** Batch 36 in 5.052212476730347 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0675, 0.1976, 0.0826, 0.1144]) \n",
      "Test Loss tensor([0.0681, 0.2024, 0.0825, 0.1132])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2104, 0.5680, 0.0365, 0.2129, 0.3591]) \n",
      "Test Loss tensor([0.2138, 0.5794, 0.0374, 0.2126, 0.3728])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0920, 0.0132, 0.2846, 0.2468, 0.2175, 0.0972, 0.0232]) \n",
      "Test Loss tensor([0.0915, 0.0141, 0.2750, 0.2562, 0.2186, 0.0953, 0.0208])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0315, 0.2355, 0.0465, 0.0858, 0.2809, 0.2491, 0.0504]) \n",
      "Test Loss tensor([0.0311, 0.2244, 0.0437, 0.0824, 0.2824, 0.2633, 0.0504])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0338, 0.0268, 0.0372, 0.0217, 0.0281, 0.0145, 0.1225, 0.0073]) \n",
      "Test Loss tensor([0.0278, 0.0222, 0.0371, 0.0213, 0.0275, 0.0153, 0.1296, 0.0078])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6023, 0.6607, 0.1092, 0.1815, 0.3798, 0.5194])\n",
      "Valid Idx 3 | Loss tensor([0.4209, 0.1932, 0.1225, 0.7282, 0.7992])\n",
      "\n",
      "************** Batch 40 in 5.056002378463745 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0689, 0.1922, 0.0808, 0.1178]) \n",
      "Test Loss tensor([0.0655, 0.2007, 0.0812, 0.1127])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2212, 0.5694, 0.0389, 0.2079, 0.3812]) \n",
      "Test Loss tensor([0.2139, 0.5702, 0.0408, 0.2109, 0.3673])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0882, 0.0152, 0.2710, 0.2509, 0.2300, 0.0989, 0.0217]) \n",
      "Test Loss tensor([0.0931, 0.0132, 0.2793, 0.2568, 0.2202, 0.0959, 0.0234])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0309, 0.2206, 0.0364, 0.0842, 0.2837, 0.2698, 0.0449]) \n",
      "Test Loss tensor([0.0331, 0.2197, 0.0431, 0.0817, 0.2789, 0.2652, 0.0483])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0302, 0.0217, 0.0370, 0.0226, 0.0273, 0.0162, 0.1216, 0.0084]) \n",
      "Test Loss tensor([0.0287, 0.0221, 0.0372, 0.0209, 0.0264, 0.0163, 0.1288, 0.0084])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6003, 0.6630, 0.1103, 0.1828, 0.3842, 0.5067])\n",
      "Valid Idx 3 | Loss tensor([0.3945, 0.1772, 0.1215, 0.7154, 0.7818])\n",
      "Gradients: Input 0.24619048833847046 | Message 0.30955928564071655 | Update 0.34067368507385254 | Output 0.04639289900660515\n",
      "\n",
      "************** Batch 44 in 4.967148542404175 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0615, 0.2001, 0.0801, 0.1105]) \n",
      "Test Loss tensor([0.0662, 0.2000, 0.0809, 0.1104])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2187, 0.5572, 0.0378, 0.1999, 0.3756]) \n",
      "Test Loss tensor([0.2172, 0.5756, 0.0377, 0.2127, 0.3661])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0991, 0.0136, 0.2863, 0.2477, 0.2211, 0.0928, 0.0232]) \n",
      "Test Loss tensor([0.0902, 0.0135, 0.2809, 0.2581, 0.2218, 0.0975, 0.0209])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0334, 0.2229, 0.0417, 0.0799, 0.2886, 0.2655, 0.0528]) \n",
      "Test Loss tensor([0.0323, 0.2292, 0.0429, 0.0809, 0.2811, 0.2645, 0.0490])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0230, 0.0238, 0.0348, 0.0218, 0.0238, 0.0171, 0.1238, 0.0082]) \n",
      "Test Loss tensor([0.0280, 0.0229, 0.0364, 0.0217, 0.0268, 0.0155, 0.1273, 0.0077])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6031, 0.6574, 0.1078, 0.1824, 0.3843, 0.5125])\n",
      "Valid Idx 3 | Loss tensor([0.4207, 0.1862, 0.1229, 0.7204, 0.7971])\n",
      "\n",
      "************** Batch 48 in 4.9861743450164795 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0669, 0.1932, 0.0820, 0.1173]) \n",
      "Test Loss tensor([0.0669, 0.2035, 0.0818, 0.1114])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2190, 0.5791, 0.0333, 0.2140, 0.3721]) \n",
      "Test Loss tensor([0.2140, 0.5685, 0.0381, 0.2086, 0.3659])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0866, 0.0149, 0.2709, 0.2440, 0.2067, 0.0950, 0.0200]) \n",
      "Test Loss tensor([0.0882, 0.0136, 0.2784, 0.2537, 0.2161, 0.0964, 0.0200])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0371, 0.2384, 0.0461, 0.0827, 0.2830, 0.2533, 0.0522]) \n",
      "Test Loss tensor([0.0343, 0.2243, 0.0427, 0.0817, 0.2791, 0.2634, 0.0493])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0282, 0.0211, 0.0328, 0.0202, 0.0268, 0.0155, 0.1271, 0.0074]) \n",
      "Test Loss tensor([0.0275, 0.0231, 0.0360, 0.0210, 0.0264, 0.0157, 0.1271, 0.0076])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6018, 0.6508, 0.1082, 0.1831, 0.3854, 0.5082])\n",
      "Valid Idx 3 | Loss tensor([0.4225, 0.1878, 0.1234, 0.7280, 0.7976])\n",
      "\n",
      "************** Batch 52 in 5.083575248718262 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0670, 0.1943, 0.0847, 0.1149]) \n",
      "Test Loss tensor([0.0670, 0.2004, 0.0817, 0.1133])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2238, 0.5997, 0.0396, 0.2237, 0.3536]) \n",
      "Test Loss tensor([0.2143, 0.5689, 0.0397, 0.2101, 0.3617])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0908, 0.0136, 0.2733, 0.2613, 0.2053, 0.0927, 0.0193]) \n",
      "Test Loss tensor([0.0896, 0.0129, 0.2812, 0.2557, 0.2181, 0.0940, 0.0217])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0362, 0.2172, 0.0437, 0.0813, 0.2665, 0.2636, 0.0438]) \n",
      "Test Loss tensor([0.0323, 0.2330, 0.0430, 0.0831, 0.2798, 0.2645, 0.0502])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0283, 0.0180, 0.0411, 0.0176, 0.0281, 0.0152, 0.1322, 0.0070]) \n",
      "Test Loss tensor([0.0280, 0.0218, 0.0375, 0.0215, 0.0263, 0.0168, 0.1257, 0.0079])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6029, 0.6580, 0.1067, 0.1806, 0.3823, 0.5053])\n",
      "Valid Idx 3 | Loss tensor([0.4006, 0.1739, 0.1219, 0.7153, 0.7916])\n",
      "\n",
      "************** Batch 56 in 4.994953632354736 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0711, 0.1907, 0.0841, 0.1138]) \n",
      "Test Loss tensor([0.0672, 0.2023, 0.0815, 0.1119])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2150, 0.5816, 0.0380, 0.2130, 0.3649]) \n",
      "Test Loss tensor([0.2148, 0.5710, 0.0396, 0.2124, 0.3610])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0852, 0.0122, 0.2759, 0.2595, 0.2124, 0.0992, 0.0202]) \n",
      "Test Loss tensor([0.0894, 0.0138, 0.2788, 0.2573, 0.2159, 0.0984, 0.0200])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0292, 0.2262, 0.0404, 0.0762, 0.2683, 0.2593, 0.0421]) \n",
      "Test Loss tensor([0.0326, 0.2299, 0.0436, 0.0812, 0.2800, 0.2626, 0.0509])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0270, 0.0221, 0.0342, 0.0202, 0.0247, 0.0179, 0.1374, 0.0079]) \n",
      "Test Loss tensor([0.0278, 0.0221, 0.0378, 0.0204, 0.0264, 0.0159, 0.1264, 0.0078])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5985, 0.6506, 0.1095, 0.1837, 0.3861, 0.5108])\n",
      "Valid Idx 3 | Loss tensor([0.4128, 0.1800, 0.1226, 0.7242, 0.7959])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 60 in 5.0770423412323 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0652, 0.2027, 0.0854, 0.1107]) \n",
      "Test Loss tensor([0.0676, 0.2018, 0.0811, 0.1100])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2175, 0.5762, 0.0378, 0.2123, 0.3712]) \n",
      "Test Loss tensor([0.2164, 0.5759, 0.0375, 0.2138, 0.3610])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0900, 0.0130, 0.2764, 0.2516, 0.2159, 0.0949, 0.0195]) \n",
      "Test Loss tensor([0.0889, 0.0137, 0.2775, 0.2533, 0.2144, 0.0996, 0.0189])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0316, 0.2455, 0.0427, 0.0790, 0.2654, 0.2557, 0.0527]) \n",
      "Test Loss tensor([0.0344, 0.2273, 0.0437, 0.0811, 0.2821, 0.2624, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0222, 0.0233, 0.0347, 0.0191, 0.0243, 0.0140, 0.1242, 0.0075]) \n",
      "Test Loss tensor([0.0281, 0.0231, 0.0382, 0.0208, 0.0267, 0.0150, 0.1267, 0.0074])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6028, 0.6404, 0.1058, 0.1847, 0.3800, 0.5158])\n",
      "Valid Idx 3 | Loss tensor([0.4289, 0.1934, 0.1256, 0.7332, 0.8026])\n",
      "Gradients: Input 0.11160559952259064 | Message 0.10664908587932587 | Update 0.10505938529968262 | Output 0.014932547695934772\n",
      "\n",
      "************** Batch 64 in 5.0740320682525635 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0633, 0.2052, 0.0795, 0.1160]) \n",
      "Test Loss tensor([0.0684, 0.2033, 0.0825, 0.1114])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2130, 0.5802, 0.0358, 0.2125, 0.3622]) \n",
      "Test Loss tensor([0.2148, 0.5665, 0.0415, 0.2132, 0.3565])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0907, 0.0142, 0.2892, 0.2538, 0.2167, 0.0989, 0.0198]) \n",
      "Test Loss tensor([0.0921, 0.0133, 0.2769, 0.2553, 0.2154, 0.0965, 0.0206])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0318, 0.2264, 0.0435, 0.0882, 0.2800, 0.2747, 0.0523]) \n",
      "Test Loss tensor([0.0321, 0.2250, 0.0434, 0.0810, 0.2807, 0.2610, 0.0514])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0282, 0.0248, 0.0419, 0.0215, 0.0286, 0.0163, 0.1290, 0.0072]) \n",
      "Test Loss tensor([0.0279, 0.0227, 0.0371, 0.0205, 0.0262, 0.0163, 0.1260, 0.0083])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6015, 0.6472, 0.1108, 0.1794, 0.3772, 0.5140])\n",
      "Valid Idx 3 | Loss tensor([0.3919, 0.1757, 0.1250, 0.7216, 0.7918])\n",
      "\n",
      "************** Batch 68 in 5.020512580871582 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0688, 0.2049, 0.0841, 0.1084]) \n",
      "Test Loss tensor([0.0667, 0.2030, 0.0808, 0.1097])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2132, 0.5634, 0.0430, 0.2160, 0.3686]) \n",
      "Test Loss tensor([0.2123, 0.5704, 0.0413, 0.2112, 0.3545])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0917, 0.0129, 0.2791, 0.2553, 0.2152, 0.0923, 0.0204]) \n",
      "Test Loss tensor([0.0912, 0.0132, 0.2787, 0.2551, 0.2155, 0.0952, 0.0204])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0309, 0.2336, 0.0443, 0.0807, 0.2882, 0.2726, 0.0514]) \n",
      "Test Loss tensor([0.0316, 0.2265, 0.0423, 0.0806, 0.2838, 0.2632, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0302, 0.0233, 0.0432, 0.0193, 0.0274, 0.0191, 0.1347, 0.0078]) \n",
      "Test Loss tensor([0.0283, 0.0221, 0.0352, 0.0205, 0.0251, 0.0170, 0.1278, 0.0086])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5951, 0.6544, 0.1100, 0.1802, 0.3812, 0.5102])\n",
      "Valid Idx 3 | Loss tensor([0.3832, 0.1744, 0.1229, 0.7138, 0.7912])\n",
      "\n",
      "************** Batch 72 in 5.13930869102478 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0683, 0.2036, 0.0823, 0.1085]) \n",
      "Test Loss tensor([0.0655, 0.2028, 0.0803, 0.1089])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2129, 0.5642, 0.0411, 0.2134, 0.3587]) \n",
      "Test Loss tensor([0.2141, 0.5650, 0.0364, 0.2133, 0.3597])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0903, 0.0121, 0.2813, 0.2523, 0.2190, 0.1026, 0.0177]) \n",
      "Test Loss tensor([0.0928, 0.0130, 0.2770, 0.2572, 0.2158, 0.0968, 0.0183])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0302, 0.2229, 0.0401, 0.0771, 0.2743, 0.2692, 0.0474]) \n",
      "Test Loss tensor([0.0307, 0.2305, 0.0436, 0.0820, 0.2810, 0.2598, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0265, 0.0187, 0.0356, 0.0252, 0.0263, 0.0179, 0.1311, 0.0079]) \n",
      "Test Loss tensor([0.0281, 0.0217, 0.0363, 0.0199, 0.0259, 0.0155, 0.1269, 0.0077])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6031, 0.6522, 0.1092, 0.1850, 0.3791, 0.5154])\n",
      "Valid Idx 3 | Loss tensor([0.3875, 0.1876, 0.1216, 0.7247, 0.7996])\n",
      "\n",
      "************** Batch 76 in 5.044481039047241 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0610, 0.1934, 0.0792, 0.1070]) \n",
      "Test Loss tensor([0.0656, 0.2003, 0.0818, 0.1085])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2203, 0.5731, 0.0362, 0.2172, 0.3522]) \n",
      "Test Loss tensor([0.2146, 0.5707, 0.0352, 0.2141, 0.3562])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0913, 0.0133, 0.2839, 0.2480, 0.2110, 0.1007, 0.0192]) \n",
      "Test Loss tensor([0.0928, 0.0135, 0.2837, 0.2601, 0.2183, 0.0979, 0.0184])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0290, 0.2338, 0.0412, 0.0793, 0.2770, 0.2596, 0.0471]) \n",
      "Test Loss tensor([0.0318, 0.2297, 0.0438, 0.0791, 0.2781, 0.2640, 0.0509])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0318, 0.0229, 0.0423, 0.0181, 0.0270, 0.0152, 0.1303, 0.0080]) \n",
      "Test Loss tensor([0.0287, 0.0221, 0.0356, 0.0203, 0.0264, 0.0152, 0.1246, 0.0074])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6061, 0.6464, 0.1057, 0.1833, 0.3831, 0.5257])\n",
      "Valid Idx 3 | Loss tensor([0.3997, 0.1982, 0.1225, 0.7329, 0.8106])\n",
      "\n",
      "************** Batch 80 in 5.046199798583984 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0685, 0.1938, 0.0766, 0.1159]) \n",
      "Test Loss tensor([0.0665, 0.2027, 0.0814, 0.1091])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2174, 0.5767, 0.0338, 0.2149, 0.3649]) \n",
      "Test Loss tensor([0.2136, 0.5684, 0.0366, 0.2127, 0.3548])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0914, 0.0145, 0.2727, 0.2582, 0.2141, 0.0919, 0.0207]) \n",
      "Test Loss tensor([0.0907, 0.0135, 0.2785, 0.2584, 0.2176, 0.0955, 0.0182])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0316, 0.2287, 0.0463, 0.0823, 0.2774, 0.2577, 0.0496]) \n",
      "Test Loss tensor([0.0323, 0.2259, 0.0436, 0.0821, 0.2793, 0.2614, 0.0497])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0315, 0.0260, 0.0396, 0.0224, 0.0253, 0.0161, 0.1237, 0.0077]) \n",
      "Test Loss tensor([0.0297, 0.0213, 0.0366, 0.0217, 0.0254, 0.0156, 0.1280, 0.0074])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6019, 0.6540, 0.1074, 0.1820, 0.3819, 0.5236])\n",
      "Valid Idx 3 | Loss tensor([0.3987, 0.1937, 0.1224, 0.7338, 0.8071])\n",
      "Gradients: Input 0.19467580318450928 | Message 0.22831033170223236 | Update 0.23431698977947235 | Output 0.023296527564525604\n",
      "\n",
      "************** Batch 84 in 5.191853046417236 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0646, 0.1932, 0.0854, 0.1047]) \n",
      "Test Loss tensor([0.0669, 0.2024, 0.0802, 0.1128])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2097, 0.5730, 0.0336, 0.2063, 0.3570]) \n",
      "Test Loss tensor([0.2121, 0.5688, 0.0411, 0.2107, 0.3493])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0967, 0.0134, 0.2811, 0.2659, 0.2109, 0.0964, 0.0177]) \n",
      "Test Loss tensor([0.0917, 0.0132, 0.2773, 0.2575, 0.2190, 0.0945, 0.0196])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0347, 0.2207, 0.0448, 0.0772, 0.2857, 0.2624, 0.0498]) \n",
      "Test Loss tensor([0.0301, 0.2279, 0.0438, 0.0811, 0.2791, 0.2623, 0.0512])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0288, 0.0215, 0.0383, 0.0236, 0.0264, 0.0154, 0.1273, 0.0096]) \n",
      "Test Loss tensor([0.0296, 0.0230, 0.0356, 0.0208, 0.0254, 0.0170, 0.1257, 0.0084])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6052, 0.6599, 0.1098, 0.1783, 0.3796, 0.5101])\n",
      "Valid Idx 3 | Loss tensor([0.3756, 0.1735, 0.1211, 0.7272, 0.7875])\n",
      "\n",
      "************** Batch 88 in 4.305471658706665 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0651, 0.2004, 0.0795, 0.1117]) \n",
      "Test Loss tensor([0.0659, 0.2012, 0.0820, 0.1077])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2078, 0.5587, 0.0404, 0.2032, 0.3499]) \n",
      "Test Loss tensor([0.2127, 0.5710, 0.0365, 0.2116, 0.3470])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0905, 0.0134, 0.2842, 0.2561, 0.2176, 0.0959, 0.0203]) \n",
      "Test Loss tensor([0.0900, 0.0145, 0.2792, 0.2591, 0.2155, 0.0964, 0.0180])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0287, 0.2314, 0.0471, 0.0855, 0.2735, 0.2687, 0.0438]) \n",
      "Test Loss tensor([0.0334, 0.2286, 0.0423, 0.0806, 0.2791, 0.2634, 0.0479])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0312, 0.0225, 0.0366, 0.0181, 0.0252, 0.0159, 0.1238, 0.0088]) \n",
      "Test Loss tensor([0.0276, 0.0223, 0.0361, 0.0203, 0.0269, 0.0150, 0.1253, 0.0077])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6020, 0.6440, 0.1090, 0.1872, 0.3792, 0.5235])\n",
      "Valid Idx 3 | Loss tensor([0.4262, 0.1959, 0.1242, 0.7490, 0.8045])\n",
      "\n",
      "************** Batch 92 in 4.182767629623413 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0665, 0.1976, 0.0775, 0.1012]) \n",
      "Test Loss tensor([0.0650, 0.2027, 0.0814, 0.1090])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2138, 0.5718, 0.0348, 0.2122, 0.3473]) \n",
      "Test Loss tensor([0.2104, 0.5667, 0.0372, 0.2104, 0.3475])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0873, 0.0163, 0.2800, 0.2585, 0.2233, 0.0978, 0.0177]) \n",
      "Test Loss tensor([0.0902, 0.0140, 0.2833, 0.2574, 0.2149, 0.0971, 0.0189])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0372, 0.2334, 0.0472, 0.0872, 0.2710, 0.2664, 0.0457]) \n",
      "Test Loss tensor([0.0330, 0.2275, 0.0437, 0.0787, 0.2812, 0.2585, 0.0504])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0319, 0.0252, 0.0393, 0.0183, 0.0276, 0.0156, 0.1243, 0.0082]) \n",
      "Test Loss tensor([0.0276, 0.0221, 0.0372, 0.0218, 0.0275, 0.0151, 0.1269, 0.0073])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6021, 0.6446, 0.1100, 0.1899, 0.3825, 0.5255])\n",
      "Valid Idx 3 | Loss tensor([0.4229, 0.1924, 0.1224, 0.7513, 0.7998])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 96 in 5.063138961791992 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0672, 0.1990, 0.0807, 0.1075]) \n",
      "Test Loss tensor([0.0651, 0.1967, 0.0804, 0.1105])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2046, 0.5685, 0.0363, 0.2054, 0.3549]) \n",
      "Test Loss tensor([0.2094, 0.5685, 0.0411, 0.2066, 0.3402])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0919, 0.0132, 0.2805, 0.2563, 0.2153, 0.0951, 0.0186]) \n",
      "Test Loss tensor([0.0932, 0.0127, 0.2787, 0.2567, 0.2195, 0.0933, 0.0197])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0356, 0.2193, 0.0397, 0.0731, 0.2695, 0.2620, 0.0478]) \n",
      "Test Loss tensor([0.0313, 0.2256, 0.0420, 0.0803, 0.2771, 0.2630, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0278, 0.0251, 0.0394, 0.0225, 0.0276, 0.0137, 0.1259, 0.0072]) \n",
      "Test Loss tensor([0.0284, 0.0227, 0.0370, 0.0206, 0.0254, 0.0174, 0.1283, 0.0080])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6018, 0.6690, 0.1096, 0.1832, 0.3854, 0.5198])\n",
      "Valid Idx 3 | Loss tensor([0.3681, 0.1617, 0.1197, 0.7243, 0.7838])\n",
      "\n",
      "************** Batch 100 in 5.027223587036133 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0620, 0.1963, 0.0845, 0.1078]) \n",
      "Test Loss tensor([0.0657, 0.2040, 0.0818, 0.1087])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2184, 0.5608, 0.0406, 0.2149, 0.3397]) \n",
      "Test Loss tensor([0.2172, 0.5638, 0.0301, 0.2105, 0.3535])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0904, 0.0127, 0.2834, 0.2632, 0.2079, 0.0959, 0.0175]) \n",
      "Test Loss tensor([0.0917, 0.0145, 0.2802, 0.2638, 0.2159, 0.0958, 0.0157])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0305, 0.2269, 0.0414, 0.0794, 0.2747, 0.2560, 0.0457]) \n",
      "Test Loss tensor([0.0332, 0.2284, 0.0444, 0.0814, 0.2770, 0.2599, 0.0504])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0290, 0.0247, 0.0412, 0.0179, 0.0261, 0.0174, 0.1241, 0.0083]) \n",
      "Test Loss tensor([0.0272, 0.0230, 0.0371, 0.0205, 0.0277, 0.0135, 0.1258, 0.0063])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6067, 0.6444, 0.1062, 0.1905, 0.3808, 0.5411])\n",
      "Valid Idx 3 | Loss tensor([0.4466, 0.2149, 0.1248, 0.7593, 0.8174])\n",
      "Gradients: Input 0.777407169342041 | Message 1.1390681266784668 | Update 1.2384755611419678 | Output 0.16293729841709137\n",
      "\n",
      "************** Batch 104 in 5.0755064487457275 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0674, 0.1938, 0.0777, 0.1053]) \n",
      "Test Loss tensor([0.0656, 0.2016, 0.0809, 0.1093])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2177, 0.5709, 0.0314, 0.2064, 0.3462]) \n",
      "Test Loss tensor([0.2131, 0.5648, 0.0363, 0.2076, 0.3460])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0863, 0.0136, 0.2720, 0.2791, 0.2212, 0.1062, 0.0159]) \n",
      "Test Loss tensor([0.0910, 0.0131, 0.2786, 0.2590, 0.2179, 0.0942, 0.0180])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0343, 0.2366, 0.0444, 0.0823, 0.2769, 0.2557, 0.0485]) \n",
      "Test Loss tensor([0.0314, 0.2270, 0.0436, 0.0797, 0.2806, 0.2603, 0.0510])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0290, 0.0273, 0.0375, 0.0208, 0.0281, 0.0142, 0.1283, 0.0061]) \n",
      "Test Loss tensor([0.0270, 0.0225, 0.0366, 0.0214, 0.0270, 0.0155, 0.1248, 0.0068])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6010, 0.6602, 0.1102, 0.1859, 0.3778, 0.5305])\n",
      "Valid Idx 3 | Loss tensor([0.3932, 0.1815, 0.1216, 0.7428, 0.8045])\n",
      "\n",
      "************** Batch 108 in 5.019654750823975 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0660, 0.2074, 0.0764, 0.1091]) \n",
      "Test Loss tensor([0.0670, 0.2038, 0.0810, 0.1119])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2291, 0.5544, 0.0358, 0.2115, 0.3507]) \n",
      "Test Loss tensor([0.2103, 0.5667, 0.0408, 0.2108, 0.3387])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0999, 0.0149, 0.2629, 0.2508, 0.2256, 0.0952, 0.0185]) \n",
      "Test Loss tensor([0.0948, 0.0130, 0.2789, 0.2560, 0.2212, 0.0955, 0.0201])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0316, 0.2281, 0.0439, 0.0864, 0.2729, 0.2666, 0.0488]) \n",
      "Test Loss tensor([0.0311, 0.2240, 0.0427, 0.0803, 0.2799, 0.2597, 0.0502])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0271, 0.0256, 0.0368, 0.0231, 0.0297, 0.0148, 0.1313, 0.0068]) \n",
      "Test Loss tensor([0.0298, 0.0219, 0.0373, 0.0221, 0.0260, 0.0174, 0.1263, 0.0075])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6005, 0.6708, 0.1119, 0.1839, 0.3913, 0.5189])\n",
      "Valid Idx 3 | Loss tensor([0.3498, 0.1578, 0.1183, 0.7242, 0.7811])\n",
      "\n",
      "************** Batch 112 in 4.9744157791137695 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0668, 0.2086, 0.0776, 0.1179]) \n",
      "Test Loss tensor([0.0673, 0.2038, 0.0825, 0.1070])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2156, 0.5838, 0.0431, 0.2135, 0.3304]) \n",
      "Test Loss tensor([0.2193, 0.5643, 0.0260, 0.2097, 0.3614])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0968, 0.0125, 0.2847, 0.2451, 0.2254, 0.0958, 0.0250]) \n",
      "Test Loss tensor([0.0870, 0.0157, 0.2799, 0.2657, 0.2147, 0.1033, 0.0146])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0292, 0.2300, 0.0384, 0.0804, 0.2811, 0.2565, 0.0408]) \n",
      "Test Loss tensor([0.0353, 0.2263, 0.0437, 0.0810, 0.2805, 0.2602, 0.0501])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0281, 0.0212, 0.0312, 0.0206, 0.0255, 0.0176, 0.1172, 0.0075]) \n",
      "Test Loss tensor([0.0254, 0.0220, 0.0376, 0.0208, 0.0320, 0.0115, 0.1314, 0.0054])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6013, 0.6308, 0.1034, 0.1991, 0.3731, 0.5614])\n",
      "Valid Idx 3 | Loss tensor([0.4980, 0.2522, 0.1290, 0.7868, 0.8413])\n",
      "\n",
      "************** Batch 116 in 4.974188566207886 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0721, 0.1897, 0.0812, 0.1039]) \n",
      "Test Loss tensor([0.0667, 0.2022, 0.0800, 0.1148])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2179, 0.5531, 0.0239, 0.2169, 0.3673]) \n",
      "Test Loss tensor([0.2092, 0.5647, 0.0457, 0.2080, 0.3326])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0868, 0.0162, 0.2917, 0.2732, 0.2111, 0.1039, 0.0151]) \n",
      "Test Loss tensor([0.0974, 0.0122, 0.2814, 0.2558, 0.2256, 0.0929, 0.0210])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0320, 0.2259, 0.0395, 0.0878, 0.2700, 0.2518, 0.0552]) \n",
      "Test Loss tensor([0.0296, 0.2307, 0.0429, 0.0795, 0.2803, 0.2618, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0261, 0.0251, 0.0392, 0.0206, 0.0327, 0.0124, 0.1332, 0.0051]) \n",
      "Test Loss tensor([0.0307, 0.0207, 0.0359, 0.0207, 0.0252, 0.0188, 0.1259, 0.0082])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5945, 0.6830, 0.1119, 0.1824, 0.3846, 0.5160])\n",
      "Valid Idx 3 | Loss tensor([0.3146, 0.1435, 0.1157, 0.7144, 0.7667])\n",
      "\n",
      "************** Batch 120 in 4.98378324508667 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0698, 0.1925, 0.0784, 0.1196]) \n",
      "Test Loss tensor([0.0653, 0.1993, 0.0819, 0.1073])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2248, 0.5736, 0.0453, 0.2157, 0.3337]) \n",
      "Test Loss tensor([0.2130, 0.5583, 0.0275, 0.2108, 0.3508])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1009, 0.0134, 0.2745, 0.2464, 0.2175, 0.0985, 0.0184]) \n",
      "Test Loss tensor([0.0878, 0.0156, 0.2843, 0.2646, 0.2145, 0.0989, 0.0168])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0291, 0.2152, 0.0375, 0.0816, 0.2914, 0.2710, 0.0501]) \n",
      "Test Loss tensor([0.0337, 0.2244, 0.0431, 0.0794, 0.2823, 0.2577, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0299, 0.0251, 0.0381, 0.0191, 0.0238, 0.0180, 0.1216, 0.0078]) \n",
      "Test Loss tensor([0.0266, 0.0230, 0.0393, 0.0215, 0.0307, 0.0122, 0.1260, 0.0057])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6054, 0.6444, 0.1044, 0.1924, 0.3721, 0.5551])\n",
      "Valid Idx 3 | Loss tensor([0.4607, 0.2344, 0.1242, 0.7755, 0.8290])\n",
      "Gradients: Input 1.9663935899734497 | Message 2.649696111679077 | Update 2.927039384841919 | Output 0.3527296781539917\n",
      "\n",
      "************** Batch 124 in 4.9636406898498535 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0624, 0.1925, 0.0795, 0.1080]) \n",
      "Test Loss tensor([0.0650, 0.2020, 0.0803, 0.1091])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2076, 0.5528, 0.0283, 0.2015, 0.3562]) \n",
      "Test Loss tensor([0.2067, 0.5637, 0.0344, 0.2099, 0.3355])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0913, 0.0164, 0.2933, 0.2592, 0.2067, 0.1030, 0.0149]) \n",
      "Test Loss tensor([0.0924, 0.0136, 0.2815, 0.2643, 0.2195, 0.0947, 0.0185])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0350, 0.2280, 0.0417, 0.0834, 0.2885, 0.2583, 0.0526]) \n",
      "Test Loss tensor([0.0304, 0.2267, 0.0434, 0.0797, 0.2797, 0.2622, 0.0495])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0272, 0.0191, 0.0380, 0.0216, 0.0301, 0.0121, 0.1214, 0.0059]) \n",
      "Test Loss tensor([0.0291, 0.0229, 0.0389, 0.0212, 0.0272, 0.0153, 0.1273, 0.0067])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5969, 0.6683, 0.1093, 0.1854, 0.3820, 0.5343])\n",
      "Valid Idx 3 | Loss tensor([0.3842, 0.1880, 0.1212, 0.7408, 0.7998])\n",
      "\n",
      "************** Batch 128 in 5.0617218017578125 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0601, 0.2003, 0.0736, 0.1132]) \n",
      "Test Loss tensor([0.0645, 0.2016, 0.0790, 0.1100])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2213, 0.5583, 0.0314, 0.2072, 0.3472]) \n",
      "Test Loss tensor([0.2102, 0.5564, 0.0380, 0.2093, 0.3331])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0884, 0.0127, 0.2797, 0.2495, 0.2207, 0.0939, 0.0205]) \n",
      "Test Loss tensor([0.0923, 0.0132, 0.2790, 0.2570, 0.2241, 0.0925, 0.0198])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0270, 0.2369, 0.0444, 0.0794, 0.2805, 0.2696, 0.0529]) \n",
      "Test Loss tensor([0.0293, 0.2259, 0.0422, 0.0796, 0.2798, 0.2614, 0.0509])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0268, 0.0214, 0.0355, 0.0214, 0.0255, 0.0149, 0.1226, 0.0065]) \n",
      "Test Loss tensor([0.0301, 0.0224, 0.0370, 0.0194, 0.0260, 0.0160, 0.1248, 0.0075])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5999, 0.6787, 0.1101, 0.1837, 0.3837, 0.5277])\n",
      "Valid Idx 3 | Loss tensor([0.3668, 0.1770, 0.1213, 0.7281, 0.7885])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 132 in 5.073239088058472 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0667, 0.1892, 0.0822, 0.1145]) \n",
      "Test Loss tensor([0.0668, 0.2028, 0.0838, 0.1076])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2080, 0.5608, 0.0405, 0.2029, 0.3417]) \n",
      "Test Loss tensor([0.2188, 0.5571, 0.0260, 0.2109, 0.3531])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0936, 0.0153, 0.2716, 0.2562, 0.2301, 0.0986, 0.0222]) \n",
      "Test Loss tensor([0.0862, 0.0164, 0.2793, 0.2698, 0.2119, 0.0986, 0.0142])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0283, 0.2289, 0.0437, 0.0809, 0.2713, 0.2650, 0.0483]) \n",
      "Test Loss tensor([0.0355, 0.2212, 0.0443, 0.0806, 0.2819, 0.2590, 0.0494])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0285, 0.0217, 0.0323, 0.0200, 0.0244, 0.0179, 0.1238, 0.0068]) \n",
      "Test Loss tensor([0.0261, 0.0229, 0.0406, 0.0203, 0.0320, 0.0110, 0.1263, 0.0056])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6104, 0.6365, 0.1055, 0.1976, 0.3729, 0.5688])\n",
      "Valid Idx 3 | Loss tensor([0.5186, 0.2831, 0.1264, 0.7944, 0.8406])\n",
      "\n",
      "************** Batch 136 in 5.0719146728515625 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0733, 0.1880, 0.0779, 0.1039]) \n",
      "Test Loss tensor([0.0656, 0.2010, 0.0794, 0.1138])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2206, 0.5589, 0.0275, 0.2070, 0.3608]) \n",
      "Test Loss tensor([0.2120, 0.5612, 0.0384, 0.2118, 0.3266])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0861, 0.0166, 0.2868, 0.2664, 0.2090, 0.1001, 0.0129]) \n",
      "Test Loss tensor([0.0913, 0.0132, 0.2819, 0.2578, 0.2183, 0.0931, 0.0197])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0360, 0.2353, 0.0464, 0.0789, 0.2786, 0.2631, 0.0473]) \n",
      "Test Loss tensor([0.0319, 0.2257, 0.0430, 0.0799, 0.2795, 0.2631, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0251, 0.0259, 0.0365, 0.0190, 0.0311, 0.0112, 0.1241, 0.0052]) \n",
      "Test Loss tensor([0.0287, 0.0225, 0.0370, 0.0206, 0.0258, 0.0166, 0.1273, 0.0075])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5956, 0.6749, 0.1102, 0.1852, 0.3791, 0.5275])\n",
      "Valid Idx 3 | Loss tensor([0.3888, 0.1903, 0.1211, 0.7393, 0.7904])\n",
      "\n",
      "************** Batch 140 in 5.024380207061768 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0649, 0.1905, 0.0792, 0.1129]) \n",
      "Test Loss tensor([0.0653, 0.2020, 0.0781, 0.1121])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2065, 0.5672, 0.0429, 0.2024, 0.3192]) \n",
      "Test Loss tensor([0.2078, 0.5608, 0.0370, 0.2091, 0.3324])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0919, 0.0146, 0.2831, 0.2616, 0.2239, 0.0970, 0.0171]) \n",
      "Test Loss tensor([0.0889, 0.0132, 0.2775, 0.2561, 0.2213, 0.0938, 0.0176])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0305, 0.2270, 0.0389, 0.0751, 0.2828, 0.2491, 0.0494]) \n",
      "Test Loss tensor([0.0309, 0.2244, 0.0424, 0.0810, 0.2756, 0.2626, 0.0477])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0292, 0.0211, 0.0372, 0.0230, 0.0275, 0.0181, 0.1250, 0.0072]) \n",
      "Test Loss tensor([0.0287, 0.0213, 0.0361, 0.0209, 0.0267, 0.0155, 0.1241, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5976, 0.6745, 0.1092, 0.1830, 0.3842, 0.5316])\n",
      "Valid Idx 3 | Loss tensor([0.3994, 0.1978, 0.1207, 0.7542, 0.7941])\n",
      "Gradients: Input 0.8879789113998413 | Message 1.2415599822998047 | Update 1.3438720703125 | Output 0.1446361392736435\n",
      "\n",
      "************** Batch 144 in 5.043950796127319 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0712, 0.1991, 0.0823, 0.1149]) \n",
      "Test Loss tensor([0.0655, 0.2038, 0.0790, 0.1083])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2101, 0.5527, 0.0390, 0.2177, 0.3342]) \n",
      "Test Loss tensor([0.2165, 0.5523, 0.0287, 0.2077, 0.3414])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0966, 0.0152, 0.2829, 0.2578, 0.2179, 0.0968, 0.0195]) \n",
      "Test Loss tensor([0.0885, 0.0154, 0.2780, 0.2631, 0.2153, 0.0973, 0.0150])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0275, 0.2322, 0.0466, 0.0867, 0.2774, 0.2673, 0.0471]) \n",
      "Test Loss tensor([0.0336, 0.2262, 0.0450, 0.0820, 0.2778, 0.2621, 0.0502])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0327, 0.0232, 0.0378, 0.0205, 0.0273, 0.0155, 0.1296, 0.0077]) \n",
      "Test Loss tensor([0.0267, 0.0224, 0.0375, 0.0224, 0.0293, 0.0129, 0.1286, 0.0060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6072, 0.6484, 0.1075, 0.1937, 0.3715, 0.5496])\n",
      "Valid Idx 3 | Loss tensor([0.4838, 0.2622, 0.1235, 0.7884, 0.8320])\n",
      "\n",
      "************** Batch 148 in 5.027217388153076 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0655, 0.1949, 0.0743, 0.1068]) \n",
      "Test Loss tensor([0.0649, 0.2014, 0.0784, 0.1114])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2220, 0.5497, 0.0300, 0.2041, 0.3293]) \n",
      "Test Loss tensor([0.2066, 0.5569, 0.0384, 0.2108, 0.3238])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0949, 0.0162, 0.2904, 0.2668, 0.2226, 0.1008, 0.0151]) \n",
      "Test Loss tensor([0.0938, 0.0134, 0.2806, 0.2572, 0.2196, 0.0929, 0.0193])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0286, 0.2325, 0.0383, 0.0758, 0.2818, 0.2501, 0.0494]) \n",
      "Test Loss tensor([0.0296, 0.2282, 0.0441, 0.0803, 0.2782, 0.2604, 0.0519])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0240, 0.0219, 0.0366, 0.0174, 0.0265, 0.0122, 0.1261, 0.0054]) \n",
      "Test Loss tensor([0.0289, 0.0209, 0.0369, 0.0196, 0.0262, 0.0173, 0.1247, 0.0075])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5979, 0.6781, 0.1117, 0.1793, 0.3855, 0.5212])\n",
      "Valid Idx 3 | Loss tensor([0.3704, 0.1869, 0.1164, 0.7427, 0.7903])\n",
      "\n",
      "************** Batch 152 in 5.053130388259888 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0639, 0.2043, 0.0801, 0.1172]) \n",
      "Test Loss tensor([0.0647, 0.2046, 0.0788, 0.1084])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2094, 0.5568, 0.0384, 0.2011, 0.3317]) \n",
      "Test Loss tensor([0.2101, 0.5589, 0.0306, 0.2077, 0.3349])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0914, 0.0134, 0.2814, 0.2562, 0.2199, 0.0963, 0.0159]) \n",
      "Test Loss tensor([0.0916, 0.0144, 0.2840, 0.2617, 0.2179, 0.0960, 0.0157])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0299, 0.2323, 0.0486, 0.0769, 0.2845, 0.2633, 0.0489]) \n",
      "Test Loss tensor([0.0321, 0.2284, 0.0458, 0.0825, 0.2767, 0.2631, 0.0527])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0196, 0.0377, 0.0207, 0.0239, 0.0173, 0.1314, 0.0068]) \n",
      "Test Loss tensor([0.0279, 0.0234, 0.0368, 0.0204, 0.0273, 0.0140, 0.1239, 0.0061])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6020, 0.6635, 0.1087, 0.1893, 0.3804, 0.5460])\n",
      "Valid Idx 3 | Loss tensor([0.4428, 0.2363, 0.1215, 0.7695, 0.8239])\n",
      "\n",
      "************** Batch 156 in 5.229436159133911 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0645, 0.2020, 0.0823, 0.1063]) \n",
      "Test Loss tensor([0.0639, 0.2024, 0.0795, 0.1097])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2158, 0.5578, 0.0317, 0.2076, 0.3366]) \n",
      "Test Loss tensor([0.2081, 0.5559, 0.0323, 0.2084, 0.3288])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0879, 0.0152, 0.2864, 0.2637, 0.2219, 0.0986, 0.0145]) \n",
      "Test Loss tensor([0.0910, 0.0137, 0.2812, 0.2612, 0.2229, 0.0944, 0.0165])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0326, 0.2333, 0.0472, 0.0776, 0.2757, 0.2665, 0.0487]) \n",
      "Test Loss tensor([0.0306, 0.2275, 0.0445, 0.0779, 0.2808, 0.2662, 0.0515])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0272, 0.0245, 0.0425, 0.0216, 0.0292, 0.0146, 0.1263, 0.0062]) \n",
      "Test Loss tensor([0.0271, 0.0230, 0.0368, 0.0210, 0.0266, 0.0155, 0.1237, 0.0063])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6031, 0.6721, 0.1086, 0.1806, 0.3797, 0.5396])\n",
      "Valid Idx 3 | Loss tensor([0.4056, 0.2213, 0.1217, 0.7541, 0.8154])\n",
      "\n",
      "************** Batch 160 in 5.13673996925354 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0601, 0.1921, 0.0778, 0.1025]) \n",
      "Test Loss tensor([0.0645, 0.2031, 0.0809, 0.1088])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2196, 0.5621, 0.0316, 0.2154, 0.3385]) \n",
      "Test Loss tensor([0.2086, 0.5534, 0.0339, 0.2090, 0.3269])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0911, 0.0138, 0.2665, 0.2393, 0.2103, 0.0942, 0.0177]) \n",
      "Test Loss tensor([0.0886, 0.0130, 0.2801, 0.2603, 0.2231, 0.0924, 0.0169])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0302, 0.2329, 0.0467, 0.0811, 0.2749, 0.2601, 0.0454]) \n",
      "Test Loss tensor([0.0309, 0.2263, 0.0436, 0.0796, 0.2806, 0.2587, 0.0487])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0296, 0.0181, 0.0359, 0.0186, 0.0297, 0.0160, 0.1188, 0.0063]) \n",
      "Test Loss tensor([0.0283, 0.0220, 0.0373, 0.0217, 0.0270, 0.0161, 0.1237, 0.0068])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5992, 0.6742, 0.1088, 0.1834, 0.3855, 0.5336])\n",
      "Valid Idx 3 | Loss tensor([0.3906, 0.2063, 0.1204, 0.7481, 0.8042])\n",
      "Gradients: Input 0.15549249947071075 | Message 0.12347786873579025 | Update 0.10976085811853409 | Output 0.025011898949742317\n",
      "\n",
      "************** Batch 164 in 5.055840730667114 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0624, 0.2009, 0.0785, 0.1085]) \n",
      "Test Loss tensor([0.0639, 0.1992, 0.0804, 0.1072])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2076, 0.5584, 0.0365, 0.2116, 0.3158]) \n",
      "Test Loss tensor([0.2143, 0.5568, 0.0290, 0.2100, 0.3282])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0921, 0.0134, 0.2929, 0.2539, 0.2078, 0.0901, 0.0152]) \n",
      "Test Loss tensor([0.0887, 0.0142, 0.2817, 0.2583, 0.2197, 0.0972, 0.0155])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0266, 0.2326, 0.0436, 0.0844, 0.2709, 0.2449, 0.0518]) \n",
      "Test Loss tensor([0.0332, 0.2262, 0.0424, 0.0800, 0.2775, 0.2600, 0.0487])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0274, 0.0215, 0.0389, 0.0198, 0.0254, 0.0169, 0.1223, 0.0066]) \n",
      "Test Loss tensor([0.0255, 0.0206, 0.0371, 0.0217, 0.0289, 0.0140, 0.1247, 0.0060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6036, 0.6606, 0.1075, 0.1869, 0.3805, 0.5535])\n",
      "Valid Idx 3 | Loss tensor([0.4475, 0.2451, 0.1231, 0.7691, 0.8270])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 168 in 5.0901172161102295 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0578, 0.1834, 0.0759, 0.1057]) \n",
      "Test Loss tensor([0.0626, 0.1998, 0.0786, 0.1086])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2032, 0.5532, 0.0269, 0.2060, 0.3221]) \n",
      "Test Loss tensor([0.2125, 0.5505, 0.0327, 0.2083, 0.3312])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0855, 0.0142, 0.2879, 0.2598, 0.2186, 0.0911, 0.0182]) \n",
      "Test Loss tensor([0.0901, 0.0134, 0.2766, 0.2580, 0.2203, 0.0954, 0.0170])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0329, 0.2422, 0.0435, 0.0884, 0.2831, 0.2600, 0.0531]) \n",
      "Test Loss tensor([0.0316, 0.2273, 0.0433, 0.0802, 0.2792, 0.2606, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0255, 0.0226, 0.0346, 0.0228, 0.0300, 0.0146, 0.1288, 0.0060]) \n",
      "Test Loss tensor([0.0287, 0.0236, 0.0370, 0.0212, 0.0279, 0.0153, 0.1249, 0.0066])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6109, 0.6663, 0.1073, 0.1855, 0.3808, 0.5447])\n",
      "Valid Idx 3 | Loss tensor([0.4160, 0.2197, 0.1194, 0.7584, 0.8065])\n",
      "\n",
      "************** Batch 172 in 5.070688962936401 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0608, 0.2048, 0.0806, 0.1147]) \n",
      "Test Loss tensor([0.0644, 0.2036, 0.0790, 0.1088])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2193, 0.5451, 0.0315, 0.2071, 0.3315]) \n",
      "Test Loss tensor([0.2131, 0.5527, 0.0336, 0.2089, 0.3253])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0805, 0.0137, 0.2591, 0.2542, 0.2205, 0.0973, 0.0152]) \n",
      "Test Loss tensor([0.0893, 0.0132, 0.2771, 0.2556, 0.2174, 0.0932, 0.0168])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0279, 0.2326, 0.0435, 0.0785, 0.2721, 0.2703, 0.0508]) \n",
      "Test Loss tensor([0.0317, 0.2284, 0.0439, 0.0806, 0.2765, 0.2603, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0281, 0.0208, 0.0355, 0.0200, 0.0268, 0.0150, 0.1246, 0.0071]) \n",
      "Test Loss tensor([0.0280, 0.0219, 0.0373, 0.0203, 0.0276, 0.0152, 0.1238, 0.0067])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5989, 0.6684, 0.1090, 0.1866, 0.3796, 0.5484])\n",
      "Valid Idx 3 | Loss tensor([0.4149, 0.2159, 0.1243, 0.7599, 0.8033])\n",
      "\n",
      "************** Batch 176 in 4.977367639541626 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0654, 0.1911, 0.0836, 0.1132]) \n",
      "Test Loss tensor([0.0629, 0.2008, 0.0797, 0.1067])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2212, 0.5474, 0.0372, 0.2138, 0.3202]) \n",
      "Test Loss tensor([0.2120, 0.5533, 0.0338, 0.2086, 0.3231])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0901, 0.0138, 0.2798, 0.2583, 0.2201, 0.0994, 0.0172]) \n",
      "Test Loss tensor([0.0911, 0.0136, 0.2809, 0.2539, 0.2184, 0.0951, 0.0169])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0292, 0.2363, 0.0462, 0.0733, 0.2757, 0.2500, 0.0499]) \n",
      "Test Loss tensor([0.0327, 0.2244, 0.0445, 0.0811, 0.2753, 0.2584, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0323, 0.0207, 0.0372, 0.0171, 0.0253, 0.0144, 0.1245, 0.0065]) \n",
      "Test Loss tensor([0.0275, 0.0214, 0.0377, 0.0214, 0.0278, 0.0148, 0.1251, 0.0065])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6031, 0.6632, 0.1083, 0.1877, 0.3780, 0.5543])\n",
      "Valid Idx 3 | Loss tensor([0.4248, 0.2258, 0.1194, 0.7642, 0.8091])\n",
      "\n",
      "************** Batch 180 in 4.970996379852295 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0647, 0.1897, 0.0796, 0.1096]) \n",
      "Test Loss tensor([0.0626, 0.2020, 0.0793, 0.1078])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2088, 0.5244, 0.0317, 0.2103, 0.3472]) \n",
      "Test Loss tensor([0.2145, 0.5460, 0.0306, 0.2078, 0.3282])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0945, 0.0148, 0.2860, 0.2577, 0.2142, 0.0926, 0.0177]) \n",
      "Test Loss tensor([0.0899, 0.0140, 0.2804, 0.2562, 0.2172, 0.0947, 0.0162])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0355, 0.2312, 0.0455, 0.0857, 0.2794, 0.2641, 0.0485]) \n",
      "Test Loss tensor([0.0329, 0.2266, 0.0425, 0.0817, 0.2783, 0.2559, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0291, 0.0217, 0.0343, 0.0235, 0.0281, 0.0147, 0.1202, 0.0060]) \n",
      "Test Loss tensor([0.0267, 0.0216, 0.0389, 0.0205, 0.0284, 0.0137, 0.1244, 0.0060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6002, 0.6612, 0.1053, 0.1882, 0.3803, 0.5580])\n",
      "Valid Idx 3 | Loss tensor([0.4399, 0.2390, 0.1251, 0.7696, 0.8226])\n",
      "Gradients: Input 0.1010625958442688 | Message 0.08550126105546951 | Update 0.07587814331054688 | Output 0.01336781308054924\n",
      "\n",
      "************** Batch 184 in 5.046672582626343 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0614, 0.2015, 0.0782, 0.1003]) \n",
      "Test Loss tensor([0.0653, 0.2014, 0.0787, 0.1083])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2107, 0.5476, 0.0347, 0.2065, 0.3209]) \n",
      "Test Loss tensor([0.2088, 0.5473, 0.0321, 0.2085, 0.3219])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0957, 0.0123, 0.2709, 0.2574, 0.2141, 0.0967, 0.0164]) \n",
      "Test Loss tensor([0.0927, 0.0136, 0.2826, 0.2565, 0.2201, 0.0945, 0.0167])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0333, 0.2314, 0.0392, 0.0770, 0.2707, 0.2526, 0.0470]) \n",
      "Test Loss tensor([0.0327, 0.2297, 0.0422, 0.0785, 0.2767, 0.2629, 0.0483])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0270, 0.0201, 0.0402, 0.0219, 0.0272, 0.0146, 0.1203, 0.0062]) \n",
      "Test Loss tensor([0.0281, 0.0220, 0.0365, 0.0211, 0.0270, 0.0140, 0.1239, 0.0061])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6036, 0.6640, 0.1034, 0.1853, 0.3795, 0.5583])\n",
      "Valid Idx 3 | Loss tensor([0.4214, 0.2302, 0.1196, 0.7607, 0.8159])\n",
      "\n",
      "************** Batch 188 in 5.10591721534729 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0639, 0.2024, 0.0752, 0.1087]) \n",
      "Test Loss tensor([0.0642, 0.2001, 0.0778, 0.1105])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2198, 0.5508, 0.0311, 0.2195, 0.3458]) \n",
      "Test Loss tensor([0.2096, 0.5540, 0.0346, 0.2138, 0.3166])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0931, 0.0136, 0.2784, 0.2549, 0.2167, 0.0969, 0.0165]) \n",
      "Test Loss tensor([0.0911, 0.0129, 0.2790, 0.2580, 0.2174, 0.0936, 0.0172])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0378, 0.2347, 0.0410, 0.0784, 0.2746, 0.2616, 0.0452]) \n",
      "Test Loss tensor([0.0304, 0.2243, 0.0424, 0.0767, 0.2741, 0.2612, 0.0480])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0259, 0.0222, 0.0329, 0.0181, 0.0268, 0.0146, 0.1231, 0.0058]) \n",
      "Test Loss tensor([0.0284, 0.0237, 0.0367, 0.0213, 0.0259, 0.0153, 0.1256, 0.0063])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6035, 0.6785, 0.1071, 0.1823, 0.3837, 0.5465])\n",
      "Valid Idx 3 | Loss tensor([0.3829, 0.2128, 0.1192, 0.7418, 0.8106])\n",
      "\n",
      "************** Batch 192 in 5.110871076583862 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0645, 0.1906, 0.0761, 0.1169]) \n",
      "Test Loss tensor([0.0624, 0.1983, 0.0787, 0.1067])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2151, 0.5472, 0.0331, 0.2178, 0.3211]) \n",
      "Test Loss tensor([0.2133, 0.5458, 0.0290, 0.2087, 0.3250])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0957, 0.0123, 0.2862, 0.2532, 0.2184, 0.0886, 0.0168]) \n",
      "Test Loss tensor([0.0902, 0.0131, 0.2822, 0.2607, 0.2172, 0.0946, 0.0144])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0306, 0.2292, 0.0420, 0.0802, 0.2751, 0.2586, 0.0522]) \n",
      "Test Loss tensor([0.0306, 0.2249, 0.0441, 0.0791, 0.2756, 0.2582, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0276, 0.0250, 0.0362, 0.0228, 0.0258, 0.0159, 0.1236, 0.0058]) \n",
      "Test Loss tensor([0.0281, 0.0223, 0.0366, 0.0208, 0.0266, 0.0135, 0.1239, 0.0055])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6043, 0.6756, 0.1063, 0.1865, 0.3792, 0.5589])\n",
      "Valid Idx 3 | Loss tensor([0.4074, 0.2408, 0.1196, 0.7536, 0.8305])\n",
      "\n",
      "************** Batch 196 in 4.0081868171691895 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0682, 0.2015, 0.0737, 0.1074]) \n",
      "Test Loss tensor([0.0649, 0.2068, 0.0786, 0.1079])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2127, 0.5405, 0.0270, 0.2098, 0.3205]) \n",
      "Test Loss tensor([0.2128, 0.5403, 0.0272, 0.2071, 0.3276])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0940, 0.0126, 0.2878, 0.2620, 0.2075, 0.0916, 0.0159]) \n",
      "Test Loss tensor([0.0900, 0.0138, 0.2822, 0.2625, 0.2147, 0.0940, 0.0145])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0310, 0.2342, 0.0459, 0.0763, 0.2745, 0.2647, 0.0481]) \n",
      "Test Loss tensor([0.0306, 0.2286, 0.0450, 0.0800, 0.2763, 0.2595, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0307, 0.0235, 0.0403, 0.0203, 0.0250, 0.0127, 0.1174, 0.0054]) \n",
      "Test Loss tensor([0.0274, 0.0224, 0.0370, 0.0205, 0.0261, 0.0134, 0.1226, 0.0055])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6039, 0.6781, 0.1067, 0.1827, 0.3840, 0.5559])\n",
      "Valid Idx 3 | Loss tensor([0.4070, 0.2403, 0.1221, 0.7595, 0.8321])\n",
      "\n",
      "************** Batch 200 in 5.149198532104492 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0668, 0.2002, 0.0807, 0.1093]) \n",
      "Test Loss tensor([0.0636, 0.2013, 0.0776, 0.1073])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2228, 0.5405, 0.0261, 0.2124, 0.3185]) \n",
      "Test Loss tensor([0.2065, 0.5451, 0.0340, 0.2068, 0.3203])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0909, 0.0118, 0.2720, 0.2550, 0.2196, 0.0967, 0.0110]) \n",
      "Test Loss tensor([0.0936, 0.0124, 0.2796, 0.2546, 0.2197, 0.0915, 0.0158])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0308, 0.2421, 0.0479, 0.0868, 0.2639, 0.2578, 0.0575]) \n",
      "Test Loss tensor([0.0302, 0.2293, 0.0440, 0.0812, 0.2802, 0.2598, 0.0524])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0284, 0.0221, 0.0372, 0.0228, 0.0270, 0.0126, 0.1285, 0.0049]) \n",
      "Test Loss tensor([0.0292, 0.0231, 0.0353, 0.0200, 0.0250, 0.0161, 0.1233, 0.0064])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6013, 0.6872, 0.1093, 0.1832, 0.3832, 0.5420])\n",
      "Valid Idx 3 | Loss tensor([0.3648, 0.2011, 0.1208, 0.7423, 0.8081])\n",
      "Gradients: Input 0.3853453993797302 | Message 0.4838064908981323 | Update 0.531399130821228 | Output 0.057883601635694504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 204 in 5.532155275344849 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0627, 0.2103, 0.0756, 0.1158]) \n",
      "Test Loss tensor([0.0633, 0.2031, 0.0787, 0.1056])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2081, 0.5392, 0.0349, 0.2093, 0.3213]) \n",
      "Test Loss tensor([0.2088, 0.5397, 0.0285, 0.2101, 0.3254])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0891, 0.0120, 0.2876, 0.2509, 0.2154, 0.0871, 0.0154]) \n",
      "Test Loss tensor([0.0873, 0.0137, 0.2795, 0.2607, 0.2159, 0.0937, 0.0139])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0306, 0.2378, 0.0394, 0.0781, 0.2799, 0.2518, 0.0425]) \n",
      "Test Loss tensor([0.0327, 0.2244, 0.0432, 0.0794, 0.2771, 0.2608, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0230, 0.0369, 0.0192, 0.0223, 0.0159, 0.1241, 0.0058]) \n",
      "Test Loss tensor([0.0274, 0.0232, 0.0376, 0.0218, 0.0282, 0.0131, 0.1259, 0.0055])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5997, 0.6616, 0.1066, 0.1926, 0.3735, 0.5601])\n",
      "Valid Idx 3 | Loss tensor([0.4400, 0.2529, 0.1210, 0.7837, 0.8378])\n",
      "\n",
      "************** Batch 208 in 5.352059602737427 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0621, 0.2025, 0.0798, 0.1008]) \n",
      "Test Loss tensor([0.0625, 0.1996, 0.0778, 0.1133])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2088, 0.5223, 0.0262, 0.2123, 0.3306]) \n",
      "Test Loss tensor([0.2088, 0.5417, 0.0380, 0.2041, 0.3149])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0881, 0.0173, 0.2804, 0.2547, 0.2135, 0.0989, 0.0170]) \n",
      "Test Loss tensor([0.0942, 0.0126, 0.2788, 0.2580, 0.2202, 0.0927, 0.0179])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0288, 0.2402, 0.0463, 0.0793, 0.2881, 0.2538, 0.0477]) \n",
      "Test Loss tensor([0.0307, 0.2300, 0.0429, 0.0769, 0.2780, 0.2595, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0269, 0.0239, 0.0363, 0.0238, 0.0303, 0.0127, 0.1272, 0.0052]) \n",
      "Test Loss tensor([0.0286, 0.0214, 0.0355, 0.0207, 0.0252, 0.0161, 0.1237, 0.0070])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6013, 0.6832, 0.1097, 0.1816, 0.3784, 0.5307])\n",
      "Valid Idx 3 | Loss tensor([0.3700, 0.1910, 0.1197, 0.7590, 0.7979])\n",
      "\n",
      "************** Batch 212 in 5.393879652023315 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0652, 0.1974, 0.0721, 0.1145]) \n",
      "Test Loss tensor([0.0642, 0.1993, 0.0790, 0.1061])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2141, 0.5300, 0.0349, 0.2003, 0.3147]) \n",
      "Test Loss tensor([0.2196, 0.5400, 0.0273, 0.2095, 0.3243])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0894, 0.0126, 0.2679, 0.2542, 0.2265, 0.0920, 0.0172]) \n",
      "Test Loss tensor([0.0880, 0.0152, 0.2849, 0.2612, 0.2155, 0.1010, 0.0147])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0297, 0.2409, 0.0410, 0.0874, 0.2871, 0.2574, 0.0502]) \n",
      "Test Loss tensor([0.0347, 0.2254, 0.0443, 0.0803, 0.2750, 0.2603, 0.0493])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0269, 0.0239, 0.0353, 0.0195, 0.0251, 0.0177, 0.1265, 0.0067]) \n",
      "Test Loss tensor([0.0267, 0.0225, 0.0384, 0.0223, 0.0285, 0.0129, 0.1271, 0.0053])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6088, 0.6529, 0.1036, 0.1930, 0.3718, 0.5616])\n",
      "Valid Idx 3 | Loss tensor([0.4886, 0.2706, 0.1215, 0.8033, 0.8423])\n",
      "\n",
      "************** Batch 216 in 5.415736675262451 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0670, 0.1974, 0.0792, 0.1028]) \n",
      "Test Loss tensor([0.0643, 0.1996, 0.0792, 0.1136])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2156, 0.5337, 0.0244, 0.1997, 0.3214]) \n",
      "Test Loss tensor([0.2040, 0.5393, 0.0395, 0.2086, 0.3125])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0800, 0.0144, 0.2935, 0.2636, 0.2029, 0.0979, 0.0119]) \n",
      "Test Loss tensor([0.0935, 0.0122, 0.2786, 0.2516, 0.2205, 0.0929, 0.0177])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0295, 0.2269, 0.0406, 0.0798, 0.2682, 0.2562, 0.0454]) \n",
      "Test Loss tensor([0.0305, 0.2227, 0.0431, 0.0788, 0.2733, 0.2602, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0271, 0.0205, 0.0398, 0.0202, 0.0287, 0.0134, 0.1144, 0.0051]) \n",
      "Test Loss tensor([0.0301, 0.0234, 0.0358, 0.0203, 0.0242, 0.0177, 0.1229, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6016, 0.6921, 0.1091, 0.1825, 0.3807, 0.5253])\n",
      "Valid Idx 3 | Loss tensor([0.3631, 0.1836, 0.1170, 0.7561, 0.7885])\n",
      "\n",
      "************** Batch 220 in 5.436371564865112 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0687, 0.1977, 0.0807, 0.1135]) \n",
      "Test Loss tensor([0.0629, 0.2003, 0.0788, 0.1077])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2087, 0.5453, 0.0380, 0.2085, 0.3056]) \n",
      "Test Loss tensor([0.2115, 0.5360, 0.0312, 0.2054, 0.3174])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0917, 0.0131, 0.2858, 0.2594, 0.2195, 0.0906, 0.0183]) \n",
      "Test Loss tensor([0.0901, 0.0131, 0.2797, 0.2553, 0.2156, 0.0981, 0.0154])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0279, 0.2336, 0.0456, 0.0781, 0.2679, 0.2567, 0.0490]) \n",
      "Test Loss tensor([0.0306, 0.2301, 0.0427, 0.0799, 0.2743, 0.2615, 0.0517])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0289, 0.0232, 0.0393, 0.0165, 0.0268, 0.0152, 0.1122, 0.0069]) \n",
      "Test Loss tensor([0.0281, 0.0221, 0.0354, 0.0202, 0.0256, 0.0153, 0.1235, 0.0061])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6002, 0.6687, 0.1062, 0.1908, 0.3766, 0.5374])\n",
      "Valid Idx 3 | Loss tensor([0.4213, 0.2292, 0.1195, 0.7762, 0.8243])\n",
      "Gradients: Input 1.1341781616210938 | Message 1.5972740650177002 | Update 1.7233973741531372 | Output 0.20496457815170288\n",
      "\n",
      "************** Batch 224 in 5.362964391708374 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0656, 0.2064, 0.0780, 0.1021]) \n",
      "Test Loss tensor([0.0622, 0.1995, 0.0788, 0.1024])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2070, 0.5368, 0.0301, 0.2042, 0.3207]) \n",
      "Test Loss tensor([0.2139, 0.5383, 0.0260, 0.2071, 0.3237])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0893, 0.0123, 0.2787, 0.2535, 0.2052, 0.0939, 0.0124]) \n",
      "Test Loss tensor([0.0902, 0.0139, 0.2782, 0.2632, 0.2161, 0.1000, 0.0136])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0337, 0.2371, 0.0397, 0.0851, 0.2691, 0.2644, 0.0458]) \n",
      "Test Loss tensor([0.0321, 0.2278, 0.0442, 0.0789, 0.2755, 0.2590, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0267, 0.0198, 0.0328, 0.0163, 0.0263, 0.0148, 0.1206, 0.0060]) \n",
      "Test Loss tensor([0.0263, 0.0225, 0.0356, 0.0206, 0.0266, 0.0127, 0.1226, 0.0052])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5983, 0.6569, 0.1070, 0.1967, 0.3714, 0.5492])\n",
      "Valid Idx 3 | Loss tensor([0.4620, 0.2758, 0.1233, 0.7959, 0.8457])\n",
      "\n",
      "************** Batch 228 in 5.326724052429199 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0654, 0.2016, 0.0764, 0.1043]) \n",
      "Test Loss tensor([0.0656, 0.1991, 0.0801, 0.1117])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2156, 0.5426, 0.0251, 0.2123, 0.3386]) \n",
      "Test Loss tensor([0.2080, 0.5391, 0.0397, 0.2048, 0.3085])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0847, 0.0135, 0.2702, 0.2612, 0.2119, 0.0982, 0.0118]) \n",
      "Test Loss tensor([0.0978, 0.0113, 0.2783, 0.2580, 0.2244, 0.0900, 0.0173])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0327, 0.2301, 0.0370, 0.0729, 0.2714, 0.2521, 0.0528]) \n",
      "Test Loss tensor([0.0283, 0.2269, 0.0432, 0.0799, 0.2747, 0.2643, 0.0505])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0262, 0.0231, 0.0379, 0.0213, 0.0292, 0.0123, 0.1222, 0.0049]) \n",
      "Test Loss tensor([0.0315, 0.0225, 0.0359, 0.0205, 0.0231, 0.0188, 0.1232, 0.0070])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5975, 0.7031, 0.1090, 0.1739, 0.3844, 0.5144])\n",
      "Valid Idx 3 | Loss tensor([0.3123, 0.1759, 0.1168, 0.7341, 0.7918])\n",
      "\n",
      "************** Batch 232 in 5.345307350158691 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0618, 0.1987, 0.0829, 0.1097]) \n",
      "Test Loss tensor([0.0661, 0.1998, 0.0818, 0.1057])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2080, 0.5296, 0.0394, 0.2228, 0.3103]) \n",
      "Test Loss tensor([0.2203, 0.5300, 0.0219, 0.2091, 0.3374])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0910, 0.0119, 0.2893, 0.2520, 0.2232, 0.0921, 0.0149]) \n",
      "Test Loss tensor([0.0877, 0.0147, 0.2840, 0.2731, 0.2117, 0.1010, 0.0120])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0267, 0.2249, 0.0426, 0.0743, 0.2726, 0.2603, 0.0461]) \n",
      "Test Loss tensor([0.0334, 0.2192, 0.0431, 0.0801, 0.2779, 0.2586, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0282, 0.0194, 0.0338, 0.0160, 0.0230, 0.0183, 0.1225, 0.0069]) \n",
      "Test Loss tensor([0.0273, 0.0226, 0.0379, 0.0211, 0.0293, 0.0116, 0.1250, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6111, 0.6431, 0.1021, 0.1985, 0.3704, 0.5748])\n",
      "Valid Idx 3 | Loss tensor([0.5040, 0.3379, 0.1277, 0.8087, 0.8643])\n",
      "\n",
      "************** Batch 236 in 5.482616901397705 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0645, 0.1952, 0.0780, 0.1055]) \n",
      "Test Loss tensor([0.0628, 0.2024, 0.0778, 0.1068])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2214, 0.5356, 0.0224, 0.1926, 0.3264]) \n",
      "Test Loss tensor([0.2049, 0.5328, 0.0337, 0.2062, 0.3092])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0808, 0.0143, 0.2654, 0.2609, 0.2056, 0.0964, 0.0112]) \n",
      "Test Loss tensor([0.0926, 0.0136, 0.2804, 0.2549, 0.2171, 0.0913, 0.0159])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0347, 0.2225, 0.0464, 0.0805, 0.2749, 0.2458, 0.0433]) \n",
      "Test Loss tensor([0.0302, 0.2245, 0.0419, 0.0769, 0.2765, 0.2605, 0.0473])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0252, 0.0258, 0.0390, 0.0261, 0.0287, 0.0117, 0.1263, 0.0051]) \n",
      "Test Loss tensor([0.0291, 0.0228, 0.0356, 0.0206, 0.0260, 0.0157, 0.1219, 0.0062])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5966, 0.6759, 0.1085, 0.1861, 0.3795, 0.5370])\n",
      "Valid Idx 3 | Loss tensor([0.3711, 0.2261, 0.1198, 0.7617, 0.8119])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 240 in 5.299047231674194 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0630, 0.2050, 0.0796, 0.1032]) \n",
      "Test Loss tensor([0.0647, 0.1999, 0.0774, 0.1124])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2172, 0.5310, 0.0320, 0.2035, 0.3076]) \n",
      "Test Loss tensor([0.2049, 0.5293, 0.0401, 0.2060, 0.3055])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0933, 0.0139, 0.2795, 0.2494, 0.2079, 0.0893, 0.0153]) \n",
      "Test Loss tensor([0.0963, 0.0125, 0.2813, 0.2538, 0.2250, 0.0951, 0.0180])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0273, 0.2377, 0.0399, 0.0778, 0.2742, 0.2642, 0.0524]) \n",
      "Test Loss tensor([0.0302, 0.2263, 0.0424, 0.0789, 0.2792, 0.2614, 0.0500])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0284, 0.0195, 0.0403, 0.0213, 0.0253, 0.0159, 0.1266, 0.0066]) \n",
      "Test Loss tensor([0.0310, 0.0221, 0.0364, 0.0200, 0.0252, 0.0175, 0.1236, 0.0081])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5928, 0.6858, 0.1084, 0.1804, 0.3783, 0.5247])\n",
      "Valid Idx 3 | Loss tensor([0.3409, 0.1953, 0.1178, 0.7473, 0.7871])\n",
      "Gradients: Input 0.4856094419956207 | Message 0.6572340726852417 | Update 0.7011551260948181 | Output 0.0746818408370018\n",
      "\n",
      "************** Batch 244 in 5.237970590591431 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0628, 0.1965, 0.0759, 0.1088]) \n",
      "Test Loss tensor([0.0645, 0.1983, 0.0820, 0.1072])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2062, 0.5170, 0.0406, 0.2047, 0.3079]) \n",
      "Test Loss tensor([0.2166, 0.5256, 0.0225, 0.2100, 0.3268])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0879, 0.0117, 0.2840, 0.2586, 0.2256, 0.0928, 0.0192]) \n",
      "Test Loss tensor([0.0876, 0.0166, 0.2840, 0.2694, 0.2137, 0.1031, 0.0139])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0279, 0.2289, 0.0344, 0.0767, 0.2788, 0.2485, 0.0446]) \n",
      "Test Loss tensor([0.0359, 0.2253, 0.0449, 0.0827, 0.2787, 0.2568, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0298, 0.0182, 0.0355, 0.0181, 0.0255, 0.0171, 0.1283, 0.0081]) \n",
      "Test Loss tensor([0.0261, 0.0231, 0.0400, 0.0212, 0.0315, 0.0110, 0.1307, 0.0054])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6147, 0.6241, 0.1020, 0.2039, 0.3607, 0.5762])\n",
      "Valid Idx 3 | Loss tensor([0.5452, 0.3634, 0.1263, 0.8143, 0.8525])\n",
      "\n",
      "************** Batch 248 in 5.291555404663086 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0637, 0.2008, 0.0813, 0.1062]) \n",
      "Test Loss tensor([0.0643, 0.1998, 0.0775, 0.1136])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2170, 0.5124, 0.0240, 0.2152, 0.3233]) \n",
      "Test Loss tensor([0.1995, 0.5313, 0.0429, 0.2082, 0.3045])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0880, 0.0190, 0.2961, 0.2760, 0.2192, 0.1018, 0.0174]) \n",
      "Test Loss tensor([0.0934, 0.0124, 0.2792, 0.2532, 0.2248, 0.0935, 0.0195])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0349, 0.2172, 0.0460, 0.0778, 0.2679, 0.2572, 0.0509]) \n",
      "Test Loss tensor([0.0301, 0.2234, 0.0444, 0.0778, 0.2733, 0.2597, 0.0517])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0281, 0.0262, 0.0412, 0.0189, 0.0300, 0.0119, 0.1341, 0.0057]) \n",
      "Test Loss tensor([0.0305, 0.0223, 0.0361, 0.0195, 0.0250, 0.0187, 0.1225, 0.0086])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6020, 0.6864, 0.1097, 0.1803, 0.3749, 0.5206])\n",
      "Valid Idx 3 | Loss tensor([0.3422, 0.1946, 0.1200, 0.7350, 0.7780])\n",
      "\n",
      "************** Batch 252 in 5.36360502243042 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0625, 0.1835, 0.0766, 0.1172]) \n",
      "Test Loss tensor([0.0630, 0.1998, 0.0782, 0.1059])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1998, 0.5246, 0.0423, 0.2110, 0.2968]) \n",
      "Test Loss tensor([0.2068, 0.5275, 0.0310, 0.2039, 0.3058])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0996, 0.0146, 0.2757, 0.2655, 0.2321, 0.0956, 0.0180]) \n",
      "Test Loss tensor([0.0902, 0.0136, 0.2774, 0.2526, 0.2204, 0.0901, 0.0162])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0298, 0.2259, 0.0410, 0.0743, 0.2787, 0.2590, 0.0538]) \n",
      "Test Loss tensor([0.0311, 0.2241, 0.0434, 0.0771, 0.2762, 0.2599, 0.0481])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0279, 0.0227, 0.0380, 0.0185, 0.0233, 0.0197, 0.1291, 0.0086]) \n",
      "Test Loss tensor([0.0290, 0.0223, 0.0362, 0.0200, 0.0262, 0.0158, 0.1236, 0.0066])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6000, 0.6752, 0.1067, 0.1850, 0.3768, 0.5374])\n",
      "Valid Idx 3 | Loss tensor([0.4031, 0.2483, 0.1210, 0.7590, 0.8128])\n",
      "\n",
      "************** Batch 256 in 5.2133729457855225 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0622, 0.1938, 0.0809, 0.1046]) \n",
      "Test Loss tensor([0.0655, 0.2027, 0.0803, 0.1037])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2018, 0.5248, 0.0339, 0.2100, 0.3167]) \n",
      "Test Loss tensor([0.2151, 0.5219, 0.0237, 0.2038, 0.3201])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0919, 0.0130, 0.2963, 0.2665, 0.2205, 0.0872, 0.0166]) \n",
      "Test Loss tensor([0.0862, 0.0156, 0.2803, 0.2672, 0.2152, 0.0955, 0.0129])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0248, 0.2293, 0.0434, 0.0851, 0.2741, 0.2620, 0.0496]) \n",
      "Test Loss tensor([0.0337, 0.2252, 0.0453, 0.0801, 0.2742, 0.2565, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0225, 0.0436, 0.0207, 0.0267, 0.0147, 0.1275, 0.0068]) \n",
      "Test Loss tensor([0.0265, 0.0234, 0.0379, 0.0206, 0.0292, 0.0125, 0.1244, 0.0050])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6089, 0.6507, 0.1055, 0.1958, 0.3701, 0.5630])\n",
      "Valid Idx 3 | Loss tensor([0.4869, 0.3277, 0.1219, 0.7870, 0.8483])\n",
      "\n",
      "************** Batch 260 in 5.457901477813721 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0650, 0.1976, 0.0802, 0.1056]) \n",
      "Test Loss tensor([0.0613, 0.2009, 0.0811, 0.1111])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2169, 0.5336, 0.0239, 0.2045, 0.3185]) \n",
      "Test Loss tensor([0.2021, 0.5283, 0.0365, 0.2064, 0.3048])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0860, 0.0149, 0.2872, 0.2606, 0.2219, 0.1093, 0.0118]) \n",
      "Test Loss tensor([0.0959, 0.0125, 0.2778, 0.2533, 0.2238, 0.0909, 0.0162])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0324, 0.2267, 0.0444, 0.0777, 0.2811, 0.2670, 0.0563]) \n",
      "Test Loss tensor([0.0291, 0.2278, 0.0427, 0.0770, 0.2766, 0.2596, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0266, 0.0247, 0.0394, 0.0216, 0.0287, 0.0144, 0.1293, 0.0056]) \n",
      "Test Loss tensor([0.0307, 0.0222, 0.0370, 0.0201, 0.0243, 0.0190, 0.1229, 0.0070])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5972, 0.6993, 0.1086, 0.1767, 0.3864, 0.5169])\n",
      "Valid Idx 3 | Loss tensor([0.3354, 0.2015, 0.1170, 0.7243, 0.7924])\n",
      "Gradients: Input 1.5370664596557617 | Message 2.152822971343994 | Update 2.324442148208618 | Output 0.2723974883556366\n",
      "\n",
      "************** Batch 264 in 5.253011226654053 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0591, 0.2005, 0.0832, 0.1071]) \n",
      "Test Loss tensor([0.0623, 0.2009, 0.0794, 0.1050])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2027, 0.5160, 0.0341, 0.2070, 0.3049]) \n",
      "Test Loss tensor([0.2083, 0.5232, 0.0294, 0.2049, 0.3126])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0900, 0.0128, 0.2731, 0.2540, 0.2192, 0.0929, 0.0151]) \n",
      "Test Loss tensor([0.0907, 0.0137, 0.2824, 0.2563, 0.2185, 0.0927, 0.0137])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0308, 0.2338, 0.0379, 0.0837, 0.2792, 0.2666, 0.0449]) \n",
      "Test Loss tensor([0.0313, 0.2241, 0.0441, 0.0787, 0.2731, 0.2605, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0310, 0.0182, 0.0365, 0.0210, 0.0231, 0.0175, 0.1218, 0.0070]) \n",
      "Test Loss tensor([0.0276, 0.0234, 0.0383, 0.0201, 0.0265, 0.0153, 0.1230, 0.0058])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6048, 0.6777, 0.1061, 0.1885, 0.3727, 0.5420])\n",
      "Valid Idx 3 | Loss tensor([0.4141, 0.2664, 0.1206, 0.7570, 0.8283])\n",
      "\n",
      "************** Batch 268 in 5.415724277496338 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0657, 0.1911, 0.0770, 0.1028]) \n",
      "Test Loss tensor([0.0619, 0.2033, 0.0794, 0.1056])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2187, 0.5203, 0.0290, 0.2029, 0.3172]) \n",
      "Test Loss tensor([0.2081, 0.5230, 0.0285, 0.2071, 0.3103])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0953, 0.0140, 0.2867, 0.2482, 0.2161, 0.0924, 0.0163]) \n",
      "Test Loss tensor([0.0899, 0.0140, 0.2817, 0.2577, 0.2182, 0.0948, 0.0144])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0359, 0.2260, 0.0435, 0.0767, 0.2871, 0.2515, 0.0503]) \n",
      "Test Loss tensor([0.0337, 0.2245, 0.0427, 0.0778, 0.2777, 0.2564, 0.0490])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0287, 0.0235, 0.0429, 0.0217, 0.0279, 0.0139, 0.1207, 0.0059]) \n",
      "Test Loss tensor([0.0280, 0.0228, 0.0375, 0.0205, 0.0276, 0.0151, 0.1200, 0.0060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6004, 0.6698, 0.1038, 0.1895, 0.3786, 0.5469])\n",
      "Valid Idx 3 | Loss tensor([0.4218, 0.2683, 0.1214, 0.7622, 0.8299])\n",
      "\n",
      "************** Batch 272 in 5.395727157592773 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0657, 0.2074, 0.0765, 0.1060]) \n",
      "Test Loss tensor([0.0622, 0.2019, 0.0770, 0.1116])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2124, 0.4981, 0.0272, 0.1993, 0.3200]) \n",
      "Test Loss tensor([0.2004, 0.5209, 0.0364, 0.2062, 0.3051])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0920, 0.0160, 0.2859, 0.2530, 0.2279, 0.0994, 0.0140]) \n",
      "Test Loss tensor([0.0985, 0.0128, 0.2825, 0.2541, 0.2213, 0.0929, 0.0171])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0333, 0.2169, 0.0477, 0.0828, 0.2770, 0.2550, 0.0522]) \n",
      "Test Loss tensor([0.0307, 0.2280, 0.0435, 0.0785, 0.2759, 0.2593, 0.0510])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0279, 0.0187, 0.0357, 0.0187, 0.0268, 0.0136, 0.1162, 0.0062]) \n",
      "Test Loss tensor([0.0302, 0.0217, 0.0366, 0.0193, 0.0259, 0.0177, 0.1213, 0.0071])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6042, 0.6818, 0.1099, 0.1853, 0.3787, 0.5312])\n",
      "Valid Idx 3 | Loss tensor([0.3614, 0.2170, 0.1183, 0.7423, 0.8000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 276 in 5.259966611862183 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0642, 0.2024, 0.0761, 0.1078]) \n",
      "Test Loss tensor([0.0624, 0.2021, 0.0788, 0.1040])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2109, 0.5255, 0.0306, 0.2084, 0.3058]) \n",
      "Test Loss tensor([0.2077, 0.5085, 0.0240, 0.2045, 0.3184])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0943, 0.0130, 0.2762, 0.2375, 0.2117, 0.0894, 0.0162]) \n",
      "Test Loss tensor([0.0893, 0.0155, 0.2809, 0.2604, 0.2161, 0.0997, 0.0135])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0299, 0.2314, 0.0443, 0.0769, 0.2776, 0.2490, 0.0450]) \n",
      "Test Loss tensor([0.0345, 0.2234, 0.0422, 0.0796, 0.2756, 0.2552, 0.0505])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0306, 0.0223, 0.0358, 0.0211, 0.0274, 0.0154, 0.1198, 0.0076]) \n",
      "Test Loss tensor([0.0269, 0.0225, 0.0380, 0.0196, 0.0310, 0.0132, 0.1251, 0.0055])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6020, 0.6492, 0.1071, 0.1982, 0.3651, 0.5595])\n",
      "Valid Idx 3 | Loss tensor([0.4771, 0.3181, 0.1215, 0.7833, 0.8481])\n",
      "\n",
      "************** Batch 280 in 5.4356043338775635 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0670, 0.2096, 0.0795, 0.1065]) \n",
      "Test Loss tensor([0.0627, 0.1999, 0.0771, 0.1116])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2134, 0.5075, 0.0277, 0.1958, 0.3267]) \n",
      "Test Loss tensor([0.2009, 0.5168, 0.0361, 0.2068, 0.3023])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0924, 0.0149, 0.2864, 0.2549, 0.2117, 0.0981, 0.0163]) \n",
      "Test Loss tensor([0.0963, 0.0134, 0.2791, 0.2532, 0.2220, 0.0907, 0.0172])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0339, 0.2258, 0.0405, 0.0812, 0.2791, 0.2639, 0.0447]) \n",
      "Test Loss tensor([0.0312, 0.2237, 0.0426, 0.0766, 0.2725, 0.2595, 0.0497])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0244, 0.0257, 0.0395, 0.0169, 0.0318, 0.0135, 0.1277, 0.0059]) \n",
      "Test Loss tensor([0.0291, 0.0224, 0.0354, 0.0205, 0.0253, 0.0175, 0.1212, 0.0072])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5996, 0.6878, 0.1048, 0.1836, 0.3786, 0.5281])\n",
      "Valid Idx 3 | Loss tensor([0.3486, 0.2176, 0.1166, 0.7379, 0.8002])\n",
      "Gradients: Input 1.5255874395370483 | Message 2.1622018814086914 | Update 2.3217074871063232 | Output 0.28429150581359863\n",
      "\n",
      "************** Batch 284 in 5.373620271682739 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0649, 0.2015, 0.0766, 0.1094]) \n",
      "Test Loss tensor([0.0632, 0.2002, 0.0779, 0.1036])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2080, 0.5227, 0.0381, 0.2060, 0.3108]) \n",
      "Test Loss tensor([0.2047, 0.5111, 0.0263, 0.2060, 0.3144])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0974, 0.0143, 0.2952, 0.2645, 0.2199, 0.0915, 0.0168]) \n",
      "Test Loss tensor([0.0930, 0.0150, 0.2816, 0.2577, 0.2192, 0.0954, 0.0147])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0269, 0.2095, 0.0425, 0.0847, 0.2836, 0.2494, 0.0576]) \n",
      "Test Loss tensor([0.0326, 0.2231, 0.0457, 0.0799, 0.2781, 0.2582, 0.0518])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0310, 0.0304, 0.0324, 0.0243, 0.0259, 0.0182, 0.1201, 0.0068]) \n",
      "Test Loss tensor([0.0287, 0.0222, 0.0379, 0.0209, 0.0271, 0.0142, 0.1228, 0.0056])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5968, 0.6727, 0.1030, 0.1911, 0.3765, 0.5502])\n",
      "Valid Idx 3 | Loss tensor([0.4227, 0.2834, 0.1187, 0.7674, 0.8368])\n",
      "\n",
      "************** Batch 288 in 5.298572301864624 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0603, 0.2030, 0.0843, 0.0985]) \n",
      "Test Loss tensor([0.0617, 0.1996, 0.0778, 0.1026])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2021, 0.5148, 0.0255, 0.2119, 0.3155]) \n",
      "Test Loss tensor([0.2043, 0.5151, 0.0258, 0.2066, 0.3133])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0921, 0.0157, 0.2751, 0.2669, 0.2169, 0.0953, 0.0146]) \n",
      "Test Loss tensor([0.0908, 0.0142, 0.2846, 0.2577, 0.2169, 0.0923, 0.0135])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0381, 0.2253, 0.0424, 0.0843, 0.2645, 0.2597, 0.0458]) \n",
      "Test Loss tensor([0.0325, 0.2230, 0.0445, 0.0796, 0.2736, 0.2611, 0.0484])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0289, 0.0275, 0.0389, 0.0191, 0.0283, 0.0148, 0.1245, 0.0052]) \n",
      "Test Loss tensor([0.0272, 0.0223, 0.0378, 0.0208, 0.0270, 0.0144, 0.1222, 0.0051])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5980, 0.6803, 0.1032, 0.1903, 0.3748, 0.5513])\n",
      "Valid Idx 3 | Loss tensor([0.4277, 0.2928, 0.1206, 0.7643, 0.8415])\n",
      "\n",
      "************** Batch 292 in 5.73065972328186 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0606, 0.2005, 0.0734, 0.1007]) \n",
      "Test Loss tensor([0.0640, 0.1976, 0.0783, 0.1085])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1950, 0.5165, 0.0257, 0.2040, 0.3166]) \n",
      "Test Loss tensor([0.2007, 0.5191, 0.0335, 0.2057, 0.3023])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0888, 0.0128, 0.2839, 0.2634, 0.2058, 0.0946, 0.0143]) \n",
      "Test Loss tensor([0.0974, 0.0133, 0.2826, 0.2543, 0.2234, 0.0904, 0.0156])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0308, 0.2264, 0.0424, 0.0761, 0.2825, 0.2533, 0.0473]) \n",
      "Test Loss tensor([0.0299, 0.2216, 0.0433, 0.0777, 0.2724, 0.2617, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0248, 0.0201, 0.0377, 0.0201, 0.0268, 0.0131, 0.1188, 0.0055]) \n",
      "Test Loss tensor([0.0298, 0.0222, 0.0376, 0.0200, 0.0238, 0.0176, 0.1215, 0.0060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5922, 0.7093, 0.1060, 0.1810, 0.3830, 0.5226])\n",
      "Valid Idx 3 | Loss tensor([0.3399, 0.2257, 0.1154, 0.7346, 0.8060])\n",
      "\n",
      "************** Batch 296 in 5.398399114608765 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0638, 0.1920, 0.0808, 0.1134]) \n",
      "Test Loss tensor([0.0622, 0.2017, 0.0801, 0.1028])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2001, 0.5179, 0.0326, 0.1920, 0.3064]) \n",
      "Test Loss tensor([0.2090, 0.5101, 0.0229, 0.2017, 0.3223])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1046, 0.0133, 0.2771, 0.2494, 0.2298, 0.0925, 0.0125]) \n",
      "Test Loss tensor([0.0882, 0.0159, 0.2851, 0.2664, 0.2132, 0.0933, 0.0134])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0297, 0.2339, 0.0454, 0.0803, 0.2709, 0.2676, 0.0463]) \n",
      "Test Loss tensor([0.0350, 0.2191, 0.0454, 0.0783, 0.2746, 0.2545, 0.0494])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0272, 0.0197, 0.0342, 0.0177, 0.0248, 0.0145, 0.1237, 0.0055]) \n",
      "Test Loss tensor([0.0283, 0.0225, 0.0385, 0.0215, 0.0278, 0.0123, 0.1236, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6055, 0.6613, 0.1056, 0.1982, 0.3697, 0.5647])\n",
      "Valid Idx 3 | Loss tensor([0.5000, 0.3591, 0.1241, 0.7919, 0.8631])\n",
      "\n",
      "************** Batch 300 in 5.290464162826538 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0570, 0.1908, 0.0811, 0.1061]) \n",
      "Test Loss tensor([0.0615, 0.2018, 0.0779, 0.1082])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2092, 0.5071, 0.0195, 0.2076, 0.3275]) \n",
      "Test Loss tensor([0.2026, 0.5144, 0.0331, 0.1991, 0.3026])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0828, 0.0182, 0.2680, 0.2509, 0.2184, 0.0956, 0.0124]) \n",
      "Test Loss tensor([0.0909, 0.0136, 0.2823, 0.2525, 0.2205, 0.0882, 0.0170])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0318, 0.2309, 0.0404, 0.0824, 0.2670, 0.2555, 0.0481]) \n",
      "Test Loss tensor([0.0302, 0.2263, 0.0432, 0.0761, 0.2781, 0.2565, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0239, 0.0237, 0.0428, 0.0199, 0.0301, 0.0114, 0.1200, 0.0043]) \n",
      "Test Loss tensor([0.0289, 0.0229, 0.0367, 0.0207, 0.0248, 0.0155, 0.1233, 0.0060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6031, 0.6886, 0.1089, 0.1844, 0.3790, 0.5311])\n",
      "Valid Idx 3 | Loss tensor([0.3875, 0.2479, 0.1184, 0.7543, 0.8106])\n",
      "Gradients: Input 1.709852933883667 | Message 2.409207344055176 | Update 2.59898042678833 | Output 0.31605976819992065\n",
      "\n",
      "************** Batch 304 in 5.376704931259155 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0651, 0.2013, 0.0761, 0.1070]) \n",
      "Test Loss tensor([0.0607, 0.1998, 0.0786, 0.1082])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2002, 0.5171, 0.0347, 0.1990, 0.3008]) \n",
      "Test Loss tensor([0.2022, 0.5139, 0.0345, 0.2037, 0.2995])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0835, 0.0148, 0.2807, 0.2497, 0.2209, 0.0893, 0.0161]) \n",
      "Test Loss tensor([0.0908, 0.0141, 0.2814, 0.2530, 0.2220, 0.0886, 0.0155])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0329, 0.2344, 0.0473, 0.0805, 0.2668, 0.2515, 0.0526]) \n",
      "Test Loss tensor([0.0317, 0.2256, 0.0420, 0.0784, 0.2738, 0.2585, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0310, 0.0238, 0.0443, 0.0199, 0.0245, 0.0170, 0.1206, 0.0060]) \n",
      "Test Loss tensor([0.0290, 0.0233, 0.0369, 0.0198, 0.0243, 0.0159, 0.1229, 0.0063])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6010, 0.6846, 0.1060, 0.1843, 0.3758, 0.5259])\n",
      "Valid Idx 3 | Loss tensor([0.3844, 0.2442, 0.1146, 0.7537, 0.8099])\n",
      "\n",
      "************** Batch 308 in 5.357006549835205 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0606, 0.2000, 0.0768, 0.1139]) \n",
      "Test Loss tensor([0.0625, 0.1987, 0.0788, 0.1021])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2111, 0.5050, 0.0309, 0.2008, 0.3118]) \n",
      "Test Loss tensor([0.2116, 0.5076, 0.0239, 0.2032, 0.3113])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0879, 0.0177, 0.2835, 0.2500, 0.2264, 0.0890, 0.0145]) \n",
      "Test Loss tensor([0.0885, 0.0157, 0.2872, 0.2627, 0.2153, 0.0925, 0.0129])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0264, 0.2271, 0.0430, 0.0720, 0.2803, 0.2569, 0.0449]) \n",
      "Test Loss tensor([0.0341, 0.2237, 0.0437, 0.0793, 0.2770, 0.2575, 0.0495])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0271, 0.0275, 0.0370, 0.0215, 0.0252, 0.0153, 0.1253, 0.0067]) \n",
      "Test Loss tensor([0.0267, 0.0236, 0.0385, 0.0208, 0.0273, 0.0126, 0.1260, 0.0050])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6102, 0.6560, 0.1052, 0.1961, 0.3679, 0.5585])\n",
      "Valid Idx 3 | Loss tensor([0.4826, 0.3333, 0.1247, 0.7911, 0.8523])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 312 in 5.415785789489746 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0642, 0.1988, 0.0811, 0.0976]) \n",
      "Test Loss tensor([0.0628, 0.2008, 0.0791, 0.1052])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1999, 0.5227, 0.0263, 0.2107, 0.3306]) \n",
      "Test Loss tensor([0.2038, 0.5120, 0.0297, 0.2049, 0.3030])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0813, 0.0161, 0.2716, 0.2647, 0.2142, 0.0893, 0.0120]) \n",
      "Test Loss tensor([0.0913, 0.0139, 0.2843, 0.2568, 0.2205, 0.0908, 0.0149])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0377, 0.2289, 0.0437, 0.0881, 0.2569, 0.2558, 0.0490]) \n",
      "Test Loss tensor([0.0312, 0.2258, 0.0434, 0.0777, 0.2742, 0.2554, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0259, 0.0197, 0.0425, 0.0217, 0.0270, 0.0127, 0.1268, 0.0051]) \n",
      "Test Loss tensor([0.0302, 0.0219, 0.0368, 0.0198, 0.0242, 0.0158, 0.1228, 0.0057])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6038, 0.6744, 0.1063, 0.1853, 0.3768, 0.5370])\n",
      "Valid Idx 3 | Loss tensor([0.4017, 0.2646, 0.1182, 0.7676, 0.8256])\n",
      "\n",
      "************** Batch 316 in 5.346689462661743 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0543, 0.2010, 0.0768, 0.0984]) \n",
      "Test Loss tensor([0.0622, 0.1978, 0.0785, 0.1067])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2077, 0.5232, 0.0294, 0.1978, 0.3086]) \n",
      "Test Loss tensor([0.2032, 0.5085, 0.0317, 0.2032, 0.3021])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0918, 0.0124, 0.2874, 0.2585, 0.2164, 0.0907, 0.0161]) \n",
      "Test Loss tensor([0.0915, 0.0135, 0.2817, 0.2552, 0.2194, 0.0900, 0.0153])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0281, 0.2354, 0.0455, 0.0762, 0.2786, 0.2561, 0.0521]) \n",
      "Test Loss tensor([0.0309, 0.2259, 0.0436, 0.0783, 0.2734, 0.2592, 0.0501])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0290, 0.0228, 0.0365, 0.0195, 0.0233, 0.0153, 0.1202, 0.0058]) \n",
      "Test Loss tensor([0.0292, 0.0230, 0.0370, 0.0196, 0.0240, 0.0153, 0.1230, 0.0060])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6035, 0.6836, 0.1041, 0.1866, 0.3775, 0.5286])\n",
      "Valid Idx 3 | Loss tensor([0.3779, 0.2477, 0.1161, 0.7575, 0.8149])\n",
      "\n",
      "************** Batch 320 in 5.319231271743774 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0644, 0.2035, 0.0803, 0.1096]) \n",
      "Test Loss tensor([0.0619, 0.2019, 0.0794, 0.1028])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2072, 0.5128, 0.0329, 0.2024, 0.2961]) \n",
      "Test Loss tensor([0.2086, 0.5084, 0.0249, 0.2029, 0.3092])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0798, 0.0124, 0.2805, 0.2492, 0.2233, 0.0892, 0.0173]) \n",
      "Test Loss tensor([0.0870, 0.0156, 0.2818, 0.2610, 0.2130, 0.0952, 0.0125])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0289, 0.2340, 0.0376, 0.0763, 0.2735, 0.2468, 0.0482]) \n",
      "Test Loss tensor([0.0342, 0.2248, 0.0445, 0.0784, 0.2732, 0.2569, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0201, 0.0395, 0.0140, 0.0228, 0.0156, 0.1204, 0.0058]) \n",
      "Test Loss tensor([0.0266, 0.0227, 0.0385, 0.0208, 0.0271, 0.0125, 0.1258, 0.0046])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6040, 0.6583, 0.1030, 0.1981, 0.3655, 0.5634])\n",
      "Valid Idx 3 | Loss tensor([0.4648, 0.3330, 0.1212, 0.7891, 0.8552])\n",
      "Gradients: Input 0.7873520851135254 | Message 1.1345281600952148 | Update 1.1900708675384521 | Output 0.13254205882549286\n",
      "\n",
      "************** Batch 324 in 5.371855974197388 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0609, 0.1968, 0.0835, 0.1067]) \n",
      "Test Loss tensor([0.0607, 0.1972, 0.0778, 0.1076])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2131, 0.5182, 0.0213, 0.2077, 0.3114]) \n",
      "Test Loss tensor([0.1983, 0.5101, 0.0334, 0.2013, 0.2954])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0885, 0.0135, 0.2821, 0.2499, 0.2084, 0.0972, 0.0103]) \n",
      "Test Loss tensor([0.0953, 0.0124, 0.2824, 0.2537, 0.2221, 0.0888, 0.0156])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0337, 0.2214, 0.0433, 0.0814, 0.2721, 0.2583, 0.0501]) \n",
      "Test Loss tensor([0.0294, 0.2278, 0.0447, 0.0772, 0.2704, 0.2583, 0.0493])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0317, 0.0235, 0.0399, 0.0192, 0.0276, 0.0122, 0.1276, 0.0050]) \n",
      "Test Loss tensor([0.0306, 0.0228, 0.0367, 0.0201, 0.0235, 0.0165, 0.1224, 0.0061])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6020, 0.6888, 0.1072, 0.1850, 0.3781, 0.5291])\n",
      "Valid Idx 3 | Loss tensor([0.3436, 0.2311, 0.1156, 0.7465, 0.8055])\n",
      "\n",
      "************** Batch 328 in 5.445787668228149 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0696, 0.1881, 0.0778, 0.1101]) \n",
      "Test Loss tensor([0.0617, 0.2028, 0.0793, 0.1023])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2049, 0.4870, 0.0299, 0.1991, 0.3056]) \n",
      "Test Loss tensor([0.1999, 0.5089, 0.0290, 0.2029, 0.3003])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0908, 0.0124, 0.2888, 0.2550, 0.2160, 0.0891, 0.0184]) \n",
      "Test Loss tensor([0.0924, 0.0141, 0.2821, 0.2557, 0.2190, 0.0913, 0.0150])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0296, 0.2314, 0.0418, 0.0839, 0.2807, 0.2539, 0.0512]) \n",
      "Test Loss tensor([0.0316, 0.2270, 0.0440, 0.0774, 0.2707, 0.2564, 0.0494])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0300, 0.0234, 0.0402, 0.0195, 0.0243, 0.0144, 0.1263, 0.0060]) \n",
      "Test Loss tensor([0.0286, 0.0215, 0.0357, 0.0199, 0.0256, 0.0142, 0.1206, 0.0052])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6046, 0.6853, 0.1020, 0.1898, 0.3745, 0.5494])\n",
      "Valid Idx 3 | Loss tensor([0.3821, 0.2721, 0.1198, 0.7606, 0.8281])\n",
      "\n",
      "************** Batch 332 in 5.528534173965454 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0615, 0.1924, 0.0777, 0.1044]) \n",
      "Test Loss tensor([0.0620, 0.2006, 0.0810, 0.1036])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1991, 0.5081, 0.0289, 0.1904, 0.3139]) \n",
      "Test Loss tensor([0.2035, 0.5045, 0.0244, 0.2010, 0.3108])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0970, 0.0137, 0.2868, 0.2605, 0.2139, 0.0956, 0.0155]) \n",
      "Test Loss tensor([0.0907, 0.0149, 0.2842, 0.2607, 0.2137, 0.0936, 0.0127])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0318, 0.2319, 0.0471, 0.0855, 0.2671, 0.2805, 0.0477]) \n",
      "Test Loss tensor([0.0318, 0.2199, 0.0459, 0.0776, 0.2716, 0.2580, 0.0502])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0319, 0.0214, 0.0327, 0.0187, 0.0234, 0.0150, 0.1222, 0.0056]) \n",
      "Test Loss tensor([0.0276, 0.0218, 0.0382, 0.0212, 0.0272, 0.0119, 0.1225, 0.0045])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6072, 0.6721, 0.1023, 0.1978, 0.3712, 0.5732])\n",
      "Valid Idx 3 | Loss tensor([0.4479, 0.3389, 0.1196, 0.7833, 0.8525])\n",
      "\n",
      "************** Batch 336 in 5.386429071426392 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0670, 0.2066, 0.0825, 0.1073]) \n",
      "Test Loss tensor([0.0645, 0.2002, 0.0787, 0.1110])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2086, 0.4997, 0.0243, 0.2018, 0.3053]) \n",
      "Test Loss tensor([0.1979, 0.5097, 0.0373, 0.2049, 0.2943])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0923, 0.0149, 0.2947, 0.2586, 0.2175, 0.0934, 0.0130]) \n",
      "Test Loss tensor([0.0974, 0.0119, 0.2809, 0.2527, 0.2267, 0.0887, 0.0176])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0310, 0.2128, 0.0434, 0.0827, 0.2785, 0.2604, 0.0450]) \n",
      "Test Loss tensor([0.0291, 0.2256, 0.0433, 0.0767, 0.2733, 0.2582, 0.0509])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0251, 0.0215, 0.0432, 0.0170, 0.0267, 0.0118, 0.1192, 0.0048]) \n",
      "Test Loss tensor([0.0326, 0.0225, 0.0367, 0.0198, 0.0230, 0.0172, 0.1217, 0.0065])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5999, 0.7077, 0.1070, 0.1886, 0.3830, 0.5250])\n",
      "Valid Idx 3 | Loss tensor([0.3055, 0.2078, 0.1137, 0.7293, 0.7950])\n",
      "\n",
      "************** Batch 340 in 5.222483158111572 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0613, 0.1845, 0.0736, 0.1139]) \n",
      "Test Loss tensor([0.0615, 0.2016, 0.0795, 0.1017])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1940, 0.5091, 0.0414, 0.2098, 0.2965]) \n",
      "Test Loss tensor([0.2072, 0.5026, 0.0237, 0.2041, 0.3123])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0955, 0.0126, 0.2770, 0.2533, 0.2141, 0.0923, 0.0181]) \n",
      "Test Loss tensor([0.0890, 0.0152, 0.2860, 0.2621, 0.2134, 0.0925, 0.0124])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0273, 0.2227, 0.0487, 0.0888, 0.2643, 0.2653, 0.0630]) \n",
      "Test Loss tensor([0.0333, 0.2236, 0.0436, 0.0812, 0.2728, 0.2545, 0.0494])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0298, 0.0241, 0.0423, 0.0185, 0.0223, 0.0182, 0.1249, 0.0073]) \n",
      "Test Loss tensor([0.0281, 0.0225, 0.0392, 0.0206, 0.0270, 0.0117, 0.1221, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6059, 0.6638, 0.1040, 0.2033, 0.3647, 0.5759])\n",
      "Valid Idx 3 | Loss tensor([0.4627, 0.3530, 0.1187, 0.7851, 0.8539])\n",
      "Gradients: Input 1.9870274066925049 | Message 2.825533390045166 | Update 3.001854181289673 | Output 0.32712239027023315\n",
      "\n",
      "************** Batch 344 in 5.813412427902222 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0594, 0.1989, 0.0741, 0.0943]) \n",
      "Test Loss tensor([0.0617, 0.2023, 0.0769, 0.1033])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2024, 0.4941, 0.0206, 0.2029, 0.3093]) \n",
      "Test Loss tensor([0.2009, 0.5044, 0.0310, 0.2009, 0.2991])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0864, 0.0161, 0.2869, 0.2561, 0.2166, 0.0929, 0.0116]) \n",
      "Test Loss tensor([0.0920, 0.0122, 0.2832, 0.2526, 0.2206, 0.0880, 0.0150])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0357, 0.2163, 0.0500, 0.0887, 0.2786, 0.2642, 0.0497]) \n",
      "Test Loss tensor([0.0318, 0.2257, 0.0449, 0.0772, 0.2770, 0.2547, 0.0511])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0265, 0.0250, 0.0354, 0.0205, 0.0292, 0.0125, 0.1266, 0.0052]) \n",
      "Test Loss tensor([0.0303, 0.0233, 0.0374, 0.0195, 0.0241, 0.0150, 0.1222, 0.0058])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6010, 0.6864, 0.1042, 0.1890, 0.3751, 0.5419])\n",
      "Valid Idx 3 | Loss tensor([0.3496, 0.2479, 0.1193, 0.7406, 0.8261])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 348 in 5.282353162765503 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0651, 0.1866, 0.0757, 0.1062]) \n",
      "Test Loss tensor([0.0616, 0.1992, 0.0762, 0.1023])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1949, 0.5043, 0.0329, 0.2009, 0.2891]) \n",
      "Test Loss tensor([0.2018, 0.5007, 0.0284, 0.2054, 0.3004])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0920, 0.0138, 0.2898, 0.2581, 0.2148, 0.0922, 0.0141]) \n",
      "Test Loss tensor([0.0903, 0.0132, 0.2845, 0.2556, 0.2171, 0.0900, 0.0139])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0304, 0.2244, 0.0437, 0.0772, 0.2719, 0.2623, 0.0457]) \n",
      "Test Loss tensor([0.0299, 0.2230, 0.0442, 0.0771, 0.2757, 0.2558, 0.0493])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0302, 0.0209, 0.0353, 0.0176, 0.0241, 0.0150, 0.1181, 0.0059]) \n",
      "Test Loss tensor([0.0305, 0.0228, 0.0380, 0.0203, 0.0242, 0.0138, 0.1215, 0.0055])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6012, 0.6813, 0.1055, 0.1918, 0.3713, 0.5475])\n",
      "Valid Idx 3 | Loss tensor([0.3767, 0.2737, 0.1167, 0.7508, 0.8338])\n",
      "\n",
      "************** Batch 352 in 5.1208343505859375 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0604, 0.2042, 0.0713, 0.1081]) \n",
      "Test Loss tensor([0.0617, 0.2025, 0.0769, 0.1036])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1971, 0.5052, 0.0301, 0.2098, 0.2829]) \n",
      "Test Loss tensor([0.2013, 0.4970, 0.0256, 0.2045, 0.3034])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0917, 0.0180, 0.2890, 0.2530, 0.2247, 0.0976, 0.0141]) \n",
      "Test Loss tensor([0.0882, 0.0136, 0.2836, 0.2605, 0.2190, 0.0922, 0.0138])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0275, 0.2278, 0.0432, 0.0873, 0.2783, 0.2612, 0.0523]) \n",
      "Test Loss tensor([0.0326, 0.2220, 0.0430, 0.0772, 0.2742, 0.2574, 0.0477])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0289, 0.0214, 0.0374, 0.0180, 0.0234, 0.0139, 0.1237, 0.0056]) \n",
      "Test Loss tensor([0.0299, 0.0228, 0.0383, 0.0204, 0.0251, 0.0127, 0.1237, 0.0048])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6033, 0.6648, 0.1045, 0.1941, 0.3694, 0.5513])\n",
      "Valid Idx 3 | Loss tensor([0.4082, 0.3060, 0.1181, 0.7619, 0.8463])\n",
      "\n",
      "************** Batch 356 in 5.464446067810059 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0626, 0.1948, 0.0791, 0.1057]) \n",
      "Test Loss tensor([0.0636, 0.2018, 0.0759, 0.1053])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2116, 0.5021, 0.0241, 0.2012, 0.2976]) \n",
      "Test Loss tensor([0.1992, 0.4990, 0.0317, 0.2039, 0.2916])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0902, 0.0150, 0.2714, 0.2562, 0.2096, 0.0951, 0.0158]) \n",
      "Test Loss tensor([0.0926, 0.0126, 0.2808, 0.2523, 0.2232, 0.0911, 0.0145])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0285, 0.2288, 0.0460, 0.0832, 0.2737, 0.2548, 0.0590]) \n",
      "Test Loss tensor([0.0294, 0.2280, 0.0436, 0.0754, 0.2719, 0.2550, 0.0495])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0307, 0.0214, 0.0353, 0.0196, 0.0241, 0.0113, 0.1270, 0.0046]) \n",
      "Test Loss tensor([0.0329, 0.0220, 0.0366, 0.0207, 0.0235, 0.0161, 0.1238, 0.0058])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5959, 0.6860, 0.1090, 0.1880, 0.3715, 0.5314])\n",
      "Valid Idx 3 | Loss tensor([0.3447, 0.2480, 0.1143, 0.7355, 0.8187])\n",
      "\n",
      "************** Batch 360 in 5.214911222457886 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0641, 0.2100, 0.0734, 0.1100]) \n",
      "Test Loss tensor([0.0613, 0.2018, 0.0756, 0.1030])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1938, 0.4733, 0.0345, 0.1990, 0.3089]) \n",
      "Test Loss tensor([0.2025, 0.4933, 0.0244, 0.2005, 0.3038])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0906, 0.0123, 0.2740, 0.2531, 0.2318, 0.0891, 0.0150]) \n",
      "Test Loss tensor([0.0888, 0.0147, 0.2885, 0.2624, 0.2198, 0.0936, 0.0129])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0318, 0.2207, 0.0415, 0.0764, 0.2711, 0.2653, 0.0451]) \n",
      "Test Loss tensor([0.0321, 0.2227, 0.0423, 0.0766, 0.2761, 0.2519, 0.0474])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0316, 0.0233, 0.0299, 0.0195, 0.0228, 0.0150, 0.1140, 0.0066]) \n",
      "Test Loss tensor([0.0312, 0.0221, 0.0391, 0.0208, 0.0260, 0.0130, 0.1245, 0.0047])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6087, 0.6640, 0.1064, 0.1936, 0.3659, 0.5589])\n",
      "Valid Idx 3 | Loss tensor([0.4296, 0.3359, 0.1187, 0.7700, 0.8512])\n",
      "Gradients: Input 1.0827093124389648 | Message 1.4954869747161865 | Update 1.5973238945007324 | Output 0.1601468026638031\n",
      "\n",
      "************** Batch 364 in 5.275524854660034 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0632, 0.2114, 0.0823, 0.1084]) \n",
      "Test Loss tensor([0.0607, 0.2018, 0.0746, 0.1060])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2031, 0.4918, 0.0304, 0.1968, 0.3038]) \n",
      "Test Loss tensor([0.2002, 0.4931, 0.0322, 0.2023, 0.2936])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0803, 0.0149, 0.2870, 0.2511, 0.2233, 0.0949, 0.0140]) \n",
      "Test Loss tensor([0.0921, 0.0134, 0.2828, 0.2527, 0.2204, 0.0884, 0.0145])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0332, 0.2204, 0.0427, 0.0775, 0.2737, 0.2503, 0.0463]) \n",
      "Test Loss tensor([0.0301, 0.2272, 0.0431, 0.0771, 0.2727, 0.2555, 0.0493])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0290, 0.0216, 0.0379, 0.0170, 0.0284, 0.0116, 0.1110, 0.0047]) \n",
      "Test Loss tensor([0.0318, 0.0220, 0.0365, 0.0195, 0.0234, 0.0154, 0.1211, 0.0058])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5953, 0.6922, 0.1109, 0.1842, 0.3723, 0.5366])\n",
      "Valid Idx 3 | Loss tensor([0.3553, 0.2642, 0.1153, 0.7387, 0.8249])\n",
      "\n",
      "************** Batch 368 in 5.1995954513549805 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0572, 0.1984, 0.0781, 0.0982]) \n",
      "Test Loss tensor([0.0631, 0.2024, 0.0758, 0.1038])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2083, 0.4972, 0.0273, 0.2058, 0.3009]) \n",
      "Test Loss tensor([0.2007, 0.4912, 0.0280, 0.2018, 0.2985])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0936, 0.0135, 0.2957, 0.2489, 0.2163, 0.0948, 0.0144]) \n",
      "Test Loss tensor([0.0900, 0.0137, 0.2828, 0.2560, 0.2187, 0.0905, 0.0142])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0312, 0.2395, 0.0424, 0.0722, 0.2711, 0.2511, 0.0490]) \n",
      "Test Loss tensor([0.0305, 0.2235, 0.0442, 0.0780, 0.2718, 0.2563, 0.0476])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0334, 0.0238, 0.0383, 0.0185, 0.0265, 0.0148, 0.1267, 0.0052]) \n",
      "Test Loss tensor([0.0304, 0.0234, 0.0384, 0.0206, 0.0257, 0.0143, 0.1230, 0.0052])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5979, 0.6780, 0.1042, 0.1876, 0.3729, 0.5459])\n",
      "Valid Idx 3 | Loss tensor([0.4005, 0.3047, 0.1184, 0.7554, 0.8393])\n",
      "\n",
      "************** Batch 372 in 5.200483083724976 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0633, 0.2030, 0.0748, 0.1025]) \n",
      "Test Loss tensor([0.0605, 0.1982, 0.0748, 0.1047])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2068, 0.4804, 0.0288, 0.2003, 0.3036]) \n",
      "Test Loss tensor([0.2008, 0.4887, 0.0287, 0.2049, 0.2970])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0936, 0.0148, 0.2846, 0.2573, 0.2083, 0.0872, 0.0156]) \n",
      "Test Loss tensor([0.0879, 0.0136, 0.2807, 0.2550, 0.2187, 0.0905, 0.0136])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0309, 0.2290, 0.0422, 0.0814, 0.2698, 0.2613, 0.0530]) \n",
      "Test Loss tensor([0.0316, 0.2261, 0.0437, 0.0774, 0.2740, 0.2561, 0.0484])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0341, 0.0224, 0.0408, 0.0216, 0.0232, 0.0126, 0.1272, 0.0046]) \n",
      "Test Loss tensor([0.0313, 0.0230, 0.0389, 0.0214, 0.0248, 0.0132, 0.1249, 0.0052])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5974, 0.6695, 0.1060, 0.1929, 0.3667, 0.5471])\n",
      "Valid Idx 3 | Loss tensor([0.4101, 0.3027, 0.1176, 0.7607, 0.8390])\n",
      "\n",
      "************** Batch 376 in 5.185123682022095 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0607, 0.1902, 0.0741, 0.1086]) \n",
      "Test Loss tensor([0.0597, 0.1998, 0.0757, 0.1050])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2120, 0.4832, 0.0280, 0.2029, 0.3035]) \n",
      "Test Loss tensor([0.1990, 0.4905, 0.0286, 0.2035, 0.2951])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0869, 0.0140, 0.2852, 0.2600, 0.2158, 0.0905, 0.0120]) \n",
      "Test Loss tensor([0.0911, 0.0143, 0.2850, 0.2536, 0.2193, 0.0927, 0.0144])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0296, 0.2264, 0.0431, 0.0728, 0.2680, 0.2549, 0.0427]) \n",
      "Test Loss tensor([0.0317, 0.2210, 0.0438, 0.0767, 0.2736, 0.2547, 0.0495])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0298, 0.0239, 0.0434, 0.0199, 0.0263, 0.0138, 0.1269, 0.0060]) \n",
      "Test Loss tensor([0.0302, 0.0233, 0.0383, 0.0190, 0.0258, 0.0134, 0.1243, 0.0052])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5977, 0.6651, 0.1060, 0.1937, 0.3684, 0.5485])\n",
      "Valid Idx 3 | Loss tensor([0.4006, 0.2939, 0.1180, 0.7562, 0.8351])\n",
      "\n",
      "************** Batch 380 in 5.168496131896973 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0582, 0.1963, 0.0743, 0.0976]) \n",
      "Test Loss tensor([0.0610, 0.2008, 0.0759, 0.1053])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1993, 0.4801, 0.0289, 0.1986, 0.2964]) \n",
      "Test Loss tensor([0.1962, 0.4879, 0.0292, 0.2060, 0.2943])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0924, 0.0148, 0.2750, 0.2583, 0.2221, 0.0965, 0.0162]) \n",
      "Test Loss tensor([0.0891, 0.0136, 0.2834, 0.2524, 0.2174, 0.0898, 0.0142])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0356, 0.2207, 0.0447, 0.0694, 0.2613, 0.2615, 0.0415]) \n",
      "Test Loss tensor([0.0319, 0.2212, 0.0434, 0.0763, 0.2726, 0.2574, 0.0475])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0209, 0.0370, 0.0202, 0.0252, 0.0136, 0.1258, 0.0049]) \n",
      "Test Loss tensor([0.0290, 0.0232, 0.0371, 0.0194, 0.0249, 0.0136, 0.1223, 0.0051])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6002, 0.6724, 0.1014, 0.1919, 0.3689, 0.5549])\n",
      "Valid Idx 3 | Loss tensor([0.3898, 0.2879, 0.1175, 0.7549, 0.8348])\n",
      "Gradients: Input 0.08879103511571884 | Message 0.06677142530679703 | Update 0.040895894169807434 | Output 0.036857202649116516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 384 in 5.304347038269043 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0602, 0.1951, 0.0747, 0.1039]) \n",
      "Test Loss tensor([0.0601, 0.2014, 0.0767, 0.1017])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2121, 0.4786, 0.0282, 0.2029, 0.3035]) \n",
      "Test Loss tensor([0.1987, 0.4915, 0.0264, 0.2010, 0.2973])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0879, 0.0139, 0.2870, 0.2567, 0.2208, 0.0850, 0.0140]) \n",
      "Test Loss tensor([0.0890, 0.0132, 0.2857, 0.2582, 0.2166, 0.0920, 0.0130])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0342, 0.2255, 0.0416, 0.0770, 0.2670, 0.2486, 0.0437]) \n",
      "Test Loss tensor([0.0307, 0.2269, 0.0461, 0.0800, 0.2721, 0.2554, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0290, 0.0220, 0.0377, 0.0205, 0.0230, 0.0135, 0.1226, 0.0047]) \n",
      "Test Loss tensor([0.0288, 0.0225, 0.0384, 0.0198, 0.0255, 0.0129, 0.1222, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5998, 0.6624, 0.1043, 0.1986, 0.3690, 0.5579])\n",
      "Valid Idx 3 | Loss tensor([0.4023, 0.3058, 0.1185, 0.7629, 0.8458])\n",
      "\n",
      "************** Batch 388 in 5.310874700546265 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0598, 0.2078, 0.0728, 0.1073]) \n",
      "Test Loss tensor([0.0614, 0.2011, 0.0772, 0.1066])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2103, 0.4691, 0.0268, 0.2006, 0.2989]) \n",
      "Test Loss tensor([0.2000, 0.4904, 0.0284, 0.2049, 0.2966])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0969, 0.0146, 0.2831, 0.2429, 0.2175, 0.0948, 0.0116]) \n",
      "Test Loss tensor([0.0911, 0.0132, 0.2803, 0.2542, 0.2190, 0.0897, 0.0136])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0322, 0.2125, 0.0395, 0.0836, 0.2738, 0.2511, 0.0446]) \n",
      "Test Loss tensor([0.0310, 0.2242, 0.0444, 0.0766, 0.2722, 0.2538, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0312, 0.0228, 0.0418, 0.0189, 0.0250, 0.0127, 0.1236, 0.0050]) \n",
      "Test Loss tensor([0.0289, 0.0231, 0.0359, 0.0193, 0.0247, 0.0133, 0.1200, 0.0046])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6050, 0.6750, 0.1028, 0.1935, 0.3692, 0.5613])\n",
      "Valid Idx 3 | Loss tensor([0.3670, 0.2791, 0.1146, 0.7572, 0.8375])\n",
      "\n",
      "************** Batch 392 in 5.241704225540161 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0608, 0.1903, 0.0739, 0.1074]) \n",
      "Test Loss tensor([0.0611, 0.2012, 0.0785, 0.1036])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1949, 0.4745, 0.0280, 0.1995, 0.3006]) \n",
      "Test Loss tensor([0.1975, 0.4860, 0.0271, 0.1997, 0.2976])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0864, 0.0118, 0.2719, 0.2438, 0.2078, 0.0833, 0.0125]) \n",
      "Test Loss tensor([0.0920, 0.0131, 0.2838, 0.2539, 0.2207, 0.0900, 0.0131])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0295, 0.2197, 0.0447, 0.0754, 0.2621, 0.2598, 0.0503]) \n",
      "Test Loss tensor([0.0306, 0.2234, 0.0447, 0.0784, 0.2738, 0.2545, 0.0521])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0345, 0.0258, 0.0388, 0.0202, 0.0243, 0.0137, 0.1142, 0.0056]) \n",
      "Test Loss tensor([0.0284, 0.0233, 0.0381, 0.0199, 0.0250, 0.0126, 0.1234, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6037, 0.6773, 0.1016, 0.1950, 0.3724, 0.5713])\n",
      "Valid Idx 3 | Loss tensor([0.3765, 0.2916, 0.1163, 0.7562, 0.8441])\n",
      "\n",
      "************** Batch 396 in 5.246805906295776 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0593, 0.2037, 0.0723, 0.0942]) \n",
      "Test Loss tensor([0.0587, 0.1984, 0.0795, 0.1029])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2061, 0.4945, 0.0268, 0.2047, 0.2941]) \n",
      "Test Loss tensor([0.1970, 0.4827, 0.0265, 0.2009, 0.2985])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0858, 0.0130, 0.2864, 0.2569, 0.2108, 0.0878, 0.0121]) \n",
      "Test Loss tensor([0.0917, 0.0133, 0.2863, 0.2530, 0.2193, 0.0912, 0.0123])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0265, 0.2282, 0.0461, 0.0836, 0.2691, 0.2695, 0.0524]) \n",
      "Test Loss tensor([0.0321, 0.2223, 0.0444, 0.0764, 0.2727, 0.2558, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0242, 0.0215, 0.0325, 0.0201, 0.0255, 0.0123, 0.1218, 0.0042]) \n",
      "Test Loss tensor([0.0281, 0.0218, 0.0379, 0.0201, 0.0263, 0.0130, 0.1214, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6081, 0.6715, 0.1051, 0.2010, 0.3671, 0.5739])\n",
      "Valid Idx 3 | Loss tensor([0.3826, 0.2972, 0.1173, 0.7622, 0.8442])\n",
      "\n",
      "************** Batch 400 in 5.17678689956665 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0598, 0.1934, 0.0762, 0.1007]) \n",
      "Test Loss tensor([0.0605, 0.1986, 0.0793, 0.1033])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2034, 0.4844, 0.0262, 0.2026, 0.2948]) \n",
      "Test Loss tensor([0.1974, 0.4853, 0.0281, 0.1970, 0.2943])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0917, 0.0132, 0.2825, 0.2506, 0.2230, 0.0867, 0.0164]) \n",
      "Test Loss tensor([0.0924, 0.0136, 0.2846, 0.2526, 0.2176, 0.0899, 0.0129])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0298, 0.2152, 0.0454, 0.0774, 0.2699, 0.2535, 0.0457]) \n",
      "Test Loss tensor([0.0313, 0.2223, 0.0448, 0.0779, 0.2718, 0.2546, 0.0492])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0314, 0.0219, 0.0386, 0.0222, 0.0251, 0.0156, 0.1216, 0.0038]) \n",
      "Test Loss tensor([0.0285, 0.0239, 0.0374, 0.0195, 0.0254, 0.0139, 0.1221, 0.0047])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6071, 0.6755, 0.1023, 0.1986, 0.3710, 0.5787])\n",
      "Valid Idx 3 | Loss tensor([0.3796, 0.2903, 0.1179, 0.7589, 0.8399])\n",
      "Gradients: Input 0.19214580953121185 | Message 0.2796924114227295 | Update 0.2726132273674011 | Output 0.05066612362861633\n",
      "\n",
      "************** Batch 404 in 5.2204413414001465 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0618, 0.1910, 0.0746, 0.1056]) \n",
      "Test Loss tensor([0.0601, 0.1995, 0.0781, 0.1034])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1998, 0.4738, 0.0275, 0.2050, 0.2912]) \n",
      "Test Loss tensor([0.1912, 0.4807, 0.0297, 0.1985, 0.2894])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0945, 0.0149, 0.2721, 0.2558, 0.2114, 0.0861, 0.0120]) \n",
      "Test Loss tensor([0.0953, 0.0133, 0.2782, 0.2499, 0.2202, 0.0883, 0.0131])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0305, 0.2282, 0.0420, 0.0816, 0.2695, 0.2497, 0.0470]) \n",
      "Test Loss tensor([0.0323, 0.2249, 0.0440, 0.0765, 0.2725, 0.2544, 0.0473])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0299, 0.0262, 0.0343, 0.0203, 0.0248, 0.0117, 0.1217, 0.0051]) \n",
      "Test Loss tensor([0.0297, 0.0228, 0.0377, 0.0198, 0.0256, 0.0138, 0.1206, 0.0051])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6024, 0.6771, 0.1027, 0.2001, 0.3749, 0.5694])\n",
      "Valid Idx 3 | Loss tensor([0.3768, 0.2835, 0.1127, 0.7581, 0.8316])\n",
      "\n",
      "************** Batch 408 in 5.317689895629883 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0597, 0.1927, 0.0750, 0.1113]) \n",
      "Test Loss tensor([0.0610, 0.2018, 0.0807, 0.1032])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1901, 0.4885, 0.0264, 0.2026, 0.2874]) \n",
      "Test Loss tensor([0.1975, 0.4747, 0.0250, 0.2016, 0.2986])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0924, 0.0137, 0.2833, 0.2405, 0.2261, 0.0908, 0.0126]) \n",
      "Test Loss tensor([0.0868, 0.0152, 0.2826, 0.2588, 0.2158, 0.0928, 0.0123])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0299, 0.2224, 0.0423, 0.0794, 0.2653, 0.2572, 0.0453]) \n",
      "Test Loss tensor([0.0344, 0.2218, 0.0458, 0.0804, 0.2731, 0.2526, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0316, 0.0230, 0.0443, 0.0190, 0.0254, 0.0133, 0.1236, 0.0050]) \n",
      "Test Loss tensor([0.0275, 0.0231, 0.0374, 0.0204, 0.0271, 0.0119, 0.1228, 0.0045])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6023, 0.6629, 0.1007, 0.2036, 0.3659, 0.5948])\n",
      "Valid Idx 3 | Loss tensor([0.4611, 0.3660, 0.1182, 0.7789, 0.8574])\n",
      "\n",
      "************** Batch 412 in 5.364797353744507 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0584, 0.2008, 0.0809, 0.1021]) \n",
      "Test Loss tensor([0.0616, 0.2035, 0.0782, 0.1092])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2013, 0.4667, 0.0232, 0.2123, 0.3014]) \n",
      "Test Loss tensor([0.1901, 0.4815, 0.0355, 0.2001, 0.2850])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0853, 0.0170, 0.2931, 0.2738, 0.2203, 0.0917, 0.0138]) \n",
      "Test Loss tensor([0.0950, 0.0135, 0.2842, 0.2550, 0.2243, 0.0894, 0.0144])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0370, 0.2288, 0.0425, 0.0804, 0.2599, 0.2468, 0.0429]) \n",
      "Test Loss tensor([0.0297, 0.2283, 0.0441, 0.0750, 0.2692, 0.2536, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0273, 0.0194, 0.0387, 0.0184, 0.0283, 0.0118, 0.1270, 0.0041]) \n",
      "Test Loss tensor([0.0307, 0.0244, 0.0372, 0.0196, 0.0234, 0.0163, 0.1222, 0.0056])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6022, 0.6967, 0.1058, 0.1925, 0.3756, 0.5560])\n",
      "Valid Idx 3 | Loss tensor([0.3413, 0.2521, 0.1142, 0.7422, 0.8135])\n",
      "\n",
      "************** Batch 416 in 5.384448528289795 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0614, 0.2028, 0.0789, 0.1077]) \n",
      "Test Loss tensor([0.0601, 0.2014, 0.0792, 0.1038])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1918, 0.4757, 0.0351, 0.1874, 0.2802]) \n",
      "Test Loss tensor([0.2011, 0.4750, 0.0219, 0.2006, 0.2986])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0941, 0.0132, 0.2878, 0.2421, 0.2241, 0.0900, 0.0167]) \n",
      "Test Loss tensor([0.0888, 0.0158, 0.2811, 0.2601, 0.2149, 0.0907, 0.0119])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0294, 0.2231, 0.0452, 0.0843, 0.2788, 0.2622, 0.0520]) \n",
      "Test Loss tensor([0.0333, 0.2231, 0.0451, 0.0803, 0.2719, 0.2569, 0.0480])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0334, 0.0227, 0.0372, 0.0199, 0.0255, 0.0168, 0.1245, 0.0052]) \n",
      "Test Loss tensor([0.0276, 0.0238, 0.0394, 0.0202, 0.0264, 0.0120, 0.1211, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6119, 0.6622, 0.1019, 0.2061, 0.3648, 0.5921])\n",
      "Valid Idx 3 | Loss tensor([0.4704, 0.3970, 0.1168, 0.7869, 0.8615])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 420 in 5.162477731704712 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0672, 0.2042, 0.0784, 0.1033]) \n",
      "Test Loss tensor([0.0599, 0.1985, 0.0759, 0.1059])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1861, 0.4833, 0.0220, 0.2070, 0.3066]) \n",
      "Test Loss tensor([0.1917, 0.4765, 0.0282, 0.2016, 0.2891])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0811, 0.0193, 0.2736, 0.2499, 0.2127, 0.0941, 0.0135]) \n",
      "Test Loss tensor([0.0906, 0.0133, 0.2819, 0.2543, 0.2186, 0.0880, 0.0136])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0380, 0.2168, 0.0439, 0.0779, 0.2697, 0.2602, 0.0513]) \n",
      "Test Loss tensor([0.0315, 0.2254, 0.0451, 0.0791, 0.2721, 0.2537, 0.0511])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0261, 0.0206, 0.0447, 0.0195, 0.0275, 0.0113, 0.1252, 0.0043]) \n",
      "Test Loss tensor([0.0297, 0.0230, 0.0381, 0.0194, 0.0235, 0.0149, 0.1211, 0.0049])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6024, 0.6915, 0.1022, 0.1916, 0.3777, 0.5629])\n",
      "Valid Idx 3 | Loss tensor([0.3816, 0.3023, 0.1172, 0.7561, 0.8363])\n",
      "Gradients: Input 1.4149582386016846 | Message 2.0419728755950928 | Update 2.154193878173828 | Output 0.2560032606124878\n",
      "\n",
      "************** Batch 424 in 5.2778096199035645 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0593, 0.2134, 0.0760, 0.1071]) \n",
      "Test Loss tensor([0.0613, 0.2020, 0.0765, 0.1062])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1946, 0.4748, 0.0305, 0.2024, 0.2837]) \n",
      "Test Loss tensor([0.1950, 0.4772, 0.0296, 0.2022, 0.2879])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0960, 0.0140, 0.2815, 0.2542, 0.2210, 0.0887, 0.0131]) \n",
      "Test Loss tensor([0.0917, 0.0133, 0.2829, 0.2500, 0.2189, 0.0872, 0.0138])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0271, 0.2293, 0.0363, 0.0803, 0.2631, 0.2535, 0.0431]) \n",
      "Test Loss tensor([0.0307, 0.2217, 0.0436, 0.0750, 0.2699, 0.2531, 0.0490])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0278, 0.0188, 0.0372, 0.0185, 0.0231, 0.0156, 0.1175, 0.0053]) \n",
      "Test Loss tensor([0.0300, 0.0236, 0.0367, 0.0199, 0.0241, 0.0149, 0.1183, 0.0049])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6048, 0.6940, 0.1035, 0.1940, 0.3772, 0.5585])\n",
      "Valid Idx 3 | Loss tensor([0.3790, 0.3004, 0.1168, 0.7526, 0.8368])\n",
      "\n",
      "************** Batch 428 in 6.052264451980591 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0580, 0.2059, 0.0772, 0.1034]) \n",
      "Test Loss tensor([0.0597, 0.2044, 0.0762, 0.1010])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1854, 0.4704, 0.0320, 0.1944, 0.2786]) \n",
      "Test Loss tensor([0.1990, 0.4749, 0.0235, 0.2021, 0.2957])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0912, 0.0155, 0.2762, 0.2456, 0.2264, 0.0922, 0.0126]) \n",
      "Test Loss tensor([0.0892, 0.0158, 0.2855, 0.2618, 0.2155, 0.0906, 0.0114])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0355, 0.2224, 0.0474, 0.0778, 0.2753, 0.2560, 0.0493]) \n",
      "Test Loss tensor([0.0342, 0.2201, 0.0463, 0.0784, 0.2749, 0.2525, 0.0498])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0311, 0.0207, 0.0413, 0.0196, 0.0218, 0.0170, 0.1210, 0.0049]) \n",
      "Test Loss tensor([0.0285, 0.0221, 0.0390, 0.0213, 0.0239, 0.0126, 0.1224, 0.0042])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6001, 0.6720, 0.1014, 0.2036, 0.3712, 0.5690])\n",
      "Valid Idx 3 | Loss tensor([0.4567, 0.3775, 0.1190, 0.7831, 0.8595])\n",
      "\n",
      "************** Batch 432 in 5.216247797012329 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0643, 0.2048, 0.0739, 0.1024]) \n",
      "Test Loss tensor([0.0600, 0.2022, 0.0750, 0.1066])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2024, 0.4715, 0.0231, 0.2040, 0.2946]) \n",
      "Test Loss tensor([0.1952, 0.4745, 0.0319, 0.2000, 0.2852])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0818, 0.0158, 0.2890, 0.2502, 0.2158, 0.0869, 0.0098]) \n",
      "Test Loss tensor([0.0919, 0.0124, 0.2811, 0.2533, 0.2230, 0.0877, 0.0139])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0308, 0.2244, 0.0446, 0.0812, 0.2682, 0.2519, 0.0480]) \n",
      "Test Loss tensor([0.0312, 0.2209, 0.0447, 0.0775, 0.2722, 0.2550, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0297, 0.0219, 0.0362, 0.0211, 0.0235, 0.0122, 0.1151, 0.0038]) \n",
      "Test Loss tensor([0.0309, 0.0236, 0.0373, 0.0200, 0.0226, 0.0152, 0.1207, 0.0053])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5980, 0.6900, 0.1040, 0.1896, 0.3742, 0.5495])\n",
      "Valid Idx 3 | Loss tensor([0.3765, 0.2820, 0.1166, 0.7548, 0.8250])\n",
      "\n",
      "************** Batch 436 in 5.185043573379517 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0613, 0.1988, 0.0803, 0.1052]) \n",
      "Test Loss tensor([0.0609, 0.2005, 0.0767, 0.1045])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1934, 0.4704, 0.0308, 0.1882, 0.2930]) \n",
      "Test Loss tensor([0.1986, 0.4671, 0.0247, 0.1999, 0.2933])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0947, 0.0154, 0.2817, 0.2593, 0.2200, 0.0804, 0.0140]) \n",
      "Test Loss tensor([0.0877, 0.0138, 0.2862, 0.2565, 0.2165, 0.0923, 0.0130])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0324, 0.2237, 0.0521, 0.0831, 0.2743, 0.2612, 0.0498]) \n",
      "Test Loss tensor([0.0340, 0.2226, 0.0445, 0.0781, 0.2742, 0.2534, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0362, 0.0256, 0.0434, 0.0190, 0.0253, 0.0145, 0.1272, 0.0051]) \n",
      "Test Loss tensor([0.0287, 0.0228, 0.0386, 0.0198, 0.0252, 0.0128, 0.1223, 0.0044])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6055, 0.6612, 0.1035, 0.1993, 0.3705, 0.5684])\n",
      "Valid Idx 3 | Loss tensor([0.4568, 0.3483, 0.1161, 0.7869, 0.8455])\n",
      "\n",
      "************** Batch 440 in 5.120945692062378 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0556, 0.1878, 0.0778, 0.1032]) \n",
      "Test Loss tensor([0.0589, 0.1995, 0.0754, 0.1076])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2019, 0.4698, 0.0236, 0.1985, 0.2946]) \n",
      "Test Loss tensor([0.1945, 0.4688, 0.0327, 0.1995, 0.2819])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0890, 0.0170, 0.2742, 0.2520, 0.2240, 0.0877, 0.0136]) \n",
      "Test Loss tensor([0.0910, 0.0130, 0.2818, 0.2540, 0.2194, 0.0879, 0.0141])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0293, 0.2328, 0.0461, 0.0793, 0.2703, 0.2447, 0.0465]) \n",
      "Test Loss tensor([0.0320, 0.2257, 0.0446, 0.0774, 0.2739, 0.2531, 0.0509])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0268, 0.0217, 0.0378, 0.0202, 0.0241, 0.0142, 0.1287, 0.0040]) \n",
      "Test Loss tensor([0.0309, 0.0240, 0.0389, 0.0194, 0.0232, 0.0149, 0.1211, 0.0056])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6034, 0.6832, 0.1059, 0.1908, 0.3746, 0.5561])\n",
      "Valid Idx 3 | Loss tensor([0.3968, 0.2779, 0.1155, 0.7637, 0.8248])\n",
      "Gradients: Input 0.8750410079956055 | Message 1.3470497131347656 | Update 1.4011433124542236 | Output 0.17849311232566833\n",
      "\n",
      "************** Batch 444 in 5.313493728637695 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0621, 0.1873, 0.0699, 0.1064]) \n",
      "Test Loss tensor([0.0602, 0.2004, 0.0758, 0.1039])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1953, 0.4639, 0.0297, 0.2009, 0.2880]) \n",
      "Test Loss tensor([0.1989, 0.4641, 0.0246, 0.2016, 0.2908])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0888, 0.0142, 0.2694, 0.2411, 0.2103, 0.0881, 0.0126]) \n",
      "Test Loss tensor([0.0902, 0.0155, 0.2821, 0.2559, 0.2169, 0.0919, 0.0130])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0318, 0.2272, 0.0471, 0.0719, 0.2795, 0.2532, 0.0542]) \n",
      "Test Loss tensor([0.0332, 0.2225, 0.0448, 0.0775, 0.2723, 0.2531, 0.0487])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0273, 0.0236, 0.0397, 0.0197, 0.0231, 0.0140, 0.1248, 0.0054]) \n",
      "Test Loss tensor([0.0282, 0.0233, 0.0388, 0.0193, 0.0251, 0.0126, 0.1220, 0.0045])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6056, 0.6665, 0.1003, 0.2002, 0.3691, 0.5764])\n",
      "Valid Idx 3 | Loss tensor([0.4684, 0.3544, 0.1161, 0.7865, 0.8514])\n",
      "\n",
      "************** Batch 448 in 5.086731672286987 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0588, 0.2079, 0.0764, 0.1046]) \n",
      "Test Loss tensor([0.0591, 0.2003, 0.0766, 0.1052])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2013, 0.4665, 0.0258, 0.1928, 0.2941]) \n",
      "Test Loss tensor([0.1935, 0.4647, 0.0296, 0.2020, 0.2838])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0773, 0.0146, 0.2882, 0.2625, 0.2200, 0.0916, 0.0107]) \n",
      "Test Loss tensor([0.0926, 0.0139, 0.2783, 0.2475, 0.2190, 0.0888, 0.0126])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0339, 0.2255, 0.0469, 0.0745, 0.2751, 0.2494, 0.0462]) \n",
      "Test Loss tensor([0.0313, 0.2209, 0.0441, 0.0772, 0.2713, 0.2535, 0.0487])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0321, 0.0261, 0.0410, 0.0171, 0.0275, 0.0120, 0.1245, 0.0042]) \n",
      "Test Loss tensor([0.0298, 0.0228, 0.0379, 0.0203, 0.0233, 0.0146, 0.1215, 0.0048])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6022, 0.6891, 0.1020, 0.1956, 0.3700, 0.5633])\n",
      "Valid Idx 3 | Loss tensor([0.4060, 0.2971, 0.1113, 0.7685, 0.8314])\n",
      "\n",
      "************** Batch 452 in 4.450428485870361 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0614, 0.2041, 0.0770, 0.1102]) \n",
      "Test Loss tensor([0.0600, 0.1999, 0.0782, 0.1025])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1955, 0.4688, 0.0313, 0.2028, 0.2847]) \n",
      "Test Loss tensor([0.1963, 0.4689, 0.0247, 0.1991, 0.2907])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0900, 0.0153, 0.2730, 0.2405, 0.2129, 0.0902, 0.0166]) \n",
      "Test Loss tensor([0.0929, 0.0149, 0.2810, 0.2489, 0.2136, 0.0900, 0.0119])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0288, 0.2270, 0.0406, 0.0777, 0.2790, 0.2592, 0.0508]) \n",
      "Test Loss tensor([0.0329, 0.2249, 0.0458, 0.0766, 0.2723, 0.2532, 0.0493])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0310, 0.0250, 0.0362, 0.0178, 0.0244, 0.0156, 0.1177, 0.0056]) \n",
      "Test Loss tensor([0.0291, 0.0245, 0.0391, 0.0190, 0.0256, 0.0131, 0.1213, 0.0043])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6072, 0.6754, 0.0995, 0.2031, 0.3678, 0.5812])\n",
      "Valid Idx 3 | Loss tensor([0.4585, 0.3513, 0.1179, 0.7866, 0.8490])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 456 in 4.54585337638855 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0584, 0.1956, 0.0762, 0.1021]) \n",
      "Test Loss tensor([0.0586, 0.2002, 0.0779, 0.1052])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1897, 0.4633, 0.0269, 0.1905, 0.2976]) \n",
      "Test Loss tensor([0.1924, 0.4626, 0.0278, 0.1976, 0.2859])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0872, 0.0148, 0.2767, 0.2634, 0.2123, 0.0942, 0.0121]) \n",
      "Test Loss tensor([0.0935, 0.0133, 0.2817, 0.2539, 0.2172, 0.0911, 0.0125])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0370, 0.2197, 0.0526, 0.0826, 0.2574, 0.2557, 0.0555]) \n",
      "Test Loss tensor([0.0320, 0.2231, 0.0453, 0.0750, 0.2708, 0.2546, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0204, 0.0405, 0.0179, 0.0267, 0.0141, 0.1186, 0.0044]) \n",
      "Test Loss tensor([0.0299, 0.0248, 0.0366, 0.0203, 0.0241, 0.0144, 0.1198, 0.0048])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6038, 0.6820, 0.1026, 0.1963, 0.3715, 0.5750])\n",
      "Valid Idx 3 | Loss tensor([0.4025, 0.3032, 0.1149, 0.7697, 0.8339])\n",
      "\n",
      "************** Batch 460 in 4.563809394836426 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0636, 0.2029, 0.0742, 0.1114]) \n",
      "Test Loss tensor([0.0569, 0.1999, 0.0777, 0.1012])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1935, 0.4665, 0.0277, 0.1992, 0.2858]) \n",
      "Test Loss tensor([0.1930, 0.4678, 0.0250, 0.2008, 0.2893])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0988, 0.0139, 0.2849, 0.2406, 0.2201, 0.0867, 0.0128]) \n",
      "Test Loss tensor([0.0924, 0.0145, 0.2854, 0.2560, 0.2161, 0.0901, 0.0119])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0306, 0.2277, 0.0410, 0.0942, 0.2783, 0.2513, 0.0514]) \n",
      "Test Loss tensor([0.0318, 0.2245, 0.0460, 0.0772, 0.2740, 0.2532, 0.0508])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0259, 0.0242, 0.0384, 0.0210, 0.0237, 0.0142, 0.1199, 0.0045]) \n",
      "Test Loss tensor([0.0280, 0.0228, 0.0368, 0.0190, 0.0252, 0.0119, 0.1191, 0.0043])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6048, 0.6731, 0.1010, 0.2014, 0.3663, 0.5853])\n",
      "Valid Idx 3 | Loss tensor([0.4312, 0.3431, 0.1176, 0.7812, 0.8541])\n",
      "Gradients: Input 0.5730694532394409 | Message 0.856775164604187 | Update 0.9026995301246643 | Output 0.08703266084194183\n",
      "\n",
      "************** Batch 464 in 4.59319281578064 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0557, 0.1929, 0.0781, 0.1001]) \n",
      "Test Loss tensor([0.0581, 0.2016, 0.0779, 0.1019])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1982, 0.4599, 0.0272, 0.1917, 0.2913]) \n",
      "Test Loss tensor([0.1920, 0.4654, 0.0252, 0.2024, 0.2906])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0926, 0.0160, 0.2817, 0.2500, 0.2033, 0.0882, 0.0106]) \n",
      "Test Loss tensor([0.0949, 0.0137, 0.2875, 0.2564, 0.2175, 0.0895, 0.0115])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0364, 0.2207, 0.0362, 0.0765, 0.2616, 0.2436, 0.0434]) \n",
      "Test Loss tensor([0.0315, 0.2203, 0.0449, 0.0772, 0.2741, 0.2527, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0242, 0.0434, 0.0197, 0.0264, 0.0138, 0.1205, 0.0043]) \n",
      "Test Loss tensor([0.0292, 0.0234, 0.0373, 0.0196, 0.0245, 0.0128, 0.1217, 0.0041])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6000, 0.6746, 0.1032, 0.2007, 0.3663, 0.5814])\n",
      "Valid Idx 3 | Loss tensor([0.4151, 0.3263, 0.1171, 0.7748, 0.8504])\n",
      "\n",
      "************** Batch 468 in 4.697202444076538 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0662, 0.2058, 0.0802, 0.1080]) \n",
      "Test Loss tensor([0.0580, 0.2014, 0.0763, 0.1036])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2018, 0.4708, 0.0300, 0.1958, 0.2926]) \n",
      "Test Loss tensor([0.1898, 0.4594, 0.0280, 0.1996, 0.2865])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0932, 0.0146, 0.2697, 0.2469, 0.2206, 0.0817, 0.0132]) \n",
      "Test Loss tensor([0.0940, 0.0129, 0.2824, 0.2519, 0.2189, 0.0877, 0.0126])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0330, 0.2295, 0.0447, 0.0764, 0.2676, 0.2545, 0.0465]) \n",
      "Test Loss tensor([0.0328, 0.2212, 0.0434, 0.0761, 0.2762, 0.2524, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0317, 0.0247, 0.0422, 0.0192, 0.0244, 0.0120, 0.1147, 0.0038]) \n",
      "Test Loss tensor([0.0306, 0.0228, 0.0384, 0.0194, 0.0243, 0.0136, 0.1203, 0.0043])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5985, 0.6802, 0.1034, 0.2002, 0.3680, 0.5725])\n",
      "Valid Idx 3 | Loss tensor([0.3969, 0.3035, 0.1127, 0.7699, 0.8380])\n",
      "\n",
      "************** Batch 472 in 4.767932891845703 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0557, 0.1937, 0.0755, 0.1060]) \n",
      "Test Loss tensor([0.0568, 0.1988, 0.0767, 0.1021])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1902, 0.4621, 0.0277, 0.2026, 0.2856]) \n",
      "Test Loss tensor([0.1930, 0.4598, 0.0260, 0.2012, 0.2884])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0916, 0.0130, 0.2908, 0.2488, 0.2161, 0.0876, 0.0123]) \n",
      "Test Loss tensor([0.0914, 0.0139, 0.2832, 0.2522, 0.2189, 0.0884, 0.0123])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0324, 0.2094, 0.0442, 0.0739, 0.2760, 0.2431, 0.0398]) \n",
      "Test Loss tensor([0.0340, 0.2240, 0.0452, 0.0779, 0.2718, 0.2488, 0.0497])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0305, 0.0249, 0.0365, 0.0209, 0.0237, 0.0124, 0.1246, 0.0044]) \n",
      "Test Loss tensor([0.0287, 0.0236, 0.0395, 0.0194, 0.0246, 0.0132, 0.1214, 0.0045])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5945, 0.6721, 0.0998, 0.2049, 0.3687, 0.5807])\n",
      "Valid Idx 3 | Loss tensor([0.4310, 0.3279, 0.1154, 0.7765, 0.8501])\n",
      "\n",
      "************** Batch 476 in 4.788268089294434 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0598, 0.1949, 0.0766, 0.1046]) \n",
      "Test Loss tensor([0.0599, 0.1979, 0.0757, 0.1050])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1958, 0.4576, 0.0279, 0.1897, 0.2940]) \n",
      "Test Loss tensor([0.1866, 0.4621, 0.0304, 0.1976, 0.2812])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0914, 0.0149, 0.2827, 0.2534, 0.2119, 0.0876, 0.0115]) \n",
      "Test Loss tensor([0.0914, 0.0136, 0.2810, 0.2524, 0.2168, 0.0880, 0.0130])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0337, 0.2239, 0.0436, 0.0788, 0.2657, 0.2456, 0.0540]) \n",
      "Test Loss tensor([0.0324, 0.2232, 0.0445, 0.0767, 0.2748, 0.2526, 0.0500])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0240, 0.0375, 0.0184, 0.0229, 0.0116, 0.1172, 0.0043]) \n",
      "Test Loss tensor([0.0298, 0.0240, 0.0392, 0.0201, 0.0239, 0.0137, 0.1231, 0.0047])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6051, 0.6719, 0.1027, 0.2017, 0.3661, 0.5707])\n",
      "Valid Idx 3 | Loss tensor([0.4195, 0.2976, 0.1159, 0.7788, 0.8330])\n",
      "\n",
      "************** Batch 480 in 4.878443002700806 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0525, 0.1965, 0.0767, 0.0988]) \n",
      "Test Loss tensor([0.0589, 0.1986, 0.0779, 0.1019])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1881, 0.4724, 0.0280, 0.1991, 0.2836]) \n",
      "Test Loss tensor([0.1954, 0.4602, 0.0257, 0.1996, 0.2846])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0951, 0.0128, 0.2781, 0.2484, 0.2152, 0.0895, 0.0145]) \n",
      "Test Loss tensor([0.0893, 0.0138, 0.2815, 0.2523, 0.2186, 0.0900, 0.0123])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0315, 0.2276, 0.0457, 0.0794, 0.2747, 0.2560, 0.0477]) \n",
      "Test Loss tensor([0.0344, 0.2248, 0.0466, 0.0786, 0.2729, 0.2521, 0.0514])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0293, 0.0223, 0.0378, 0.0174, 0.0252, 0.0138, 0.1159, 0.0044]) \n",
      "Test Loss tensor([0.0277, 0.0245, 0.0391, 0.0192, 0.0246, 0.0122, 0.1230, 0.0042])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6038, 0.6620, 0.1018, 0.2043, 0.3647, 0.5793])\n",
      "Valid Idx 3 | Loss tensor([0.4728, 0.3415, 0.1156, 0.7982, 0.8467])\n",
      "Gradients: Input 0.41178447008132935 | Message 0.6323326826095581 | Update 0.6539024114608765 | Output 0.06372298300266266\n",
      "\n",
      "************** Batch 484 in 4.77925968170166 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0574, 0.1961, 0.0745, 0.1002]) \n",
      "Test Loss tensor([0.0586, 0.1975, 0.0772, 0.1066])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1820, 0.4339, 0.0245, 0.1892, 0.3027]) \n",
      "Test Loss tensor([0.1908, 0.4596, 0.0323, 0.1967, 0.2808])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0897, 0.0114, 0.2859, 0.2557, 0.2242, 0.0902, 0.0113]) \n",
      "Test Loss tensor([0.0924, 0.0122, 0.2788, 0.2500, 0.2251, 0.0880, 0.0138])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0363, 0.2229, 0.0469, 0.0812, 0.2657, 0.2490, 0.0446]) \n",
      "Test Loss tensor([0.0314, 0.2232, 0.0432, 0.0778, 0.2716, 0.2585, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0345, 0.0239, 0.0399, 0.0203, 0.0247, 0.0129, 0.1295, 0.0042]) \n",
      "Test Loss tensor([0.0303, 0.0238, 0.0375, 0.0189, 0.0224, 0.0145, 0.1193, 0.0049])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5987, 0.6812, 0.1059, 0.1993, 0.3736, 0.5561])\n",
      "Valid Idx 3 | Loss tensor([0.4066, 0.2731, 0.1150, 0.7756, 0.8239])\n",
      "\n",
      "************** Batch 488 in 4.790575742721558 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0578, 0.1971, 0.0748, 0.1122]) \n",
      "Test Loss tensor([0.0593, 0.1989, 0.0778, 0.1013])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2010, 0.4541, 0.0292, 0.1941, 0.2899]) \n",
      "Test Loss tensor([0.1975, 0.4591, 0.0223, 0.1963, 0.2896])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0939, 0.0129, 0.2955, 0.2466, 0.2235, 0.0893, 0.0116]) \n",
      "Test Loss tensor([0.0885, 0.0153, 0.2853, 0.2594, 0.2165, 0.0877, 0.0109])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0311, 0.2241, 0.0450, 0.0802, 0.2617, 0.2524, 0.0480]) \n",
      "Test Loss tensor([0.0344, 0.2182, 0.0451, 0.0758, 0.2722, 0.2520, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0302, 0.0247, 0.0369, 0.0221, 0.0205, 0.0140, 0.1211, 0.0044]) \n",
      "Test Loss tensor([0.0276, 0.0228, 0.0393, 0.0206, 0.0237, 0.0120, 0.1203, 0.0037])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6059, 0.6630, 0.1048, 0.2070, 0.3675, 0.5820])\n",
      "Valid Idx 3 | Loss tensor([0.5117, 0.3962, 0.1166, 0.8083, 0.8712])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 492 in 4.760124206542969 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0629, 0.1896, 0.0832, 0.1002]) \n",
      "Test Loss tensor([0.0568, 0.1993, 0.0773, 0.1035])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1983, 0.4758, 0.0224, 0.1955, 0.2884]) \n",
      "Test Loss tensor([0.1908, 0.4638, 0.0296, 0.1949, 0.2807])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0850, 0.0148, 0.2937, 0.2702, 0.2152, 0.0883, 0.0127]) \n",
      "Test Loss tensor([0.0918, 0.0124, 0.2845, 0.2507, 0.2262, 0.0876, 0.0128])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0339, 0.2234, 0.0468, 0.0786, 0.2821, 0.2563, 0.0488]) \n",
      "Test Loss tensor([0.0300, 0.2216, 0.0455, 0.0785, 0.2702, 0.2542, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0257, 0.0220, 0.0343, 0.0205, 0.0225, 0.0133, 0.1123, 0.0034]) \n",
      "Test Loss tensor([0.0298, 0.0226, 0.0374, 0.0197, 0.0216, 0.0152, 0.1200, 0.0042])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5984, 0.6950, 0.1026, 0.1942, 0.3801, 0.5579])\n",
      "Valid Idx 3 | Loss tensor([0.4009, 0.2851, 0.1128, 0.7772, 0.8341])\n",
      "\n",
      "************** Batch 496 in 4.739205360412598 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0565, 0.2009, 0.0704, 0.1071]) \n",
      "Test Loss tensor([0.0575, 0.1993, 0.0783, 0.1013])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1893, 0.4579, 0.0304, 0.1960, 0.2770]) \n",
      "Test Loss tensor([0.1942, 0.4560, 0.0239, 0.1958, 0.2892])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0937, 0.0128, 0.2810, 0.2539, 0.2228, 0.0896, 0.0120]) \n",
      "Test Loss tensor([0.0897, 0.0141, 0.2802, 0.2539, 0.2177, 0.0862, 0.0111])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0293, 0.2116, 0.0415, 0.0772, 0.2643, 0.2626, 0.0489]) \n",
      "Test Loss tensor([0.0333, 0.2210, 0.0450, 0.0762, 0.2672, 0.2528, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0321, 0.0248, 0.0401, 0.0201, 0.0195, 0.0129, 0.1258, 0.0047]) \n",
      "Test Loss tensor([0.0275, 0.0234, 0.0386, 0.0198, 0.0223, 0.0129, 0.1204, 0.0040])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6020, 0.6834, 0.1026, 0.2003, 0.3766, 0.5691])\n",
      "Valid Idx 3 | Loss tensor([0.4629, 0.3536, 0.1142, 0.7977, 0.8582])\n",
      "\n",
      "************** Batch 500 in 4.80560302734375 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0593, 0.2010, 0.0810, 0.1032]) \n",
      "Test Loss tensor([0.0577, 0.2001, 0.0782, 0.1014])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1869, 0.4634, 0.0246, 0.1901, 0.2831]) \n",
      "Test Loss tensor([0.1962, 0.4605, 0.0232, 0.1961, 0.2860])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0824, 0.0138, 0.2956, 0.2418, 0.2163, 0.0911, 0.0095]) \n",
      "Test Loss tensor([0.0908, 0.0136, 0.2795, 0.2553, 0.2168, 0.0863, 0.0113])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0290, 0.2170, 0.0481, 0.0736, 0.2670, 0.2535, 0.0486]) \n",
      "Test Loss tensor([0.0338, 0.2193, 0.0450, 0.0779, 0.2716, 0.2551, 0.0483])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0303, 0.0231, 0.0377, 0.0214, 0.0216, 0.0132, 0.1143, 0.0040]) \n",
      "Test Loss tensor([0.0285, 0.0230, 0.0378, 0.0197, 0.0222, 0.0128, 0.1197, 0.0037])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6035, 0.6712, 0.1024, 0.2013, 0.3699, 0.5717])\n",
      "Valid Idx 3 | Loss tensor([0.4768, 0.3618, 0.1158, 0.8061, 0.8627])\n",
      "Gradients: Input 0.39120620489120483 | Message 0.479808509349823 | Update 0.4970797598361969 | Output 0.0509563572704792\n",
      "\n",
      "************** Batch 504 in 4.734326601028442 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0518, 0.2020, 0.0732, 0.0992]) \n",
      "Test Loss tensor([0.0602, 0.1986, 0.0774, 0.1091])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1932, 0.4578, 0.0233, 0.1977, 0.2863]) \n",
      "Test Loss tensor([0.1871, 0.4558, 0.0314, 0.1959, 0.2803])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0917, 0.0154, 0.2947, 0.2632, 0.2253, 0.0858, 0.0107]) \n",
      "Test Loss tensor([0.0977, 0.0123, 0.2829, 0.2503, 0.2266, 0.0877, 0.0138])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0305, 0.2277, 0.0456, 0.0792, 0.2677, 0.2563, 0.0566]) \n",
      "Test Loss tensor([0.0299, 0.2277, 0.0452, 0.0763, 0.2747, 0.2534, 0.0508])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0254, 0.0252, 0.0383, 0.0167, 0.0216, 0.0130, 0.1198, 0.0036]) \n",
      "Test Loss tensor([0.0307, 0.0223, 0.0381, 0.0193, 0.0205, 0.0157, 0.1207, 0.0046])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5994, 0.6939, 0.1054, 0.1923, 0.3778, 0.5515])\n",
      "Valid Idx 3 | Loss tensor([0.3865, 0.2588, 0.1109, 0.7759, 0.8242])\n",
      "\n",
      "************** Batch 508 in 4.738725423812866 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0608, 0.2127, 0.0736, 0.1012]) \n",
      "Test Loss tensor([0.0599, 0.2001, 0.0805, 0.1029])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1830, 0.4476, 0.0331, 0.1919, 0.2834]) \n",
      "Test Loss tensor([0.1956, 0.4484, 0.0200, 0.1968, 0.2977])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0941, 0.0113, 0.2829, 0.2489, 0.2178, 0.0847, 0.0147]) \n",
      "Test Loss tensor([0.0872, 0.0166, 0.2878, 0.2640, 0.2105, 0.0909, 0.0111])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0281, 0.2270, 0.0419, 0.0771, 0.2687, 0.2532, 0.0488]) \n",
      "Test Loss tensor([0.0355, 0.2222, 0.0472, 0.0801, 0.2741, 0.2506, 0.0495])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0281, 0.0274, 0.0387, 0.0175, 0.0208, 0.0144, 0.1197, 0.0040]) \n",
      "Test Loss tensor([0.0267, 0.0251, 0.0407, 0.0202, 0.0255, 0.0105, 0.1262, 0.0036])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6060, 0.6396, 0.1028, 0.2127, 0.3660, 0.5921])\n",
      "Valid Idx 3 | Loss tensor([0.5744, 0.4442, 0.1177, 0.8290, 0.8818])\n",
      "\n",
      "************** Batch 512 in 4.842217922210693 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0652, 0.1987, 0.0836, 0.0989]) \n",
      "Test Loss tensor([0.0630, 0.2015, 0.0771, 0.1167])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1945, 0.4362, 0.0215, 0.1953, 0.2938]) \n",
      "Test Loss tensor([0.1888, 0.4521, 0.0388, 0.1964, 0.2730])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0885, 0.0168, 0.2916, 0.2484, 0.2205, 0.1004, 0.0128]) \n",
      "Test Loss tensor([0.1012, 0.0116, 0.2808, 0.2546, 0.2266, 0.0847, 0.0155])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0382, 0.2281, 0.0449, 0.0816, 0.2719, 0.2426, 0.0470]) \n",
      "Test Loss tensor([0.0296, 0.2225, 0.0434, 0.0778, 0.2776, 0.2574, 0.0498])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0264, 0.0247, 0.0427, 0.0196, 0.0266, 0.0101, 0.1237, 0.0040]) \n",
      "Test Loss tensor([0.0327, 0.0242, 0.0367, 0.0184, 0.0201, 0.0184, 0.1226, 0.0053])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5903, 0.6946, 0.1066, 0.1897, 0.3771, 0.5377])\n",
      "Valid Idx 3 | Loss tensor([0.3633, 0.2220, 0.1103, 0.7669, 0.7986])\n",
      "\n",
      "************** Batch 516 in 4.745959758758545 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0615, 0.1998, 0.0792, 0.1170]) \n",
      "Test Loss tensor([0.0597, 0.1999, 0.0783, 0.1012])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1913, 0.4511, 0.0391, 0.1950, 0.2701]) \n",
      "Test Loss tensor([0.1979, 0.4462, 0.0209, 0.1983, 0.2947])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.1062, 0.0122, 0.2880, 0.2494, 0.2180, 0.0839, 0.0167]) \n",
      "Test Loss tensor([0.0883, 0.0161, 0.2841, 0.2611, 0.2140, 0.0913, 0.0110])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0295, 0.2221, 0.0423, 0.0717, 0.2761, 0.2628, 0.0456]) \n",
      "Test Loss tensor([0.0360, 0.2233, 0.0479, 0.0787, 0.2753, 0.2495, 0.0505])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0326, 0.0218, 0.0425, 0.0196, 0.0204, 0.0166, 0.1173, 0.0051]) \n",
      "Test Loss tensor([0.0272, 0.0237, 0.0403, 0.0194, 0.0249, 0.0105, 0.1213, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6082, 0.6489, 0.1025, 0.2094, 0.3617, 0.5896])\n",
      "Valid Idx 3 | Loss tensor([0.5728, 0.4381, 0.1158, 0.8257, 0.8835])\n",
      "\n",
      "************** Batch 520 in 4.740144491195679 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0640, 0.1947, 0.0804, 0.0989]) \n",
      "Test Loss tensor([0.0564, 0.1993, 0.0772, 0.1035])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2022, 0.4489, 0.0166, 0.1898, 0.3020]) \n",
      "Test Loss tensor([0.1878, 0.4529, 0.0273, 0.1971, 0.2792])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0863, 0.0181, 0.2845, 0.2564, 0.2133, 0.0826, 0.0119]) \n",
      "Test Loss tensor([0.0918, 0.0136, 0.2829, 0.2522, 0.2221, 0.0879, 0.0126])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0373, 0.2170, 0.0360, 0.0737, 0.2739, 0.2450, 0.0425]) \n",
      "Test Loss tensor([0.0316, 0.2243, 0.0457, 0.0761, 0.2724, 0.2509, 0.0527])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0310, 0.0176, 0.0431, 0.0209, 0.0246, 0.0118, 0.1208, 0.0033]) \n",
      "Test Loss tensor([0.0308, 0.0240, 0.0383, 0.0198, 0.0217, 0.0142, 0.1202, 0.0039])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5932, 0.6870, 0.1025, 0.1949, 0.3832, 0.5554])\n",
      "Valid Idx 3 | Loss tensor([0.4537, 0.3191, 0.1141, 0.7909, 0.8427])\n",
      "Gradients: Input 2.0318198204040527 | Message 3.0907037258148193 | Update 3.1971964836120605 | Output 0.37621235847473145\n",
      "\n",
      "************** Batch 524 in 4.9111645221710205 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0566, 0.1990, 0.0748, 0.1048]) \n",
      "Test Loss tensor([0.0566, 0.2015, 0.0767, 0.1051])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1927, 0.4613, 0.0292, 0.2059, 0.2741]) \n",
      "Test Loss tensor([0.1914, 0.4481, 0.0289, 0.1983, 0.2792])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0836, 0.0130, 0.2847, 0.2445, 0.2143, 0.0868, 0.0105]) \n",
      "Test Loss tensor([0.0926, 0.0131, 0.2793, 0.2501, 0.2249, 0.0875, 0.0133])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0327, 0.2232, 0.0363, 0.0769, 0.2785, 0.2514, 0.0431]) \n",
      "Test Loss tensor([0.0303, 0.2232, 0.0441, 0.0762, 0.2730, 0.2560, 0.0477])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0281, 0.0199, 0.0369, 0.0174, 0.0205, 0.0148, 0.1135, 0.0036]) \n",
      "Test Loss tensor([0.0317, 0.0248, 0.0384, 0.0197, 0.0206, 0.0151, 0.1215, 0.0043])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5962, 0.6977, 0.1028, 0.1896, 0.3809, 0.5564])\n",
      "Valid Idx 3 | Loss tensor([0.4347, 0.2996, 0.1099, 0.7853, 0.8407])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 528 in 4.842132568359375 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0616, 0.1933, 0.0796, 0.1120]) \n",
      "Test Loss tensor([0.0606, 0.2006, 0.0772, 0.1015])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1904, 0.4421, 0.0276, 0.1861, 0.2896]) \n",
      "Test Loss tensor([0.1984, 0.4420, 0.0177, 0.1997, 0.3001])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0934, 0.0134, 0.2791, 0.2477, 0.2279, 0.0843, 0.0138]) \n",
      "Test Loss tensor([0.0871, 0.0165, 0.2841, 0.2675, 0.2132, 0.0919, 0.0106])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0326, 0.2135, 0.0470, 0.0831, 0.2673, 0.2522, 0.0494]) \n",
      "Test Loss tensor([0.0366, 0.2212, 0.0476, 0.0782, 0.2752, 0.2519, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0275, 0.0240, 0.0419, 0.0193, 0.0201, 0.0164, 0.1246, 0.0046]) \n",
      "Test Loss tensor([0.0272, 0.0241, 0.0425, 0.0205, 0.0249, 0.0101, 0.1223, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6107, 0.6452, 0.1019, 0.2110, 0.3648, 0.5989])\n",
      "Valid Idx 3 | Loss tensor([0.6174, 0.4837, 0.1201, 0.8281, 0.8841])\n",
      "\n",
      "************** Batch 532 in 4.832495212554932 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0559, 0.2030, 0.0736, 0.1031]) \n",
      "Test Loss tensor([0.0584, 0.1990, 0.0784, 0.1105])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1991, 0.4532, 0.0170, 0.1860, 0.3028]) \n",
      "Test Loss tensor([0.1911, 0.4474, 0.0343, 0.1989, 0.2761])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0854, 0.0186, 0.2883, 0.2668, 0.2026, 0.0897, 0.0129]) \n",
      "Test Loss tensor([0.0944, 0.0119, 0.2800, 0.2522, 0.2252, 0.0856, 0.0153])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0376, 0.2192, 0.0464, 0.0779, 0.2734, 0.2420, 0.0463]) \n",
      "Test Loss tensor([0.0301, 0.2279, 0.0441, 0.0759, 0.2738, 0.2548, 0.0492])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0250, 0.0201, 0.0396, 0.0184, 0.0278, 0.0113, 0.1282, 0.0035]) \n",
      "Test Loss tensor([0.0317, 0.0231, 0.0385, 0.0179, 0.0210, 0.0164, 0.1194, 0.0046])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5908, 0.6986, 0.1059, 0.1903, 0.3795, 0.5444])\n",
      "Valid Idx 3 | Loss tensor([0.4056, 0.2485, 0.1131, 0.7737, 0.8141])\n",
      "\n",
      "************** Batch 536 in 4.78615140914917 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0593, 0.1887, 0.0736, 0.1101]) \n",
      "Test Loss tensor([0.0564, 0.1996, 0.0764, 0.0996])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1959, 0.4504, 0.0347, 0.1955, 0.2795]) \n",
      "Test Loss tensor([0.1944, 0.4399, 0.0228, 0.1999, 0.2855])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0897, 0.0138, 0.2810, 0.2620, 0.2295, 0.0808, 0.0152]) \n",
      "Test Loss tensor([0.0893, 0.0149, 0.2877, 0.2561, 0.2177, 0.0904, 0.0121])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0308, 0.2353, 0.0398, 0.0801, 0.2615, 0.2477, 0.0445]) \n",
      "Test Loss tensor([0.0353, 0.2237, 0.0454, 0.0758, 0.2722, 0.2519, 0.0484])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0297, 0.0245, 0.0380, 0.0178, 0.0208, 0.0174, 0.1144, 0.0045]) \n",
      "Test Loss tensor([0.0289, 0.0243, 0.0410, 0.0205, 0.0239, 0.0128, 0.1226, 0.0039])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6042, 0.6594, 0.1002, 0.2043, 0.3678, 0.5817])\n",
      "Valid Idx 3 | Loss tensor([0.5503, 0.3896, 0.1169, 0.8143, 0.8632])\n",
      "\n",
      "************** Batch 540 in 4.774531126022339 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0598, 0.2091, 0.0748, 0.1032]) \n",
      "Test Loss tensor([0.0568, 0.1987, 0.0762, 0.1005])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1867, 0.4311, 0.0183, 0.1896, 0.2987]) \n",
      "Test Loss tensor([0.1907, 0.4390, 0.0242, 0.1992, 0.2839])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0909, 0.0172, 0.2985, 0.2596, 0.2195, 0.0896, 0.0093]) \n",
      "Test Loss tensor([0.0897, 0.0144, 0.2816, 0.2487, 0.2185, 0.0888, 0.0133])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0349, 0.2265, 0.0492, 0.0821, 0.2857, 0.2512, 0.0589]) \n",
      "Test Loss tensor([0.0348, 0.2221, 0.0448, 0.0763, 0.2714, 0.2503, 0.0495])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0267, 0.0241, 0.0377, 0.0184, 0.0246, 0.0114, 0.1077, 0.0033]) \n",
      "Test Loss tensor([0.0282, 0.0244, 0.0390, 0.0198, 0.0220, 0.0129, 0.1207, 0.0040])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5984, 0.6592, 0.1011, 0.2033, 0.3668, 0.5752])\n",
      "Valid Idx 3 | Loss tensor([0.5191, 0.3486, 0.1139, 0.8066, 0.8535])\n",
      "Gradients: Input 1.0585145950317383 | Message 1.5490474700927734 | Update 1.6009721755981445 | Output 0.18449853360652924\n",
      "\n",
      "************** Batch 544 in 4.831528902053833 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0625, 0.1910, 0.0832, 0.1032]) \n",
      "Test Loss tensor([0.0556, 0.2009, 0.0760, 0.1045])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1958, 0.4375, 0.0208, 0.1975, 0.2797]) \n",
      "Test Loss tensor([0.1891, 0.4416, 0.0306, 0.1989, 0.2776])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0903, 0.0165, 0.2821, 0.2422, 0.2160, 0.0927, 0.0124]) \n",
      "Test Loss tensor([0.0952, 0.0128, 0.2825, 0.2468, 0.2271, 0.0875, 0.0141])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0312, 0.2293, 0.0367, 0.0749, 0.2608, 0.2523, 0.0454]) \n",
      "Test Loss tensor([0.0312, 0.2244, 0.0441, 0.0776, 0.2730, 0.2535, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0273, 0.0247, 0.0415, 0.0213, 0.0240, 0.0122, 0.1269, 0.0042]) \n",
      "Test Loss tensor([0.0298, 0.0232, 0.0380, 0.0189, 0.0206, 0.0151, 0.1174, 0.0045])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5930, 0.6728, 0.1025, 0.2000, 0.3748, 0.5561])\n",
      "Valid Idx 3 | Loss tensor([0.4515, 0.2813, 0.1155, 0.7889, 0.8236])\n",
      "\n",
      "************** Batch 548 in 4.762805461883545 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0604, 0.1942, 0.0716, 0.1077]) \n",
      "Test Loss tensor([0.0573, 0.2004, 0.0760, 0.0993])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1878, 0.4263, 0.0299, 0.2014, 0.2863]) \n",
      "Test Loss tensor([0.1912, 0.4354, 0.0219, 0.1950, 0.2858])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0968, 0.0131, 0.2817, 0.2546, 0.2224, 0.0945, 0.0180]) \n",
      "Test Loss tensor([0.0901, 0.0153, 0.2799, 0.2552, 0.2147, 0.0885, 0.0118])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0369, 0.2155, 0.0416, 0.0769, 0.2644, 0.2494, 0.0484]) \n",
      "Test Loss tensor([0.0342, 0.2179, 0.0453, 0.0762, 0.2747, 0.2537, 0.0481])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0255, 0.0220, 0.0357, 0.0156, 0.0220, 0.0149, 0.1128, 0.0046]) \n",
      "Test Loss tensor([0.0283, 0.0240, 0.0404, 0.0200, 0.0234, 0.0125, 0.1224, 0.0036])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5976, 0.6494, 0.1049, 0.2064, 0.3638, 0.5875])\n",
      "Valid Idx 3 | Loss tensor([0.5448, 0.3884, 0.1163, 0.8177, 0.8655])\n",
      "\n",
      "************** Batch 552 in 4.874635934829712 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0576, 0.1951, 0.0814, 0.0963]) \n",
      "Test Loss tensor([0.0575, 0.1965, 0.0778, 0.0992])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1971, 0.4330, 0.0237, 0.1956, 0.2958]) \n",
      "Test Loss tensor([0.1876, 0.4372, 0.0249, 0.1956, 0.2832])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0863, 0.0170, 0.2786, 0.2608, 0.2197, 0.0910, 0.0143]) \n",
      "Test Loss tensor([0.0918, 0.0134, 0.2796, 0.2500, 0.2220, 0.0891, 0.0127])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0334, 0.2284, 0.0473, 0.0826, 0.2839, 0.2465, 0.0482]) \n",
      "Test Loss tensor([0.0328, 0.2229, 0.0453, 0.0776, 0.2729, 0.2496, 0.0499])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0297, 0.0234, 0.0372, 0.0209, 0.0234, 0.0125, 0.1203, 0.0042]) \n",
      "Test Loss tensor([0.0278, 0.0231, 0.0405, 0.0201, 0.0215, 0.0133, 0.1211, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5967, 0.6694, 0.1027, 0.2031, 0.3728, 0.5747])\n",
      "Valid Idx 3 | Loss tensor([0.4993, 0.3466, 0.1089, 0.8095, 0.8579])\n",
      "\n",
      "************** Batch 556 in 4.82288670539856 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0557, 0.2009, 0.0730, 0.0984]) \n",
      "Test Loss tensor([0.0569, 0.1977, 0.0786, 0.1046])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1870, 0.4452, 0.0255, 0.1943, 0.2756]) \n",
      "Test Loss tensor([0.1867, 0.4451, 0.0273, 0.1941, 0.2790])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0931, 0.0120, 0.2771, 0.2457, 0.2278, 0.0897, 0.0113]) \n",
      "Test Loss tensor([0.0945, 0.0143, 0.2761, 0.2462, 0.2201, 0.0842, 0.0129])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0361, 0.2264, 0.0462, 0.0755, 0.2701, 0.2514, 0.0592]) \n",
      "Test Loss tensor([0.0313, 0.2262, 0.0464, 0.0758, 0.2685, 0.2509, 0.0505])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0278, 0.0276, 0.0444, 0.0189, 0.0205, 0.0148, 0.1188, 0.0035]) \n",
      "Test Loss tensor([0.0298, 0.0242, 0.0382, 0.0196, 0.0209, 0.0149, 0.1186, 0.0039])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6006, 0.6777, 0.1020, 0.1965, 0.3743, 0.5701])\n",
      "Valid Idx 3 | Loss tensor([0.4639, 0.3170, 0.1121, 0.7975, 0.8438])\n",
      "\n",
      "************** Batch 560 in 4.812074184417725 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0586, 0.1906, 0.0725, 0.1025]) \n",
      "Test Loss tensor([0.0569, 0.1995, 0.0772, 0.1002])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1949, 0.4402, 0.0262, 0.1945, 0.2717]) \n",
      "Test Loss tensor([0.1883, 0.4333, 0.0226, 0.1961, 0.2856])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0867, 0.0122, 0.2841, 0.2420, 0.2154, 0.0785, 0.0104]) \n",
      "Test Loss tensor([0.0914, 0.0159, 0.2846, 0.2573, 0.2191, 0.0890, 0.0115])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0300, 0.2199, 0.0396, 0.0737, 0.2672, 0.2516, 0.0443]) \n",
      "Test Loss tensor([0.0345, 0.2249, 0.0456, 0.0777, 0.2739, 0.2521, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0297, 0.0212, 0.0399, 0.0204, 0.0195, 0.0134, 0.1124, 0.0036]) \n",
      "Test Loss tensor([0.0284, 0.0236, 0.0390, 0.0198, 0.0225, 0.0128, 0.1199, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6022, 0.6576, 0.1017, 0.2064, 0.3664, 0.5870])\n",
      "Valid Idx 3 | Loss tensor([0.5466, 0.4043, 0.1155, 0.8243, 0.8692])\n",
      "Gradients: Input 0.6954776048660278 | Message 0.9806091785430908 | Update 1.019291639328003 | Output 0.10651852190494537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 564 in 4.871590852737427 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0543, 0.2023, 0.0787, 0.0956]) \n",
      "Test Loss tensor([0.0566, 0.1988, 0.0776, 0.1021])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1825, 0.4275, 0.0231, 0.1923, 0.2738]) \n",
      "Test Loss tensor([0.1824, 0.4375, 0.0260, 0.1934, 0.2785])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0870, 0.0171, 0.2832, 0.2500, 0.2274, 0.0944, 0.0129]) \n",
      "Test Loss tensor([0.0925, 0.0147, 0.2801, 0.2498, 0.2207, 0.0850, 0.0118])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0354, 0.2163, 0.0422, 0.0738, 0.2703, 0.2480, 0.0520]) \n",
      "Test Loss tensor([0.0350, 0.2264, 0.0459, 0.0761, 0.2735, 0.2532, 0.0487])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0290, 0.0234, 0.0414, 0.0159, 0.0226, 0.0129, 0.1212, 0.0031]) \n",
      "Test Loss tensor([0.0283, 0.0246, 0.0392, 0.0191, 0.0214, 0.0135, 0.1176, 0.0036])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6019, 0.6601, 0.1028, 0.2025, 0.3725, 0.5780])\n",
      "Valid Idx 3 | Loss tensor([0.5250, 0.3631, 0.1121, 0.8186, 0.8516])\n",
      "\n",
      "************** Batch 568 in 4.973083972930908 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0565, 0.1959, 0.0798, 0.1009]) \n",
      "Test Loss tensor([0.0565, 0.1992, 0.0769, 0.1061])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1875, 0.4274, 0.0238, 0.1888, 0.2735]) \n",
      "Test Loss tensor([0.1844, 0.4347, 0.0299, 0.1913, 0.2740])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0931, 0.0164, 0.2997, 0.2540, 0.2074, 0.0856, 0.0102]) \n",
      "Test Loss tensor([0.0964, 0.0148, 0.2831, 0.2478, 0.2227, 0.0863, 0.0136])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0300, 0.2207, 0.0510, 0.0789, 0.2730, 0.2439, 0.0518]) \n",
      "Test Loss tensor([0.0329, 0.2237, 0.0446, 0.0771, 0.2711, 0.2495, 0.0494])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0291, 0.0210, 0.0403, 0.0192, 0.0212, 0.0123, 0.1161, 0.0037]) \n",
      "Test Loss tensor([0.0286, 0.0247, 0.0393, 0.0193, 0.0213, 0.0148, 0.1180, 0.0042])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5965, 0.6689, 0.1025, 0.2015, 0.3702, 0.5613])\n",
      "Valid Idx 3 | Loss tensor([0.4933, 0.3192, 0.1133, 0.8111, 0.8295])\n",
      "\n",
      "************** Batch 572 in 4.965318202972412 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0558, 0.1850, 0.0735, 0.1032]) \n",
      "Test Loss tensor([0.0593, 0.1998, 0.0783, 0.0998])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1834, 0.4204, 0.0261, 0.1960, 0.2842]) \n",
      "Test Loss tensor([0.1896, 0.4357, 0.0215, 0.1934, 0.2841])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0900, 0.0158, 0.2889, 0.2423, 0.2292, 0.0918, 0.0149]) \n",
      "Test Loss tensor([0.0872, 0.0174, 0.2873, 0.2573, 0.2176, 0.0890, 0.0113])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0291, 0.2111, 0.0461, 0.0769, 0.2634, 0.2455, 0.0478]) \n",
      "Test Loss tensor([0.0382, 0.2238, 0.0456, 0.0751, 0.2721, 0.2496, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0310, 0.0284, 0.0407, 0.0157, 0.0223, 0.0151, 0.1129, 0.0040]) \n",
      "Test Loss tensor([0.0266, 0.0242, 0.0418, 0.0208, 0.0228, 0.0111, 0.1224, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6039, 0.6327, 0.1055, 0.2143, 0.3624, 0.5947])\n",
      "Valid Idx 3 | Loss tensor([0.6254, 0.4606, 0.1143, 0.8461, 0.8730])\n",
      "\n",
      "************** Batch 576 in 4.949277877807617 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0576, 0.2005, 0.0735, 0.0991]) \n",
      "Test Loss tensor([0.0561, 0.2009, 0.0765, 0.1046])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.2027, 0.4280, 0.0197, 0.1918, 0.2815]) \n",
      "Test Loss tensor([0.1859, 0.4339, 0.0285, 0.1946, 0.2741])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0801, 0.0181, 0.2679, 0.2627, 0.2145, 0.0862, 0.0127]) \n",
      "Test Loss tensor([0.0920, 0.0137, 0.2841, 0.2489, 0.2228, 0.0860, 0.0130])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0355, 0.2262, 0.0519, 0.0879, 0.2862, 0.2605, 0.0580]) \n",
      "Test Loss tensor([0.0325, 0.2237, 0.0446, 0.0760, 0.2722, 0.2535, 0.0498])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0284, 0.0257, 0.0392, 0.0210, 0.0214, 0.0121, 0.1234, 0.0035]) \n",
      "Test Loss tensor([0.0288, 0.0238, 0.0388, 0.0194, 0.0204, 0.0149, 0.1190, 0.0040])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6011, 0.6665, 0.1025, 0.2008, 0.3691, 0.5643])\n",
      "Valid Idx 3 | Loss tensor([0.5127, 0.3294, 0.1125, 0.8129, 0.8342])\n",
      "\n",
      "************** Batch 580 in 4.817039966583252 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0526, 0.1959, 0.0784, 0.1084]) \n",
      "Test Loss tensor([0.0586, 0.1988, 0.0767, 0.1038])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1894, 0.4353, 0.0296, 0.1923, 0.2797]) \n",
      "Test Loss tensor([0.1875, 0.4326, 0.0263, 0.1960, 0.2758])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0836, 0.0152, 0.2845, 0.2349, 0.2318, 0.0837, 0.0114]) \n",
      "Test Loss tensor([0.0909, 0.0145, 0.2850, 0.2521, 0.2242, 0.0853, 0.0124])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0345, 0.2195, 0.0429, 0.0812, 0.2669, 0.2485, 0.0475]) \n",
      "Test Loss tensor([0.0336, 0.2191, 0.0435, 0.0763, 0.2688, 0.2487, 0.0477])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0271, 0.0245, 0.0385, 0.0214, 0.0195, 0.0136, 0.1212, 0.0038]) \n",
      "Test Loss tensor([0.0283, 0.0244, 0.0391, 0.0187, 0.0196, 0.0138, 0.1185, 0.0037])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6031, 0.6618, 0.1012, 0.1963, 0.3698, 0.5682])\n",
      "Valid Idx 3 | Loss tensor([0.5263, 0.3537, 0.1119, 0.8192, 0.8460])\n",
      "Gradients: Input 0.8790996074676514 | Message 1.2902116775512695 | Update 1.32951819896698 | Output 0.11962099373340607\n",
      "\n",
      "************** Batch 584 in 4.8752477169036865 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0606, 0.2022, 0.0758, 0.1059]) \n",
      "Test Loss tensor([0.0582, 0.2019, 0.0757, 0.0994])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1818, 0.4517, 0.0271, 0.1891, 0.2886]) \n",
      "Test Loss tensor([0.1904, 0.4353, 0.0208, 0.1941, 0.2860])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0929, 0.0140, 0.2728, 0.2482, 0.2162, 0.0867, 0.0121]) \n",
      "Test Loss tensor([0.0886, 0.0147, 0.2843, 0.2541, 0.2204, 0.0877, 0.0114])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0353, 0.2272, 0.0432, 0.0734, 0.2752, 0.2426, 0.0488]) \n",
      "Test Loss tensor([0.0344, 0.2160, 0.0463, 0.0770, 0.2707, 0.2493, 0.0502])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0285, 0.0280, 0.0367, 0.0166, 0.0190, 0.0147, 0.1107, 0.0042]) \n",
      "Test Loss tensor([0.0275, 0.0229, 0.0405, 0.0197, 0.0205, 0.0122, 0.1185, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6042, 0.6505, 0.1006, 0.2002, 0.3693, 0.5813])\n",
      "Valid Idx 3 | Loss tensor([0.5829, 0.4232, 0.1145, 0.8299, 0.8706])\n",
      "\n",
      "************** Batch 588 in 4.9345738887786865 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0557, 0.1977, 0.0726, 0.0967]) \n",
      "Test Loss tensor([0.0581, 0.2011, 0.0775, 0.1048])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1855, 0.4389, 0.0197, 0.1933, 0.2888]) \n",
      "Test Loss tensor([0.1858, 0.4403, 0.0282, 0.1942, 0.2756])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0747, 0.0148, 0.2842, 0.2558, 0.2104, 0.0871, 0.0105]) \n",
      "Test Loss tensor([0.0968, 0.0131, 0.2826, 0.2439, 0.2239, 0.0866, 0.0120])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0326, 0.2447, 0.0438, 0.0774, 0.2758, 0.2593, 0.0491]) \n",
      "Test Loss tensor([0.0306, 0.2160, 0.0437, 0.0752, 0.2709, 0.2536, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0252, 0.0262, 0.0390, 0.0228, 0.0187, 0.0118, 0.1247, 0.0033]) \n",
      "Test Loss tensor([0.0300, 0.0236, 0.0380, 0.0198, 0.0184, 0.0155, 0.1192, 0.0039])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5950, 0.6804, 0.1009, 0.1903, 0.3758, 0.5550])\n",
      "Valid Idx 3 | Loss tensor([0.4704, 0.3080, 0.1094, 0.7982, 0.8380])\n",
      "\n",
      "************** Batch 592 in 4.966047763824463 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0579, 0.1983, 0.0717, 0.1021]) \n",
      "Test Loss tensor([0.0568, 0.1974, 0.0755, 0.0994])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1897, 0.4298, 0.0262, 0.1933, 0.2658]) \n",
      "Test Loss tensor([0.1888, 0.4315, 0.0213, 0.1917, 0.2849])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0984, 0.0141, 0.2740, 0.2379, 0.2180, 0.0861, 0.0116]) \n",
      "Test Loss tensor([0.0905, 0.0144, 0.2831, 0.2537, 0.2185, 0.0883, 0.0112])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0287, 0.2236, 0.0447, 0.0805, 0.2708, 0.2524, 0.0486]) \n",
      "Test Loss tensor([0.0328, 0.2220, 0.0468, 0.0774, 0.2683, 0.2511, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0319, 0.0337, 0.0406, 0.0260, 0.0211, 0.0149, 0.1186, 0.0035]) \n",
      "Test Loss tensor([0.0283, 0.0241, 0.0408, 0.0195, 0.0204, 0.0125, 0.1191, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6007, 0.6525, 0.1026, 0.2032, 0.3698, 0.5805])\n",
      "Valid Idx 3 | Loss tensor([0.5645, 0.4062, 0.1120, 0.8225, 0.8720])\n",
      "\n",
      "************** Batch 596 in 4.96930456161499 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0586, 0.1945, 0.0763, 0.1025]) \n",
      "Test Loss tensor([0.0557, 0.1994, 0.0765, 0.0996])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1795, 0.4142, 0.0162, 0.1933, 0.2724]) \n",
      "Test Loss tensor([0.1865, 0.4268, 0.0232, 0.1923, 0.2826])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0871, 0.0148, 0.2855, 0.2498, 0.2224, 0.0874, 0.0122]) \n",
      "Test Loss tensor([0.0912, 0.0142, 0.2820, 0.2507, 0.2195, 0.0862, 0.0109])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0372, 0.2247, 0.0461, 0.0747, 0.2666, 0.2498, 0.0522]) \n",
      "Test Loss tensor([0.0341, 0.2221, 0.0451, 0.0769, 0.2710, 0.2490, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0259, 0.0215, 0.0399, 0.0179, 0.0191, 0.0150, 0.1240, 0.0036]) \n",
      "Test Loss tensor([0.0283, 0.0240, 0.0409, 0.0203, 0.0207, 0.0133, 0.1203, 0.0036])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5987, 0.6565, 0.1035, 0.2038, 0.3739, 0.5750])\n",
      "Valid Idx 3 | Loss tensor([0.5514, 0.3900, 0.1138, 0.8170, 0.8691])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 600 in 4.883480548858643 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0604, 0.2094, 0.0750, 0.1068]) \n",
      "Test Loss tensor([0.0569, 0.1981, 0.0761, 0.1042])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1856, 0.4317, 0.0245, 0.1986, 0.2810]) \n",
      "Test Loss tensor([0.1824, 0.4318, 0.0288, 0.1943, 0.2707])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0998, 0.0150, 0.2826, 0.2446, 0.2223, 0.0994, 0.0128]) \n",
      "Test Loss tensor([0.0957, 0.0129, 0.2821, 0.2448, 0.2207, 0.0886, 0.0125])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0368, 0.2340, 0.0445, 0.0783, 0.2736, 0.2463, 0.0522]) \n",
      "Test Loss tensor([0.0322, 0.2180, 0.0451, 0.0736, 0.2724, 0.2488, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0307, 0.0198, 0.0417, 0.0170, 0.0200, 0.0140, 0.1207, 0.0030]) \n",
      "Test Loss tensor([0.0292, 0.0257, 0.0387, 0.0190, 0.0192, 0.0141, 0.1180, 0.0038])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5966, 0.6679, 0.1012, 0.1970, 0.3759, 0.5700])\n",
      "Valid Idx 3 | Loss tensor([0.5116, 0.3358, 0.1129, 0.8038, 0.8454])\n",
      "Gradients: Input 0.2669673264026642 | Message 0.36367976665496826 | Update 0.37295272946357727 | Output 0.03882988542318344\n",
      "\n",
      "************** Batch 604 in 4.873312711715698 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0580, 0.1971, 0.0772, 0.1064]) \n",
      "Test Loss tensor([0.0560, 0.1990, 0.0767, 0.1018])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1829, 0.4379, 0.0312, 0.1903, 0.2637]) \n",
      "Test Loss tensor([0.1835, 0.4245, 0.0251, 0.1933, 0.2754])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0926, 0.0136, 0.3027, 0.2407, 0.2192, 0.0850, 0.0120]) \n",
      "Test Loss tensor([0.0911, 0.0137, 0.2821, 0.2471, 0.2171, 0.0891, 0.0122])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0332, 0.2281, 0.0409, 0.0782, 0.2742, 0.2501, 0.0520]) \n",
      "Test Loss tensor([0.0345, 0.2226, 0.0463, 0.0770, 0.2701, 0.2501, 0.0511])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0285, 0.0243, 0.0409, 0.0193, 0.0206, 0.0139, 0.1205, 0.0041]) \n",
      "Test Loss tensor([0.0291, 0.0245, 0.0412, 0.0200, 0.0201, 0.0136, 0.1191, 0.0037])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5989, 0.6558, 0.1035, 0.2032, 0.3737, 0.5821])\n",
      "Valid Idx 3 | Loss tensor([0.5574, 0.3784, 0.1120, 0.8157, 0.8564])\n",
      "\n",
      "************** Batch 608 in 4.88601279258728 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0590, 0.1765, 0.0754, 0.1031]) \n",
      "Test Loss tensor([0.0561, 0.1965, 0.0769, 0.1020])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1851, 0.4085, 0.0271, 0.1865, 0.2780]) \n",
      "Test Loss tensor([0.1878, 0.4278, 0.0238, 0.1936, 0.2788])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0901, 0.0134, 0.2906, 0.2543, 0.2258, 0.0885, 0.0144]) \n",
      "Test Loss tensor([0.0884, 0.0142, 0.2812, 0.2505, 0.2181, 0.0905, 0.0119])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0335, 0.2154, 0.0472, 0.0722, 0.2638, 0.2511, 0.0487]) \n",
      "Test Loss tensor([0.0350, 0.2215, 0.0461, 0.0778, 0.2681, 0.2505, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0333, 0.0281, 0.0453, 0.0207, 0.0207, 0.0137, 0.1184, 0.0036]) \n",
      "Test Loss tensor([0.0285, 0.0239, 0.0412, 0.0188, 0.0213, 0.0128, 0.1192, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6015, 0.6460, 0.1001, 0.2082, 0.3660, 0.5917])\n",
      "Valid Idx 3 | Loss tensor([0.6009, 0.4121, 0.1146, 0.8217, 0.8628])\n",
      "\n",
      "************** Batch 612 in 4.967912197113037 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0546, 0.1900, 0.0752, 0.1087]) \n",
      "Test Loss tensor([0.0562, 0.1961, 0.0755, 0.1086])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1855, 0.4213, 0.0256, 0.1944, 0.2833]) \n",
      "Test Loss tensor([0.1856, 0.4263, 0.0323, 0.1905, 0.2673])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0944, 0.0146, 0.2909, 0.2498, 0.2164, 0.0953, 0.0106]) \n",
      "Test Loss tensor([0.0952, 0.0132, 0.2833, 0.2470, 0.2255, 0.0883, 0.0147])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0378, 0.2162, 0.0395, 0.0750, 0.2701, 0.2500, 0.0445]) \n",
      "Test Loss tensor([0.0325, 0.2220, 0.0437, 0.0752, 0.2668, 0.2490, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0233, 0.0200, 0.0410, 0.0182, 0.0198, 0.0123, 0.1219, 0.0035]) \n",
      "Test Loss tensor([0.0304, 0.0239, 0.0395, 0.0192, 0.0196, 0.0152, 0.1167, 0.0042])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5963, 0.6689, 0.1052, 0.1977, 0.3720, 0.5680])\n",
      "Valid Idx 3 | Loss tensor([0.5222, 0.3215, 0.1119, 0.8023, 0.8308])\n",
      "\n",
      "************** Batch 616 in 5.07885217666626 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0588, 0.2071, 0.0687, 0.1076]) \n",
      "Test Loss tensor([0.0588, 0.2030, 0.0760, 0.0998])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1877, 0.4244, 0.0326, 0.1862, 0.2690]) \n",
      "Test Loss tensor([0.1916, 0.4229, 0.0186, 0.1938, 0.2861])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0893, 0.0116, 0.2685, 0.2418, 0.2240, 0.0836, 0.0157]) \n",
      "Test Loss tensor([0.0866, 0.0164, 0.2862, 0.2595, 0.2151, 0.0929, 0.0105])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0317, 0.2110, 0.0409, 0.0716, 0.2622, 0.2346, 0.0518]) \n",
      "Test Loss tensor([0.0388, 0.2192, 0.0489, 0.0755, 0.2703, 0.2466, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0297, 0.0241, 0.0384, 0.0182, 0.0190, 0.0128, 0.1123, 0.0047]) \n",
      "Test Loss tensor([0.0255, 0.0249, 0.0411, 0.0201, 0.0224, 0.0107, 0.1188, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6114, 0.6285, 0.1024, 0.2113, 0.3660, 0.6092])\n",
      "Valid Idx 3 | Loss tensor([0.6813, 0.5147, 0.1140, 0.8440, 0.8839])\n",
      "\n",
      "************** Batch 620 in 4.978434324264526 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0586, 0.1912, 0.0775, 0.0979]) \n",
      "Test Loss tensor([0.0575, 0.1985, 0.0759, 0.1053])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1911, 0.4210, 0.0226, 0.1953, 0.2885]) \n",
      "Test Loss tensor([0.1861, 0.4256, 0.0283, 0.1961, 0.2702])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0860, 0.0173, 0.2832, 0.2591, 0.2048, 0.0922, 0.0083]) \n",
      "Test Loss tensor([0.0917, 0.0126, 0.2826, 0.2471, 0.2263, 0.0861, 0.0128])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0387, 0.2189, 0.0423, 0.0759, 0.2643, 0.2444, 0.0509]) \n",
      "Test Loss tensor([0.0328, 0.2236, 0.0467, 0.0766, 0.2691, 0.2514, 0.0500])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0269, 0.0245, 0.0417, 0.0203, 0.0225, 0.0114, 0.1100, 0.0032]) \n",
      "Test Loss tensor([0.0288, 0.0234, 0.0400, 0.0198, 0.0183, 0.0148, 0.1181, 0.0036])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5969, 0.6784, 0.1032, 0.1958, 0.3767, 0.5692])\n",
      "Valid Idx 3 | Loss tensor([0.5328, 0.3558, 0.1133, 0.8027, 0.8421])\n",
      "Gradients: Input 1.983672022819519 | Message 3.0305821895599365 | Update 3.098104238510132 | Output 0.35414499044418335\n",
      "\n",
      "************** Batch 624 in 4.866329193115234 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0562, 0.1913, 0.0771, 0.1074]) \n",
      "Test Loss tensor([0.0563, 0.1995, 0.0753, 0.1013])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1894, 0.4089, 0.0279, 0.1917, 0.2776]) \n",
      "Test Loss tensor([0.1872, 0.4295, 0.0242, 0.1922, 0.2753])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0901, 0.0130, 0.2850, 0.2328, 0.2221, 0.0847, 0.0128]) \n",
      "Test Loss tensor([0.0892, 0.0140, 0.2858, 0.2514, 0.2229, 0.0858, 0.0125])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0351, 0.2264, 0.0410, 0.0831, 0.2638, 0.2627, 0.0452]) \n",
      "Test Loss tensor([0.0335, 0.2216, 0.0443, 0.0761, 0.2709, 0.2530, 0.0481])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0273, 0.0228, 0.0387, 0.0201, 0.0197, 0.0150, 0.1167, 0.0043]) \n",
      "Test Loss tensor([0.0286, 0.0247, 0.0399, 0.0198, 0.0190, 0.0132, 0.1173, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6016, 0.6697, 0.1008, 0.1979, 0.3769, 0.5809])\n",
      "Valid Idx 3 | Loss tensor([0.5777, 0.4171, 0.1125, 0.8168, 0.8605])\n",
      "\n",
      "************** Batch 628 in 5.020295858383179 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0583, 0.2021, 0.0731, 0.1006]) \n",
      "Test Loss tensor([0.0575, 0.2005, 0.0759, 0.1018])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1941, 0.4118, 0.0243, 0.1934, 0.2762]) \n",
      "Test Loss tensor([0.1848, 0.4264, 0.0249, 0.1950, 0.2756])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0892, 0.0146, 0.2907, 0.2474, 0.2258, 0.0886, 0.0092]) \n",
      "Test Loss tensor([0.0879, 0.0139, 0.2781, 0.2454, 0.2199, 0.0852, 0.0109])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0300, 0.2300, 0.0424, 0.0815, 0.2668, 0.2462, 0.0520]) \n",
      "Test Loss tensor([0.0349, 0.2193, 0.0468, 0.0765, 0.2704, 0.2502, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0247, 0.0450, 0.0152, 0.0201, 0.0130, 0.1240, 0.0038]) \n",
      "Test Loss tensor([0.0283, 0.0243, 0.0402, 0.0188, 0.0189, 0.0131, 0.1168, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6023, 0.6678, 0.0982, 0.1942, 0.3725, 0.5841])\n",
      "Valid Idx 3 | Loss tensor([0.5798, 0.4153, 0.1101, 0.8183, 0.8639])\n",
      "\n",
      "************** Batch 632 in 4.973187685012817 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0540, 0.1953, 0.0755, 0.1007]) \n",
      "Test Loss tensor([0.0579, 0.1975, 0.0746, 0.1010])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1874, 0.4329, 0.0258, 0.1815, 0.2698]) \n",
      "Test Loss tensor([0.1875, 0.4171, 0.0247, 0.1941, 0.2723])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0940, 0.0142, 0.2825, 0.2589, 0.2190, 0.0876, 0.0117]) \n",
      "Test Loss tensor([0.0896, 0.0142, 0.2809, 0.2475, 0.2189, 0.0865, 0.0120])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0345, 0.2258, 0.0420, 0.0804, 0.2678, 0.2437, 0.0469]) \n",
      "Test Loss tensor([0.0353, 0.2258, 0.0461, 0.0760, 0.2708, 0.2493, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0311, 0.0232, 0.0380, 0.0193, 0.0185, 0.0124, 0.1171, 0.0030]) \n",
      "Test Loss tensor([0.0290, 0.0243, 0.0411, 0.0201, 0.0189, 0.0129, 0.1194, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6010, 0.6659, 0.1013, 0.1991, 0.3727, 0.5846])\n",
      "Valid Idx 3 | Loss tensor([0.5993, 0.4283, 0.1139, 0.8247, 0.8662])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 636 in 4.928152322769165 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0562, 0.2047, 0.0749, 0.1023]) \n",
      "Test Loss tensor([0.0565, 0.1995, 0.0727, 0.1049])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1864, 0.4035, 0.0268, 0.1893, 0.2791]) \n",
      "Test Loss tensor([0.1863, 0.4202, 0.0267, 0.1976, 0.2705])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0862, 0.0133, 0.2813, 0.2538, 0.2216, 0.0881, 0.0111]) \n",
      "Test Loss tensor([0.0904, 0.0137, 0.2807, 0.2447, 0.2237, 0.0856, 0.0132])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0324, 0.2266, 0.0477, 0.0838, 0.2604, 0.2512, 0.0490]) \n",
      "Test Loss tensor([0.0346, 0.2209, 0.0456, 0.0758, 0.2704, 0.2479, 0.0501])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0274, 0.0258, 0.0429, 0.0222, 0.0202, 0.0135, 0.1124, 0.0037]) \n",
      "Test Loss tensor([0.0286, 0.0238, 0.0397, 0.0198, 0.0197, 0.0142, 0.1183, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6005, 0.6654, 0.1025, 0.1971, 0.3754, 0.5738])\n",
      "Valid Idx 3 | Loss tensor([0.5749, 0.3886, 0.1103, 0.8153, 0.8491])\n",
      "\n",
      "************** Batch 640 in 4.902262926101685 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0544, 0.1850, 0.0718, 0.0995]) \n",
      "Test Loss tensor([0.0577, 0.2017, 0.0731, 0.0993])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1897, 0.4160, 0.0279, 0.1931, 0.2765]) \n",
      "Test Loss tensor([0.1857, 0.4199, 0.0248, 0.1961, 0.2727])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0978, 0.0157, 0.2699, 0.2487, 0.2184, 0.0891, 0.0113]) \n",
      "Test Loss tensor([0.0898, 0.0156, 0.2884, 0.2507, 0.2196, 0.0898, 0.0122])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0306, 0.2404, 0.0460, 0.0813, 0.2736, 0.2536, 0.0551]) \n",
      "Test Loss tensor([0.0382, 0.2183, 0.0453, 0.0779, 0.2741, 0.2490, 0.0484])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0284, 0.0245, 0.0387, 0.0223, 0.0184, 0.0159, 0.1200, 0.0036]) \n",
      "Test Loss tensor([0.0276, 0.0249, 0.0421, 0.0204, 0.0215, 0.0119, 0.1216, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6070, 0.6392, 0.1032, 0.2045, 0.3696, 0.5946])\n",
      "Valid Idx 3 | Loss tensor([0.6508, 0.4673, 0.1144, 0.8313, 0.8712])\n",
      "Gradients: Input 0.7758499383926392 | Message 1.0602680444717407 | Update 1.0887866020202637 | Output 0.090578593313694\n",
      "\n",
      "************** Batch 644 in 4.847136735916138 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0529, 0.2005, 0.0810, 0.1011]) \n",
      "Test Loss tensor([0.0554, 0.1965, 0.0733, 0.1043])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1935, 0.4339, 0.0253, 0.1917, 0.2829]) \n",
      "Test Loss tensor([0.1828, 0.4157, 0.0267, 0.1935, 0.2706])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0831, 0.0164, 0.2845, 0.2545, 0.2246, 0.0880, 0.0106]) \n",
      "Test Loss tensor([0.0922, 0.0139, 0.2838, 0.2466, 0.2190, 0.0873, 0.0131])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0321, 0.2279, 0.0473, 0.0777, 0.2707, 0.2412, 0.0489]) \n",
      "Test Loss tensor([0.0351, 0.2217, 0.0465, 0.0774, 0.2751, 0.2471, 0.0509])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0304, 0.0233, 0.0453, 0.0223, 0.0225, 0.0120, 0.1178, 0.0033]) \n",
      "Test Loss tensor([0.0292, 0.0259, 0.0391, 0.0188, 0.0195, 0.0132, 0.1193, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6000, 0.6573, 0.1013, 0.2009, 0.3736, 0.5860])\n",
      "Valid Idx 3 | Loss tensor([0.6043, 0.4067, 0.1096, 0.8223, 0.8620])\n",
      "\n",
      "************** Batch 648 in 4.909430980682373 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0560, 0.1933, 0.0743, 0.1015]) \n",
      "Test Loss tensor([0.0547, 0.1993, 0.0732, 0.1009])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1885, 0.4212, 0.0259, 0.2015, 0.2697]) \n",
      "Test Loss tensor([0.1889, 0.4151, 0.0248, 0.1950, 0.2718])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0915, 0.0147, 0.2827, 0.2412, 0.2226, 0.0877, 0.0134]) \n",
      "Test Loss tensor([0.0907, 0.0141, 0.2835, 0.2482, 0.2178, 0.0864, 0.0108])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0369, 0.2270, 0.0452, 0.0807, 0.2683, 0.2415, 0.0478]) \n",
      "Test Loss tensor([0.0345, 0.2210, 0.0443, 0.0745, 0.2690, 0.2483, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0281, 0.0216, 0.0327, 0.0170, 0.0185, 0.0124, 0.1175, 0.0033]) \n",
      "Test Loss tensor([0.0276, 0.0235, 0.0397, 0.0192, 0.0196, 0.0124, 0.1179, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6015, 0.6547, 0.1008, 0.2042, 0.3720, 0.5929])\n",
      "Valid Idx 3 | Loss tensor([0.6211, 0.4240, 0.1117, 0.8267, 0.8631])\n",
      "\n",
      "************** Batch 652 in 4.860334873199463 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0521, 0.1946, 0.0693, 0.1010]) \n",
      "Test Loss tensor([0.0552, 0.2000, 0.0763, 0.1026])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1906, 0.4017, 0.0271, 0.1894, 0.2743]) \n",
      "Test Loss tensor([0.1846, 0.4147, 0.0232, 0.1940, 0.2740])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0916, 0.0153, 0.2852, 0.2530, 0.2291, 0.0866, 0.0144]) \n",
      "Test Loss tensor([0.0904, 0.0144, 0.2811, 0.2475, 0.2194, 0.0889, 0.0115])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0363, 0.2222, 0.0450, 0.0767, 0.2680, 0.2542, 0.0475]) \n",
      "Test Loss tensor([0.0347, 0.2250, 0.0456, 0.0746, 0.2684, 0.2480, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0257, 0.0254, 0.0376, 0.0183, 0.0190, 0.0128, 0.1172, 0.0037]) \n",
      "Test Loss tensor([0.0283, 0.0239, 0.0390, 0.0202, 0.0193, 0.0120, 0.1155, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6037, 0.6636, 0.0981, 0.2013, 0.3704, 0.5960])\n",
      "Valid Idx 3 | Loss tensor([0.6246, 0.4272, 0.1120, 0.8295, 0.8689])\n",
      "\n",
      "************** Batch 656 in 4.840872049331665 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0543, 0.1946, 0.0708, 0.0974]) \n",
      "Test Loss tensor([0.0559, 0.2021, 0.0768, 0.1045])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1905, 0.4178, 0.0236, 0.1915, 0.2762]) \n",
      "Test Loss tensor([0.1830, 0.4149, 0.0271, 0.1906, 0.2682])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0877, 0.0168, 0.2949, 0.2556, 0.2202, 0.0827, 0.0114]) \n",
      "Test Loss tensor([0.0931, 0.0126, 0.2834, 0.2472, 0.2236, 0.0886, 0.0120])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0344, 0.2205, 0.0441, 0.0798, 0.2780, 0.2492, 0.0519]) \n",
      "Test Loss tensor([0.0324, 0.2245, 0.0438, 0.0747, 0.2674, 0.2526, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0280, 0.0243, 0.0432, 0.0200, 0.0180, 0.0127, 0.1172, 0.0029]) \n",
      "Test Loss tensor([0.0289, 0.0249, 0.0395, 0.0188, 0.0179, 0.0146, 0.1181, 0.0037])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6011, 0.6661, 0.1025, 0.1969, 0.3705, 0.5767])\n",
      "Valid Idx 3 | Loss tensor([0.5754, 0.3519, 0.1105, 0.8091, 0.8499])\n",
      "\n",
      "************** Batch 660 in 4.887503385543823 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0473, 0.2059, 0.0765, 0.1047]) \n",
      "Test Loss tensor([0.0561, 0.1994, 0.0744, 0.0987])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1793, 0.4118, 0.0256, 0.1899, 0.2691]) \n",
      "Test Loss tensor([0.1899, 0.4150, 0.0222, 0.1914, 0.2725])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0959, 0.0127, 0.2902, 0.2415, 0.2229, 0.0939, 0.0137]) \n",
      "Test Loss tensor([0.0870, 0.0139, 0.2864, 0.2488, 0.2202, 0.0880, 0.0113])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0352, 0.2288, 0.0457, 0.0697, 0.2589, 0.2522, 0.0549]) \n",
      "Test Loss tensor([0.0369, 0.2216, 0.0460, 0.0768, 0.2686, 0.2510, 0.0490])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0233, 0.0385, 0.0175, 0.0180, 0.0134, 0.1097, 0.0030]) \n",
      "Test Loss tensor([0.0262, 0.0239, 0.0411, 0.0181, 0.0201, 0.0122, 0.1161, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6062, 0.6413, 0.1012, 0.2084, 0.3645, 0.5984])\n",
      "Valid Idx 3 | Loss tensor([0.6619, 0.4266, 0.1117, 0.8321, 0.8688])\n",
      "Gradients: Input 0.8708888292312622 | Message 1.3361455202102661 | Update 1.365096926689148 | Output 0.15403775870800018\n",
      "\n",
      "************** Batch 664 in 4.925776243209839 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0515, 0.1993, 0.0791, 0.0987]) \n",
      "Test Loss tensor([0.0550, 0.1988, 0.0761, 0.1005])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1823, 0.3992, 0.0238, 0.1805, 0.2612]) \n",
      "Test Loss tensor([0.1859, 0.4160, 0.0270, 0.1903, 0.2696])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0907, 0.0158, 0.2852, 0.2544, 0.2128, 0.0871, 0.0129]) \n",
      "Test Loss tensor([0.0918, 0.0134, 0.2826, 0.2476, 0.2219, 0.0888, 0.0118])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0321, 0.2247, 0.0446, 0.0883, 0.2722, 0.2559, 0.0476]) \n",
      "Test Loss tensor([0.0359, 0.2218, 0.0454, 0.0777, 0.2706, 0.2515, 0.0495])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0333, 0.0293, 0.0387, 0.0202, 0.0200, 0.0117, 0.1162, 0.0028]) \n",
      "Test Loss tensor([0.0279, 0.0240, 0.0393, 0.0191, 0.0188, 0.0128, 0.1155, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6017, 0.6432, 0.1049, 0.2057, 0.3698, 0.5849])\n",
      "Valid Idx 3 | Loss tensor([0.6231, 0.3570, 0.1101, 0.8188, 0.8531])\n",
      "\n",
      "************** Batch 668 in 5.024825096130371 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0519, 0.2009, 0.0758, 0.1015]) \n",
      "Test Loss tensor([0.0548, 0.1985, 0.0755, 0.0994])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1784, 0.4014, 0.0235, 0.1877, 0.2757]) \n",
      "Test Loss tensor([0.1886, 0.4113, 0.0265, 0.1904, 0.2718])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0877, 0.0130, 0.2888, 0.2564, 0.2309, 0.0870, 0.0116]) \n",
      "Test Loss tensor([0.0885, 0.0131, 0.2810, 0.2468, 0.2231, 0.0858, 0.0117])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0343, 0.2212, 0.0433, 0.0734, 0.2657, 0.2401, 0.0454]) \n",
      "Test Loss tensor([0.0367, 0.2216, 0.0439, 0.0739, 0.2665, 0.2465, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0284, 0.0205, 0.0429, 0.0207, 0.0178, 0.0117, 0.1194, 0.0036]) \n",
      "Test Loss tensor([0.0266, 0.0242, 0.0418, 0.0183, 0.0184, 0.0123, 0.1180, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6095, 0.6347, 0.1019, 0.2070, 0.3690, 0.5857])\n",
      "Valid Idx 3 | Loss tensor([0.6450, 0.3667, 0.1101, 0.8294, 0.8543])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 672 in 4.992272138595581 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0585, 0.2008, 0.0767, 0.0996]) \n",
      "Test Loss tensor([0.0571, 0.2012, 0.0749, 0.1018])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1850, 0.4190, 0.0272, 0.1841, 0.2725]) \n",
      "Test Loss tensor([0.1898, 0.4101, 0.0233, 0.1917, 0.2712])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0877, 0.0145, 0.2842, 0.2402, 0.2035, 0.0807, 0.0101]) \n",
      "Test Loss tensor([0.0866, 0.0137, 0.2814, 0.2449, 0.2187, 0.0870, 0.0117])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0366, 0.2138, 0.0451, 0.0794, 0.2691, 0.2432, 0.0493]) \n",
      "Test Loss tensor([0.0384, 0.2234, 0.0466, 0.0776, 0.2714, 0.2487, 0.0512])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0225, 0.0236, 0.0381, 0.0172, 0.0197, 0.0127, 0.1132, 0.0034]) \n",
      "Test Loss tensor([0.0283, 0.0256, 0.0396, 0.0194, 0.0186, 0.0117, 0.1168, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6006, 0.6336, 0.1000, 0.2079, 0.3683, 0.5895])\n",
      "Valid Idx 3 | Loss tensor([0.6699, 0.3918, 0.1117, 0.8354, 0.8626])\n",
      "\n",
      "************** Batch 676 in 4.945788621902466 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0580, 0.1902, 0.0745, 0.1030]) \n",
      "Test Loss tensor([0.0548, 0.1992, 0.0745, 0.1012])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1874, 0.4188, 0.0254, 0.1823, 0.2708]) \n",
      "Test Loss tensor([0.1856, 0.4122, 0.0273, 0.1937, 0.2728])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0814, 0.0125, 0.2894, 0.2434, 0.2205, 0.0863, 0.0128]) \n",
      "Test Loss tensor([0.0903, 0.0131, 0.2859, 0.2481, 0.2232, 0.0861, 0.0128])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0344, 0.2193, 0.0414, 0.0713, 0.2695, 0.2492, 0.0429]) \n",
      "Test Loss tensor([0.0352, 0.2222, 0.0461, 0.0749, 0.2682, 0.2500, 0.0498])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0281, 0.0227, 0.0446, 0.0205, 0.0202, 0.0130, 0.1167, 0.0029]) \n",
      "Test Loss tensor([0.0283, 0.0249, 0.0399, 0.0192, 0.0178, 0.0131, 0.1185, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5964, 0.6542, 0.1021, 0.2025, 0.3738, 0.5767])\n",
      "Valid Idx 3 | Loss tensor([0.6132, 0.3383, 0.1124, 0.8201, 0.8507])\n",
      "\n",
      "************** Batch 680 in 4.971518516540527 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0576, 0.2024, 0.0725, 0.1001]) \n",
      "Test Loss tensor([0.0552, 0.1978, 0.0751, 0.0988])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1917, 0.3986, 0.0253, 0.1891, 0.2703]) \n",
      "Test Loss tensor([0.1867, 0.4073, 0.0220, 0.1881, 0.2732])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0862, 0.0128, 0.2707, 0.2390, 0.2086, 0.0766, 0.0122]) \n",
      "Test Loss tensor([0.0884, 0.0135, 0.2813, 0.2491, 0.2211, 0.0862, 0.0115])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0286, 0.2173, 0.0427, 0.0763, 0.2734, 0.2470, 0.0478]) \n",
      "Test Loss tensor([0.0363, 0.2177, 0.0448, 0.0745, 0.2677, 0.2516, 0.0478])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0303, 0.0251, 0.0424, 0.0181, 0.0172, 0.0142, 0.1145, 0.0036]) \n",
      "Test Loss tensor([0.0272, 0.0235, 0.0391, 0.0190, 0.0185, 0.0125, 0.1175, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5974, 0.6496, 0.1003, 0.2039, 0.3678, 0.5861])\n",
      "Valid Idx 3 | Loss tensor([0.6455, 0.3869, 0.1109, 0.8248, 0.8627])\n",
      "Gradients: Input 0.5253298282623291 | Message 0.8129091858863831 | Update 0.8129287958145142 | Output 0.08302053809165955\n",
      "\n",
      "************** Batch 684 in 4.906748056411743 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0570, 0.1995, 0.0748, 0.1021]) \n",
      "Test Loss tensor([0.0554, 0.1977, 0.0740, 0.1006])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1868, 0.4207, 0.0242, 0.1937, 0.2732]) \n",
      "Test Loss tensor([0.1857, 0.4084, 0.0216, 0.1890, 0.2766])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0842, 0.0119, 0.2785, 0.2358, 0.2262, 0.0901, 0.0122]) \n",
      "Test Loss tensor([0.0901, 0.0146, 0.2842, 0.2523, 0.2193, 0.0876, 0.0106])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0392, 0.2151, 0.0484, 0.0784, 0.2742, 0.2478, 0.0458]) \n",
      "Test Loss tensor([0.0346, 0.2222, 0.0464, 0.0758, 0.2679, 0.2487, 0.0500])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0280, 0.0212, 0.0377, 0.0197, 0.0171, 0.0133, 0.1175, 0.0031]) \n",
      "Test Loss tensor([0.0273, 0.0251, 0.0398, 0.0193, 0.0191, 0.0119, 0.1177, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5976, 0.6487, 0.1001, 0.2068, 0.3688, 0.5990])\n",
      "Valid Idx 3 | Loss tensor([0.6679, 0.4272, 0.1135, 0.8267, 0.8784])\n",
      "\n",
      "************** Batch 688 in 5.040794610977173 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0566, 0.1957, 0.0724, 0.1015]) \n",
      "Test Loss tensor([0.0561, 0.1985, 0.0767, 0.1018])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1850, 0.3999, 0.0203, 0.1812, 0.2858]) \n",
      "Test Loss tensor([0.1821, 0.4098, 0.0272, 0.1920, 0.2674])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0842, 0.0161, 0.2979, 0.2530, 0.2221, 0.0840, 0.0098]) \n",
      "Test Loss tensor([0.0952, 0.0119, 0.2786, 0.2440, 0.2205, 0.0838, 0.0121])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0344, 0.2126, 0.0441, 0.0711, 0.2769, 0.2434, 0.0488]) \n",
      "Test Loss tensor([0.0343, 0.2181, 0.0444, 0.0747, 0.2683, 0.2476, 0.0472])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0290, 0.0245, 0.0341, 0.0175, 0.0179, 0.0128, 0.1126, 0.0036]) \n",
      "Test Loss tensor([0.0313, 0.0246, 0.0392, 0.0199, 0.0182, 0.0144, 0.1192, 0.0036])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5900, 0.6738, 0.1008, 0.2007, 0.3749, 0.5758])\n",
      "Valid Idx 3 | Loss tensor([0.5695, 0.3368, 0.1115, 0.8045, 0.8460])\n",
      "\n",
      "************** Batch 692 in 5.020063877105713 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0530, 0.1998, 0.0797, 0.1028]) \n",
      "Test Loss tensor([0.0559, 0.1954, 0.0748, 0.0988])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1894, 0.4052, 0.0295, 0.1931, 0.2760]) \n",
      "Test Loss tensor([0.1845, 0.4007, 0.0193, 0.1936, 0.2823])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0907, 0.0142, 0.2714, 0.2469, 0.2261, 0.0859, 0.0119]) \n",
      "Test Loss tensor([0.0895, 0.0155, 0.2846, 0.2571, 0.2156, 0.0871, 0.0120])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0321, 0.2269, 0.0479, 0.0689, 0.2698, 0.2504, 0.0523]) \n",
      "Test Loss tensor([0.0369, 0.2203, 0.0466, 0.0758, 0.2702, 0.2470, 0.0507])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0270, 0.0249, 0.0366, 0.0202, 0.0183, 0.0147, 0.1177, 0.0029]) \n",
      "Test Loss tensor([0.0270, 0.0233, 0.0418, 0.0195, 0.0202, 0.0108, 0.1178, 0.0029])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6009, 0.6421, 0.0998, 0.2096, 0.3667, 0.6162])\n",
      "Valid Idx 3 | Loss tensor([0.6962, 0.4751, 0.1127, 0.8308, 0.8870])\n",
      "\n",
      "************** Batch 696 in 5.159198522567749 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0495, 0.1903, 0.0767, 0.0999]) \n",
      "Test Loss tensor([0.0551, 0.1989, 0.0752, 0.1008])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1966, 0.3971, 0.0211, 0.1902, 0.2842]) \n",
      "Test Loss tensor([0.1807, 0.4072, 0.0272, 0.1926, 0.2690])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0885, 0.0177, 0.2807, 0.2427, 0.2149, 0.0910, 0.0105]) \n",
      "Test Loss tensor([0.0942, 0.0125, 0.2817, 0.2440, 0.2220, 0.0856, 0.0118])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0396, 0.2140, 0.0476, 0.0722, 0.2631, 0.2493, 0.0470]) \n",
      "Test Loss tensor([0.0350, 0.2231, 0.0440, 0.0761, 0.2697, 0.2484, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0293, 0.0240, 0.0421, 0.0187, 0.0200, 0.0122, 0.1206, 0.0030]) \n",
      "Test Loss tensor([0.0291, 0.0237, 0.0397, 0.0197, 0.0181, 0.0135, 0.1177, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5916, 0.6677, 0.1035, 0.2052, 0.3780, 0.5923])\n",
      "Valid Idx 3 | Loss tensor([0.6119, 0.3731, 0.1069, 0.8073, 0.8620])\n",
      "\n",
      "************** Batch 700 in 4.999568462371826 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0527, 0.2007, 0.0722, 0.1053]) \n",
      "Test Loss tensor([0.0544, 0.1976, 0.0743, 0.0979])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1789, 0.4022, 0.0270, 0.1899, 0.2816]) \n",
      "Test Loss tensor([0.1844, 0.4005, 0.0235, 0.1916, 0.2731])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0934, 0.0137, 0.2884, 0.2474, 0.2205, 0.0875, 0.0146]) \n",
      "Test Loss tensor([0.0902, 0.0143, 0.2837, 0.2480, 0.2189, 0.0849, 0.0111])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0348, 0.2221, 0.0468, 0.0779, 0.2622, 0.2421, 0.0510]) \n",
      "Test Loss tensor([0.0354, 0.2227, 0.0456, 0.0766, 0.2691, 0.2485, 0.0487])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0300, 0.0268, 0.0374, 0.0189, 0.0173, 0.0125, 0.1165, 0.0030]) \n",
      "Test Loss tensor([0.0265, 0.0236, 0.0417, 0.0180, 0.0198, 0.0125, 0.1150, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6026, 0.6518, 0.1036, 0.2113, 0.3652, 0.5988])\n",
      "Valid Idx 3 | Loss tensor([0.6768, 0.4264, 0.1087, 0.8245, 0.8743])\n",
      "Gradients: Input 0.9382360577583313 | Message 1.3529114723205566 | Update 1.4001706838607788 | Output 0.14180709421634674\n",
      "\n",
      "************** Batch 704 in 5.068034410476685 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0580, 0.2026, 0.0742, 0.1038]) \n",
      "Test Loss tensor([0.0554, 0.1987, 0.0748, 0.1024])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1888, 0.3957, 0.0233, 0.1945, 0.2825]) \n",
      "Test Loss tensor([0.1855, 0.4063, 0.0235, 0.1938, 0.2706])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0949, 0.0144, 0.2825, 0.2489, 0.2089, 0.0812, 0.0111]) \n",
      "Test Loss tensor([0.0896, 0.0150, 0.2836, 0.2461, 0.2182, 0.0855, 0.0108])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0398, 0.2177, 0.0510, 0.0863, 0.2681, 0.2524, 0.0565]) \n",
      "Test Loss tensor([0.0368, 0.2166, 0.0464, 0.0755, 0.2705, 0.2492, 0.0469])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0274, 0.0291, 0.0389, 0.0189, 0.0172, 0.0123, 0.1087, 0.0034]) \n",
      "Test Loss tensor([0.0274, 0.0234, 0.0402, 0.0187, 0.0199, 0.0118, 0.1167, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6005, 0.6557, 0.0993, 0.2062, 0.3695, 0.6048])\n",
      "Valid Idx 3 | Loss tensor([0.7030, 0.4316, 0.1112, 0.8312, 0.8662])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 708 in 4.98145055770874 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0543, 0.1970, 0.0729, 0.0972]) \n",
      "Test Loss tensor([0.0554, 0.1964, 0.0754, 0.1040])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1929, 0.4152, 0.0249, 0.1846, 0.2794]) \n",
      "Test Loss tensor([0.1841, 0.3973, 0.0257, 0.1905, 0.2686])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0951, 0.0151, 0.2788, 0.2471, 0.2192, 0.0898, 0.0128]) \n",
      "Test Loss tensor([0.0885, 0.0133, 0.2803, 0.2467, 0.2235, 0.0853, 0.0119])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0326, 0.2225, 0.0468, 0.0672, 0.2646, 0.2465, 0.0463]) \n",
      "Test Loss tensor([0.0379, 0.2175, 0.0454, 0.0771, 0.2672, 0.2522, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0274, 0.0254, 0.0435, 0.0197, 0.0208, 0.0128, 0.1201, 0.0030]) \n",
      "Test Loss tensor([0.0288, 0.0250, 0.0412, 0.0187, 0.0178, 0.0136, 0.1195, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6056, 0.6642, 0.0996, 0.2064, 0.3757, 0.5989])\n",
      "Valid Idx 3 | Loss tensor([0.6754, 0.3873, 0.1112, 0.8238, 0.8590])\n",
      "\n",
      "************** Batch 712 in 5.083759546279907 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0502, 0.1936, 0.0737, 0.0999]) \n",
      "Test Loss tensor([0.0550, 0.1976, 0.0763, 0.1034])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1805, 0.4006, 0.0229, 0.1891, 0.2784]) \n",
      "Test Loss tensor([0.1835, 0.3967, 0.0261, 0.1885, 0.2642])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0895, 0.0163, 0.2717, 0.2493, 0.2267, 0.0834, 0.0154]) \n",
      "Test Loss tensor([0.0887, 0.0145, 0.2784, 0.2441, 0.2189, 0.0844, 0.0119])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0364, 0.2317, 0.0463, 0.0870, 0.2710, 0.2653, 0.0588]) \n",
      "Test Loss tensor([0.0386, 0.2238, 0.0456, 0.0739, 0.2688, 0.2511, 0.0483])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0277, 0.0289, 0.0367, 0.0177, 0.0179, 0.0128, 0.1130, 0.0034]) \n",
      "Test Loss tensor([0.0268, 0.0240, 0.0419, 0.0192, 0.0187, 0.0135, 0.1165, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6035, 0.6607, 0.1013, 0.2072, 0.3755, 0.5967])\n",
      "Valid Idx 3 | Loss tensor([0.6983, 0.3994, 0.1120, 0.8286, 0.8556])\n",
      "\n",
      "************** Batch 716 in 5.090815544128418 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0579, 0.2029, 0.0729, 0.1041]) \n",
      "Test Loss tensor([0.0584, 0.2005, 0.0781, 0.0996])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1866, 0.3995, 0.0283, 0.1880, 0.2759]) \n",
      "Test Loss tensor([0.1865, 0.3989, 0.0211, 0.1899, 0.2747])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0879, 0.0145, 0.2891, 0.2550, 0.2152, 0.0869, 0.0138]) \n",
      "Test Loss tensor([0.0854, 0.0174, 0.2820, 0.2495, 0.2180, 0.0865, 0.0119])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0330, 0.2304, 0.0483, 0.0765, 0.2645, 0.2472, 0.0510]) \n",
      "Test Loss tensor([0.0396, 0.2201, 0.0468, 0.0751, 0.2672, 0.2472, 0.0500])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0351, 0.0295, 0.0381, 0.0205, 0.0158, 0.0139, 0.1179, 0.0035]) \n",
      "Test Loss tensor([0.0261, 0.0246, 0.0424, 0.0189, 0.0202, 0.0115, 0.1186, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6077, 0.6393, 0.0982, 0.2158, 0.3661, 0.6163])\n",
      "Valid Idx 3 | Loss tensor([0.7791, 0.4765, 0.1136, 0.8470, 0.8783])\n",
      "\n",
      "************** Batch 720 in 4.924189567565918 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0593, 0.1993, 0.0780, 0.0964]) \n",
      "Test Loss tensor([0.0557, 0.2010, 0.0775, 0.1090])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1869, 0.3993, 0.0248, 0.1804, 0.2751]) \n",
      "Test Loss tensor([0.1814, 0.3997, 0.0329, 0.1915, 0.2617])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0869, 0.0138, 0.2774, 0.2494, 0.2215, 0.0860, 0.0108]) \n",
      "Test Loss tensor([0.0922, 0.0141, 0.2817, 0.2454, 0.2255, 0.0838, 0.0134])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0391, 0.2235, 0.0513, 0.0762, 0.2685, 0.2436, 0.0528]) \n",
      "Test Loss tensor([0.0371, 0.2230, 0.0455, 0.0772, 0.2676, 0.2447, 0.0504])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0265, 0.0275, 0.0403, 0.0174, 0.0187, 0.0120, 0.1181, 0.0027]) \n",
      "Test Loss tensor([0.0285, 0.0261, 0.0391, 0.0192, 0.0172, 0.0147, 0.1172, 0.0037])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6059, 0.6668, 0.1018, 0.2091, 0.3744, 0.5833])\n",
      "Valid Idx 3 | Loss tensor([0.6609, 0.3333, 0.1106, 0.8168, 0.8333])\n",
      "Gradients: Input 1.3004212379455566 | Message 1.9326987266540527 | Update 1.9775580167770386 | Output 0.2124766707420349\n",
      "\n",
      "************** Batch 724 in 5.159461736679077 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0582, 0.1948, 0.0787, 0.1090]) \n",
      "Test Loss tensor([0.0578, 0.1969, 0.0766, 0.1014])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1813, 0.3843, 0.0308, 0.1831, 0.2649]) \n",
      "Test Loss tensor([0.1860, 0.3894, 0.0219, 0.1860, 0.2708])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0865, 0.0158, 0.2795, 0.2401, 0.2215, 0.0821, 0.0112]) \n",
      "Test Loss tensor([0.0857, 0.0161, 0.2810, 0.2499, 0.2172, 0.0857, 0.0112])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0347, 0.2177, 0.0399, 0.0722, 0.2633, 0.2467, 0.0430]) \n",
      "Test Loss tensor([0.0412, 0.2181, 0.0476, 0.0760, 0.2689, 0.2501, 0.0493])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0310, 0.0303, 0.0423, 0.0190, 0.0163, 0.0142, 0.1172, 0.0045]) \n",
      "Test Loss tensor([0.0272, 0.0239, 0.0421, 0.0198, 0.0199, 0.0114, 0.1164, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6088, 0.6332, 0.1020, 0.2164, 0.3668, 0.6167])\n",
      "Valid Idx 3 | Loss tensor([0.7772, 0.4673, 0.1153, 0.8426, 0.8756])\n",
      "\n",
      "************** Batch 728 in 5.161795377731323 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0604, 0.1920, 0.0767, 0.1075]) \n",
      "Test Loss tensor([0.0551, 0.1970, 0.0757, 0.1026])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1886, 0.3934, 0.0232, 0.1990, 0.2750]) \n",
      "Test Loss tensor([0.1831, 0.3987, 0.0264, 0.1919, 0.2643])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0876, 0.0160, 0.2893, 0.2390, 0.2106, 0.0826, 0.0117]) \n",
      "Test Loss tensor([0.0893, 0.0138, 0.2802, 0.2472, 0.2225, 0.0835, 0.0124])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0399, 0.2186, 0.0492, 0.0722, 0.2655, 0.2555, 0.0496]) \n",
      "Test Loss tensor([0.0367, 0.2199, 0.0450, 0.0751, 0.2692, 0.2449, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0219, 0.0219, 0.0364, 0.0173, 0.0198, 0.0130, 0.1169, 0.0033]) \n",
      "Test Loss tensor([0.0278, 0.0258, 0.0403, 0.0190, 0.0178, 0.0126, 0.1149, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6019, 0.6503, 0.1036, 0.2100, 0.3733, 0.5958])\n",
      "Valid Idx 3 | Loss tensor([0.7013, 0.3873, 0.1091, 0.8246, 0.8578])\n",
      "\n",
      "************** Batch 732 in 4.998716354370117 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0522, 0.2008, 0.0670, 0.1022]) \n",
      "Test Loss tensor([0.0569, 0.1996, 0.0754, 0.1003])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1874, 0.3951, 0.0273, 0.1893, 0.2693]) \n",
      "Test Loss tensor([0.1846, 0.3970, 0.0260, 0.1892, 0.2686])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0937, 0.0122, 0.2790, 0.2435, 0.2226, 0.0876, 0.0144]) \n",
      "Test Loss tensor([0.0887, 0.0140, 0.2832, 0.2488, 0.2210, 0.0847, 0.0120])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0344, 0.2209, 0.0420, 0.0777, 0.2665, 0.2403, 0.0575]) \n",
      "Test Loss tensor([0.0355, 0.2218, 0.0459, 0.0742, 0.2691, 0.2489, 0.0493])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0264, 0.0279, 0.0427, 0.0226, 0.0177, 0.0111, 0.1189, 0.0033]) \n",
      "Test Loss tensor([0.0283, 0.0243, 0.0410, 0.0197, 0.0179, 0.0124, 0.1187, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5966, 0.6392, 0.1032, 0.2098, 0.3691, 0.5964])\n",
      "Valid Idx 3 | Loss tensor([0.7068, 0.3963, 0.1113, 0.8196, 0.8657])\n",
      "\n",
      "************** Batch 736 in 4.983672142028809 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0532, 0.1996, 0.0705, 0.1005]) \n",
      "Test Loss tensor([0.0556, 0.1986, 0.0738, 0.1002])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1814, 0.4004, 0.0250, 0.1892, 0.2553]) \n",
      "Test Loss tensor([0.1875, 0.3951, 0.0223, 0.1909, 0.2718])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0896, 0.0135, 0.2847, 0.2313, 0.2256, 0.0837, 0.0122]) \n",
      "Test Loss tensor([0.0873, 0.0145, 0.2818, 0.2503, 0.2153, 0.0848, 0.0113])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0378, 0.2166, 0.0467, 0.0793, 0.2747, 0.2528, 0.0491]) \n",
      "Test Loss tensor([0.0383, 0.2176, 0.0460, 0.0735, 0.2690, 0.2464, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0280, 0.0269, 0.0356, 0.0203, 0.0180, 0.0121, 0.1107, 0.0030]) \n",
      "Test Loss tensor([0.0259, 0.0239, 0.0430, 0.0195, 0.0195, 0.0106, 0.1188, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6028, 0.6241, 0.0998, 0.2133, 0.3683, 0.6135])\n",
      "Valid Idx 3 | Loss tensor([0.7533, 0.4498, 0.1145, 0.8355, 0.8800])\n",
      "\n",
      "************** Batch 740 in 4.98286509513855 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0565, 0.1865, 0.0753, 0.0988]) \n",
      "Test Loss tensor([0.0552, 0.1990, 0.0752, 0.1049])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1839, 0.3848, 0.0248, 0.1786, 0.2733]) \n",
      "Test Loss tensor([0.1825, 0.3971, 0.0288, 0.1907, 0.2621])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0888, 0.0157, 0.2810, 0.2473, 0.2114, 0.0816, 0.0103]) \n",
      "Test Loss tensor([0.0939, 0.0124, 0.2813, 0.2428, 0.2247, 0.0847, 0.0133])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0367, 0.2233, 0.0463, 0.0831, 0.2663, 0.2576, 0.0474]) \n",
      "Test Loss tensor([0.0342, 0.2219, 0.0438, 0.0754, 0.2686, 0.2507, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0252, 0.0254, 0.0439, 0.0180, 0.0195, 0.0118, 0.1169, 0.0028]) \n",
      "Test Loss tensor([0.0285, 0.0254, 0.0388, 0.0185, 0.0168, 0.0140, 0.1162, 0.0035])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5938, 0.6637, 0.1012, 0.2046, 0.3726, 0.5843])\n",
      "Valid Idx 3 | Loss tensor([0.6231, 0.3151, 0.1132, 0.7991, 0.8412])\n",
      "Gradients: Input 1.0730595588684082 | Message 1.7692031860351562 | Update 1.7767292261123657 | Output 0.1899709552526474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 744 in 5.009112358093262 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0563, 0.1951, 0.0711, 0.1097]) \n",
      "Test Loss tensor([0.0558, 0.1990, 0.0748, 0.0993])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1692, 0.3886, 0.0310, 0.1899, 0.2627]) \n",
      "Test Loss tensor([0.1858, 0.3915, 0.0211, 0.1916, 0.2756])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0956, 0.0132, 0.2995, 0.2420, 0.2341, 0.0871, 0.0135]) \n",
      "Test Loss tensor([0.0881, 0.0154, 0.2852, 0.2516, 0.2154, 0.0870, 0.0111])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0325, 0.2239, 0.0389, 0.0788, 0.2644, 0.2396, 0.0480]) \n",
      "Test Loss tensor([0.0398, 0.2144, 0.0465, 0.0757, 0.2687, 0.2471, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0260, 0.0390, 0.0160, 0.0145, 0.0124, 0.1076, 0.0032]) \n",
      "Test Loss tensor([0.0268, 0.0243, 0.0398, 0.0201, 0.0178, 0.0110, 0.1183, 0.0029])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5991, 0.6294, 0.0993, 0.2154, 0.3661, 0.6215])\n",
      "Valid Idx 3 | Loss tensor([0.7553, 0.4601, 0.1127, 0.8313, 0.8897])\n",
      "\n",
      "************** Batch 748 in 5.066482782363892 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0508, 0.1933, 0.0771, 0.0957]) \n",
      "Test Loss tensor([0.0540, 0.1983, 0.0756, 0.1012])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1895, 0.3810, 0.0203, 0.1872, 0.2683]) \n",
      "Test Loss tensor([0.1837, 0.3918, 0.0263, 0.1903, 0.2673])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0865, 0.0164, 0.2847, 0.2470, 0.2123, 0.0838, 0.0123]) \n",
      "Test Loss tensor([0.0877, 0.0132, 0.2821, 0.2449, 0.2189, 0.0848, 0.0116])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0344, 0.2158, 0.0461, 0.0791, 0.2666, 0.2618, 0.0476]) \n",
      "Test Loss tensor([0.0364, 0.2185, 0.0464, 0.0768, 0.2672, 0.2503, 0.0511])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0239, 0.0236, 0.0402, 0.0181, 0.0169, 0.0114, 0.1126, 0.0030]) \n",
      "Test Loss tensor([0.0273, 0.0252, 0.0400, 0.0191, 0.0178, 0.0124, 0.1173, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5961, 0.6479, 0.1012, 0.2080, 0.3695, 0.5998])\n",
      "Valid Idx 3 | Loss tensor([0.6888, 0.3815, 0.1087, 0.8157, 0.8607])\n",
      "\n",
      "************** Batch 752 in 5.035731792449951 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0551, 0.2064, 0.0767, 0.0992]) \n",
      "Test Loss tensor([0.0546, 0.1980, 0.0751, 0.1040])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1836, 0.3795, 0.0260, 0.1888, 0.2750]) \n",
      "Test Loss tensor([0.1827, 0.3930, 0.0264, 0.1879, 0.2622])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0996, 0.0135, 0.2843, 0.2472, 0.2237, 0.0849, 0.0144]) \n",
      "Test Loss tensor([0.0910, 0.0131, 0.2782, 0.2417, 0.2203, 0.0845, 0.0125])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0338, 0.2277, 0.0436, 0.0720, 0.2706, 0.2513, 0.0488]) \n",
      "Test Loss tensor([0.0363, 0.2178, 0.0460, 0.0755, 0.2694, 0.2491, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0237, 0.0243, 0.0360, 0.0201, 0.0178, 0.0110, 0.1118, 0.0029]) \n",
      "Test Loss tensor([0.0280, 0.0248, 0.0377, 0.0192, 0.0174, 0.0132, 0.1158, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6004, 0.6479, 0.1032, 0.2084, 0.3713, 0.6027])\n",
      "Valid Idx 3 | Loss tensor([0.6809, 0.3640, 0.1130, 0.8183, 0.8562])\n",
      "\n",
      "************** Batch 756 in 4.997089147567749 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0617, 0.2041, 0.0820, 0.1056]) \n",
      "Test Loss tensor([0.0542, 0.1979, 0.0749, 0.0996])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1850, 0.4038, 0.0252, 0.1885, 0.2588]) \n",
      "Test Loss tensor([0.1851, 0.3851, 0.0220, 0.1867, 0.2723])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0922, 0.0150, 0.2897, 0.2458, 0.2191, 0.0792, 0.0118]) \n",
      "Test Loss tensor([0.0899, 0.0160, 0.2841, 0.2527, 0.2157, 0.0874, 0.0108])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0391, 0.2323, 0.0450, 0.0740, 0.2585, 0.2412, 0.0457]) \n",
      "Test Loss tensor([0.0393, 0.2198, 0.0478, 0.0751, 0.2677, 0.2429, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0275, 0.0203, 0.0423, 0.0172, 0.0185, 0.0121, 0.1165, 0.0031]) \n",
      "Test Loss tensor([0.0272, 0.0258, 0.0424, 0.0189, 0.0198, 0.0109, 0.1185, 0.0031])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6023, 0.6327, 0.1009, 0.2163, 0.3687, 0.6229])\n",
      "Valid Idx 3 | Loss tensor([0.7728, 0.4464, 0.1102, 0.8402, 0.8793])\n",
      "\n",
      "************** Batch 760 in 5.0147271156311035 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0534, 0.1968, 0.0791, 0.0986]) \n",
      "Test Loss tensor([0.0556, 0.1974, 0.0747, 0.1023])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1704, 0.3764, 0.0224, 0.1958, 0.2632]) \n",
      "Test Loss tensor([0.1799, 0.3910, 0.0286, 0.1918, 0.2621])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0854, 0.0141, 0.2879, 0.2461, 0.2205, 0.0867, 0.0127]) \n",
      "Test Loss tensor([0.0901, 0.0138, 0.2806, 0.2435, 0.2168, 0.0844, 0.0120])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0415, 0.2150, 0.0427, 0.0775, 0.2635, 0.2479, 0.0487]) \n",
      "Test Loss tensor([0.0367, 0.2177, 0.0460, 0.0755, 0.2692, 0.2454, 0.0495])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0313, 0.0290, 0.0430, 0.0212, 0.0195, 0.0107, 0.1196, 0.0027]) \n",
      "Test Loss tensor([0.0272, 0.0252, 0.0395, 0.0182, 0.0186, 0.0130, 0.1162, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5986, 0.6501, 0.1011, 0.2116, 0.3708, 0.6012])\n",
      "Valid Idx 3 | Loss tensor([0.7092, 0.3603, 0.1140, 0.8293, 0.8510])\n",
      "Gradients: Input 0.9369540214538574 | Message 1.4645929336547852 | Update 1.4693331718444824 | Output 0.16156119108200073\n",
      "\n",
      "************** Batch 764 in 4.974053144454956 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0567, 0.1917, 0.0743, 0.1097]) \n",
      "Test Loss tensor([0.0554, 0.1975, 0.0764, 0.0979])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1839, 0.3848, 0.0313, 0.1900, 0.2628]) \n",
      "Test Loss tensor([0.1862, 0.3842, 0.0218, 0.1881, 0.2711])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0884, 0.0120, 0.2819, 0.2391, 0.2270, 0.0880, 0.0135]) \n",
      "Test Loss tensor([0.0873, 0.0159, 0.2821, 0.2519, 0.2181, 0.0865, 0.0116])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0355, 0.2099, 0.0475, 0.0748, 0.2772, 0.2476, 0.0420]) \n",
      "Test Loss tensor([0.0394, 0.2189, 0.0456, 0.0764, 0.2711, 0.2451, 0.0486])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0311, 0.0241, 0.0479, 0.0225, 0.0210, 0.0135, 0.1248, 0.0033]) \n",
      "Test Loss tensor([0.0257, 0.0237, 0.0404, 0.0189, 0.0185, 0.0110, 0.1170, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6022, 0.6298, 0.0995, 0.2169, 0.3689, 0.6192])\n",
      "Valid Idx 3 | Loss tensor([0.8006, 0.4505, 0.1108, 0.8505, 0.8788])\n",
      "\n",
      "************** Batch 768 in 5.015440940856934 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0526, 0.1977, 0.0751, 0.0993]) \n",
      "Test Loss tensor([0.0548, 0.1993, 0.0754, 0.1016])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1925, 0.3913, 0.0213, 0.1968, 0.2664]) \n",
      "Test Loss tensor([0.1790, 0.3847, 0.0256, 0.1879, 0.2638])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0851, 0.0163, 0.2791, 0.2512, 0.2127, 0.0880, 0.0105]) \n",
      "Test Loss tensor([0.0897, 0.0141, 0.2822, 0.2467, 0.2223, 0.0843, 0.0118])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0419, 0.2176, 0.0393, 0.0713, 0.2689, 0.2527, 0.0485]) \n",
      "Test Loss tensor([0.0355, 0.2234, 0.0457, 0.0742, 0.2668, 0.2491, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0262, 0.0239, 0.0479, 0.0208, 0.0195, 0.0101, 0.1196, 0.0029]) \n",
      "Test Loss tensor([0.0277, 0.0239, 0.0394, 0.0183, 0.0177, 0.0125, 0.1149, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5939, 0.6554, 0.1003, 0.2122, 0.3723, 0.5969])\n",
      "Valid Idx 3 | Loss tensor([0.7270, 0.3718, 0.1084, 0.8327, 0.8611])\n",
      "\n",
      "************** Batch 772 in 5.092217922210693 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0555, 0.1947, 0.0797, 0.0977]) \n",
      "Test Loss tensor([0.0556, 0.1995, 0.0744, 0.1018])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1804, 0.3888, 0.0252, 0.1821, 0.2694]) \n",
      "Test Loss tensor([0.1802, 0.3871, 0.0255, 0.1854, 0.2622])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0918, 0.0156, 0.2784, 0.2518, 0.2296, 0.0837, 0.0115]) \n",
      "Test Loss tensor([0.0905, 0.0134, 0.2817, 0.2477, 0.2251, 0.0845, 0.0119])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0393, 0.2273, 0.0453, 0.0749, 0.2712, 0.2582, 0.0520]) \n",
      "Test Loss tensor([0.0342, 0.2183, 0.0444, 0.0742, 0.2662, 0.2498, 0.0481])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0253, 0.0272, 0.0364, 0.0195, 0.0171, 0.0129, 0.1175, 0.0029]) \n",
      "Test Loss tensor([0.0285, 0.0249, 0.0391, 0.0186, 0.0163, 0.0130, 0.1154, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5990, 0.6602, 0.1007, 0.2088, 0.3720, 0.5895])\n",
      "Valid Idx 3 | Loss tensor([0.7098, 0.3525, 0.1091, 0.8302, 0.8615])\n",
      "\n",
      "************** Batch 776 in 5.0754969120025635 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0579, 0.1968, 0.0803, 0.1075]) \n",
      "Test Loss tensor([0.0560, 0.1985, 0.0741, 0.0970])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1794, 0.3719, 0.0273, 0.1842, 0.2685]) \n",
      "Test Loss tensor([0.1838, 0.3879, 0.0199, 0.1925, 0.2716])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0854, 0.0119, 0.2811, 0.2304, 0.2180, 0.0862, 0.0124]) \n",
      "Test Loss tensor([0.0869, 0.0164, 0.2814, 0.2555, 0.2151, 0.0864, 0.0108])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0339, 0.2165, 0.0360, 0.0781, 0.2647, 0.2559, 0.0480]) \n",
      "Test Loss tensor([0.0379, 0.2219, 0.0480, 0.0759, 0.2652, 0.2459, 0.0503])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0294, 0.0265, 0.0405, 0.0197, 0.0176, 0.0140, 0.1180, 0.0034]) \n",
      "Test Loss tensor([0.0260, 0.0236, 0.0412, 0.0195, 0.0183, 0.0104, 0.1148, 0.0028])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6019, 0.6310, 0.1007, 0.2128, 0.3647, 0.6166])\n",
      "Valid Idx 3 | Loss tensor([0.8035, 0.4624, 0.1121, 0.8486, 0.8909])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 780 in 5.044205904006958 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0607, 0.1980, 0.0765, 0.0994]) \n",
      "Test Loss tensor([0.0560, 0.1973, 0.0747, 0.1010])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1911, 0.3776, 0.0179, 0.1910, 0.2687]) \n",
      "Test Loss tensor([0.1818, 0.3877, 0.0263, 0.1905, 0.2634])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0859, 0.0189, 0.2865, 0.2402, 0.2147, 0.0783, 0.0095]) \n",
      "Test Loss tensor([0.0917, 0.0134, 0.2845, 0.2470, 0.2237, 0.0846, 0.0122])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0389, 0.2094, 0.0415, 0.0696, 0.2626, 0.2404, 0.0444]) \n",
      "Test Loss tensor([0.0348, 0.2162, 0.0441, 0.0744, 0.2611, 0.2487, 0.0485])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0295, 0.0273, 0.0432, 0.0182, 0.0162, 0.0116, 0.1156, 0.0035]) \n",
      "Test Loss tensor([0.0280, 0.0251, 0.0370, 0.0183, 0.0159, 0.0135, 0.1136, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5931, 0.6668, 0.0998, 0.2016, 0.3783, 0.5891])\n",
      "Valid Idx 3 | Loss tensor([0.6840, 0.3360, 0.1110, 0.8126, 0.8578])\n",
      "Gradients: Input 1.4104392528533936 | Message 2.230677843093872 | Update 2.24239444732666 | Output 0.24034607410430908\n",
      "\n",
      "************** Batch 784 in 5.134017467498779 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0555, 0.1784, 0.0747, 0.0969]) \n",
      "Test Loss tensor([0.0544, 0.1980, 0.0736, 0.0980])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1911, 0.3928, 0.0275, 0.1867, 0.2752]) \n",
      "Test Loss tensor([0.1821, 0.3847, 0.0225, 0.1879, 0.2695])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0940, 0.0148, 0.2744, 0.2472, 0.2290, 0.0860, 0.0113]) \n",
      "Test Loss tensor([0.0888, 0.0144, 0.2806, 0.2497, 0.2190, 0.0837, 0.0110])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0359, 0.2234, 0.0423, 0.0774, 0.2705, 0.2450, 0.0437]) \n",
      "Test Loss tensor([0.0357, 0.2192, 0.0454, 0.0769, 0.2672, 0.2502, 0.0498])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0268, 0.0183, 0.0391, 0.0173, 0.0153, 0.0123, 0.1149, 0.0031]) \n",
      "Test Loss tensor([0.0286, 0.0257, 0.0416, 0.0200, 0.0179, 0.0121, 0.1146, 0.0029])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5957, 0.6475, 0.1048, 0.2076, 0.3719, 0.6040])\n",
      "Valid Idx 3 | Loss tensor([0.7479, 0.3916, 0.1114, 0.8291, 0.8728])\n",
      "\n",
      "************** Batch 788 in 5.019845962524414 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0554, 0.1993, 0.0791, 0.0963]) \n",
      "Test Loss tensor([0.0547, 0.2016, 0.0739, 0.0961])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1864, 0.3794, 0.0215, 0.1855, 0.2658]) \n",
      "Test Loss tensor([0.1817, 0.3888, 0.0223, 0.1884, 0.2672])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0916, 0.0169, 0.2964, 0.2618, 0.2175, 0.0876, 0.0107]) \n",
      "Test Loss tensor([0.0879, 0.0151, 0.2777, 0.2528, 0.2189, 0.0838, 0.0114])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0389, 0.2304, 0.0468, 0.0806, 0.2616, 0.2355, 0.0516]) \n",
      "Test Loss tensor([0.0355, 0.2201, 0.0467, 0.0759, 0.2649, 0.2471, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0287, 0.0261, 0.0373, 0.0204, 0.0171, 0.0129, 0.1130, 0.0034]) \n",
      "Test Loss tensor([0.0270, 0.0246, 0.0389, 0.0183, 0.0173, 0.0118, 0.1130, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5989, 0.6499, 0.1004, 0.2077, 0.3731, 0.6060])\n",
      "Valid Idx 3 | Loss tensor([0.7720, 0.4057, 0.1130, 0.8360, 0.8744])\n",
      "\n",
      "************** Batch 792 in 5.209467887878418 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0499, 0.2010, 0.0719, 0.0967]) \n",
      "Test Loss tensor([0.0555, 0.1988, 0.0750, 0.1032])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1811, 0.3694, 0.0231, 0.1975, 0.2688]) \n",
      "Test Loss tensor([0.1830, 0.3813, 0.0258, 0.1914, 0.2630])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0886, 0.0132, 0.2775, 0.2517, 0.2041, 0.0878, 0.0141]) \n",
      "Test Loss tensor([0.0902, 0.0134, 0.2779, 0.2453, 0.2252, 0.0843, 0.0131])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0380, 0.2254, 0.0452, 0.0783, 0.2641, 0.2512, 0.0495]) \n",
      "Test Loss tensor([0.0339, 0.2162, 0.0444, 0.0782, 0.2705, 0.2497, 0.0480])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0237, 0.0289, 0.0379, 0.0218, 0.0175, 0.0122, 0.1118, 0.0036]) \n",
      "Test Loss tensor([0.0282, 0.0257, 0.0405, 0.0192, 0.0171, 0.0131, 0.1154, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5968, 0.6581, 0.0996, 0.2044, 0.3731, 0.5930])\n",
      "Valid Idx 3 | Loss tensor([0.7318, 0.3435, 0.1141, 0.8243, 0.8607])\n",
      "\n",
      "************** Batch 796 in 5.066611289978027 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0577, 0.1925, 0.0717, 0.1049]) \n",
      "Test Loss tensor([0.0565, 0.1991, 0.0740, 0.0999])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1806, 0.3715, 0.0283, 0.1907, 0.2599]) \n",
      "Test Loss tensor([0.1811, 0.3812, 0.0228, 0.1875, 0.2679])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0893, 0.0122, 0.2804, 0.2452, 0.2262, 0.0807, 0.0114]) \n",
      "Test Loss tensor([0.0873, 0.0153, 0.2771, 0.2480, 0.2180, 0.0844, 0.0120])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0347, 0.2298, 0.0427, 0.0779, 0.2652, 0.2539, 0.0461]) \n",
      "Test Loss tensor([0.0362, 0.2182, 0.0459, 0.0756, 0.2691, 0.2437, 0.0488])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0252, 0.0254, 0.0452, 0.0210, 0.0181, 0.0137, 0.1210, 0.0042]) \n",
      "Test Loss tensor([0.0265, 0.0249, 0.0404, 0.0194, 0.0178, 0.0119, 0.1144, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5942, 0.6437, 0.1018, 0.2101, 0.3658, 0.6052])\n",
      "Valid Idx 3 | Loss tensor([0.7930, 0.3989, 0.1111, 0.8397, 0.8715])\n",
      "\n",
      "************** Batch 800 in 5.087637662887573 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0507, 0.1910, 0.0745, 0.1010]) \n",
      "Test Loss tensor([0.0567, 0.2002, 0.0752, 0.0994])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1776, 0.3747, 0.0200, 0.1912, 0.2587]) \n",
      "Test Loss tensor([0.1801, 0.3785, 0.0240, 0.1860, 0.2639])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0927, 0.0133, 0.2930, 0.2501, 0.2177, 0.0871, 0.0130]) \n",
      "Test Loss tensor([0.0871, 0.0156, 0.2804, 0.2460, 0.2182, 0.0834, 0.0120])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0401, 0.2191, 0.0414, 0.0753, 0.2723, 0.2460, 0.0475]) \n",
      "Test Loss tensor([0.0381, 0.2195, 0.0448, 0.0765, 0.2644, 0.2507, 0.0483])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0245, 0.0240, 0.0438, 0.0166, 0.0167, 0.0115, 0.1189, 0.0030]) \n",
      "Test Loss tensor([0.0270, 0.0249, 0.0403, 0.0179, 0.0174, 0.0120, 0.1149, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5979, 0.6415, 0.1033, 0.2087, 0.3660, 0.6131])\n",
      "Valid Idx 3 | Loss tensor([0.8008, 0.3856, 0.1120, 0.8395, 0.8656])\n",
      "Gradients: Input 0.3482266068458557 | Message 0.5090092420578003 | Update 0.5202974081039429 | Output 0.05091206729412079\n",
      "\n",
      "************** Batch 804 in 5.050264596939087 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0527, 0.1953, 0.0795, 0.1011]) \n",
      "Test Loss tensor([0.0547, 0.1974, 0.0746, 0.1019])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1732, 0.3740, 0.0205, 0.1853, 0.2636]) \n",
      "Test Loss tensor([0.1796, 0.3773, 0.0260, 0.1866, 0.2638])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0879, 0.0123, 0.2733, 0.2395, 0.2197, 0.0885, 0.0117]) \n",
      "Test Loss tensor([0.0883, 0.0145, 0.2796, 0.2471, 0.2224, 0.0818, 0.0124])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0343, 0.2155, 0.0494, 0.0791, 0.2664, 0.2626, 0.0430]) \n",
      "Test Loss tensor([0.0372, 0.2180, 0.0455, 0.0762, 0.2660, 0.2465, 0.0489])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0270, 0.0297, 0.0396, 0.0206, 0.0183, 0.0140, 0.1129, 0.0030]) \n",
      "Test Loss tensor([0.0278, 0.0241, 0.0402, 0.0184, 0.0173, 0.0130, 0.1150, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5979, 0.6454, 0.1016, 0.2078, 0.3703, 0.5988])\n",
      "Valid Idx 3 | Loss tensor([0.7716, 0.3502, 0.1112, 0.8337, 0.8510])\n",
      "\n",
      "************** Batch 808 in 5.1622724533081055 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0523, 0.1855, 0.0743, 0.1018]) \n",
      "Test Loss tensor([0.0551, 0.1989, 0.0759, 0.1020])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1829, 0.3827, 0.0236, 0.1859, 0.2721]) \n",
      "Test Loss tensor([0.1836, 0.3765, 0.0257, 0.1892, 0.2618])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0861, 0.0146, 0.3007, 0.2533, 0.2184, 0.0807, 0.0152]) \n",
      "Test Loss tensor([0.0911, 0.0150, 0.2775, 0.2449, 0.2180, 0.0824, 0.0129])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0345, 0.2215, 0.0445, 0.0795, 0.2637, 0.2597, 0.0518]) \n",
      "Test Loss tensor([0.0381, 0.2173, 0.0455, 0.0750, 0.2686, 0.2475, 0.0498])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0304, 0.0254, 0.0396, 0.0207, 0.0176, 0.0125, 0.1168, 0.0032]) \n",
      "Test Loss tensor([0.0271, 0.0257, 0.0424, 0.0196, 0.0178, 0.0122, 0.1159, 0.0033])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5997, 0.6380, 0.0997, 0.2103, 0.3698, 0.6008])\n",
      "Valid Idx 3 | Loss tensor([0.8037, 0.3775, 0.1096, 0.8352, 0.8638])\n",
      "\n",
      "************** Batch 812 in 5.052875995635986 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0561, 0.1860, 0.0765, 0.1050]) \n",
      "Test Loss tensor([0.0560, 0.1984, 0.0742, 0.0990])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1884, 0.3555, 0.0245, 0.1793, 0.2591]) \n",
      "Test Loss tensor([0.1840, 0.3807, 0.0214, 0.1859, 0.2666])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0879, 0.0120, 0.2808, 0.2444, 0.2222, 0.0836, 0.0124]) \n",
      "Test Loss tensor([0.0863, 0.0170, 0.2829, 0.2528, 0.2172, 0.0835, 0.0112])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0365, 0.2168, 0.0426, 0.0746, 0.2502, 0.2347, 0.0497]) \n",
      "Test Loss tensor([0.0379, 0.2127, 0.0454, 0.0766, 0.2654, 0.2483, 0.0471])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0287, 0.0210, 0.0409, 0.0193, 0.0160, 0.0128, 0.1227, 0.0033]) \n",
      "Test Loss tensor([0.0255, 0.0243, 0.0420, 0.0192, 0.0180, 0.0108, 0.1150, 0.0029])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6016, 0.6328, 0.1013, 0.2144, 0.3663, 0.6196])\n",
      "Valid Idx 3 | Loss tensor([0.8551, 0.4335, 0.1118, 0.8444, 0.8825])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 816 in 5.0356903076171875 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0561, 0.1993, 0.0682, 0.1023]) \n",
      "Test Loss tensor([0.0549, 0.1957, 0.0755, 0.1062])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1891, 0.3604, 0.0198, 0.1891, 0.2592]) \n",
      "Test Loss tensor([0.1809, 0.3798, 0.0311, 0.1864, 0.2593])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0899, 0.0167, 0.2982, 0.2555, 0.2114, 0.0827, 0.0128]) \n",
      "Test Loss tensor([0.0913, 0.0132, 0.2765, 0.2476, 0.2279, 0.0821, 0.0128])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0384, 0.2185, 0.0439, 0.0789, 0.2562, 0.2502, 0.0503]) \n",
      "Test Loss tensor([0.0339, 0.2202, 0.0440, 0.0736, 0.2655, 0.2526, 0.0482])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0258, 0.0261, 0.0445, 0.0210, 0.0163, 0.0113, 0.1246, 0.0040]) \n",
      "Test Loss tensor([0.0290, 0.0237, 0.0389, 0.0187, 0.0154, 0.0142, 0.1147, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5913, 0.6619, 0.1012, 0.2030, 0.3819, 0.5849])\n",
      "Valid Idx 3 | Loss tensor([0.7372, 0.3028, 0.1099, 0.8164, 0.8401])\n",
      "\n",
      "************** Batch 820 in 5.072553634643555 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0587, 0.2024, 0.0762, 0.1038]) \n",
      "Test Loss tensor([0.0573, 0.1974, 0.0749, 0.0992])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1786, 0.3877, 0.0303, 0.1868, 0.2602]) \n",
      "Test Loss tensor([0.1854, 0.3716, 0.0185, 0.1904, 0.2745])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0910, 0.0130, 0.2724, 0.2414, 0.2258, 0.0833, 0.0115]) \n",
      "Test Loss tensor([0.0879, 0.0175, 0.2829, 0.2600, 0.2146, 0.0865, 0.0109])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0349, 0.2150, 0.0421, 0.0854, 0.2574, 0.2519, 0.0453]) \n",
      "Test Loss tensor([0.0385, 0.2172, 0.0462, 0.0798, 0.2668, 0.2469, 0.0487])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0243, 0.0214, 0.0416, 0.0171, 0.0164, 0.0159, 0.1199, 0.0037]) \n",
      "Test Loss tensor([0.0248, 0.0240, 0.0437, 0.0186, 0.0183, 0.0098, 0.1147, 0.0028])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6007, 0.6212, 0.1007, 0.2131, 0.3618, 0.6306])\n",
      "Valid Idx 3 | Loss tensor([0.8944, 0.4788, 0.1123, 0.8516, 0.8953])\n",
      "Gradients: Input 1.8938243389129639 | Message 3.0418128967285156 | Update 3.0511951446533203 | Output 0.30807507038116455\n",
      "\n",
      "************** Batch 824 in 5.138701915740967 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0576, 0.1886, 0.0726, 0.1001]) \n",
      "Test Loss tensor([0.0552, 0.2008, 0.0743, 0.1033])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1883, 0.3658, 0.0156, 0.1907, 0.2739]) \n",
      "Test Loss tensor([0.1827, 0.3800, 0.0281, 0.1861, 0.2608])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0792, 0.0169, 0.2734, 0.2625, 0.2172, 0.0800, 0.0087]) \n",
      "Test Loss tensor([0.0911, 0.0131, 0.2829, 0.2445, 0.2272, 0.0855, 0.0124])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0371, 0.2035, 0.0432, 0.0709, 0.2633, 0.2454, 0.0525]) \n",
      "Test Loss tensor([0.0344, 0.2182, 0.0433, 0.0760, 0.2642, 0.2503, 0.0487])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0241, 0.0245, 0.0454, 0.0185, 0.0172, 0.0093, 0.1164, 0.0030]) \n",
      "Test Loss tensor([0.0287, 0.0256, 0.0384, 0.0185, 0.0154, 0.0130, 0.1127, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5912, 0.6609, 0.1016, 0.2019, 0.3774, 0.5907])\n",
      "Valid Idx 3 | Loss tensor([0.7581, 0.3239, 0.1077, 0.8228, 0.8542])\n",
      "\n",
      "************** Batch 828 in 5.0932393074035645 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0543, 0.2075, 0.0744, 0.1009]) \n",
      "Test Loss tensor([0.0541, 0.1974, 0.0724, 0.0996])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1799, 0.3669, 0.0261, 0.1914, 0.2666]) \n",
      "Test Loss tensor([0.1798, 0.3785, 0.0264, 0.1899, 0.2589])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0920, 0.0166, 0.2897, 0.2524, 0.2280, 0.0847, 0.0105]) \n",
      "Test Loss tensor([0.0878, 0.0140, 0.2778, 0.2449, 0.2243, 0.0839, 0.0119])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0345, 0.2191, 0.0373, 0.0796, 0.2649, 0.2554, 0.0420]) \n",
      "Test Loss tensor([0.0348, 0.2199, 0.0428, 0.0759, 0.2674, 0.2461, 0.0493])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0288, 0.0290, 0.0361, 0.0190, 0.0153, 0.0132, 0.1128, 0.0032]) \n",
      "Test Loss tensor([0.0281, 0.0249, 0.0404, 0.0186, 0.0163, 0.0122, 0.1130, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5902, 0.6495, 0.1046, 0.2092, 0.3757, 0.6029])\n",
      "Valid Idx 3 | Loss tensor([0.8009, 0.3572, 0.1118, 0.8278, 0.8624])\n",
      "\n",
      "************** Batch 832 in 4.96166729927063 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0554, 0.2051, 0.0700, 0.1005]) \n",
      "Test Loss tensor([0.0557, 0.2002, 0.0736, 0.0979])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1835, 0.3761, 0.0236, 0.1911, 0.2668]) \n",
      "Test Loss tensor([0.1856, 0.3703, 0.0190, 0.1889, 0.2723])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0840, 0.0140, 0.2645, 0.2544, 0.2229, 0.0826, 0.0086]) \n",
      "Test Loss tensor([0.0867, 0.0178, 0.2823, 0.2575, 0.2139, 0.0848, 0.0109])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0370, 0.2111, 0.0420, 0.0709, 0.2661, 0.2375, 0.0432]) \n",
      "Test Loss tensor([0.0410, 0.2195, 0.0478, 0.0777, 0.2696, 0.2464, 0.0507])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0291, 0.0256, 0.0484, 0.0190, 0.0188, 0.0125, 0.1166, 0.0038]) \n",
      "Test Loss tensor([0.0273, 0.0258, 0.0451, 0.0202, 0.0184, 0.0100, 0.1188, 0.0028])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5997, 0.6195, 0.1014, 0.2172, 0.3667, 0.6392])\n",
      "Valid Idx 3 | Loss tensor([0.9018, 0.4648, 0.1113, 0.8468, 0.8913])\n",
      "\n",
      "************** Batch 836 in 5.035339117050171 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0563, 0.2042, 0.0684, 0.0948]) \n",
      "Test Loss tensor([0.0541, 0.1990, 0.0723, 0.1034])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1758, 0.3691, 0.0221, 0.1946, 0.2684]) \n",
      "Test Loss tensor([0.1814, 0.3726, 0.0309, 0.1848, 0.2590])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0795, 0.0170, 0.2669, 0.2454, 0.2026, 0.0853, 0.0102]) \n",
      "Test Loss tensor([0.0921, 0.0127, 0.2763, 0.2444, 0.2300, 0.0832, 0.0130])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0459, 0.2171, 0.0451, 0.0708, 0.2639, 0.2418, 0.0409]) \n",
      "Test Loss tensor([0.0344, 0.2208, 0.0450, 0.0774, 0.2676, 0.2481, 0.0512])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0226, 0.0253, 0.0390, 0.0172, 0.0178, 0.0091, 0.1164, 0.0026]) \n",
      "Test Loss tensor([0.0296, 0.0253, 0.0392, 0.0204, 0.0160, 0.0136, 0.1164, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5867, 0.6506, 0.1028, 0.2017, 0.3778, 0.5944])\n",
      "Valid Idx 3 | Loss tensor([0.7761, 0.3039, 0.1091, 0.8148, 0.8497])\n",
      "\n",
      "************** Batch 840 in 5.06949520111084 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0566, 0.2101, 0.0770, 0.1062]) \n",
      "Test Loss tensor([0.0559, 0.1980, 0.0730, 0.0975])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1842, 0.3818, 0.0329, 0.1869, 0.2581]) \n",
      "Test Loss tensor([0.1837, 0.3664, 0.0205, 0.1901, 0.2687])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0933, 0.0132, 0.2803, 0.2330, 0.2307, 0.0835, 0.0131]) \n",
      "Test Loss tensor([0.0854, 0.0168, 0.2786, 0.2534, 0.2158, 0.0856, 0.0113])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0320, 0.2416, 0.0404, 0.0731, 0.2663, 0.2392, 0.0437]) \n",
      "Test Loss tensor([0.0401, 0.2201, 0.0454, 0.0766, 0.2649, 0.2436, 0.0496])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0298, 0.0273, 0.0419, 0.0191, 0.0168, 0.0129, 0.1140, 0.0037]) \n",
      "Test Loss tensor([0.0266, 0.0260, 0.0433, 0.0197, 0.0185, 0.0101, 0.1188, 0.0028])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6040, 0.6103, 0.1021, 0.2173, 0.3701, 0.6287])\n",
      "Valid Idx 3 | Loss tensor([0.9177, 0.4492, 0.1110, 0.8480, 0.8899])\n",
      "Gradients: Input 1.9476426839828491 | Message 3.035574436187744 | Update 3.040708065032959 | Output 0.30463001132011414\n",
      "\n",
      "************** Batch 844 in 5.054596424102783 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0614, 0.2028, 0.0724, 0.0999]) \n",
      "Test Loss tensor([0.0530, 0.1971, 0.0726, 0.0985])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1820, 0.3767, 0.0200, 0.1880, 0.2722]) \n",
      "Test Loss tensor([0.1813, 0.3709, 0.0270, 0.1863, 0.2599])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0891, 0.0154, 0.2873, 0.2576, 0.2121, 0.0885, 0.0115]) \n",
      "Test Loss tensor([0.0902, 0.0146, 0.2773, 0.2456, 0.2229, 0.0831, 0.0115])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0411, 0.2154, 0.0475, 0.0877, 0.2781, 0.2479, 0.0495]) \n",
      "Test Loss tensor([0.0360, 0.2209, 0.0430, 0.0763, 0.2687, 0.2495, 0.0492])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0269, 0.0255, 0.0440, 0.0182, 0.0184, 0.0101, 0.1161, 0.0027]) \n",
      "Test Loss tensor([0.0274, 0.0243, 0.0394, 0.0185, 0.0165, 0.0124, 0.1138, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5951, 0.6464, 0.1004, 0.2059, 0.3761, 0.6083])\n",
      "Valid Idx 3 | Loss tensor([0.8275, 0.3450, 0.1113, 0.8332, 0.8593])\n",
      "\n",
      "************** Batch 848 in 4.991392374038696 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0500, 0.2066, 0.0712, 0.1023]) \n",
      "Test Loss tensor([0.0538, 0.1987, 0.0743, 0.0998])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1726, 0.3576, 0.0265, 0.1862, 0.2483]) \n",
      "Test Loss tensor([0.1757, 0.3718, 0.0264, 0.1873, 0.2577])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0884, 0.0140, 0.2635, 0.2464, 0.2180, 0.0810, 0.0117]) \n",
      "Test Loss tensor([0.0882, 0.0141, 0.2757, 0.2458, 0.2240, 0.0827, 0.0104])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0389, 0.2163, 0.0430, 0.0769, 0.2685, 0.2363, 0.0482]) \n",
      "Test Loss tensor([0.0362, 0.2195, 0.0448, 0.0763, 0.2671, 0.2481, 0.0506])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0257, 0.0281, 0.0438, 0.0197, 0.0166, 0.0126, 0.1060, 0.0033]) \n",
      "Test Loss tensor([0.0284, 0.0256, 0.0402, 0.0182, 0.0163, 0.0125, 0.1136, 0.0032])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5907, 0.6558, 0.1052, 0.2116, 0.3752, 0.6096])\n",
      "Valid Idx 3 | Loss tensor([0.8269, 0.3481, 0.1118, 0.8302, 0.8624])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 852 in 5.030203580856323 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0540, 0.2097, 0.0732, 0.0966]) \n",
      "Test Loss tensor([0.0564, 0.2017, 0.0748, 0.0990])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1733, 0.3609, 0.0250, 0.1779, 0.2505]) \n",
      "Test Loss tensor([0.1811, 0.3720, 0.0195, 0.1885, 0.2710])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0956, 0.0146, 0.2755, 0.2448, 0.2143, 0.0877, 0.0125]) \n",
      "Test Loss tensor([0.0886, 0.0182, 0.2768, 0.2549, 0.2169, 0.0850, 0.0099])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0356, 0.2246, 0.0445, 0.0713, 0.2662, 0.2391, 0.0437]) \n",
      "Test Loss tensor([0.0402, 0.2183, 0.0455, 0.0764, 0.2678, 0.2435, 0.0481])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0348, 0.0231, 0.0415, 0.0199, 0.0147, 0.0129, 0.1118, 0.0031]) \n",
      "Test Loss tensor([0.0254, 0.0234, 0.0408, 0.0187, 0.0183, 0.0100, 0.1133, 0.0028])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6044, 0.6284, 0.1022, 0.2185, 0.3682, 0.6340])\n",
      "Valid Idx 3 | Loss tensor([0.9203, 0.4529, 0.1094, 0.8574, 0.8899])\n",
      "\n",
      "************** Batch 856 in 5.107532262802124 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0584, 0.2108, 0.0725, 0.0985]) \n",
      "Test Loss tensor([0.0537, 0.2009, 0.0742, 0.1032])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1840, 0.3644, 0.0173, 0.1878, 0.2705]) \n",
      "Test Loss tensor([0.1781, 0.3704, 0.0289, 0.1866, 0.2563])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0811, 0.0186, 0.2923, 0.2644, 0.2175, 0.0860, 0.0112]) \n",
      "Test Loss tensor([0.0921, 0.0141, 0.2744, 0.2430, 0.2241, 0.0821, 0.0121])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0371, 0.2187, 0.0441, 0.0745, 0.2596, 0.2476, 0.0409]) \n",
      "Test Loss tensor([0.0332, 0.2209, 0.0444, 0.0756, 0.2678, 0.2486, 0.0483])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0300, 0.0269, 0.0417, 0.0176, 0.0170, 0.0086, 0.1133, 0.0024]) \n",
      "Test Loss tensor([0.0290, 0.0263, 0.0391, 0.0189, 0.0155, 0.0133, 0.1135, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5890, 0.6602, 0.1025, 0.2093, 0.3850, 0.5933])\n",
      "Valid Idx 3 | Loss tensor([0.8147, 0.3158, 0.1094, 0.8284, 0.8571])\n",
      "\n",
      "************** Batch 860 in 4.9941935539245605 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0600, 0.1853, 0.0749, 0.1082]) \n",
      "Test Loss tensor([0.0542, 0.1993, 0.0732, 0.0970])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1876, 0.3674, 0.0308, 0.1905, 0.2724]) \n",
      "Test Loss tensor([0.1793, 0.3667, 0.0249, 0.1864, 0.2635])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0944, 0.0145, 0.2740, 0.2461, 0.2154, 0.0850, 0.0138]) \n",
      "Test Loss tensor([0.0872, 0.0158, 0.2718, 0.2450, 0.2172, 0.0810, 0.0114])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0324, 0.2129, 0.0405, 0.0708, 0.2592, 0.2528, 0.0497]) \n",
      "Test Loss tensor([0.0384, 0.2162, 0.0455, 0.0740, 0.2628, 0.2462, 0.0501])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0279, 0.0247, 0.0359, 0.0141, 0.0160, 0.0125, 0.1076, 0.0030]) \n",
      "Test Loss tensor([0.0264, 0.0254, 0.0406, 0.0184, 0.0166, 0.0112, 0.1139, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5952, 0.6348, 0.1010, 0.2123, 0.3718, 0.6068])\n",
      "Valid Idx 3 | Loss tensor([0.8805, 0.3714, 0.1127, 0.8430, 0.8729])\n",
      "Gradients: Input 1.2690237760543823 | Message 2.0399370193481445 | Update 2.04606294631958 | Output 0.22294315695762634\n",
      "\n",
      "************** Batch 864 in 4.980311870574951 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0523, 0.1968, 0.0713, 0.0987]) \n",
      "Test Loss tensor([0.0548, 0.1982, 0.0742, 0.0960])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1849, 0.3604, 0.0277, 0.1833, 0.2618]) \n",
      "Test Loss tensor([0.1830, 0.3648, 0.0217, 0.1854, 0.2657])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0842, 0.0151, 0.2821, 0.2557, 0.2278, 0.0881, 0.0121]) \n",
      "Test Loss tensor([0.0882, 0.0170, 0.2745, 0.2493, 0.2172, 0.0847, 0.0107])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0345, 0.2177, 0.0461, 0.0752, 0.2753, 0.2451, 0.0489]) \n",
      "Test Loss tensor([0.0400, 0.2187, 0.0472, 0.0760, 0.2669, 0.2438, 0.0477])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0297, 0.0234, 0.0424, 0.0224, 0.0171, 0.0130, 0.1172, 0.0027]) \n",
      "Test Loss tensor([0.0264, 0.0251, 0.0426, 0.0189, 0.0175, 0.0109, 0.1166, 0.0029])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5988, 0.6133, 0.1011, 0.2133, 0.3676, 0.6203])\n",
      "Valid Idx 3 | Loss tensor([0.9328, 0.4123, 0.1167, 0.8516, 0.8888])\n",
      "\n",
      "************** Batch 868 in 4.950026273727417 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0545, 0.1950, 0.0724, 0.1024]) \n",
      "Test Loss tensor([0.0539, 0.2031, 0.0730, 0.1037])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1831, 0.3555, 0.0220, 0.1872, 0.2692]) \n",
      "Test Loss tensor([0.1808, 0.3679, 0.0320, 0.1891, 0.2571])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0843, 0.0167, 0.2685, 0.2464, 0.2115, 0.0849, 0.0085]) \n",
      "Test Loss tensor([0.0917, 0.0137, 0.2788, 0.2445, 0.2273, 0.0826, 0.0127])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0352, 0.2175, 0.0468, 0.0786, 0.2703, 0.2676, 0.0470]) \n",
      "Test Loss tensor([0.0355, 0.2141, 0.0439, 0.0754, 0.2671, 0.2459, 0.0491])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0243, 0.0242, 0.0386, 0.0180, 0.0178, 0.0100, 0.1121, 0.0028]) \n",
      "Test Loss tensor([0.0297, 0.0257, 0.0397, 0.0191, 0.0154, 0.0135, 0.1156, 0.0034])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5872, 0.6470, 0.1020, 0.2029, 0.3802, 0.5911])\n",
      "Valid Idx 3 | Loss tensor([0.8366, 0.2963, 0.1080, 0.8320, 0.8430])\n",
      "\n",
      "************** Batch 872 in 4.959326982498169 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0560, 0.2001, 0.0714, 0.1043]) \n",
      "Test Loss tensor([0.0546, 0.2007, 0.0738, 0.0971])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1862, 0.3694, 0.0273, 0.1874, 0.2529]) \n",
      "Test Loss tensor([0.1830, 0.3606, 0.0209, 0.1881, 0.2712])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0918, 0.0128, 0.2740, 0.2388, 0.2325, 0.0861, 0.0130]) \n",
      "Test Loss tensor([0.0871, 0.0185, 0.2764, 0.2577, 0.2145, 0.0825, 0.0102])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0310, 0.2089, 0.0451, 0.0694, 0.2723, 0.2453, 0.0465]) \n",
      "Test Loss tensor([0.0409, 0.2176, 0.0463, 0.0743, 0.2674, 0.2425, 0.0505])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0270, 0.0259, 0.0430, 0.0222, 0.0155, 0.0133, 0.1157, 0.0039]) \n",
      "Test Loss tensor([0.0263, 0.0250, 0.0454, 0.0184, 0.0179, 0.0101, 0.1163, 0.0028])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.6023, 0.6071, 0.0981, 0.2169, 0.3690, 0.6231])\n",
      "Valid Idx 3 | Loss tensor([0.9629, 0.4400, 0.1130, 0.8545, 0.8921])\n",
      "\n",
      "************** Batch 876 in 4.836023807525635 **************\n",
      "\n",
      "Training Idx 1 \n",
      "Train Loss tensor([0.0419, 0.1423, 0.0611, 0.0766]) \n",
      "Test Loss tensor([0.0570, 0.1998, 0.0737, 0.0974])\n",
      "\n",
      "Training Idx 2 \n",
      "Train Loss tensor([0.1341, 0.2551, 0.0163, 0.1434, 0.2067]) \n",
      "Test Loss tensor([0.1790, 0.3629, 0.0227, 0.1864, 0.2616])\n",
      "\n",
      "Training Idx 4 \n",
      "Train Loss tensor([0.0578, 0.0151, 0.2170, 0.1903, 0.1701, 0.0601, 0.0078]) \n",
      "Test Loss tensor([0.0872, 0.0164, 0.2733, 0.2486, 0.2168, 0.0823, 0.0114])\n",
      "\n",
      "Training Idx 5 \n",
      "Train Loss tensor([0.0308, 0.1688, 0.0339, 0.0574, 0.2043, 0.1797, 0.0382]) \n",
      "Test Loss tensor([0.0402, 0.2197, 0.0456, 0.0757, 0.2685, 0.2448, 0.0507])\n",
      "\n",
      "Training Idx 6 \n",
      "Train Loss tensor([0.0190, 0.0200, 0.0292, 0.0144, 0.0143, 0.0093, 0.0882, 0.0023]) \n",
      "Test Loss tensor([0.0266, 0.0250, 0.0424, 0.0190, 0.0162, 0.0114, 0.1157, 0.0030])\n",
      "\n",
      "Valid Idx 0 | Loss tensor([0.5998, 0.6307, 0.1007, 0.2134, 0.3692, 0.6222])\n",
      "Valid Idx 3 | Loss tensor([0.9379, 0.4055, 0.1115, 0.8524, 0.8844])\n"
     ]
    }
   ],
   "source": [
    "trainLosses = {}\n",
    "testLosses = {}\n",
    "validLosses = {}\n",
    "trainingIdxs = [1, 2, 4, 5, 6]\n",
    "validationIdxs = [0, 3]\n",
    "\n",
    "inputNetworkGradients = []\n",
    "messageNetworkGradients = []\n",
    "updateNetworkGradients = []\n",
    "outputNetworkGradients = []\n",
    "\n",
    "\n",
    "for morphIdx in range(7):\n",
    "    trainLosses[morphIdx] = []\n",
    "    testLosses[morphIdx] = []\n",
    "    validLosses[morphIdx] = []\n",
    "\n",
    "for epoch in range(4):\n",
    "    \n",
    "    \n",
    "    for morphIdx in trainingIdxs:\n",
    "        permutation = np.random.permutation(X_train[morphIdx].shape[0])\n",
    "        X_train[morphIdx] = X_train[morphIdx][permutation]\n",
    "        Y_train[morphIdx] = Y_train[morphIdx][permutation]\n",
    "        \n",
    "    stepLoss = None\n",
    "    graphs = []\n",
    "    numAggregatedBatches = 0\n",
    "\n",
    "    for batch in range(0, numTrainingBatches, numBatchesPerTrainingStep):\n",
    "        \n",
    "        inputNetwork.train()\n",
    "        messageNetwork.train()\n",
    "        updateNetwork.train()\n",
    "        outputNetwork.train()\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        for morphIdx in trainingIdxs:\n",
    "            numNodes = (X_train[morphIdx].shape[1] - 6) // 2\n",
    "            trainLosses[morphIdx].append(torch.zeros(numNodes))\n",
    "\n",
    "        for batchOffset in range(numBatchesPerTrainingStep):\n",
    "            \n",
    "            if batch + batchOffset >= numTrainingBatches:\n",
    "                break\n",
    "                \n",
    "            for morphIdx in trainingIdxs:\n",
    "                graphs.append(env[morphIdx].get_graph()._get_dgl_graph())\n",
    "                x = X_train[morphIdx][(batch+batchOffset) * batch_size:(batch+batchOffset+1)*batch_size].to(device)\n",
    "                y = Y_train[morphIdx][(batch+batchOffset) * batch_size:(batch+batchOffset+1)*batch_size].to(device)\n",
    "\n",
    "                y_hat = gnn.forward(graphs[-1], x)\n",
    "\n",
    "                loss_tmp = criterion(y, y_hat).mean(dim=0)\n",
    "\n",
    "                trainLosses[morphIdx][-1] += loss_tmp.cpu().detach() / numBatchesPerTrainingStep\n",
    "\n",
    "                if stepLoss is None:\n",
    "                    stepLoss = loss_tmp.mean()\n",
    "\n",
    "                else:\n",
    "                    stepLoss += loss_tmp.mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        stepLoss.backward()\n",
    "        \n",
    "        \n",
    "        s = 0\n",
    "        for parameter in inputNetwork.parameters():\n",
    "            s += torch.abs(parameter.grad).mean()\n",
    "        inputNetworkGradients.append(s.item())\n",
    "\n",
    "        s = 0\n",
    "        for parameter in messageNetwork.parameters():\n",
    "            s += torch.abs(parameter.grad).mean()\n",
    "        messageNetworkGradients.append(s.item())\n",
    "\n",
    "        s = 0        \n",
    "        for parameter in updateNetwork.parameters():\n",
    "            s += torch.abs(parameter.grad).mean()\n",
    "        updateNetworkGradients.append(s.item())\n",
    "\n",
    "        s = 0        \n",
    "        for parameter in outputNetwork.parameters():\n",
    "            s += torch.abs(parameter.grad).mean()\n",
    "        outputNetworkGradients.append(s.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        stepLoss = None\n",
    "        graphs = []\n",
    "        \n",
    "        inputNetwork.eval()\n",
    "        messageNetwork.eval()\n",
    "        updateNetwork.eval()\n",
    "        outputNetwork.eval()\n",
    "\n",
    "        numBatchesForExectution = 50\n",
    "        for morphIdx in trainingIdxs:\n",
    "            numNodes = (X_train[morphIdx].shape[1] - 6) // 2\n",
    "            testLosses[morphIdx].append(torch.zeros(numNodes))\n",
    "            for batch_ in np.random.choice(np.arange(numTestingBatches-1), numBatchesForExectution):\n",
    "                g = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                x = X_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "                y = Y_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "                y_hat = gnn.forward(g, x)\n",
    "                loss = criterion(y, y_hat).mean(dim=0)\n",
    "                testLosses[morphIdx][-1] += loss.cpu().detach()\n",
    "            testLosses[morphIdx][-1] /= numBatchesForExectution\n",
    "        \n",
    "        for morphIdx in validationIdxs:\n",
    "            numNodes = (X_train[morphIdx].shape[1] - 6) // 2\n",
    "            validLosses[morphIdx].append(torch.zeros(numNodes))\n",
    "            for batch_ in np.random.choice(np.arange(numTestingBatches-1), numBatchesForExectution):\n",
    "\n",
    "                g = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                x = X_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "                y = Y_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "\n",
    "                y_hat = gnn.forward(g, x)\n",
    "                loss = criterion(y, y_hat).mean(dim=0)\n",
    "\n",
    "                validLosses[morphIdx][-1] += loss.cpu().detach()\n",
    "            validLosses[morphIdx][-1] /= numBatchesForExectution\n",
    "\n",
    "        print('\\n************** Batch {} in {} **************\\n'.format(batch, time.time() - t0))\n",
    "        for morphIdx in trainingIdxs:\n",
    "            print('Training Idx {} \\nTrain Loss {} \\nTest Loss {}\\n'.format(morphIdx, trainLosses[morphIdx][-1], testLosses[morphIdx][-1]))\n",
    "        for morphIdx in validationIdxs:\n",
    "            print('Valid Idx {} | Loss {}'.format(morphIdx, validLosses[morphIdx][-1]))\n",
    "            \n",
    "        if batch % 20 ==0:\n",
    "            print('Gradients: Input {} | Message {} | Update {} | Output {}'.format(\n",
    "                inputNetworkGradients[-1], messageNetworkGradients[-1], updateNetworkGradients[-1], outputNetworkGradients[-1]))    \n",
    "#         if batch % 100 == 99:\n",
    "#             lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "morphIdx = 0\n",
    "lossArr = torch.stack(testLosses[morphIdx]).T\n",
    "fig, ax = plt.subplots(1, sharex=True)\n",
    "for i in range(lossArr.shape[0]):\n",
    "    ax.plot(range(lossArr.shape[1]), torch.log10(lossArr[i]))\n",
    "plt.legend(range(lossArr.shape[0]))\n",
    "plt.xlabel('Training Step')\n",
    "plt.grid()\n",
    "plt.ylabel('Smooth L1 Loss')\n",
    "plt.title('Per Node Loss Morphology {}, Train = {}'.format(morphIdx, morphIdx in trainingIdxs))\n",
    "plt.savefig('per-node-loss-{}.jpg'.format(morphIdx))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell for producing Per Node Loss for each Morphology\n",
    "\n",
    "for morphIdx in range(7):\n",
    "    if morphIdx in trainingIdxs:\n",
    "        lossArr = torch.stack(testLosses[morphIdx]).T\n",
    "    else:\n",
    "        lossArr = torch.stack(validLosses[morphIdx]).T\n",
    "    \n",
    "    fig, ax = plt.subplots(1, sharex=True)\n",
    "    for i in range(lossArr.shape[0]):\n",
    "        ax.plot(range(lossArr.shape[1]), lossArr[i])\n",
    "    plt.legend(range(lossArr.shape[0]))\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.grid()\n",
    "    plt.ylabel('Smooth L1 Loss')\n",
    "    plt.title('Per Node Loss Morphology {}, Train = {}'.format(morphIdx, morphIdx in trainingIdxs))\n",
    "    plt.savefig('per-node-loss-{}.jpg'.format(morphIdx))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, sharex=True)\n",
    "for morphIdx in trainingIdxs:\n",
    "    lossArr = torch.stack(testLosses[morphIdx]).mean(dim=1)\n",
    "    ax.plot(range(lossArr.shape[0]), lossArr)\n",
    "for morphIdx in validationIdxs:\n",
    "    lossArr = torch.stack(validLosses[morphIdx]).mean(dim=1)\n",
    "    ax.plot(range(lossArr.shape[0]), lossArr)\n",
    "\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Smooth L1 Loss')\n",
    "plt.title('Mean Node Loss per Morphology')\n",
    "plt.legend(trainingIdxs + validationIdxs)\n",
    "plt.savefig('mean-node-losses.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, sharex=True)\n",
    "ax.plot(range(len(inputNetworkGradients)), np.log10(inputNetworkGradients))\n",
    "ax.plot(range(len(messageNetworkGradients)), np.log10(messageNetworkGradients))\n",
    "ax.plot(range(len(updateNetworkGradients)), np.log10(updateNetworkGradients))\n",
    "ax.plot(range(len(outputNetworkGradients)), np.log10(outputNetworkGradients))\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Mean Absolute Gradient (Log 10)')\n",
    "plt.title('Gradient Magnitudes Over Time')\n",
    "plt.legend(['Input Network', 'Message Network', 'Update Network', ' Output Network'])\n",
    "plt.savefig('gradients.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 0 in 0.45406627655029297 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4542, 0.4080, 0.4073, 0.5089, 0.3972, 0.4879]) \n",
      "Test Loss tensor([0.4553, 0.4058, 0.4062, 0.5118, 0.3943, 0.4892])\n",
      "\n",
      "\n",
      "************** Batch 4 in 0.4866156578063965 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4541, 0.4069, 0.4068, 0.5117, 0.3903, 0.4889]) \n",
      "Test Loss tensor([0.4553, 0.4052, 0.4053, 0.5127, 0.3929, 0.4908])\n",
      "\n",
      "\n",
      "************** Batch 8 in 0.5168979167938232 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4531, 0.4069, 0.4072, 0.5125, 0.3931, 0.4876]) \n",
      "Test Loss tensor([0.4551, 0.4034, 0.4038, 0.5139, 0.3937, 0.4915])\n",
      "\n",
      "\n",
      "************** Batch 12 in 0.565899133682251 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4551, 0.4047, 0.4047, 0.5160, 0.3939, 0.4916]) \n",
      "Test Loss tensor([0.4563, 0.4023, 0.4024, 0.5158, 0.3922, 0.4931])\n",
      "\n",
      "\n",
      "************** Batch 16 in 0.5349369049072266 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4538, 0.4025, 0.4026, 0.5151, 0.3916, 0.4938]) \n",
      "Test Loss tensor([0.4556, 0.4010, 0.4014, 0.5172, 0.3911, 0.4934])\n",
      "\n",
      "\n",
      "************** Batch 20 in 0.5562644004821777 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4540, 0.4002, 0.4008, 0.5155, 0.3891, 0.4934]) \n",
      "Test Loss tensor([0.4554, 0.3992, 0.3999, 0.5177, 0.3906, 0.4948])\n",
      "\n",
      "\n",
      "************** Batch 24 in 0.5596208572387695 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4558, 0.3988, 0.3997, 0.5160, 0.3898, 0.4949]) \n",
      "Test Loss tensor([0.4555, 0.3977, 0.3985, 0.5193, 0.3903, 0.4960])\n",
      "\n",
      "\n",
      "************** Batch 28 in 0.6594791412353516 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4541, 0.3974, 0.4006, 0.5188, 0.3912, 0.4941]) \n",
      "Test Loss tensor([0.4563, 0.3965, 0.3981, 0.5212, 0.3886, 0.4968])\n",
      "\n",
      "\n",
      "************** Batch 32 in 0.6790189743041992 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4567, 0.3962, 0.3970, 0.5215, 0.3878, 0.4972]) \n",
      "Test Loss tensor([0.4560, 0.3951, 0.3966, 0.5230, 0.3880, 0.4978])\n",
      "\n",
      "\n",
      "************** Batch 36 in 0.5848147869110107 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4534, 0.3950, 0.3967, 0.5216, 0.3863, 0.4948]) \n",
      "Test Loss tensor([0.4564, 0.3940, 0.3955, 0.5239, 0.3866, 0.4982])\n",
      "\n",
      "\n",
      "************** Batch 40 in 0.5852384567260742 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4542, 0.3916, 0.3961, 0.5242, 0.3868, 0.4988]) \n",
      "Test Loss tensor([0.4555, 0.3924, 0.3935, 0.5258, 0.3855, 0.4991])\n",
      "\n",
      "\n",
      "************** Batch 44 in 0.5981452465057373 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4589, 0.3922, 0.3937, 0.5244, 0.3832, 0.5011]) \n",
      "Test Loss tensor([0.4571, 0.3910, 0.3926, 0.5270, 0.3848, 0.5012])\n",
      "\n",
      "\n",
      "************** Batch 48 in 0.599543571472168 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4563, 0.3912, 0.3937, 0.5253, 0.3829, 0.4998]) \n",
      "Test Loss tensor([0.4566, 0.3890, 0.3922, 0.5286, 0.3835, 0.5019])\n",
      "\n",
      "\n",
      "************** Batch 52 in 0.598870038986206 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4560, 0.3899, 0.3922, 0.5280, 0.3835, 0.5011]) \n",
      "Test Loss tensor([0.4576, 0.3881, 0.3909, 0.5294, 0.3831, 0.5025])\n",
      "\n",
      "\n",
      "************** Batch 56 in 0.5535140037536621 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4572, 0.3871, 0.3911, 0.5285, 0.3834, 0.5043]) \n",
      "Test Loss tensor([0.4558, 0.3865, 0.3888, 0.5308, 0.3822, 0.5036])\n",
      "\n",
      "\n",
      "************** Batch 60 in 0.5868568420410156 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4578, 0.3870, 0.3882, 0.5300, 0.3818, 0.5062]) \n",
      "Test Loss tensor([0.4562, 0.3854, 0.3882, 0.5320, 0.3815, 0.5048])\n",
      "\n",
      "\n",
      "************** Batch 64 in 0.5859065055847168 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4582, 0.3843, 0.3887, 0.5330, 0.3816, 0.5048]) \n",
      "Test Loss tensor([0.4568, 0.3841, 0.3876, 0.5340, 0.3809, 0.5059])\n",
      "\n",
      "\n",
      "************** Batch 68 in 0.6158707141876221 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4562, 0.3830, 0.3849, 0.5327, 0.3789, 0.5081]) \n",
      "Test Loss tensor([0.4576, 0.3829, 0.3857, 0.5367, 0.3788, 0.5069])\n",
      "\n",
      "\n",
      "************** Batch 72 in 0.5784027576446533 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4551, 0.3853, 0.3875, 0.5367, 0.3810, 0.5085]) \n",
      "Test Loss tensor([0.4575, 0.3818, 0.3849, 0.5369, 0.3782, 0.5070])\n",
      "\n",
      "\n",
      "************** Batch 76 in 0.6217467784881592 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4564, 0.3828, 0.3860, 0.5370, 0.3802, 0.5073]) \n",
      "Test Loss tensor([0.4578, 0.3805, 0.3833, 0.5374, 0.3781, 0.5090])\n",
      "\n",
      "\n",
      "************** Batch 80 in 0.6014542579650879 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4598, 0.3821, 0.3820, 0.5388, 0.3787, 0.5117]) \n",
      "Test Loss tensor([0.4590, 0.3792, 0.3823, 0.5394, 0.3779, 0.5101])\n",
      "\n",
      "\n",
      "************** Batch 84 in 0.5976171493530273 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4595, 0.3802, 0.3842, 0.5398, 0.3750, 0.5103]) \n",
      "Test Loss tensor([0.4583, 0.3778, 0.3817, 0.5409, 0.3763, 0.5109])\n",
      "\n",
      "\n",
      "************** Batch 88 in 0.5993540287017822 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4559, 0.3771, 0.3795, 0.5406, 0.3748, 0.5115]) \n",
      "Test Loss tensor([0.4588, 0.3770, 0.3796, 0.5418, 0.3760, 0.5123])\n",
      "\n",
      "\n",
      "************** Batch 92 in 0.5942683219909668 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4588, 0.3755, 0.3829, 0.5423, 0.3749, 0.5132]) \n",
      "Test Loss tensor([0.4591, 0.3755, 0.3795, 0.5436, 0.3746, 0.5125])\n",
      "\n",
      "\n",
      "************** Batch 96 in 0.6651508808135986 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4562, 0.3755, 0.3808, 0.5441, 0.3749, 0.5108]) \n",
      "Test Loss tensor([0.4589, 0.3744, 0.3778, 0.5449, 0.3731, 0.5143])\n",
      "\n",
      "\n",
      "************** Batch 100 in 0.5750713348388672 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4545, 0.3741, 0.3795, 0.5445, 0.3750, 0.5142]) \n",
      "Test Loss tensor([0.4580, 0.3724, 0.3768, 0.5462, 0.3723, 0.5128])\n",
      "\n",
      "\n",
      "************** Batch 104 in 0.6606695652008057 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4589, 0.3719, 0.3776, 0.5467, 0.3716, 0.5167]) \n",
      "Test Loss tensor([0.4590, 0.3717, 0.3751, 0.5473, 0.3722, 0.5161])\n",
      "\n",
      "\n",
      "************** Batch 108 in 0.560694694519043 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4628, 0.3721, 0.3756, 0.5473, 0.3708, 0.5148]) \n",
      "Test Loss tensor([0.4596, 0.3704, 0.3754, 0.5490, 0.3712, 0.5169])\n",
      "\n",
      "\n",
      "************** Batch 112 in 0.5497455596923828 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4596, 0.3703, 0.3762, 0.5495, 0.3682, 0.5162]) \n",
      "Test Loss tensor([0.4591, 0.3696, 0.3739, 0.5498, 0.3697, 0.5165])\n",
      "\n",
      "\n",
      "************** Batch 116 in 0.5648930072784424 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4551, 0.3693, 0.3740, 0.5489, 0.3701, 0.5151]) \n",
      "Test Loss tensor([0.4590, 0.3679, 0.3732, 0.5510, 0.3705, 0.5183])\n",
      "\n",
      "\n",
      "************** Batch 120 in 0.5919842720031738 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4609, 0.3670, 0.3716, 0.5501, 0.3667, 0.5219]) \n",
      "Test Loss tensor([0.4585, 0.3666, 0.3725, 0.5520, 0.3679, 0.5183])\n",
      "\n",
      "\n",
      "************** Batch 124 in 0.5622580051422119 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4582, 0.3664, 0.3716, 0.5508, 0.3692, 0.5192]) \n",
      "Test Loss tensor([0.4591, 0.3657, 0.3709, 0.5548, 0.3682, 0.5199])\n",
      "\n",
      "\n",
      "************** Batch 128 in 0.5248236656188965 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4616, 0.3638, 0.3702, 0.5517, 0.3667, 0.5186]) \n",
      "Test Loss tensor([0.4579, 0.3635, 0.3683, 0.5553, 0.3676, 0.5213])\n",
      "\n",
      "\n",
      "************** Batch 132 in 0.5647532939910889 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4557, 0.3658, 0.3712, 0.5530, 0.3694, 0.5192]) \n",
      "Test Loss tensor([0.4603, 0.3632, 0.3683, 0.5571, 0.3672, 0.5220])\n",
      "\n",
      "\n",
      "************** Batch 136 in 0.539799690246582 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4610, 0.3638, 0.3696, 0.5582, 0.3642, 0.5213]) \n",
      "Test Loss tensor([0.4599, 0.3627, 0.3675, 0.5585, 0.3660, 0.5230])\n",
      "\n",
      "\n",
      "************** Batch 140 in 0.5531497001647949 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4583, 0.3626, 0.3689, 0.5557, 0.3643, 0.5212]) \n",
      "Test Loss tensor([0.4599, 0.3612, 0.3665, 0.5591, 0.3663, 0.5242])\n",
      "\n",
      "\n",
      "************** Batch 144 in 0.5676765441894531 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4620, 0.3623, 0.3650, 0.5570, 0.3650, 0.5277]) \n",
      "Test Loss tensor([0.4592, 0.3597, 0.3647, 0.5607, 0.3646, 0.5244])\n",
      "\n",
      "\n",
      "************** Batch 148 in 0.5655257701873779 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4568, 0.3596, 0.3668, 0.5609, 0.3655, 0.5251]) \n",
      "Test Loss tensor([0.4596, 0.3587, 0.3647, 0.5613, 0.3625, 0.5255])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 152 in 0.5560059547424316 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4648, 0.3584, 0.3621, 0.5627, 0.3607, 0.5286]) \n",
      "Test Loss tensor([0.4603, 0.3571, 0.3639, 0.5633, 0.3631, 0.5260])\n",
      "\n",
      "\n",
      "************** Batch 156 in 0.5513787269592285 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4610, 0.3566, 0.3615, 0.5625, 0.3629, 0.5291]) \n",
      "Test Loss tensor([0.4613, 0.3562, 0.3621, 0.5646, 0.3613, 0.5279])\n",
      "\n",
      "\n",
      "************** Batch 160 in 0.5360031127929688 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4613, 0.3562, 0.3617, 0.5626, 0.3600, 0.5319]) \n",
      "Test Loss tensor([0.4607, 0.3551, 0.3611, 0.5663, 0.3615, 0.5282])\n",
      "\n",
      "\n",
      "************** Batch 164 in 0.5378808975219727 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4584, 0.3553, 0.3648, 0.5640, 0.3598, 0.5264]) \n",
      "Test Loss tensor([0.4612, 0.3537, 0.3605, 0.5675, 0.3602, 0.5302])\n",
      "\n",
      "\n",
      "************** Batch 168 in 0.5488462448120117 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4599, 0.3537, 0.3595, 0.5667, 0.3593, 0.5303]) \n",
      "Test Loss tensor([0.4616, 0.3522, 0.3583, 0.5687, 0.3600, 0.5309])\n",
      "\n",
      "\n",
      "************** Batch 172 in 0.5488460063934326 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4620, 0.3535, 0.3595, 0.5662, 0.3614, 0.5318]) \n",
      "Test Loss tensor([0.4605, 0.3514, 0.3581, 0.5703, 0.3583, 0.5303])\n",
      "\n",
      "\n",
      "************** Batch 176 in 0.5512256622314453 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4603, 0.3536, 0.3573, 0.5712, 0.3583, 0.5306]) \n",
      "Test Loss tensor([0.4614, 0.3501, 0.3567, 0.5711, 0.3577, 0.5311])\n",
      "\n",
      "\n",
      "************** Batch 180 in 0.5223455429077148 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4577, 0.3501, 0.3599, 0.5729, 0.3584, 0.5319]) \n",
      "Test Loss tensor([0.4619, 0.3500, 0.3557, 0.5726, 0.3572, 0.5328])\n",
      "\n",
      "\n",
      "************** Batch 184 in 0.5787951946258545 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4610, 0.3487, 0.3580, 0.5716, 0.3625, 0.5318]) \n",
      "Test Loss tensor([0.4619, 0.3491, 0.3560, 0.5737, 0.3574, 0.5341])\n",
      "\n",
      "\n",
      "************** Batch 188 in 0.5439724922180176 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4623, 0.3478, 0.3531, 0.5743, 0.3556, 0.5352]) \n",
      "Test Loss tensor([0.4611, 0.3472, 0.3544, 0.5736, 0.3568, 0.5335])\n",
      "\n",
      "\n",
      "************** Batch 192 in 0.5428657531738281 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4608, 0.3462, 0.3566, 0.5724, 0.3548, 0.5350]) \n",
      "Test Loss tensor([0.4619, 0.3465, 0.3531, 0.5759, 0.3562, 0.5361])\n",
      "\n",
      "\n",
      "************** Batch 196 in 0.5271544456481934 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4607, 0.3470, 0.3536, 0.5770, 0.3576, 0.5376]) \n",
      "Test Loss tensor([0.4623, 0.3450, 0.3525, 0.5776, 0.3550, 0.5373])\n",
      "\n",
      "\n",
      "************** Batch 200 in 0.5225322246551514 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4594, 0.3451, 0.3540, 0.5777, 0.3582, 0.5350]) \n",
      "Test Loss tensor([0.4621, 0.3447, 0.3521, 0.5777, 0.3542, 0.5366])\n",
      "\n",
      "\n",
      "************** Batch 204 in 0.5229067802429199 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4628, 0.3466, 0.3524, 0.5769, 0.3571, 0.5423]) \n",
      "Test Loss tensor([0.4631, 0.3430, 0.3508, 0.5793, 0.3534, 0.5387])\n",
      "\n",
      "\n",
      "************** Batch 208 in 0.5430681705474854 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4623, 0.3444, 0.3538, 0.5783, 0.3530, 0.5364]) \n",
      "Test Loss tensor([0.4620, 0.3419, 0.3501, 0.5800, 0.3538, 0.5388])\n",
      "\n",
      "\n",
      "************** Batch 212 in 0.5528886318206787 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4638, 0.3418, 0.3504, 0.5797, 0.3532, 0.5406]) \n",
      "Test Loss tensor([0.4646, 0.3416, 0.3498, 0.5810, 0.3522, 0.5392])\n",
      "\n",
      "\n",
      "************** Batch 216 in 0.55973219871521 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4586, 0.3426, 0.3465, 0.5778, 0.3538, 0.5465]) \n",
      "Test Loss tensor([0.4624, 0.3399, 0.3483, 0.5824, 0.3518, 0.5409])\n",
      "\n",
      "\n",
      "************** Batch 220 in 0.5585198402404785 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4648, 0.3424, 0.3464, 0.5845, 0.3552, 0.5422]) \n",
      "Test Loss tensor([0.4635, 0.3392, 0.3472, 0.5834, 0.3513, 0.5413])\n",
      "\n",
      "\n",
      "************** Batch 224 in 0.5296294689178467 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4626, 0.3413, 0.3482, 0.5805, 0.3511, 0.5435]) \n",
      "Test Loss tensor([0.4643, 0.3384, 0.3467, 0.5841, 0.3509, 0.5419])\n",
      "\n",
      "\n",
      "************** Batch 228 in 0.5487463474273682 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4637, 0.3394, 0.3445, 0.5812, 0.3530, 0.5445]) \n",
      "Test Loss tensor([0.4634, 0.3376, 0.3463, 0.5862, 0.3502, 0.5425])\n",
      "\n",
      "\n",
      "************** Batch 232 in 0.5794572830200195 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4618, 0.3362, 0.3470, 0.5873, 0.3485, 0.5424]) \n",
      "Test Loss tensor([0.4634, 0.3363, 0.3445, 0.5875, 0.3491, 0.5429])\n",
      "\n",
      "\n",
      "************** Batch 236 in 0.5554006099700928 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4643, 0.3388, 0.3438, 0.5856, 0.3496, 0.5473]) \n",
      "Test Loss tensor([0.4657, 0.3362, 0.3438, 0.5880, 0.3485, 0.5455])\n",
      "\n",
      "\n",
      "************** Batch 240 in 0.5635302066802979 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4640, 0.3335, 0.3461, 0.5881, 0.3455, 0.5435]) \n",
      "Test Loss tensor([0.4640, 0.3349, 0.3431, 0.5891, 0.3488, 0.5455])\n",
      "\n",
      "\n",
      "************** Batch 244 in 0.5669996738433838 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4615, 0.3334, 0.3443, 0.5860, 0.3446, 0.5423]) \n",
      "Test Loss tensor([0.4646, 0.3335, 0.3439, 0.5900, 0.3487, 0.5455])\n",
      "\n",
      "\n",
      "************** Batch 248 in 0.535175085067749 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4670, 0.3367, 0.3397, 0.5913, 0.3480, 0.5464]) \n",
      "Test Loss tensor([0.4646, 0.3332, 0.3420, 0.5917, 0.3469, 0.5469])\n",
      "\n",
      "\n",
      "************** Batch 252 in 0.5421781539916992 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4654, 0.3322, 0.3405, 0.5908, 0.3471, 0.5489]) \n",
      "Test Loss tensor([0.4649, 0.3324, 0.3417, 0.5925, 0.3463, 0.5474])\n",
      "\n",
      "\n",
      "************** Batch 256 in 0.5361030101776123 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4675, 0.3311, 0.3421, 0.5922, 0.3463, 0.5450]) \n",
      "Test Loss tensor([0.4638, 0.3315, 0.3411, 0.5936, 0.3457, 0.5477])\n",
      "\n",
      "\n",
      "************** Batch 260 in 0.5305707454681396 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4654, 0.3296, 0.3404, 0.5941, 0.3467, 0.5479]) \n",
      "Test Loss tensor([0.4640, 0.3311, 0.3390, 0.5942, 0.3453, 0.5494])\n",
      "\n",
      "\n",
      "************** Batch 264 in 0.5538709163665771 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4633, 0.3298, 0.3374, 0.5947, 0.3487, 0.5473]) \n",
      "Test Loss tensor([0.4654, 0.3297, 0.3384, 0.5950, 0.3446, 0.5500])\n",
      "\n",
      "\n",
      "************** Batch 268 in 0.5492753982543945 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4672, 0.3284, 0.3399, 0.5958, 0.3456, 0.5497]) \n",
      "Test Loss tensor([0.4660, 0.3282, 0.3376, 0.5966, 0.3442, 0.5510])\n",
      "\n",
      "\n",
      "************** Batch 272 in 0.5810742378234863 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4584, 0.3277, 0.3426, 0.5991, 0.3426, 0.5479]) \n",
      "Test Loss tensor([0.4651, 0.3280, 0.3376, 0.5971, 0.3448, 0.5512])\n",
      "\n",
      "\n",
      "************** Batch 276 in 0.5310637950897217 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4642, 0.3281, 0.3371, 0.5981, 0.3482, 0.5526]) \n",
      "Test Loss tensor([0.4650, 0.3270, 0.3368, 0.5977, 0.3426, 0.5510])\n",
      "\n",
      "\n",
      "************** Batch 280 in 0.5338852405548096 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4654, 0.3275, 0.3397, 0.5991, 0.3410, 0.5501]) \n",
      "Test Loss tensor([0.4638, 0.3257, 0.3365, 0.5986, 0.3427, 0.5522])\n",
      "\n",
      "\n",
      "************** Batch 284 in 0.5357680320739746 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4607, 0.3257, 0.3354, 0.5980, 0.3391, 0.5534]) \n",
      "Test Loss tensor([0.4668, 0.3250, 0.3348, 0.6011, 0.3415, 0.5534])\n",
      "\n",
      "\n",
      "************** Batch 288 in 0.5709202289581299 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4704, 0.3246, 0.3363, 0.6019, 0.3428, 0.5542]) \n",
      "Test Loss tensor([0.4656, 0.3244, 0.3352, 0.6019, 0.3425, 0.5543])\n",
      "\n",
      "\n",
      "************** Batch 292 in 0.5523886680603027 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4629, 0.3254, 0.3336, 0.6009, 0.3416, 0.5569]) \n",
      "Test Loss tensor([0.4676, 0.3229, 0.3347, 0.6030, 0.3412, 0.5550])\n",
      "\n",
      "\n",
      "************** Batch 296 in 0.5431942939758301 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4684, 0.3224, 0.3346, 0.6014, 0.3373, 0.5545]) \n",
      "Test Loss tensor([0.4667, 0.3229, 0.3336, 0.6043, 0.3409, 0.5553])\n",
      "\n",
      "\n",
      "************** Batch 300 in 0.537083625793457 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4689, 0.3220, 0.3311, 0.6062, 0.3380, 0.5601]) \n",
      "Test Loss tensor([0.4666, 0.3210, 0.3320, 0.6052, 0.3387, 0.5567])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 304 in 0.5307738780975342 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4722, 0.3235, 0.3318, 0.6048, 0.3419, 0.5607]) \n",
      "Test Loss tensor([0.4684, 0.3212, 0.3314, 0.6049, 0.3403, 0.5576])\n",
      "\n",
      "\n",
      "************** Batch 308 in 0.5523655414581299 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4645, 0.3213, 0.3314, 0.6063, 0.3349, 0.5557]) \n",
      "Test Loss tensor([0.4655, 0.3202, 0.3312, 0.6058, 0.3385, 0.5561])\n",
      "\n",
      "\n",
      "************** Batch 312 in 0.5363969802856445 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4634, 0.3221, 0.3302, 0.6043, 0.3402, 0.5538]) \n",
      "Test Loss tensor([0.4669, 0.3193, 0.3302, 0.6069, 0.3383, 0.5579])\n",
      "\n",
      "\n",
      "************** Batch 316 in 0.519451379776001 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4619, 0.3184, 0.3311, 0.6035, 0.3387, 0.5613]) \n",
      "Test Loss tensor([0.4660, 0.3187, 0.3295, 0.6083, 0.3371, 0.5595])\n",
      "\n",
      "\n",
      "************** Batch 320 in 0.535254955291748 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4690, 0.3189, 0.3291, 0.6051, 0.3388, 0.5602]) \n",
      "Test Loss tensor([0.4678, 0.3177, 0.3282, 0.6089, 0.3373, 0.5589])\n",
      "\n",
      "\n",
      "************** Batch 324 in 0.5529971122741699 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4724, 0.3170, 0.3303, 0.6098, 0.3355, 0.5614]) \n",
      "Test Loss tensor([0.4665, 0.3166, 0.3272, 0.6104, 0.3360, 0.5593])\n",
      "\n",
      "\n",
      "************** Batch 328 in 0.5551495552062988 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4674, 0.3179, 0.3304, 0.6084, 0.3362, 0.5595]) \n",
      "Test Loss tensor([0.4673, 0.3164, 0.3277, 0.6110, 0.3370, 0.5610])\n",
      "\n",
      "\n",
      "************** Batch 332 in 0.5440778732299805 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4604, 0.3190, 0.3277, 0.6075, 0.3345, 0.5615]) \n",
      "Test Loss tensor([0.4666, 0.3159, 0.3273, 0.6126, 0.3365, 0.5614])\n",
      "\n",
      "\n",
      "************** Batch 336 in 0.5393645763397217 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4682, 0.3116, 0.3281, 0.6114, 0.3335, 0.5616]) \n",
      "Test Loss tensor([0.4677, 0.3149, 0.3269, 0.6122, 0.3365, 0.5625])\n",
      "\n",
      "\n",
      "************** Batch 340 in 0.544245719909668 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4688, 0.3131, 0.3271, 0.6168, 0.3370, 0.5660]) \n",
      "Test Loss tensor([0.4685, 0.3148, 0.3262, 0.6136, 0.3343, 0.5633])\n",
      "\n",
      "\n",
      "************** Batch 344 in 0.5941610336303711 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4701, 0.3136, 0.3268, 0.6180, 0.3325, 0.5583]) \n",
      "Test Loss tensor([0.4664, 0.3142, 0.3259, 0.6140, 0.3357, 0.5640])\n",
      "\n",
      "\n",
      "************** Batch 348 in 0.5784153938293457 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4660, 0.3129, 0.3275, 0.6157, 0.3380, 0.5603]) \n",
      "Test Loss tensor([0.4678, 0.3123, 0.3236, 0.6157, 0.3335, 0.5639])\n",
      "\n",
      "\n",
      "************** Batch 352 in 0.5587425231933594 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4691, 0.3131, 0.3232, 0.6167, 0.3333, 0.5641]) \n",
      "Test Loss tensor([0.4691, 0.3128, 0.3237, 0.6158, 0.3342, 0.5644])\n",
      "\n",
      "\n",
      "************** Batch 356 in 0.5308749675750732 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4657, 0.3120, 0.3249, 0.6139, 0.3306, 0.5657]) \n",
      "Test Loss tensor([0.4683, 0.3115, 0.3236, 0.6173, 0.3329, 0.5659])\n",
      "\n",
      "\n",
      "************** Batch 360 in 0.5482223033905029 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4658, 0.3131, 0.3218, 0.6155, 0.3334, 0.5710]) \n",
      "Test Loss tensor([0.4689, 0.3111, 0.3229, 0.6179, 0.3328, 0.5660])\n",
      "\n",
      "\n",
      "************** Batch 364 in 0.5399119853973389 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4607, 0.3100, 0.3217, 0.6146, 0.3339, 0.5649]) \n",
      "Test Loss tensor([0.4693, 0.3102, 0.3225, 0.6191, 0.3325, 0.5667])\n",
      "\n",
      "\n",
      "************** Batch 368 in 0.596123456954956 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4662, 0.3105, 0.3198, 0.6207, 0.3286, 0.5692]) \n",
      "Test Loss tensor([0.4697, 0.3093, 0.3218, 0.6199, 0.3319, 0.5670])\n",
      "\n",
      "\n",
      "************** Batch 372 in 0.5180127620697021 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4644, 0.3112, 0.3228, 0.6162, 0.3355, 0.5684]) \n",
      "Test Loss tensor([0.4677, 0.3085, 0.3213, 0.6203, 0.3314, 0.5675])\n",
      "\n",
      "\n",
      "************** Batch 376 in 0.5662617683410645 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4681, 0.3093, 0.3202, 0.6193, 0.3298, 0.5687]) \n",
      "Test Loss tensor([0.4690, 0.3081, 0.3213, 0.6224, 0.3313, 0.5678])\n",
      "\n",
      "\n",
      "************** Batch 380 in 0.5802760124206543 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4702, 0.3101, 0.3225, 0.6208, 0.3354, 0.5712]) \n",
      "Test Loss tensor([0.4690, 0.3070, 0.3199, 0.6220, 0.3304, 0.5682])\n",
      "\n",
      "\n",
      "************** Batch 384 in 0.5578138828277588 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4701, 0.3087, 0.3214, 0.6207, 0.3337, 0.5670]) \n",
      "Test Loss tensor([0.4680, 0.3073, 0.3194, 0.6224, 0.3302, 0.5689])\n",
      "\n",
      "\n",
      "************** Batch 388 in 0.548743724822998 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4692, 0.3056, 0.3206, 0.6229, 0.3320, 0.5732]) \n",
      "Test Loss tensor([0.4699, 0.3064, 0.3186, 0.6238, 0.3305, 0.5707])\n",
      "\n",
      "\n",
      "************** Batch 392 in 0.5359156131744385 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4726, 0.3067, 0.3221, 0.6231, 0.3319, 0.5709]) \n",
      "Test Loss tensor([0.4687, 0.3056, 0.3181, 0.6240, 0.3307, 0.5701])\n",
      "\n",
      "\n",
      "************** Batch 396 in 0.6223852634429932 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4748, 0.3056, 0.3163, 0.6247, 0.3277, 0.5710]) \n",
      "Test Loss tensor([0.4688, 0.3045, 0.3189, 0.6253, 0.3299, 0.5697])\n",
      "\n",
      "\n",
      "************** Batch 400 in 0.7137537002563477 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4646, 0.3070, 0.3188, 0.6255, 0.3268, 0.5686]) \n",
      "Test Loss tensor([0.4694, 0.3049, 0.3174, 0.6251, 0.3295, 0.5720])\n",
      "\n",
      "\n",
      "************** Batch 404 in 0.596015453338623 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4756, 0.3037, 0.3177, 0.6239, 0.3299, 0.5734]) \n",
      "Test Loss tensor([0.4705, 0.3032, 0.3164, 0.6262, 0.3288, 0.5727])\n",
      "\n",
      "\n",
      "************** Batch 408 in 0.6626451015472412 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4693, 0.3028, 0.3154, 0.6214, 0.3249, 0.5746]) \n",
      "Test Loss tensor([0.4705, 0.3037, 0.3160, 0.6271, 0.3282, 0.5720])\n",
      "\n",
      "\n",
      "************** Batch 412 in 0.6092245578765869 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4704, 0.3030, 0.3169, 0.6275, 0.3265, 0.5745]) \n",
      "Test Loss tensor([0.4700, 0.3026, 0.3159, 0.6282, 0.3271, 0.5722])\n",
      "\n",
      "\n",
      "************** Batch 416 in 0.5891101360321045 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4727, 0.3021, 0.3181, 0.6276, 0.3311, 0.5735]) \n",
      "Test Loss tensor([0.4688, 0.3017, 0.3152, 0.6289, 0.3268, 0.5737])\n",
      "\n",
      "\n",
      "************** Batch 420 in 0.616722583770752 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4708, 0.3025, 0.3175, 0.6289, 0.3280, 0.5726]) \n",
      "Test Loss tensor([0.4724, 0.3009, 0.3146, 0.6299, 0.3269, 0.5757])\n",
      "\n",
      "\n",
      "************** Batch 424 in 0.55971360206604 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4695, 0.3023, 0.3134, 0.6271, 0.3238, 0.5739]) \n",
      "Test Loss tensor([0.4699, 0.3006, 0.3139, 0.6295, 0.3278, 0.5748])\n",
      "\n",
      "\n",
      "************** Batch 428 in 0.5791771411895752 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4727, 0.3022, 0.3167, 0.6278, 0.3266, 0.5743]) \n",
      "Test Loss tensor([0.4714, 0.3004, 0.3139, 0.6306, 0.3262, 0.5770])\n",
      "\n",
      "\n",
      "************** Batch 432 in 0.5713365077972412 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4724, 0.2990, 0.3155, 0.6322, 0.3236, 0.5763]) \n",
      "Test Loss tensor([0.4698, 0.2995, 0.3130, 0.6321, 0.3258, 0.5753])\n",
      "\n",
      "\n",
      "************** Batch 436 in 0.5924782752990723 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4712, 0.3006, 0.3095, 0.6318, 0.3280, 0.5769]) \n",
      "Test Loss tensor([0.4708, 0.2996, 0.3129, 0.6317, 0.3265, 0.5769])\n",
      "\n",
      "\n",
      "************** Batch 440 in 0.5708255767822266 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4693, 0.2990, 0.3140, 0.6326, 0.3266, 0.5784]) \n",
      "Test Loss tensor([0.4710, 0.2983, 0.3117, 0.6325, 0.3260, 0.5774])\n",
      "\n",
      "\n",
      "************** Batch 444 in 0.577172040939331 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4736, 0.2984, 0.3104, 0.6325, 0.3262, 0.5778]) \n",
      "Test Loss tensor([0.4711, 0.2978, 0.3125, 0.6317, 0.3247, 0.5772])\n",
      "\n",
      "\n",
      "************** Batch 448 in 0.5317740440368652 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4736, 0.2978, 0.3129, 0.6326, 0.3195, 0.5763]) \n",
      "Test Loss tensor([0.4714, 0.2984, 0.3120, 0.6337, 0.3250, 0.5766])\n",
      "\n",
      "\n",
      "************** Batch 452 in 0.5336968898773193 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4657, 0.3004, 0.3134, 0.6305, 0.3251, 0.5751]) \n",
      "Test Loss tensor([0.4716, 0.2967, 0.3115, 0.6351, 0.3237, 0.5777])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 456 in 0.5431110858917236 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4735, 0.2966, 0.3103, 0.6328, 0.3259, 0.5812]) \n",
      "Test Loss tensor([0.4720, 0.2970, 0.3105, 0.6360, 0.3247, 0.5792])\n",
      "\n",
      "\n",
      "************** Batch 460 in 0.5542716979980469 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4728, 0.2964, 0.3128, 0.6349, 0.3234, 0.5767]) \n",
      "Test Loss tensor([0.4718, 0.2958, 0.3108, 0.6363, 0.3240, 0.5786])\n",
      "\n",
      "\n",
      "************** Batch 464 in 0.5381941795349121 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4737, 0.2971, 0.3127, 0.6338, 0.3237, 0.5800]) \n",
      "Test Loss tensor([0.4718, 0.2951, 0.3102, 0.6362, 0.3242, 0.5802])\n",
      "\n",
      "\n",
      "************** Batch 468 in 0.5481476783752441 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4687, 0.2953, 0.3124, 0.6360, 0.3236, 0.5770]) \n",
      "Test Loss tensor([0.4715, 0.2953, 0.3100, 0.6381, 0.3236, 0.5788])\n",
      "\n",
      "\n",
      "************** Batch 472 in 0.543813943862915 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4715, 0.2956, 0.3079, 0.6391, 0.3230, 0.5782]) \n",
      "Test Loss tensor([0.4710, 0.2946, 0.3092, 0.6384, 0.3227, 0.5805])\n",
      "\n",
      "\n",
      "************** Batch 476 in 0.521629810333252 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4684, 0.2953, 0.3086, 0.6376, 0.3233, 0.5797]) \n",
      "Test Loss tensor([0.4724, 0.2939, 0.3090, 0.6397, 0.3232, 0.5810])\n",
      "\n",
      "\n",
      "************** Batch 480 in 0.5398075580596924 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4768, 0.2955, 0.3097, 0.6394, 0.3215, 0.5814]) \n",
      "Test Loss tensor([0.4739, 0.2935, 0.3094, 0.6391, 0.3224, 0.5816])\n",
      "\n",
      "\n",
      "************** Batch 484 in 0.527667760848999 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4730, 0.2929, 0.3091, 0.6366, 0.3188, 0.5799]) \n",
      "Test Loss tensor([0.4699, 0.2931, 0.3077, 0.6400, 0.3225, 0.5811])\n",
      "\n",
      "\n",
      "************** Batch 488 in 0.53165602684021 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4728, 0.2938, 0.3104, 0.6407, 0.3226, 0.5803]) \n",
      "Test Loss tensor([0.4743, 0.2924, 0.3082, 0.6408, 0.3213, 0.5822])\n",
      "\n",
      "\n",
      "************** Batch 492 in 0.550360918045044 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4674, 0.2937, 0.3119, 0.6383, 0.3189, 0.5779]) \n",
      "Test Loss tensor([0.4741, 0.2928, 0.3070, 0.6415, 0.3215, 0.5838])\n",
      "\n",
      "\n",
      "************** Batch 496 in 0.5304567813873291 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4691, 0.2919, 0.3099, 0.6402, 0.3165, 0.5800]) \n",
      "Test Loss tensor([0.4714, 0.2916, 0.3069, 0.6424, 0.3213, 0.5809])\n",
      "\n",
      "\n",
      "************** Batch 500 in 0.5498731136322021 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4705, 0.2919, 0.3069, 0.6396, 0.3194, 0.5798]) \n",
      "Test Loss tensor([0.4725, 0.2910, 0.3068, 0.6419, 0.3198, 0.5829])\n",
      "\n",
      "\n",
      "************** Batch 504 in 0.5347297191619873 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4747, 0.2914, 0.3075, 0.6411, 0.3182, 0.5826]) \n",
      "Test Loss tensor([0.4726, 0.2910, 0.3063, 0.6426, 0.3201, 0.5844])\n",
      "\n",
      "\n",
      "************** Batch 508 in 0.5485143661499023 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4718, 0.2902, 0.3081, 0.6461, 0.3229, 0.5802]) \n",
      "Test Loss tensor([0.4695, 0.2908, 0.3078, 0.6424, 0.3213, 0.5820])\n",
      "\n",
      "\n",
      "************** Batch 512 in 0.5296142101287842 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4792, 0.2912, 0.3076, 0.6461, 0.3186, 0.5840]) \n",
      "Test Loss tensor([0.4718, 0.2904, 0.3067, 0.6431, 0.3194, 0.5837])\n",
      "\n",
      "\n",
      "************** Batch 516 in 0.5636417865753174 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4724, 0.2927, 0.3043, 0.6419, 0.3150, 0.5844]) \n",
      "Test Loss tensor([0.4696, 0.2896, 0.3055, 0.6431, 0.3199, 0.5836])\n",
      "\n",
      "\n",
      "************** Batch 520 in 0.5323412418365479 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4754, 0.2892, 0.3062, 0.6440, 0.3192, 0.5886]) \n",
      "Test Loss tensor([0.4737, 0.2893, 0.3059, 0.6445, 0.3189, 0.5843])\n",
      "\n",
      "\n",
      "************** Batch 524 in 0.550534725189209 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4760, 0.2907, 0.3051, 0.6456, 0.3202, 0.5848]) \n",
      "Test Loss tensor([0.4731, 0.2899, 0.3056, 0.6447, 0.3187, 0.5846])\n",
      "\n",
      "\n",
      "************** Batch 528 in 0.5278103351593018 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4695, 0.2885, 0.3058, 0.6469, 0.3199, 0.5842]) \n",
      "Test Loss tensor([0.4737, 0.2893, 0.3055, 0.6449, 0.3188, 0.5842])\n",
      "\n",
      "\n",
      "************** Batch 532 in 0.5365340709686279 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4739, 0.2882, 0.3068, 0.6473, 0.3179, 0.5815]) \n",
      "Test Loss tensor([0.4738, 0.2890, 0.3043, 0.6450, 0.3197, 0.5861])\n",
      "\n",
      "\n",
      "************** Batch 536 in 0.5322372913360596 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4736, 0.2873, 0.3080, 0.6455, 0.3188, 0.5825]) \n",
      "Test Loss tensor([0.4724, 0.2881, 0.3059, 0.6460, 0.3186, 0.5839])\n",
      "\n",
      "\n",
      "************** Batch 540 in 0.5346050262451172 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4701, 0.2882, 0.3034, 0.6446, 0.3195, 0.5831]) \n",
      "Test Loss tensor([0.4727, 0.2877, 0.3042, 0.6471, 0.3188, 0.5861])\n",
      "\n",
      "\n",
      "************** Batch 544 in 0.5362133979797363 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4758, 0.2884, 0.3054, 0.6479, 0.3166, 0.5893]) \n",
      "Test Loss tensor([0.4742, 0.2877, 0.3049, 0.6476, 0.3172, 0.5850])\n",
      "\n",
      "\n",
      "************** Batch 548 in 0.5376753807067871 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4766, 0.2873, 0.3018, 0.6451, 0.3180, 0.5869]) \n",
      "Test Loss tensor([0.4730, 0.2880, 0.3035, 0.6471, 0.3181, 0.5857])\n",
      "\n",
      "\n",
      "************** Batch 552 in 0.5498015880584717 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4738, 0.2899, 0.3038, 0.6485, 0.3191, 0.5858]) \n",
      "Test Loss tensor([0.4699, 0.2870, 0.3038, 0.6471, 0.3177, 0.5841])\n",
      "\n",
      "\n",
      "************** Batch 556 in 0.5328083038330078 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4763, 0.2878, 0.3070, 0.6480, 0.3181, 0.5931]) \n",
      "Test Loss tensor([0.4721, 0.2874, 0.3036, 0.6475, 0.3173, 0.5857])\n",
      "\n",
      "\n",
      "************** Batch 560 in 0.5369133949279785 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4703, 0.2879, 0.3040, 0.6410, 0.3183, 0.5849]) \n",
      "Test Loss tensor([0.4733, 0.2876, 0.3036, 0.6468, 0.3186, 0.5860])\n",
      "\n",
      "\n",
      "************** Batch 564 in 0.5287339687347412 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4772, 0.2895, 0.3027, 0.6495, 0.3197, 0.5904]) \n",
      "Test Loss tensor([0.4732, 0.2869, 0.3031, 0.6480, 0.3176, 0.5864])\n",
      "\n",
      "\n",
      "************** Batch 568 in 0.5464184284210205 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4730, 0.2852, 0.2997, 0.6464, 0.3160, 0.5875]) \n",
      "Test Loss tensor([0.4715, 0.2870, 0.3034, 0.6478, 0.3181, 0.5863])\n",
      "\n",
      "\n",
      "************** Batch 572 in 0.5407192707061768 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4787, 0.2872, 0.3032, 0.6472, 0.3185, 0.5906]) \n",
      "Test Loss tensor([0.4728, 0.2859, 0.3027, 0.6477, 0.3173, 0.5860])\n",
      "\n",
      "\n",
      "************** Batch 576 in 0.5671002864837646 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4713, 0.2864, 0.3041, 0.6478, 0.3238, 0.5861]) \n",
      "Test Loss tensor([0.4715, 0.2860, 0.3042, 0.6481, 0.3175, 0.5846])\n",
      "\n",
      "\n",
      "************** Batch 580 in 0.5379941463470459 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4713, 0.2854, 0.3074, 0.6441, 0.3174, 0.5834]) \n",
      "Test Loss tensor([0.4737, 0.2860, 0.3026, 0.6484, 0.3179, 0.5874])\n",
      "\n",
      "\n",
      "************** Batch 584 in 0.5450553894042969 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4721, 0.2858, 0.3047, 0.6471, 0.3192, 0.5885]) \n",
      "Test Loss tensor([0.4722, 0.2857, 0.3031, 0.6493, 0.3169, 0.5864])\n",
      "\n",
      "\n",
      "************** Batch 588 in 0.5384907722473145 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4733, 0.2883, 0.2987, 0.6454, 0.3210, 0.5879]) \n",
      "Test Loss tensor([0.4720, 0.2851, 0.3031, 0.6497, 0.3168, 0.5860])\n",
      "\n",
      "\n",
      "************** Batch 592 in 0.5423276424407959 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4770, 0.2846, 0.3026, 0.6491, 0.3161, 0.5864]) \n",
      "Test Loss tensor([0.4726, 0.2847, 0.3044, 0.6507, 0.3162, 0.5869])\n",
      "\n",
      "\n",
      "************** Batch 596 in 0.5360107421875 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4712, 0.2848, 0.3033, 0.6473, 0.3130, 0.5856]) \n",
      "Test Loss tensor([0.4729, 0.2850, 0.3030, 0.6497, 0.3170, 0.5869])\n",
      "\n",
      "\n",
      "************** Batch 600 in 0.5429396629333496 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4737, 0.2874, 0.3062, 0.6472, 0.3165, 0.5930]) \n",
      "Test Loss tensor([0.4722, 0.2856, 0.3025, 0.6494, 0.3174, 0.5867])\n",
      "\n",
      "\n",
      "************** Batch 604 in 0.5436110496520996 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4708, 0.2852, 0.3068, 0.6493, 0.3156, 0.5843]) \n",
      "Test Loss tensor([0.4729, 0.2855, 0.3033, 0.6506, 0.3169, 0.5875])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 608 in 0.5291624069213867 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4808, 0.2858, 0.3030, 0.6466, 0.3147, 0.5916]) \n",
      "Test Loss tensor([0.4713, 0.2843, 0.3025, 0.6499, 0.3165, 0.5867])\n",
      "\n",
      "\n",
      "************** Batch 612 in 0.535383939743042 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4644, 0.2848, 0.3026, 0.6458, 0.3203, 0.5847]) \n",
      "Test Loss tensor([0.4728, 0.2849, 0.3029, 0.6501, 0.3186, 0.5881])\n",
      "\n",
      "\n",
      "************** Batch 616 in 0.5333395004272461 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4737, 0.2861, 0.3017, 0.6502, 0.3172, 0.5905]) \n",
      "Test Loss tensor([0.4739, 0.2849, 0.3036, 0.6502, 0.3160, 0.5873])\n",
      "\n",
      "\n",
      "************** Batch 620 in 0.5429649353027344 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4765, 0.2840, 0.3032, 0.6483, 0.3158, 0.5868]) \n",
      "Test Loss tensor([0.4749, 0.2842, 0.3025, 0.6517, 0.3165, 0.5875])\n",
      "\n",
      "\n",
      "************** Batch 624 in 0.5283288955688477 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4706, 0.2832, 0.3042, 0.6484, 0.3173, 0.5874]) \n",
      "Test Loss tensor([0.4711, 0.2829, 0.3029, 0.6511, 0.3160, 0.5869])\n",
      "\n",
      "\n",
      "************** Batch 628 in 0.5370051860809326 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4731, 0.2838, 0.3005, 0.6532, 0.3175, 0.5861]) \n",
      "Test Loss tensor([0.4727, 0.2838, 0.3014, 0.6522, 0.3160, 0.5860])\n",
      "\n",
      "\n",
      "************** Batch 632 in 0.5318098068237305 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4764, 0.2840, 0.3012, 0.6562, 0.3180, 0.5873]) \n",
      "Test Loss tensor([0.4733, 0.2836, 0.3021, 0.6518, 0.3147, 0.5870])\n",
      "\n",
      "\n",
      "************** Batch 636 in 0.548306941986084 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4748, 0.2821, 0.3024, 0.6501, 0.3113, 0.5876]) \n",
      "Test Loss tensor([0.4732, 0.2838, 0.3022, 0.6529, 0.3156, 0.5884])\n",
      "\n",
      "\n",
      "************** Batch 640 in 0.5327200889587402 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4720, 0.2834, 0.3019, 0.6511, 0.3148, 0.5916]) \n",
      "Test Loss tensor([0.4726, 0.2836, 0.3029, 0.6519, 0.3160, 0.5884])\n",
      "\n",
      "\n",
      "************** Batch 644 in 0.5325138568878174 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4765, 0.2834, 0.3015, 0.6543, 0.3119, 0.5837]) \n",
      "Test Loss tensor([0.4724, 0.2829, 0.3021, 0.6522, 0.3153, 0.5886])\n",
      "\n",
      "\n",
      "************** Batch 648 in 0.5356760025024414 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4733, 0.2822, 0.3028, 0.6538, 0.3193, 0.5848]) \n",
      "Test Loss tensor([0.4722, 0.2823, 0.3027, 0.6522, 0.3145, 0.5876])\n",
      "\n",
      "\n",
      "************** Batch 652 in 0.5325295925140381 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4710, 0.2848, 0.3037, 0.6503, 0.3158, 0.5872]) \n",
      "Test Loss tensor([0.4727, 0.2816, 0.3021, 0.6534, 0.3145, 0.5873])\n",
      "\n",
      "\n",
      "************** Batch 656 in 0.5535764694213867 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4753, 0.2819, 0.3021, 0.6536, 0.3135, 0.5846]) \n",
      "Test Loss tensor([0.4736, 0.2822, 0.3016, 0.6544, 0.3146, 0.5889])\n",
      "\n",
      "\n",
      "************** Batch 660 in 0.5309865474700928 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4746, 0.2820, 0.3051, 0.6541, 0.3129, 0.5853]) \n",
      "Test Loss tensor([0.4711, 0.2824, 0.3021, 0.6546, 0.3147, 0.5881])\n",
      "\n",
      "\n",
      "************** Batch 664 in 0.5437793731689453 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4645, 0.2825, 0.3009, 0.6484, 0.3160, 0.5910]) \n",
      "Test Loss tensor([0.4725, 0.2810, 0.3010, 0.6552, 0.3153, 0.5906])\n",
      "\n",
      "\n",
      "************** Batch 668 in 0.5378110408782959 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4724, 0.2802, 0.3031, 0.6535, 0.3136, 0.5896]) \n",
      "Test Loss tensor([0.4723, 0.2812, 0.3008, 0.6548, 0.3142, 0.5896])\n",
      "\n",
      "\n",
      "************** Batch 672 in 0.5395028591156006 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4717, 0.2833, 0.3015, 0.6555, 0.3197, 0.5921]) \n",
      "Test Loss tensor([0.4737, 0.2815, 0.3010, 0.6552, 0.3141, 0.5894])\n",
      "\n",
      "\n",
      "************** Batch 676 in 0.5350449085235596 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4664, 0.2827, 0.3041, 0.6535, 0.3139, 0.5869]) \n",
      "Test Loss tensor([0.4712, 0.2807, 0.3022, 0.6555, 0.3133, 0.5881])\n",
      "\n",
      "\n",
      "************** Batch 680 in 0.5459835529327393 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4715, 0.2828, 0.3029, 0.6543, 0.3179, 0.5889]) \n",
      "Test Loss tensor([0.4723, 0.2811, 0.3013, 0.6561, 0.3129, 0.5886])\n",
      "\n",
      "\n",
      "************** Batch 684 in 0.5502040386199951 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4724, 0.2792, 0.2984, 0.6537, 0.3135, 0.5849]) \n",
      "Test Loss tensor([0.4715, 0.2805, 0.2997, 0.6566, 0.3138, 0.5883])\n",
      "\n",
      "\n",
      "************** Batch 688 in 0.5547912120819092 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4654, 0.2803, 0.2984, 0.6567, 0.3153, 0.5896]) \n",
      "Test Loss tensor([0.4707, 0.2800, 0.3013, 0.6554, 0.3126, 0.5884])\n",
      "\n",
      "\n",
      "************** Batch 692 in 0.5443439483642578 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4722, 0.2782, 0.3004, 0.6580, 0.3126, 0.5918]) \n",
      "Test Loss tensor([0.4727, 0.2795, 0.3011, 0.6570, 0.3139, 0.5898])\n",
      "\n",
      "\n",
      "************** Batch 696 in 0.5279247760772705 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4718, 0.2810, 0.2974, 0.6545, 0.3174, 0.5952]) \n",
      "Test Loss tensor([0.4706, 0.2790, 0.3007, 0.6580, 0.3130, 0.5898])\n",
      "\n",
      "\n",
      "************** Batch 700 in 0.5503726005554199 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4730, 0.2756, 0.3007, 0.6579, 0.3161, 0.5885]) \n",
      "Test Loss tensor([0.4713, 0.2794, 0.3007, 0.6581, 0.3130, 0.5899])\n",
      "\n",
      "\n",
      "************** Batch 704 in 0.5253779888153076 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4761, 0.2792, 0.2994, 0.6598, 0.3170, 0.5899]) \n",
      "Test Loss tensor([0.4718, 0.2789, 0.3010, 0.6587, 0.3131, 0.5905])\n",
      "\n",
      "\n",
      "************** Batch 708 in 0.5352354049682617 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4693, 0.2771, 0.3041, 0.6566, 0.3074, 0.5888]) \n",
      "Test Loss tensor([0.4720, 0.2783, 0.3009, 0.6587, 0.3119, 0.5900])\n",
      "\n",
      "\n",
      "************** Batch 712 in 0.5312080383300781 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4659, 0.2784, 0.3009, 0.6573, 0.3145, 0.5891]) \n",
      "Test Loss tensor([0.4723, 0.2781, 0.3007, 0.6595, 0.3126, 0.5900])\n",
      "\n",
      "\n",
      "************** Batch 716 in 0.550184965133667 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4723, 0.2780, 0.3002, 0.6594, 0.3123, 0.5902]) \n",
      "Test Loss tensor([0.4730, 0.2777, 0.2997, 0.6603, 0.3121, 0.5905])\n",
      "\n",
      "\n",
      "************** Batch 720 in 0.5331540107727051 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4720, 0.2755, 0.2993, 0.6624, 0.3077, 0.5926]) \n",
      "Test Loss tensor([0.4730, 0.2779, 0.3004, 0.6604, 0.3126, 0.5895])\n",
      "\n",
      "\n",
      "************** Batch 724 in 0.5399889945983887 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4751, 0.2789, 0.3002, 0.6593, 0.3106, 0.5995]) \n",
      "Test Loss tensor([0.4734, 0.2773, 0.2996, 0.6608, 0.3116, 0.5919])\n",
      "\n",
      "\n",
      "************** Batch 728 in 0.5280182361602783 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4657, 0.2757, 0.3015, 0.6624, 0.3089, 0.5889]) \n",
      "Test Loss tensor([0.4704, 0.2755, 0.2997, 0.6607, 0.3108, 0.5896])\n",
      "\n",
      "\n",
      "************** Batch 732 in 0.5405178070068359 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4725, 0.2770, 0.2994, 0.6611, 0.3073, 0.5922]) \n",
      "Test Loss tensor([0.4714, 0.2761, 0.2998, 0.6609, 0.3119, 0.5916])\n",
      "\n",
      "\n",
      "************** Batch 736 in 0.5217568874359131 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4705, 0.2774, 0.3014, 0.6582, 0.3092, 0.5911]) \n",
      "Test Loss tensor([0.4712, 0.2757, 0.3006, 0.6621, 0.3105, 0.5904])\n",
      "\n",
      "\n",
      "************** Batch 740 in 0.544299840927124 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4671, 0.2759, 0.2998, 0.6663, 0.3118, 0.5951]) \n",
      "Test Loss tensor([0.4719, 0.2748, 0.2996, 0.6618, 0.3105, 0.5915])\n",
      "\n",
      "\n",
      "************** Batch 744 in 0.540247917175293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4674, 0.2762, 0.2970, 0.6619, 0.3081, 0.5917]) \n",
      "Test Loss tensor([0.4719, 0.2755, 0.2994, 0.6644, 0.3106, 0.5921])\n",
      "\n",
      "\n",
      "************** Batch 748 in 0.5485386848449707 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4732, 0.2769, 0.2984, 0.6616, 0.3096, 0.5946]) \n",
      "Test Loss tensor([0.4727, 0.2743, 0.2987, 0.6639, 0.3110, 0.5922])\n",
      "\n",
      "\n",
      "************** Batch 752 in 0.5419847965240479 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4668, 0.2752, 0.3003, 0.6637, 0.3125, 0.5947]) \n",
      "Test Loss tensor([0.4713, 0.2744, 0.2986, 0.6643, 0.3094, 0.5906])\n",
      "\n",
      "\n",
      "************** Batch 756 in 0.5267837047576904 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4717, 0.2732, 0.2999, 0.6640, 0.3073, 0.5917]) \n",
      "Test Loss tensor([0.4709, 0.2741, 0.2991, 0.6646, 0.3099, 0.5910])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 760 in 0.5348443984985352 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4768, 0.2728, 0.2961, 0.6692, 0.3082, 0.5919]) \n",
      "Test Loss tensor([0.4712, 0.2740, 0.2991, 0.6645, 0.3088, 0.5914])\n",
      "\n",
      "\n",
      "************** Batch 764 in 0.538771390914917 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4663, 0.2741, 0.2996, 0.6642, 0.3023, 0.5924]) \n",
      "Test Loss tensor([0.4701, 0.2728, 0.2998, 0.6663, 0.3076, 0.5902])\n",
      "\n",
      "\n",
      "************** Batch 768 in 0.538947582244873 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4712, 0.2714, 0.2968, 0.6625, 0.3078, 0.5921]) \n",
      "Test Loss tensor([0.4715, 0.2724, 0.2985, 0.6667, 0.3080, 0.5917])\n",
      "\n",
      "\n",
      "************** Batch 772 in 0.5261785984039307 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4783, 0.2713, 0.2974, 0.6642, 0.3107, 0.5965]) \n",
      "Test Loss tensor([0.4711, 0.2725, 0.2990, 0.6660, 0.3084, 0.5912])\n",
      "\n",
      "\n",
      "************** Batch 776 in 0.5457241535186768 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4749, 0.2729, 0.3024, 0.6687, 0.3118, 0.5910]) \n",
      "Test Loss tensor([0.4685, 0.2727, 0.2991, 0.6658, 0.3095, 0.5908])\n",
      "\n",
      "\n",
      "************** Batch 780 in 0.537071943283081 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4699, 0.2699, 0.2973, 0.6649, 0.3058, 0.5935]) \n",
      "Test Loss tensor([0.4713, 0.2722, 0.2994, 0.6667, 0.3074, 0.5919])\n",
      "\n",
      "\n",
      "************** Batch 784 in 0.5733487606048584 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4691, 0.2701, 0.2974, 0.6674, 0.3068, 0.5951]) \n",
      "Test Loss tensor([0.4710, 0.2719, 0.3008, 0.6683, 0.3070, 0.5902])\n",
      "\n",
      "\n",
      "************** Batch 788 in 0.5378825664520264 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4695, 0.2710, 0.3001, 0.6667, 0.3076, 0.5896]) \n",
      "Test Loss tensor([0.4705, 0.2709, 0.2984, 0.6679, 0.3073, 0.5909])\n",
      "\n",
      "\n",
      "************** Batch 792 in 0.5296118259429932 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4720, 0.2730, 0.2992, 0.6679, 0.3083, 0.5933]) \n",
      "Test Loss tensor([0.4699, 0.2708, 0.2991, 0.6693, 0.3070, 0.5916])\n",
      "\n",
      "\n",
      "************** Batch 796 in 0.5419135093688965 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4714, 0.2736, 0.2984, 0.6685, 0.3044, 0.5900]) \n",
      "Test Loss tensor([0.4695, 0.2693, 0.2991, 0.6699, 0.3043, 0.5899])\n",
      "\n",
      "\n",
      "************** Batch 800 in 0.5314631462097168 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4682, 0.2709, 0.2981, 0.6659, 0.3063, 0.5955]) \n",
      "Test Loss tensor([0.4695, 0.2700, 0.3003, 0.6694, 0.3048, 0.5907])\n",
      "\n",
      "\n",
      "************** Batch 804 in 0.55387282371521 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4648, 0.2684, 0.2968, 0.6686, 0.3057, 0.5937]) \n",
      "Test Loss tensor([0.4688, 0.2692, 0.2988, 0.6716, 0.3054, 0.5901])\n",
      "\n",
      "\n",
      "************** Batch 808 in 0.5289037227630615 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4713, 0.2679, 0.2971, 0.6668, 0.3050, 0.5923]) \n",
      "Test Loss tensor([0.4704, 0.2680, 0.2986, 0.6722, 0.3047, 0.5905])\n",
      "\n",
      "\n",
      "************** Batch 812 in 0.5486221313476562 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4651, 0.2680, 0.2971, 0.6710, 0.3087, 0.5872]) \n",
      "Test Loss tensor([0.4683, 0.2671, 0.2994, 0.6722, 0.3032, 0.5892])\n",
      "\n",
      "\n",
      "************** Batch 816 in 0.5345962047576904 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4690, 0.2666, 0.2978, 0.6713, 0.3114, 0.5948]) \n",
      "Test Loss tensor([0.4696, 0.2671, 0.3004, 0.6730, 0.3037, 0.5914])\n",
      "\n",
      "\n",
      "************** Batch 820 in 0.5403013229370117 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4625, 0.2663, 0.3025, 0.6725, 0.3055, 0.5864]) \n",
      "Test Loss tensor([0.4681, 0.2665, 0.3001, 0.6742, 0.3032, 0.5903])\n",
      "\n",
      "\n",
      "************** Batch 824 in 0.5404407978057861 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4687, 0.2657, 0.2978, 0.6779, 0.3008, 0.5936]) \n",
      "Test Loss tensor([0.4682, 0.2660, 0.2992, 0.6756, 0.3028, 0.5906])\n",
      "\n",
      "\n",
      "************** Batch 828 in 0.5366532802581787 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4712, 0.2655, 0.2984, 0.6720, 0.3074, 0.5881]) \n",
      "Test Loss tensor([0.4692, 0.2643, 0.2987, 0.6766, 0.3021, 0.5918])\n",
      "\n",
      "\n",
      "************** Batch 832 in 0.5343294143676758 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4709, 0.2656, 0.3019, 0.6748, 0.3012, 0.5925]) \n",
      "Test Loss tensor([0.4679, 0.2624, 0.2983, 0.6774, 0.3009, 0.5908])\n",
      "\n",
      "\n",
      "************** Batch 836 in 0.5294580459594727 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4667, 0.2638, 0.3008, 0.6751, 0.3016, 0.5889]) \n",
      "Test Loss tensor([0.4681, 0.2631, 0.2982, 0.6787, 0.3005, 0.5923])\n",
      "\n",
      "\n",
      "************** Batch 840 in 0.5409531593322754 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4703, 0.2609, 0.2983, 0.6791, 0.3024, 0.5932]) \n",
      "Test Loss tensor([0.4680, 0.2626, 0.2990, 0.6786, 0.3005, 0.5910])\n",
      "\n",
      "\n",
      "************** Batch 844 in 0.5503087043762207 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4703, 0.2617, 0.2984, 0.6775, 0.2989, 0.5888]) \n",
      "Test Loss tensor([0.4682, 0.2609, 0.2987, 0.6815, 0.2987, 0.5918])\n",
      "\n",
      "\n",
      "************** Batch 848 in 0.5488386154174805 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4661, 0.2594, 0.2987, 0.6772, 0.2971, 0.5912]) \n",
      "Test Loss tensor([0.4700, 0.2600, 0.2987, 0.6811, 0.2983, 0.5928])\n",
      "\n",
      "\n",
      "************** Batch 852 in 0.5366847515106201 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4625, 0.2615, 0.2997, 0.6758, 0.2985, 0.5940]) \n",
      "Test Loss tensor([0.4664, 0.2589, 0.2980, 0.6826, 0.2970, 0.5918])\n",
      "\n",
      "\n",
      "************** Batch 856 in 0.5384633541107178 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4727, 0.2584, 0.2976, 0.6859, 0.2956, 0.5907]) \n",
      "Test Loss tensor([0.4675, 0.2584, 0.2990, 0.6827, 0.2967, 0.5915])\n",
      "\n",
      "\n",
      "************** Batch 860 in 0.5291018486022949 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4648, 0.2619, 0.3009, 0.6851, 0.2958, 0.5890]) \n",
      "Test Loss tensor([0.4661, 0.2573, 0.2982, 0.6845, 0.2964, 0.5907])\n",
      "\n",
      "\n",
      "************** Batch 864 in 0.5482509136199951 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4641, 0.2579, 0.2992, 0.6843, 0.3004, 0.5857]) \n",
      "Test Loss tensor([0.4673, 0.2570, 0.2993, 0.6838, 0.2949, 0.5917])\n",
      "\n",
      "\n",
      "************** Batch 868 in 0.5168232917785645 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4709, 0.2564, 0.3040, 0.6882, 0.2977, 0.5913]) \n",
      "Test Loss tensor([0.4671, 0.2559, 0.2980, 0.6855, 0.2948, 0.5916])\n",
      "\n",
      "\n",
      "************** Batch 872 in 0.5383400917053223 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4727, 0.2534, 0.2961, 0.6867, 0.2962, 0.5932]) \n",
      "Test Loss tensor([0.4662, 0.2547, 0.2988, 0.6876, 0.2926, 0.5910])\n",
      "\n",
      "\n",
      "************** Batch 876 in 0.5260055065155029 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3510, 0.1921, 0.2258, 0.5143, 0.2238, 0.4480]) \n",
      "Test Loss tensor([0.4670, 0.2533, 0.2990, 0.6864, 0.2909, 0.5914])\n",
      "\n",
      "\n",
      "************** Batch 0 in 0.5369033813476562 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4650, 0.2512, 0.3017, 0.6872, 0.2969, 0.5950]) \n",
      "Test Loss tensor([0.4661, 0.2526, 0.2994, 0.6884, 0.2905, 0.5901])\n",
      "\n",
      "\n",
      "************** Batch 4 in 0.5516784191131592 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4624, 0.2525, 0.3013, 0.6835, 0.2882, 0.5879]) \n",
      "Test Loss tensor([0.4663, 0.2512, 0.2994, 0.6911, 0.2901, 0.5905])\n",
      "\n",
      "\n",
      "************** Batch 8 in 0.5265789031982422 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4676, 0.2548, 0.2974, 0.6872, 0.2856, 0.5919]) \n",
      "Test Loss tensor([0.4672, 0.2505, 0.3000, 0.6913, 0.2879, 0.5900])\n",
      "\n",
      "\n",
      "************** Batch 12 in 0.5666952133178711 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4649, 0.2524, 0.3030, 0.6939, 0.2910, 0.5911]) \n",
      "Test Loss tensor([0.4654, 0.2496, 0.3013, 0.6911, 0.2880, 0.5887])\n",
      "\n",
      "\n",
      "************** Batch 16 in 0.5445377826690674 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4626, 0.2501, 0.3001, 0.6920, 0.2930, 0.5887]) \n",
      "Test Loss tensor([0.4666, 0.2475, 0.3014, 0.6922, 0.2863, 0.5881])\n",
      "\n",
      "\n",
      "************** Batch 20 in 0.5371863842010498 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4649, 0.2489, 0.3026, 0.6921, 0.2879, 0.5921]) \n",
      "Test Loss tensor([0.4663, 0.2476, 0.3031, 0.6915, 0.2850, 0.5868])\n",
      "\n",
      "\n",
      "************** Batch 24 in 0.532113790512085 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4641, 0.2465, 0.3025, 0.6904, 0.2878, 0.5839]) \n",
      "Test Loss tensor([0.4649, 0.2454, 0.3022, 0.6935, 0.2852, 0.5865])\n",
      "\n",
      "\n",
      "************** Batch 28 in 0.5490648746490479 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4646, 0.2438, 0.3033, 0.6936, 0.2785, 0.5910]) \n",
      "Test Loss tensor([0.4668, 0.2442, 0.3035, 0.6950, 0.2829, 0.5860])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 32 in 0.5296206474304199 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4658, 0.2456, 0.3056, 0.6917, 0.2798, 0.5893]) \n",
      "Test Loss tensor([0.4639, 0.2422, 0.3047, 0.6952, 0.2816, 0.5845])\n",
      "\n",
      "\n",
      "************** Batch 36 in 0.554046630859375 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4647, 0.2424, 0.3039, 0.6952, 0.2805, 0.5878]) \n",
      "Test Loss tensor([0.4646, 0.2422, 0.3055, 0.6957, 0.2808, 0.5828])\n",
      "\n",
      "\n",
      "************** Batch 40 in 0.5377156734466553 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4632, 0.2413, 0.3037, 0.6954, 0.2799, 0.5822]) \n",
      "Test Loss tensor([0.4647, 0.2389, 0.3065, 0.6953, 0.2799, 0.5819])\n",
      "\n",
      "\n",
      "************** Batch 44 in 0.5578279495239258 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4598, 0.2408, 0.3089, 0.6982, 0.2891, 0.5831]) \n",
      "Test Loss tensor([0.4645, 0.2377, 0.3082, 0.6958, 0.2781, 0.5799])\n",
      "\n",
      "\n",
      "************** Batch 48 in 0.5402917861938477 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4631, 0.2404, 0.3081, 0.6953, 0.2793, 0.5785]) \n",
      "Test Loss tensor([0.4638, 0.2358, 0.3099, 0.6947, 0.2763, 0.5786])\n",
      "\n",
      "\n",
      "************** Batch 52 in 0.5374493598937988 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4646, 0.2352, 0.3102, 0.6938, 0.2731, 0.5771]) \n",
      "Test Loss tensor([0.4644, 0.2341, 0.3110, 0.6969, 0.2741, 0.5768])\n",
      "\n",
      "\n",
      "************** Batch 56 in 0.5381884574890137 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4658, 0.2369, 0.3098, 0.6980, 0.2753, 0.5814]) \n",
      "Test Loss tensor([0.4624, 0.2319, 0.3131, 0.6970, 0.2722, 0.5726])\n",
      "\n",
      "\n",
      "************** Batch 60 in 0.5293533802032471 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4643, 0.2304, 0.3143, 0.6929, 0.2740, 0.5734]) \n",
      "Test Loss tensor([0.4625, 0.2283, 0.3148, 0.6961, 0.2705, 0.5722])\n",
      "\n",
      "\n",
      "************** Batch 64 in 0.5451428890228271 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4609, 0.2289, 0.3155, 0.6919, 0.2661, 0.5672]) \n",
      "Test Loss tensor([0.4634, 0.2249, 0.3149, 0.6957, 0.2667, 0.5701])\n",
      "\n",
      "\n",
      "************** Batch 68 in 0.531731367111206 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4610, 0.2263, 0.3149, 0.6949, 0.2647, 0.5663]) \n",
      "Test Loss tensor([0.4630, 0.2218, 0.3171, 0.6957, 0.2651, 0.5678])\n",
      "\n",
      "\n",
      "************** Batch 72 in 0.5492725372314453 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4593, 0.2203, 0.3158, 0.6932, 0.2650, 0.5683]) \n",
      "Test Loss tensor([0.4631, 0.2188, 0.3181, 0.6953, 0.2636, 0.5660])\n",
      "\n",
      "\n",
      "************** Batch 76 in 0.5365962982177734 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4628, 0.2169, 0.3196, 0.6981, 0.2633, 0.5693]) \n",
      "Test Loss tensor([0.4639, 0.2141, 0.3186, 0.6944, 0.2579, 0.5638])\n",
      "\n",
      "\n",
      "************** Batch 80 in 0.5500874519348145 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4665, 0.2155, 0.3229, 0.6955, 0.2655, 0.5687]) \n",
      "Test Loss tensor([0.4611, 0.2085, 0.3224, 0.6916, 0.2553, 0.5602])\n",
      "\n",
      "\n",
      "************** Batch 84 in 0.5340385437011719 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4594, 0.2108, 0.3200, 0.6951, 0.2607, 0.5618]) \n",
      "Test Loss tensor([0.4624, 0.2035, 0.3228, 0.6906, 0.2544, 0.5584])\n",
      "\n",
      "\n",
      "************** Batch 88 in 0.5360496044158936 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4623, 0.2065, 0.3231, 0.6892, 0.2547, 0.5577]) \n",
      "Test Loss tensor([0.4610, 0.1977, 0.3257, 0.6872, 0.2495, 0.5545])\n",
      "\n",
      "\n",
      "************** Batch 92 in 0.5368177890777588 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4580, 0.1997, 0.3276, 0.6890, 0.2548, 0.5552]) \n",
      "Test Loss tensor([0.4620, 0.1926, 0.3285, 0.6824, 0.2487, 0.5531])\n",
      "\n",
      "\n",
      "************** Batch 96 in 0.5268867015838623 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4611, 0.1955, 0.3293, 0.6832, 0.2424, 0.5498]) \n",
      "Test Loss tensor([0.4618, 0.1867, 0.3304, 0.6774, 0.2455, 0.5497])\n",
      "\n",
      "\n",
      "************** Batch 100 in 0.5419423580169678 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4607, 0.1894, 0.3314, 0.6786, 0.2431, 0.5515]) \n",
      "Test Loss tensor([0.4606, 0.1813, 0.3344, 0.6712, 0.2405, 0.5454])\n",
      "\n",
      "\n",
      "************** Batch 104 in 0.5480034351348877 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4592, 0.1852, 0.3344, 0.6727, 0.2526, 0.5464]) \n",
      "Test Loss tensor([0.4602, 0.1733, 0.3366, 0.6645, 0.2373, 0.5424])\n",
      "\n",
      "\n",
      "************** Batch 108 in 0.570885181427002 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4629, 0.1748, 0.3344, 0.6643, 0.2378, 0.5444]) \n",
      "Test Loss tensor([0.4611, 0.1685, 0.3402, 0.6572, 0.2363, 0.5403])\n",
      "\n",
      "\n",
      "************** Batch 112 in 0.5297234058380127 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4644, 0.1696, 0.3407, 0.6577, 0.2388, 0.5401]) \n",
      "Test Loss tensor([0.4611, 0.1613, 0.3435, 0.6478, 0.2341, 0.5356])\n",
      "\n",
      "\n",
      "************** Batch 116 in 0.5332391262054443 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4591, 0.1632, 0.3465, 0.6488, 0.2380, 0.5343]) \n",
      "Test Loss tensor([0.4604, 0.1545, 0.3477, 0.6361, 0.2294, 0.5308])\n",
      "\n",
      "\n",
      "************** Batch 120 in 0.5308961868286133 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4573, 0.1553, 0.3496, 0.6329, 0.2336, 0.5274]) \n",
      "Test Loss tensor([0.4604, 0.1477, 0.3535, 0.6232, 0.2257, 0.5263])\n",
      "\n",
      "\n",
      "************** Batch 124 in 0.5423593521118164 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4568, 0.1478, 0.3515, 0.6196, 0.2267, 0.5274]) \n",
      "Test Loss tensor([0.4616, 0.1430, 0.3592, 0.6107, 0.2261, 0.5229])\n",
      "\n",
      "\n",
      "************** Batch 128 in 0.527393102645874 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4614, 0.1461, 0.3589, 0.6088, 0.2281, 0.5244]) \n",
      "Test Loss tensor([0.4615, 0.1380, 0.3669, 0.5966, 0.2261, 0.5182])\n",
      "\n",
      "\n",
      "************** Batch 132 in 0.5503406524658203 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4602, 0.1387, 0.3664, 0.5967, 0.2296, 0.5180]) \n",
      "Test Loss tensor([0.4595, 0.1339, 0.3738, 0.5810, 0.2213, 0.5136])\n",
      "\n",
      "\n",
      "************** Batch 136 in 0.5276329517364502 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4596, 0.1311, 0.3741, 0.5793, 0.2295, 0.5156]) \n",
      "Test Loss tensor([0.4593, 0.1291, 0.3784, 0.5662, 0.2208, 0.5110])\n",
      "\n",
      "\n",
      "************** Batch 140 in 0.5404024124145508 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4589, 0.1311, 0.3778, 0.5650, 0.2249, 0.5096]) \n",
      "Test Loss tensor([0.4608, 0.1283, 0.3815, 0.5558, 0.2250, 0.5087])\n",
      "\n",
      "\n",
      "************** Batch 144 in 0.5305745601654053 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4626, 0.1292, 0.3810, 0.5560, 0.2175, 0.5102]) \n",
      "Test Loss tensor([0.4612, 0.1244, 0.3847, 0.5470, 0.2232, 0.5067])\n",
      "\n",
      "\n",
      "************** Batch 148 in 0.5450272560119629 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4634, 0.1207, 0.3850, 0.5475, 0.2242, 0.5090]) \n",
      "Test Loss tensor([0.4608, 0.1168, 0.3879, 0.5414, 0.2187, 0.5029])\n",
      "\n",
      "\n",
      "************** Batch 152 in 0.5431303977966309 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4607, 0.1162, 0.3892, 0.5389, 0.2182, 0.5014]) \n",
      "Test Loss tensor([0.4616, 0.1104, 0.3910, 0.5371, 0.2207, 0.5011])\n",
      "\n",
      "\n",
      "************** Batch 156 in 0.5385675430297852 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4614, 0.1121, 0.3911, 0.5362, 0.2212, 0.4996]) \n",
      "Test Loss tensor([0.4628, 0.1023, 0.3939, 0.5324, 0.2212, 0.4993])\n",
      "\n",
      "\n",
      "************** Batch 160 in 0.5374691486358643 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4637, 0.1014, 0.3947, 0.5333, 0.2188, 0.5012]) \n",
      "Test Loss tensor([0.4645, 0.0957, 0.3966, 0.5290, 0.2194, 0.4972])\n",
      "\n",
      "\n",
      "************** Batch 164 in 0.5353255271911621 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4669, 0.0952, 0.3947, 0.5279, 0.2134, 0.4973]) \n",
      "Test Loss tensor([0.4672, 0.0872, 0.3987, 0.5248, 0.2178, 0.4956])\n",
      "\n",
      "\n",
      "************** Batch 168 in 0.5578744411468506 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4687, 0.0865, 0.3993, 0.5245, 0.2131, 0.4951]) \n",
      "Test Loss tensor([0.4656, 0.0833, 0.4020, 0.5213, 0.2161, 0.4920])\n",
      "\n",
      "\n",
      "************** Batch 172 in 0.5291831493377686 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4643, 0.0835, 0.4043, 0.5211, 0.2181, 0.4892]) \n",
      "Test Loss tensor([0.4661, 0.0778, 0.4046, 0.5178, 0.2151, 0.4899])\n",
      "\n",
      "\n",
      "************** Batch 176 in 0.53603196144104 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4627, 0.0789, 0.4044, 0.5181, 0.2173, 0.4926]) \n",
      "Test Loss tensor([0.4606, 0.0755, 0.4078, 0.5139, 0.2187, 0.4873])\n",
      "\n",
      "\n",
      "************** Batch 180 in 0.5291955471038818 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4629, 0.0770, 0.4078, 0.5110, 0.2208, 0.4874]) \n",
      "Test Loss tensor([0.4574, 0.0768, 0.4109, 0.5100, 0.2186, 0.4856])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 184 in 0.5474205017089844 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4550, 0.0717, 0.4095, 0.5107, 0.2236, 0.4855]) \n",
      "Test Loss tensor([0.4541, 0.0752, 0.4137, 0.5070, 0.2182, 0.4834])\n",
      "\n",
      "\n",
      "************** Batch 188 in 0.5286734104156494 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4451, 0.0777, 0.4147, 0.5056, 0.2163, 0.4817]) \n",
      "Test Loss tensor([0.4491, 0.0737, 0.4166, 0.5028, 0.2171, 0.4814])\n",
      "\n",
      "\n",
      "************** Batch 192 in 0.5516188144683838 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4504, 0.0789, 0.4164, 0.5052, 0.2211, 0.4848]) \n",
      "Test Loss tensor([0.4490, 0.0712, 0.4182, 0.4994, 0.2155, 0.4787])\n",
      "\n",
      "\n",
      "************** Batch 196 in 0.5434615612030029 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4511, 0.0672, 0.4198, 0.4996, 0.2066, 0.4794]) \n",
      "Test Loss tensor([0.4457, 0.0702, 0.4218, 0.4969, 0.2204, 0.4763])\n",
      "\n",
      "\n",
      "************** Batch 200 in 0.5419492721557617 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4435, 0.0741, 0.4226, 0.4964, 0.2224, 0.4769]) \n",
      "Test Loss tensor([0.4452, 0.0683, 0.4244, 0.4946, 0.2176, 0.4746])\n",
      "\n",
      "\n",
      "************** Batch 204 in 0.5764060020446777 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4419, 0.0652, 0.4259, 0.4936, 0.2099, 0.4754]) \n",
      "Test Loss tensor([0.4433, 0.0625, 0.4274, 0.4915, 0.2193, 0.4722])\n",
      "\n",
      "\n",
      "************** Batch 208 in 0.5340189933776855 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4396, 0.0622, 0.4268, 0.4925, 0.2193, 0.4707]) \n",
      "Test Loss tensor([0.4409, 0.0611, 0.4301, 0.4882, 0.2220, 0.4707])\n",
      "\n",
      "\n",
      "************** Batch 212 in 0.5543162822723389 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4415, 0.0656, 0.4299, 0.4871, 0.2255, 0.4722]) \n",
      "Test Loss tensor([0.4379, 0.0574, 0.4321, 0.4852, 0.2232, 0.4690])\n",
      "\n",
      "\n",
      "************** Batch 216 in 0.5437164306640625 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4382, 0.0560, 0.4339, 0.4840, 0.2186, 0.4686]) \n",
      "Test Loss tensor([0.4387, 0.0549, 0.4343, 0.4828, 0.2228, 0.4671])\n",
      "\n",
      "\n",
      "************** Batch 220 in 0.5477278232574463 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4341, 0.0563, 0.4359, 0.4800, 0.2265, 0.4655]) \n",
      "Test Loss tensor([0.4339, 0.0564, 0.4375, 0.4806, 0.2263, 0.4654])\n",
      "\n",
      "\n",
      "************** Batch 224 in 0.5369782447814941 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4318, 0.0657, 0.4371, 0.4785, 0.2198, 0.4649]) \n",
      "Test Loss tensor([0.4306, 0.0575, 0.4400, 0.4773, 0.2236, 0.4638])\n",
      "\n",
      "\n",
      "************** Batch 228 in 0.5416641235351562 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4325, 0.0580, 0.4376, 0.4766, 0.2324, 0.4640]) \n",
      "Test Loss tensor([0.4277, 0.0549, 0.4417, 0.4745, 0.2261, 0.4623])\n",
      "\n",
      "\n",
      "************** Batch 232 in 0.5341284275054932 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4255, 0.0538, 0.4413, 0.4732, 0.2245, 0.4625]) \n",
      "Test Loss tensor([0.4254, 0.0555, 0.4436, 0.4716, 0.2262, 0.4600])\n",
      "\n",
      "\n",
      "************** Batch 236 in 0.5416326522827148 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4227, 0.0581, 0.4443, 0.4726, 0.2280, 0.4611]) \n",
      "Test Loss tensor([0.4211, 0.0547, 0.4464, 0.4694, 0.2247, 0.4583])\n",
      "\n",
      "\n",
      "************** Batch 240 in 0.5507433414459229 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4195, 0.0575, 0.4451, 0.4698, 0.2379, 0.4594]) \n",
      "Test Loss tensor([0.4176, 0.0541, 0.4483, 0.4672, 0.2255, 0.4567])\n",
      "\n",
      "\n",
      "************** Batch 244 in 0.5476889610290527 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4172, 0.0569, 0.4481, 0.4679, 0.2239, 0.4565]) \n",
      "Test Loss tensor([0.4132, 0.0559, 0.4514, 0.4649, 0.2265, 0.4550])\n",
      "\n",
      "\n",
      "************** Batch 248 in 0.5475037097930908 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4135, 0.0542, 0.4509, 0.4651, 0.2172, 0.4550]) \n",
      "Test Loss tensor([0.4078, 0.0515, 0.4527, 0.4624, 0.2238, 0.4535])\n",
      "\n",
      "\n",
      "************** Batch 252 in 0.5294182300567627 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4065, 0.0499, 0.4515, 0.4630, 0.2373, 0.4522]) \n",
      "Test Loss tensor([0.4042, 0.0523, 0.4561, 0.4604, 0.2272, 0.4509])\n",
      "\n",
      "\n",
      "************** Batch 256 in 0.5368838310241699 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.4070, 0.0534, 0.4563, 0.4596, 0.2267, 0.4522]) \n",
      "Test Loss tensor([0.4002, 0.0527, 0.4583, 0.4575, 0.2271, 0.4500])\n",
      "\n",
      "\n",
      "************** Batch 260 in 0.5359311103820801 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3995, 0.0598, 0.4577, 0.4572, 0.2270, 0.4492]) \n",
      "Test Loss tensor([0.3963, 0.0522, 0.4605, 0.4547, 0.2242, 0.4482])\n",
      "\n",
      "\n",
      "************** Batch 264 in 0.5395767688751221 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3967, 0.0547, 0.4628, 0.4557, 0.2293, 0.4495]) \n",
      "Test Loss tensor([0.3911, 0.0534, 0.4626, 0.4528, 0.2239, 0.4468])\n",
      "\n",
      "\n",
      "************** Batch 268 in 0.5305967330932617 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3892, 0.0540, 0.4623, 0.4531, 0.2095, 0.4457]) \n",
      "Test Loss tensor([0.3882, 0.0533, 0.4652, 0.4504, 0.2232, 0.4443])\n",
      "\n",
      "\n",
      "************** Batch 272 in 0.5523388385772705 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3826, 0.0526, 0.4645, 0.4502, 0.2256, 0.4453]) \n",
      "Test Loss tensor([0.3832, 0.0549, 0.4679, 0.4477, 0.2267, 0.4431])\n",
      "\n",
      "\n",
      "************** Batch 276 in 0.53900146484375 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3842, 0.0561, 0.4672, 0.4463, 0.2270, 0.4446]) \n",
      "Test Loss tensor([0.3805, 0.0539, 0.4704, 0.4460, 0.2221, 0.4421])\n",
      "\n",
      "\n",
      "************** Batch 280 in 0.5505084991455078 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3809, 0.0544, 0.4708, 0.4456, 0.2330, 0.4400]) \n",
      "Test Loss tensor([0.3779, 0.0526, 0.4724, 0.4440, 0.2216, 0.4403])\n",
      "\n",
      "\n",
      "************** Batch 284 in 0.5264954566955566 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3819, 0.0529, 0.4744, 0.4426, 0.2278, 0.4408]) \n",
      "Test Loss tensor([0.3740, 0.0533, 0.4752, 0.4413, 0.2243, 0.4383])\n",
      "\n",
      "\n",
      "************** Batch 288 in 0.5411927700042725 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3709, 0.0600, 0.4739, 0.4395, 0.2224, 0.4383]) \n",
      "Test Loss tensor([0.3690, 0.0548, 0.4773, 0.4391, 0.2216, 0.4370])\n",
      "\n",
      "\n",
      "************** Batch 292 in 0.5485985279083252 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3713, 0.0578, 0.4784, 0.4379, 0.2216, 0.4370]) \n",
      "Test Loss tensor([0.3659, 0.0562, 0.4795, 0.4365, 0.2194, 0.4360])\n",
      "\n",
      "\n",
      "************** Batch 296 in 0.5366761684417725 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3652, 0.0535, 0.4804, 0.4366, 0.2352, 0.4373]) \n",
      "Test Loss tensor([0.3632, 0.0562, 0.4819, 0.4342, 0.2211, 0.4340])\n",
      "\n",
      "\n",
      "************** Batch 300 in 0.5440142154693604 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3631, 0.0582, 0.4820, 0.4349, 0.2229, 0.4335]) \n",
      "Test Loss tensor([0.3605, 0.0567, 0.4845, 0.4319, 0.2210, 0.4324])\n",
      "\n",
      "\n",
      "************** Batch 304 in 0.5376951694488525 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3623, 0.0617, 0.4821, 0.4323, 0.2230, 0.4333]) \n",
      "Test Loss tensor([0.3561, 0.0559, 0.4869, 0.4305, 0.2159, 0.4306])\n",
      "\n",
      "\n",
      "************** Batch 308 in 0.5488913059234619 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3581, 0.0613, 0.4873, 0.4303, 0.2208, 0.4333]) \n",
      "Test Loss tensor([0.3544, 0.0545, 0.4887, 0.4275, 0.2217, 0.4295])\n",
      "\n",
      "\n",
      "************** Batch 312 in 0.5340695381164551 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3567, 0.0611, 0.4903, 0.4267, 0.2165, 0.4292]) \n",
      "Test Loss tensor([0.3520, 0.0553, 0.4913, 0.4262, 0.2193, 0.4289])\n",
      "\n",
      "\n",
      "************** Batch 316 in 0.5480163097381592 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3595, 0.0570, 0.4948, 0.4271, 0.2323, 0.4264]) \n",
      "Test Loss tensor([0.3498, 0.0563, 0.4939, 0.4237, 0.2200, 0.4262])\n",
      "\n",
      "\n",
      "************** Batch 320 in 0.5377779006958008 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3495, 0.0573, 0.4961, 0.4248, 0.2264, 0.4260]) \n",
      "Test Loss tensor([0.3473, 0.0579, 0.4966, 0.4214, 0.2199, 0.4256])\n",
      "\n",
      "\n",
      "************** Batch 324 in 0.5488560199737549 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3506, 0.0579, 0.4963, 0.4221, 0.2209, 0.4243]) \n",
      "Test Loss tensor([0.3455, 0.0575, 0.4989, 0.4185, 0.2165, 0.4230])\n",
      "\n",
      "\n",
      "************** Batch 328 in 0.5382635593414307 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3483, 0.0546, 0.4977, 0.4167, 0.2030, 0.4260]) \n",
      "Test Loss tensor([0.3438, 0.0583, 0.5010, 0.4172, 0.2192, 0.4224])\n",
      "\n",
      "\n",
      "************** Batch 332 in 0.5406415462493896 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3474, 0.0565, 0.5001, 0.4185, 0.2142, 0.4237]) \n",
      "Test Loss tensor([0.3401, 0.0593, 0.5042, 0.4146, 0.2187, 0.4209])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 336 in 0.5496346950531006 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3465, 0.0545, 0.5040, 0.4146, 0.2161, 0.4217]) \n",
      "Test Loss tensor([0.3384, 0.0567, 0.5054, 0.4120, 0.2126, 0.4189])\n",
      "\n",
      "\n",
      "************** Batch 340 in 0.5305476188659668 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3392, 0.0597, 0.5036, 0.4122, 0.2044, 0.4196]) \n",
      "Test Loss tensor([0.3393, 0.0595, 0.5090, 0.4096, 0.2182, 0.4176])\n",
      "\n",
      "\n",
      "************** Batch 344 in 0.5415654182434082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3409, 0.0579, 0.5088, 0.4084, 0.2115, 0.4159]) \n",
      "Test Loss tensor([0.3334, 0.0567, 0.5109, 0.4073, 0.2156, 0.4162])\n",
      "\n",
      "\n",
      "************** Batch 348 in 0.5341329574584961 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3358, 0.0542, 0.5094, 0.4079, 0.2161, 0.4185]) \n",
      "Test Loss tensor([0.3341, 0.0568, 0.5134, 0.4052, 0.2171, 0.4145])\n",
      "\n",
      "\n",
      "************** Batch 352 in 0.5488667488098145 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3389, 0.0605, 0.5131, 0.4049, 0.2161, 0.4131]) \n",
      "Test Loss tensor([0.3319, 0.0567, 0.5165, 0.4034, 0.2158, 0.4133])\n",
      "\n",
      "\n",
      "************** Batch 356 in 0.5371425151824951 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3318, 0.0578, 0.5176, 0.4025, 0.2082, 0.4134]) \n",
      "Test Loss tensor([0.3294, 0.0564, 0.5186, 0.4012, 0.2161, 0.4113])\n",
      "\n",
      "\n",
      "************** Batch 360 in 0.5457074642181396 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3299, 0.0582, 0.5192, 0.4008, 0.2168, 0.4115]) \n",
      "Test Loss tensor([0.3289, 0.0549, 0.5213, 0.3983, 0.2175, 0.4091])\n",
      "\n",
      "\n",
      "************** Batch 364 in 0.5360326766967773 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3284, 0.0559, 0.5210, 0.3946, 0.2028, 0.4098]) \n",
      "Test Loss tensor([0.3259, 0.0535, 0.5238, 0.3957, 0.2162, 0.4087])\n",
      "\n",
      "\n",
      "************** Batch 368 in 0.5481078624725342 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3201, 0.0548, 0.5235, 0.3934, 0.2182, 0.4088]) \n",
      "Test Loss tensor([0.3256, 0.0538, 0.5264, 0.3931, 0.2123, 0.4077])\n",
      "\n",
      "\n",
      "************** Batch 372 in 0.5303671360015869 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3310, 0.0578, 0.5266, 0.3940, 0.2208, 0.4078]) \n",
      "Test Loss tensor([0.3246, 0.0569, 0.5292, 0.3910, 0.2158, 0.4062])\n",
      "\n",
      "\n",
      "************** Batch 376 in 0.5520234107971191 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3213, 0.0545, 0.5312, 0.3919, 0.2071, 0.4079]) \n",
      "Test Loss tensor([0.3222, 0.0539, 0.5312, 0.3891, 0.2135, 0.4049])\n",
      "\n",
      "\n",
      "************** Batch 380 in 0.5310981273651123 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3212, 0.0527, 0.5300, 0.3855, 0.2027, 0.4096]) \n",
      "Test Loss tensor([0.3177, 0.0550, 0.5341, 0.3867, 0.2133, 0.4034])\n",
      "\n",
      "\n",
      "************** Batch 384 in 0.5404500961303711 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3156, 0.0584, 0.5315, 0.3858, 0.2155, 0.4055]) \n",
      "Test Loss tensor([0.3195, 0.0523, 0.5369, 0.3842, 0.2147, 0.4012])\n",
      "\n",
      "\n",
      "************** Batch 388 in 0.540454626083374 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3193, 0.0494, 0.5381, 0.3867, 0.2102, 0.4004]) \n",
      "Test Loss tensor([0.3192, 0.0503, 0.5394, 0.3815, 0.2142, 0.3992])\n",
      "\n",
      "\n",
      "************** Batch 392 in 0.5384137630462646 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3173, 0.0501, 0.5389, 0.3823, 0.2185, 0.4022]) \n",
      "Test Loss tensor([0.3174, 0.0508, 0.5421, 0.3801, 0.2116, 0.3983])\n",
      "\n",
      "\n",
      "************** Batch 396 in 0.5527994632720947 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3179, 0.0486, 0.5413, 0.3781, 0.2096, 0.3973]) \n",
      "Test Loss tensor([0.3143, 0.0515, 0.5441, 0.3780, 0.2084, 0.3973])\n",
      "\n",
      "\n",
      "************** Batch 400 in 0.5381100177764893 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3148, 0.0539, 0.5431, 0.3764, 0.2100, 0.3978]) \n",
      "Test Loss tensor([0.3125, 0.0532, 0.5463, 0.3759, 0.2095, 0.3958])\n",
      "\n",
      "\n",
      "************** Batch 404 in 0.5498135089874268 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3153, 0.0543, 0.5473, 0.3765, 0.1972, 0.3955]) \n",
      "Test Loss tensor([0.3123, 0.0525, 0.5488, 0.3731, 0.2066, 0.3954])\n",
      "\n",
      "\n",
      "************** Batch 408 in 0.5307109355926514 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3099, 0.0473, 0.5460, 0.3736, 0.1985, 0.3985]) \n",
      "Test Loss tensor([0.3098, 0.0484, 0.5516, 0.3714, 0.2064, 0.3936])\n",
      "\n",
      "\n",
      "************** Batch 412 in 0.5482864379882812 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3093, 0.0482, 0.5490, 0.3693, 0.2062, 0.3931]) \n",
      "Test Loss tensor([0.3076, 0.0475, 0.5536, 0.3687, 0.2075, 0.3922])\n",
      "\n",
      "\n",
      "************** Batch 416 in 0.5407969951629639 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3092, 0.0483, 0.5535, 0.3699, 0.2005, 0.3946]) \n",
      "Test Loss tensor([0.3068, 0.0451, 0.5560, 0.3670, 0.2058, 0.3910])\n",
      "\n",
      "\n",
      "************** Batch 420 in 0.5538265705108643 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3096, 0.0480, 0.5571, 0.3674, 0.2111, 0.3915]) \n",
      "Test Loss tensor([0.3048, 0.0466, 0.5585, 0.3651, 0.2022, 0.3893])\n",
      "\n",
      "\n",
      "************** Batch 424 in 0.5385575294494629 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3115, 0.0463, 0.5597, 0.3648, 0.1949, 0.3882]) \n",
      "Test Loss tensor([0.3051, 0.0471, 0.5619, 0.3642, 0.1977, 0.3892])\n",
      "\n",
      "\n",
      "************** Batch 428 in 0.5333340167999268 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3078, 0.0488, 0.5603, 0.3625, 0.1992, 0.3916]) \n",
      "Test Loss tensor([0.3043, 0.0469, 0.5635, 0.3615, 0.1956, 0.3874])\n",
      "\n",
      "\n",
      "************** Batch 432 in 0.5490028858184814 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3064, 0.0536, 0.5656, 0.3610, 0.2131, 0.3864]) \n",
      "Test Loss tensor([0.3015, 0.0465, 0.5666, 0.3607, 0.1999, 0.3855])\n",
      "\n",
      "\n",
      "************** Batch 436 in 0.5609574317932129 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3089, 0.0470, 0.5693, 0.3622, 0.2055, 0.3832]) \n",
      "Test Loss tensor([0.2993, 0.0443, 0.5677, 0.3591, 0.1986, 0.3852])\n",
      "\n",
      "\n",
      "************** Batch 440 in 0.54681396484375 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3064, 0.0464, 0.5712, 0.3582, 0.1987, 0.3821]) \n",
      "Test Loss tensor([0.2973, 0.0414, 0.5700, 0.3571, 0.1947, 0.3839])\n",
      "\n",
      "\n",
      "************** Batch 444 in 0.5317091941833496 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3021, 0.0403, 0.5745, 0.3549, 0.1914, 0.3850]) \n",
      "Test Loss tensor([0.2974, 0.0405, 0.5728, 0.3558, 0.1983, 0.3825])\n",
      "\n",
      "\n",
      "************** Batch 448 in 0.541374921798706 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.3019, 0.0412, 0.5736, 0.3546, 0.1958, 0.3805]) \n",
      "Test Loss tensor([0.2935, 0.0396, 0.5747, 0.3543, 0.1963, 0.3822])\n",
      "\n",
      "\n",
      "************** Batch 452 in 0.5278172492980957 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2853, 0.0361, 0.5748, 0.3539, 0.1861, 0.3841]) \n",
      "Test Loss tensor([0.2939, 0.0403, 0.5777, 0.3520, 0.1954, 0.3789])\n",
      "\n",
      "\n",
      "************** Batch 456 in 0.5479369163513184 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2924, 0.0413, 0.5790, 0.3515, 0.1943, 0.3835]) \n",
      "Test Loss tensor([0.2945, 0.0386, 0.5798, 0.3503, 0.1980, 0.3790])\n",
      "\n",
      "\n",
      "************** Batch 460 in 0.5376427173614502 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2935, 0.0402, 0.5817, 0.3496, 0.1991, 0.3791]) \n",
      "Test Loss tensor([0.2899, 0.0382, 0.5819, 0.3476, 0.1955, 0.3777])\n",
      "\n",
      "\n",
      "************** Batch 464 in 0.5466217994689941 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2931, 0.0392, 0.5805, 0.3479, 0.1984, 0.3800]) \n",
      "Test Loss tensor([0.2884, 0.0370, 0.5839, 0.3450, 0.1967, 0.3763])\n",
      "\n",
      "\n",
      "************** Batch 468 in 0.5320987701416016 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2860, 0.0392, 0.5820, 0.3444, 0.1921, 0.3797]) \n",
      "Test Loss tensor([0.2866, 0.0358, 0.5861, 0.3434, 0.1915, 0.3759])\n",
      "\n",
      "\n",
      "************** Batch 472 in 0.5478754043579102 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2849, 0.0368, 0.5866, 0.3414, 0.2060, 0.3748]) \n",
      "Test Loss tensor([0.2869, 0.0363, 0.5887, 0.3415, 0.1943, 0.3745])\n",
      "\n",
      "\n",
      "************** Batch 476 in 0.5524661540985107 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2795, 0.0341, 0.5865, 0.3414, 0.1921, 0.3774]) \n",
      "Test Loss tensor([0.2845, 0.0366, 0.5904, 0.3394, 0.1938, 0.3732])\n",
      "\n",
      "\n",
      "************** Batch 480 in 0.530442476272583 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2810, 0.0367, 0.5850, 0.3413, 0.1914, 0.3759]) \n",
      "Test Loss tensor([0.2833, 0.0365, 0.5923, 0.3377, 0.1953, 0.3732])\n",
      "\n",
      "\n",
      "************** Batch 484 in 0.5424692630767822 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2882, 0.0361, 0.5911, 0.3392, 0.1880, 0.3721]) \n",
      "Test Loss tensor([0.2818, 0.0351, 0.5945, 0.3359, 0.1941, 0.3716])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 488 in 0.5324983596801758 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2823, 0.0308, 0.5931, 0.3385, 0.1875, 0.3725]) \n",
      "Test Loss tensor([0.2785, 0.0344, 0.5963, 0.3336, 0.1947, 0.3706])\n",
      "\n",
      "\n",
      "************** Batch 492 in 0.5389766693115234 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2843, 0.0385, 0.5912, 0.3353, 0.1955, 0.3722]) \n",
      "Test Loss tensor([0.2774, 0.0343, 0.5975, 0.3327, 0.1939, 0.3701])\n",
      "\n",
      "\n",
      "************** Batch 496 in 0.5296876430511475 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2842, 0.0337, 0.5989, 0.3308, 0.1892, 0.3700]) \n",
      "Test Loss tensor([0.2755, 0.0331, 0.6001, 0.3304, 0.1925, 0.3697])\n",
      "\n",
      "\n",
      "************** Batch 500 in 0.5414493083953857 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2800, 0.0301, 0.5977, 0.3291, 0.1926, 0.3669]) \n",
      "Test Loss tensor([0.2758, 0.0343, 0.6015, 0.3284, 0.1927, 0.3677])\n",
      "\n",
      "\n",
      "************** Batch 504 in 0.5322480201721191 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2706, 0.0307, 0.6004, 0.3288, 0.1866, 0.3711]) \n",
      "Test Loss tensor([0.2726, 0.0344, 0.6035, 0.3273, 0.1937, 0.3669])\n",
      "\n",
      "\n",
      "************** Batch 508 in 0.5432620048522949 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2772, 0.0354, 0.6033, 0.3283, 0.1980, 0.3633]) \n",
      "Test Loss tensor([0.2721, 0.0329, 0.6046, 0.3252, 0.1911, 0.3662])\n",
      "\n",
      "\n",
      "************** Batch 512 in 0.5467488765716553 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2678, 0.0327, 0.6047, 0.3263, 0.1916, 0.3623]) \n",
      "Test Loss tensor([0.2710, 0.0330, 0.6074, 0.3237, 0.1941, 0.3650])\n",
      "\n",
      "\n",
      "************** Batch 516 in 0.5446412563323975 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2676, 0.0306, 0.6095, 0.3229, 0.1886, 0.3632]) \n",
      "Test Loss tensor([0.2695, 0.0328, 0.6096, 0.3222, 0.1921, 0.3638])\n",
      "\n",
      "\n",
      "************** Batch 520 in 0.5312197208404541 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2691, 0.0308, 0.6087, 0.3230, 0.1900, 0.3592]) \n",
      "Test Loss tensor([0.2680, 0.0327, 0.6109, 0.3209, 0.1903, 0.3618])\n",
      "\n",
      "\n",
      "************** Batch 524 in 0.5334467887878418 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2688, 0.0331, 0.6127, 0.3205, 0.1905, 0.3629]) \n",
      "Test Loss tensor([0.2666, 0.0341, 0.6122, 0.3186, 0.1912, 0.3619])\n",
      "\n",
      "\n",
      "************** Batch 528 in 0.5458273887634277 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2656, 0.0348, 0.6143, 0.3168, 0.2000, 0.3611]) \n",
      "Test Loss tensor([0.2660, 0.0315, 0.6137, 0.3168, 0.1912, 0.3614])\n",
      "\n",
      "\n",
      "************** Batch 532 in 0.5300407409667969 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2638, 0.0367, 0.6127, 0.3158, 0.2044, 0.3598]) \n",
      "Test Loss tensor([0.2617, 0.0324, 0.6166, 0.3161, 0.1911, 0.3605])\n",
      "\n",
      "\n",
      "************** Batch 536 in 0.5369448661804199 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2636, 0.0341, 0.6150, 0.3124, 0.1908, 0.3633]) \n",
      "Test Loss tensor([0.2640, 0.0328, 0.6180, 0.3141, 0.1867, 0.3608])\n",
      "\n",
      "\n",
      "************** Batch 540 in 0.5330848693847656 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2649, 0.0379, 0.6178, 0.3128, 0.1844, 0.3585]) \n",
      "Test Loss tensor([0.2616, 0.0329, 0.6198, 0.3118, 0.1875, 0.3586])\n",
      "\n",
      "\n",
      "************** Batch 544 in 0.5467836856842041 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2565, 0.0299, 0.6195, 0.3121, 0.1839, 0.3579]) \n",
      "Test Loss tensor([0.2604, 0.0319, 0.6220, 0.3110, 0.1892, 0.3573])\n",
      "\n",
      "\n",
      "************** Batch 548 in 0.5567061901092529 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2607, 0.0282, 0.6183, 0.3088, 0.1872, 0.3564]) \n",
      "Test Loss tensor([0.2590, 0.0307, 0.6238, 0.3094, 0.1872, 0.3567])\n",
      "\n",
      "\n",
      "************** Batch 552 in 0.555018424987793 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2571, 0.0282, 0.6195, 0.3144, 0.1782, 0.3659]) \n",
      "Test Loss tensor([0.2556, 0.0312, 0.6247, 0.3076, 0.1852, 0.3571])\n",
      "\n",
      "\n",
      "************** Batch 556 in 0.5314595699310303 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2659, 0.0289, 0.6280, 0.3075, 0.1885, 0.3543]) \n",
      "Test Loss tensor([0.2603, 0.0335, 0.6274, 0.3063, 0.1883, 0.3541])\n",
      "\n",
      "\n",
      "************** Batch 560 in 0.5376131534576416 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2585, 0.0325, 0.6283, 0.3054, 0.1911, 0.3538]) \n",
      "Test Loss tensor([0.2561, 0.0316, 0.6293, 0.3044, 0.1847, 0.3532])\n",
      "\n",
      "\n",
      "************** Batch 564 in 0.5304334163665771 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2580, 0.0318, 0.6304, 0.3066, 0.1877, 0.3522]) \n",
      "Test Loss tensor([0.2548, 0.0301, 0.6293, 0.3036, 0.1894, 0.3524])\n",
      "\n",
      "\n",
      "************** Batch 568 in 0.5408360958099365 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2517, 0.0348, 0.6263, 0.3022, 0.1921, 0.3546]) \n",
      "Test Loss tensor([0.2530, 0.0298, 0.6321, 0.3018, 0.1864, 0.3526])\n",
      "\n",
      "\n",
      "************** Batch 572 in 0.5254209041595459 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2561, 0.0316, 0.6299, 0.3032, 0.1892, 0.3534]) \n",
      "Test Loss tensor([0.2525, 0.0299, 0.6337, 0.3000, 0.1830, 0.3515])\n",
      "\n",
      "\n",
      "************** Batch 576 in 0.539271354675293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2547, 0.0335, 0.6332, 0.3015, 0.1821, 0.3471]) \n",
      "Test Loss tensor([0.2546, 0.0326, 0.6354, 0.2979, 0.1837, 0.3512])\n",
      "\n",
      "\n",
      "************** Batch 580 in 0.5404694080352783 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2526, 0.0335, 0.6364, 0.2959, 0.1839, 0.3548]) \n",
      "Test Loss tensor([0.2482, 0.0303, 0.6360, 0.2969, 0.1862, 0.3491])\n",
      "\n",
      "\n",
      "************** Batch 584 in 0.5270264148712158 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2585, 0.0344, 0.6371, 0.2977, 0.1845, 0.3541]) \n",
      "Test Loss tensor([0.2489, 0.0296, 0.6370, 0.2965, 0.1852, 0.3493])\n",
      "\n",
      "\n",
      "************** Batch 588 in 0.5466170310974121 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2532, 0.0303, 0.6406, 0.2952, 0.1911, 0.3462]) \n",
      "Test Loss tensor([0.2494, 0.0294, 0.6387, 0.2950, 0.1858, 0.3488])\n",
      "\n",
      "\n",
      "************** Batch 592 in 0.531743049621582 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2478, 0.0315, 0.6427, 0.2964, 0.1847, 0.3468]) \n",
      "Test Loss tensor([0.2441, 0.0296, 0.6407, 0.2931, 0.1828, 0.3468])\n",
      "\n",
      "\n",
      "************** Batch 596 in 0.5423238277435303 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2385, 0.0314, 0.6371, 0.2938, 0.1727, 0.3514]) \n",
      "Test Loss tensor([0.2473, 0.0311, 0.6421, 0.2910, 0.1835, 0.3456])\n",
      "\n",
      "\n",
      "************** Batch 600 in 0.5315349102020264 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2487, 0.0285, 0.6450, 0.2926, 0.1799, 0.3459]) \n",
      "Test Loss tensor([0.2444, 0.0291, 0.6428, 0.2901, 0.1814, 0.3470])\n",
      "\n",
      "\n",
      "************** Batch 604 in 0.5439817905426025 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2434, 0.0308, 0.6449, 0.2917, 0.1840, 0.3454]) \n",
      "Test Loss tensor([0.2411, 0.0291, 0.6442, 0.2883, 0.1819, 0.3463])\n",
      "\n",
      "\n",
      "************** Batch 608 in 0.5281522274017334 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2376, 0.0298, 0.6424, 0.2885, 0.1868, 0.3509]) \n",
      "Test Loss tensor([0.2405, 0.0301, 0.6451, 0.2874, 0.1841, 0.3443])\n",
      "\n",
      "\n",
      "************** Batch 612 in 0.5437750816345215 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2442, 0.0291, 0.6490, 0.2866, 0.1951, 0.3422]) \n",
      "Test Loss tensor([0.2382, 0.0294, 0.6461, 0.2857, 0.1823, 0.3450])\n",
      "\n",
      "\n",
      "************** Batch 616 in 0.5265803337097168 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2392, 0.0312, 0.6431, 0.2867, 0.1791, 0.3486]) \n",
      "Test Loss tensor([0.2409, 0.0309, 0.6485, 0.2833, 0.1799, 0.3432])\n",
      "\n",
      "\n",
      "************** Batch 620 in 0.5332984924316406 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2405, 0.0291, 0.6474, 0.2854, 0.1800, 0.3420]) \n",
      "Test Loss tensor([0.2372, 0.0278, 0.6480, 0.2824, 0.1800, 0.3437])\n",
      "\n",
      "\n",
      "************** Batch 624 in 0.5282468795776367 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2433, 0.0309, 0.6458, 0.2837, 0.1827, 0.3448]) \n",
      "Test Loss tensor([0.2358, 0.0289, 0.6486, 0.2821, 0.1769, 0.3450])\n",
      "\n",
      "\n",
      "************** Batch 628 in 0.5481634140014648 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2316, 0.0294, 0.6464, 0.2801, 0.1745, 0.3448]) \n",
      "Test Loss tensor([0.2346, 0.0286, 0.6499, 0.2808, 0.1808, 0.3424])\n",
      "\n",
      "\n",
      "************** Batch 632 in 0.5500879287719727 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2323, 0.0290, 0.6531, 0.2782, 0.1912, 0.3447]) \n",
      "Test Loss tensor([0.2322, 0.0280, 0.6517, 0.2784, 0.1806, 0.3405])\n",
      "\n",
      "\n",
      "************** Batch 636 in 0.5302579402923584 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2295, 0.0281, 0.6513, 0.2783, 0.1911, 0.3412]) \n",
      "Test Loss tensor([0.2316, 0.0289, 0.6523, 0.2779, 0.1768, 0.3409])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 640 in 0.5485544204711914 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2308, 0.0299, 0.6522, 0.2760, 0.1764, 0.3431]) \n",
      "Test Loss tensor([0.2329, 0.0289, 0.6536, 0.2755, 0.1756, 0.3395])\n",
      "\n",
      "\n",
      "************** Batch 644 in 0.5494263172149658 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2350, 0.0261, 0.6540, 0.2745, 0.1730, 0.3407]) \n",
      "Test Loss tensor([0.2304, 0.0274, 0.6542, 0.2739, 0.1761, 0.3388])\n",
      "\n",
      "\n",
      "************** Batch 648 in 0.542665958404541 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2273, 0.0268, 0.6522, 0.2784, 0.1827, 0.3383]) \n",
      "Test Loss tensor([0.2284, 0.0283, 0.6540, 0.2754, 0.1726, 0.3414])\n",
      "\n",
      "\n",
      "************** Batch 652 in 0.5348613262176514 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2207, 0.0255, 0.6499, 0.2726, 0.1677, 0.3441]) \n",
      "Test Loss tensor([0.2277, 0.0286, 0.6559, 0.2737, 0.1753, 0.3382])\n",
      "\n",
      "\n",
      "************** Batch 656 in 0.5376636981964111 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2277, 0.0300, 0.6541, 0.2741, 0.1782, 0.3395]) \n",
      "Test Loss tensor([0.2278, 0.0286, 0.6572, 0.2706, 0.1752, 0.3362])\n",
      "\n",
      "\n",
      "************** Batch 660 in 0.5359213352203369 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2286, 0.0275, 0.6561, 0.2705, 0.1810, 0.3379]) \n",
      "Test Loss tensor([0.2255, 0.0293, 0.6584, 0.2691, 0.1767, 0.3352])\n",
      "\n",
      "\n",
      "************** Batch 664 in 0.5442283153533936 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2276, 0.0253, 0.6581, 0.2681, 0.1759, 0.3375]) \n",
      "Test Loss tensor([0.2228, 0.0294, 0.6575, 0.2697, 0.1749, 0.3363])\n",
      "\n",
      "\n",
      "************** Batch 668 in 0.5304160118103027 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2164, 0.0297, 0.6604, 0.2679, 0.1714, 0.3372]) \n",
      "Test Loss tensor([0.2225, 0.0292, 0.6572, 0.2701, 0.1750, 0.3365])\n",
      "\n",
      "\n",
      "************** Batch 672 in 0.549257755279541 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2266, 0.0266, 0.6554, 0.2666, 0.1792, 0.3367]) \n",
      "Test Loss tensor([0.2188, 0.0280, 0.6580, 0.2668, 0.1759, 0.3360])\n",
      "\n",
      "\n",
      "************** Batch 676 in 0.5474512577056885 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2259, 0.0277, 0.6615, 0.2659, 0.1745, 0.3327]) \n",
      "Test Loss tensor([0.2237, 0.0303, 0.6609, 0.2637, 0.1726, 0.3329])\n",
      "\n",
      "\n",
      "************** Batch 680 in 0.5387604236602783 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2222, 0.0288, 0.6609, 0.2615, 0.1785, 0.3368]) \n",
      "Test Loss tensor([0.2194, 0.0292, 0.6608, 0.2627, 0.1724, 0.3349])\n",
      "\n",
      "\n",
      "************** Batch 684 in 0.541400671005249 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2203, 0.0309, 0.6599, 0.2613, 0.1797, 0.3326]) \n",
      "Test Loss tensor([0.2167, 0.0287, 0.6594, 0.2645, 0.1692, 0.3348])\n",
      "\n",
      "\n",
      "************** Batch 688 in 0.5291767120361328 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2140, 0.0272, 0.6568, 0.2600, 0.1589, 0.3402]) \n",
      "Test Loss tensor([0.2151, 0.0291, 0.6597, 0.2641, 0.1715, 0.3361])\n",
      "\n",
      "\n",
      "************** Batch 692 in 0.5557358264923096 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2175, 0.0293, 0.6580, 0.2650, 0.1776, 0.3283]) \n",
      "Test Loss tensor([0.2143, 0.0293, 0.6626, 0.2591, 0.1723, 0.3307])\n",
      "\n",
      "\n",
      "************** Batch 696 in 0.5355100631713867 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2164, 0.0312, 0.6609, 0.2610, 0.1868, 0.3289]) \n",
      "Test Loss tensor([0.2134, 0.0296, 0.6645, 0.2584, 0.1713, 0.3300])\n",
      "\n",
      "\n",
      "************** Batch 700 in 0.5510246753692627 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2199, 0.0293, 0.6645, 0.2561, 0.1681, 0.3313]) \n",
      "Test Loss tensor([0.2117, 0.0290, 0.6620, 0.2575, 0.1678, 0.3308])\n",
      "\n",
      "\n",
      "************** Batch 704 in 0.5693039894104004 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2164, 0.0305, 0.6646, 0.2558, 0.1795, 0.3299]) \n",
      "Test Loss tensor([0.2107, 0.0283, 0.6621, 0.2574, 0.1712, 0.3308])\n",
      "\n",
      "\n",
      "************** Batch 708 in 0.5421984195709229 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2107, 0.0304, 0.6614, 0.2559, 0.1618, 0.3328]) \n",
      "Test Loss tensor([0.2110, 0.0285, 0.6614, 0.2560, 0.1714, 0.3318])\n",
      "\n",
      "\n",
      "************** Batch 712 in 0.5400416851043701 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2154, 0.0284, 0.6636, 0.2576, 0.1585, 0.3313]) \n",
      "Test Loss tensor([0.2075, 0.0285, 0.6631, 0.2525, 0.1709, 0.3291])\n",
      "\n",
      "\n",
      "************** Batch 716 in 0.5446090698242188 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2113, 0.0293, 0.6645, 0.2544, 0.1735, 0.3313]) \n",
      "Test Loss tensor([0.2071, 0.0279, 0.6629, 0.2514, 0.1702, 0.3287])\n",
      "\n",
      "\n",
      "************** Batch 720 in 0.5425615310668945 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2132, 0.0291, 0.6677, 0.2482, 0.1733, 0.3271]) \n",
      "Test Loss tensor([0.2050, 0.0278, 0.6633, 0.2515, 0.1702, 0.3283])\n",
      "\n",
      "\n",
      "************** Batch 724 in 0.5397772789001465 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2045, 0.0310, 0.6629, 0.2518, 0.1592, 0.3243]) \n",
      "Test Loss tensor([0.2048, 0.0283, 0.6622, 0.2498, 0.1700, 0.3290])\n",
      "\n",
      "\n",
      "************** Batch 728 in 0.5379304885864258 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2045, 0.0299, 0.6638, 0.2504, 0.1783, 0.3325]) \n",
      "Test Loss tensor([0.2052, 0.0293, 0.6630, 0.2480, 0.1700, 0.3261])\n",
      "\n",
      "\n",
      "************** Batch 732 in 0.5425355434417725 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2032, 0.0272, 0.6613, 0.2485, 0.1754, 0.3293]) \n",
      "Test Loss tensor([0.2022, 0.0257, 0.6625, 0.2456, 0.1710, 0.3263])\n",
      "\n",
      "\n",
      "************** Batch 736 in 0.5431206226348877 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2084, 0.0317, 0.6606, 0.2426, 0.1682, 0.3277]) \n",
      "Test Loss tensor([0.1998, 0.0270, 0.6619, 0.2462, 0.1704, 0.3260])\n",
      "\n",
      "\n",
      "************** Batch 740 in 0.5370922088623047 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2083, 0.0283, 0.6628, 0.2416, 0.1761, 0.3204]) \n",
      "Test Loss tensor([0.1982, 0.0271, 0.6593, 0.2455, 0.1689, 0.3268])\n",
      "\n",
      "\n",
      "************** Batch 744 in 0.5513801574707031 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1960, 0.0250, 0.6605, 0.2434, 0.1708, 0.3290]) \n",
      "Test Loss tensor([0.1972, 0.0266, 0.6596, 0.2437, 0.1659, 0.3256])\n",
      "\n",
      "\n",
      "************** Batch 748 in 0.5333476066589355 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1966, 0.0267, 0.6594, 0.2431, 0.1737, 0.3206]) \n",
      "Test Loss tensor([0.1950, 0.0268, 0.6590, 0.2420, 0.1676, 0.3260])\n",
      "\n",
      "\n",
      "************** Batch 752 in 0.5493512153625488 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1962, 0.0297, 0.6601, 0.2440, 0.1680, 0.3264]) \n",
      "Test Loss tensor([0.1941, 0.0276, 0.6592, 0.2396, 0.1681, 0.3222])\n",
      "\n",
      "\n",
      "************** Batch 756 in 0.523829460144043 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1975, 0.0287, 0.6563, 0.2403, 0.1704, 0.3260]) \n",
      "Test Loss tensor([0.1946, 0.0274, 0.6567, 0.2402, 0.1690, 0.3239])\n",
      "\n",
      "\n",
      "************** Batch 760 in 0.5425801277160645 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.2034, 0.0281, 0.6603, 0.2363, 0.1665, 0.3234]) \n",
      "Test Loss tensor([0.1938, 0.0274, 0.6566, 0.2376, 0.1685, 0.3225])\n",
      "\n",
      "\n",
      "************** Batch 764 in 0.529693603515625 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1919, 0.0294, 0.6573, 0.2371, 0.1638, 0.3224]) \n",
      "Test Loss tensor([0.1903, 0.0271, 0.6556, 0.2362, 0.1698, 0.3226])\n",
      "\n",
      "\n",
      "************** Batch 768 in 0.534865140914917 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1913, 0.0272, 0.6550, 0.2393, 0.1734, 0.3212]) \n",
      "Test Loss tensor([0.1888, 0.0268, 0.6531, 0.2365, 0.1684, 0.3224])\n",
      "\n",
      "\n",
      "************** Batch 772 in 0.5503277778625488 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1886, 0.0257, 0.6543, 0.2314, 0.1684, 0.3198]) \n",
      "Test Loss tensor([0.1873, 0.0267, 0.6503, 0.2349, 0.1665, 0.3235])\n",
      "\n",
      "\n",
      "************** Batch 776 in 0.5339698791503906 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1910, 0.0276, 0.6523, 0.2344, 0.1737, 0.3172]) \n",
      "Test Loss tensor([0.1877, 0.0271, 0.6508, 0.2324, 0.1702, 0.3219])\n",
      "\n",
      "\n",
      "************** Batch 780 in 0.5418646335601807 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1882, 0.0280, 0.6516, 0.2337, 0.1640, 0.3223]) \n",
      "Test Loss tensor([0.1866, 0.0272, 0.6472, 0.2322, 0.1667, 0.3214])\n",
      "\n",
      "\n",
      "************** Batch 784 in 0.54205322265625 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1821, 0.0284, 0.6449, 0.2320, 0.1673, 0.3229]) \n",
      "Test Loss tensor([0.1844, 0.0292, 0.6449, 0.2303, 0.1654, 0.3221])\n",
      "\n",
      "\n",
      "************** Batch 788 in 0.5410797595977783 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1865, 0.0311, 0.6453, 0.2278, 0.1612, 0.3202]) \n",
      "Test Loss tensor([0.1834, 0.0265, 0.6423, 0.2291, 0.1664, 0.3209])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 792 in 0.5258066654205322 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1869, 0.0258, 0.6401, 0.2307, 0.1648, 0.3221]) \n",
      "Test Loss tensor([0.1825, 0.0274, 0.6392, 0.2285, 0.1642, 0.3208])\n",
      "\n",
      "\n",
      "************** Batch 796 in 0.5531620979309082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1794, 0.0305, 0.6373, 0.2270, 0.1745, 0.3261]) \n",
      "Test Loss tensor([0.1822, 0.0281, 0.6384, 0.2259, 0.1680, 0.3193])\n",
      "\n",
      "\n",
      "************** Batch 800 in 0.5309033393859863 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1869, 0.0273, 0.6413, 0.2235, 0.1578, 0.3158]) \n",
      "Test Loss tensor([0.1809, 0.0277, 0.6342, 0.2248, 0.1681, 0.3178])\n",
      "\n",
      "\n",
      "************** Batch 804 in 0.551349401473999 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1812, 0.0274, 0.6356, 0.2262, 0.1632, 0.3180]) \n",
      "Test Loss tensor([0.1799, 0.0278, 0.6297, 0.2230, 0.1667, 0.3169])\n",
      "\n",
      "\n",
      "************** Batch 808 in 0.5349478721618652 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1749, 0.0277, 0.6324, 0.2208, 0.1675, 0.3148]) \n",
      "Test Loss tensor([0.1800, 0.0280, 0.6231, 0.2244, 0.1655, 0.3186])\n",
      "\n",
      "\n",
      "************** Batch 812 in 0.5797553062438965 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1823, 0.0318, 0.6220, 0.2235, 0.1667, 0.3215]) \n",
      "Test Loss tensor([0.1764, 0.0258, 0.6212, 0.2191, 0.1682, 0.3150])\n",
      "\n",
      "\n",
      "************** Batch 816 in 0.5461592674255371 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1776, 0.0292, 0.6199, 0.2202, 0.1628, 0.3170]) \n",
      "Test Loss tensor([0.1779, 0.0260, 0.6179, 0.2163, 0.1730, 0.3131])\n",
      "\n",
      "\n",
      "************** Batch 820 in 0.5341613292694092 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1790, 0.0300, 0.6213, 0.2155, 0.1772, 0.3107]) \n",
      "Test Loss tensor([0.1754, 0.0261, 0.6080, 0.2172, 0.1656, 0.3146])\n",
      "\n",
      "\n",
      "************** Batch 824 in 0.541236162185669 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1720, 0.0230, 0.6045, 0.2164, 0.1621, 0.3147]) \n",
      "Test Loss tensor([0.1789, 0.0271, 0.5972, 0.2180, 0.1615, 0.3166])\n",
      "\n",
      "\n",
      "************** Batch 828 in 0.5358002185821533 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1773, 0.0278, 0.5993, 0.2136, 0.1594, 0.3167]) \n",
      "Test Loss tensor([0.1749, 0.0269, 0.5966, 0.2113, 0.1666, 0.3118])\n",
      "\n",
      "\n",
      "************** Batch 832 in 0.5428817272186279 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1738, 0.0295, 0.5948, 0.2125, 0.1688, 0.3105]) \n",
      "Test Loss tensor([0.1752, 0.0277, 0.5886, 0.2105, 0.1647, 0.3108])\n",
      "\n",
      "\n",
      "************** Batch 836 in 0.5336194038391113 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1731, 0.0287, 0.5879, 0.2076, 0.1617, 0.3085]) \n",
      "Test Loss tensor([0.1725, 0.0274, 0.5729, 0.2112, 0.1622, 0.3137])\n",
      "\n",
      "\n",
      "************** Batch 840 in 0.5426895618438721 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1717, 0.0276, 0.5733, 0.2123, 0.1610, 0.3179]) \n",
      "Test Loss tensor([0.1708, 0.0282, 0.5618, 0.2099, 0.1636, 0.3137])\n",
      "\n",
      "\n",
      "************** Batch 844 in 0.5301196575164795 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1749, 0.0309, 0.5624, 0.2054, 0.1662, 0.3147]) \n",
      "Test Loss tensor([0.1723, 0.0278, 0.5559, 0.2038, 0.1671, 0.3059])\n",
      "\n",
      "\n",
      "************** Batch 848 in 0.5458226203918457 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1748, 0.0285, 0.5599, 0.1981, 0.1640, 0.3027]) \n",
      "Test Loss tensor([0.1724, 0.0272, 0.5361, 0.2038, 0.1661, 0.3086])\n",
      "\n",
      "\n",
      "************** Batch 852 in 0.5289161205291748 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1731, 0.0289, 0.5369, 0.2030, 0.1692, 0.3101]) \n",
      "Test Loss tensor([0.1720, 0.0282, 0.5151, 0.2051, 0.1629, 0.3132])\n",
      "\n",
      "\n",
      "************** Batch 856 in 0.5383086204528809 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1660, 0.0264, 0.5152, 0.2141, 0.1615, 0.3134]) \n",
      "Test Loss tensor([0.1694, 0.0281, 0.5062, 0.2002, 0.1698, 0.3014])\n",
      "\n",
      "\n",
      "************** Batch 860 in 0.5298855304718018 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1705, 0.0274, 0.5039, 0.1977, 0.1721, 0.3015]) \n",
      "Test Loss tensor([0.1689, 0.0289, 0.4873, 0.1992, 0.1691, 0.3018])\n",
      "\n",
      "\n",
      "************** Batch 864 in 0.5390598773956299 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1647, 0.0274, 0.4853, 0.1913, 0.1646, 0.2983]) \n",
      "Test Loss tensor([0.1705, 0.0286, 0.4612, 0.1961, 0.1650, 0.3079])\n",
      "\n",
      "\n",
      "************** Batch 868 in 0.5416653156280518 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1666, 0.0280, 0.4643, 0.2031, 0.1680, 0.3075]) \n",
      "Test Loss tensor([0.1703, 0.0296, 0.4475, 0.1939, 0.1670, 0.3025])\n",
      "\n",
      "\n",
      "************** Batch 872 in 0.5348470211029053 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1677, 0.0289, 0.4459, 0.1979, 0.1707, 0.3023]) \n",
      "Test Loss tensor([0.1700, 0.0286, 0.4353, 0.1878, 0.1784, 0.2902])\n",
      "\n",
      "\n",
      "************** Batch 876 in 0.5232770442962646 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1243, 0.0251, 0.3263, 0.1489, 0.1289, 0.2184]) \n",
      "Test Loss tensor([0.1691, 0.0302, 0.4139, 0.1858, 0.1754, 0.2915])\n",
      "\n",
      "\n",
      "************** Batch 0 in 0.5314862728118896 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1684, 0.0264, 0.4153, 0.1856, 0.1710, 0.2904]) \n",
      "Test Loss tensor([0.1735, 0.0332, 0.3885, 0.1899, 0.1609, 0.3050])\n",
      "\n",
      "\n",
      "************** Batch 4 in 0.5478489398956299 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1795, 0.0312, 0.3885, 0.1855, 0.1664, 0.3006]) \n",
      "Test Loss tensor([0.1688, 0.0288, 0.3802, 0.1788, 0.1732, 0.2834])\n",
      "\n",
      "\n",
      "************** Batch 8 in 0.5271575450897217 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1788, 0.0309, 0.3834, 0.1767, 0.1771, 0.2829]) \n",
      "Test Loss tensor([0.1703, 0.0302, 0.3642, 0.1766, 0.1772, 0.2777])\n",
      "\n",
      "\n",
      "************** Batch 12 in 0.5458238124847412 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1713, 0.0310, 0.3652, 0.1707, 0.1707, 0.2768]) \n",
      "Test Loss tensor([0.1681, 0.0308, 0.3327, 0.1787, 0.1617, 0.2949])\n",
      "\n",
      "\n",
      "************** Batch 16 in 0.5351712703704834 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1669, 0.0382, 0.3373, 0.1801, 0.1617, 0.2906]) \n",
      "Test Loss tensor([0.1664, 0.0306, 0.3242, 0.1706, 0.1741, 0.2733])\n",
      "\n",
      "\n",
      "************** Batch 20 in 0.5444867610931396 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1674, 0.0253, 0.3290, 0.1732, 0.1702, 0.2719]) \n",
      "Test Loss tensor([0.1682, 0.0311, 0.3149, 0.1653, 0.1797, 0.2606])\n",
      "\n",
      "\n",
      "************** Batch 24 in 0.537792444229126 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1635, 0.0281, 0.3162, 0.1637, 0.1719, 0.2572]) \n",
      "Test Loss tensor([0.1677, 0.0328, 0.2866, 0.1671, 0.1656, 0.2760])\n",
      "\n",
      "\n",
      "************** Batch 28 in 0.5286309719085693 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1710, 0.0391, 0.2878, 0.1700, 0.1672, 0.2733]) \n",
      "Test Loss tensor([0.1628, 0.0332, 0.2764, 0.1610, 0.1663, 0.2621])\n",
      "\n",
      "\n",
      "************** Batch 32 in 0.5426244735717773 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1672, 0.0314, 0.2721, 0.1621, 0.1810, 0.2570]) \n",
      "Test Loss tensor([0.1658, 0.0322, 0.2713, 0.1558, 0.1832, 0.2396])\n",
      "\n",
      "\n",
      "************** Batch 36 in 0.5297427177429199 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1663, 0.0338, 0.2718, 0.1553, 0.1804, 0.2430]) \n",
      "Test Loss tensor([0.1633, 0.0337, 0.2522, 0.1543, 0.1662, 0.2486])\n",
      "\n",
      "\n",
      "************** Batch 40 in 0.5344243049621582 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1627, 0.0333, 0.2567, 0.1543, 0.1590, 0.2528]) \n",
      "Test Loss tensor([0.1629, 0.0343, 0.2418, 0.1508, 0.1705, 0.2346])\n",
      "\n",
      "\n",
      "************** Batch 44 in 0.5335507392883301 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1652, 0.0331, 0.2456, 0.1503, 0.1649, 0.2369]) \n",
      "Test Loss tensor([0.1626, 0.0340, 0.2353, 0.1464, 0.1750, 0.2236])\n",
      "\n",
      "\n",
      "************** Batch 48 in 0.5511326789855957 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1639, 0.0344, 0.2436, 0.1440, 0.1693, 0.2263]) \n",
      "Test Loss tensor([0.1622, 0.0349, 0.2242, 0.1432, 0.1658, 0.2231])\n",
      "\n",
      "\n",
      "************** Batch 52 in 0.5280752182006836 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1514, 0.0342, 0.2311, 0.1465, 0.1579, 0.2186]) \n",
      "Test Loss tensor([0.1606, 0.0332, 0.2193, 0.1399, 0.1641, 0.2171])\n",
      "\n",
      "\n",
      "************** Batch 56 in 0.5404019355773926 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1652, 0.0361, 0.2216, 0.1391, 0.1737, 0.2211]) \n",
      "Test Loss tensor([0.1600, 0.0347, 0.2136, 0.1361, 0.1647, 0.2095])\n",
      "\n",
      "\n",
      "************** Batch 60 in 0.5307555198669434 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1688, 0.0412, 0.2078, 0.1373, 0.1702, 0.2083]) \n",
      "Test Loss tensor([0.1589, 0.0356, 0.2087, 0.1317, 0.1651, 0.1999])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 64 in 0.5434319972991943 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1616, 0.0361, 0.2093, 0.1333, 0.1659, 0.1990]) \n",
      "Test Loss tensor([0.1587, 0.0365, 0.1967, 0.1328, 0.1599, 0.2034])\n",
      "\n",
      "\n",
      "************** Batch 68 in 0.5323295593261719 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1626, 0.0375, 0.1996, 0.1423, 0.1633, 0.2079]) \n",
      "Test Loss tensor([0.1550, 0.0346, 0.1960, 0.1251, 0.1674, 0.1867])\n",
      "\n",
      "\n",
      "************** Batch 72 in 0.5457367897033691 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1545, 0.0424, 0.1969, 0.1323, 0.1703, 0.1777]) \n",
      "Test Loss tensor([0.1577, 0.0356, 0.1930, 0.1246, 0.1726, 0.1809])\n",
      "\n",
      "\n",
      "************** Batch 76 in 0.5403342247009277 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1513, 0.0388, 0.2013, 0.1240, 0.1641, 0.1835]) \n",
      "Test Loss tensor([0.1569, 0.0377, 0.1850, 0.1267, 0.1625, 0.1820])\n",
      "\n",
      "\n",
      "************** Batch 80 in 0.5394365787506104 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1611, 0.0347, 0.1882, 0.1228, 0.1588, 0.1826]) \n",
      "Test Loss tensor([0.1531, 0.0373, 0.1882, 0.1191, 0.1691, 0.1726])\n",
      "\n",
      "\n",
      "************** Batch 84 in 0.5639653205871582 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1497, 0.0370, 0.1857, 0.1187, 0.1670, 0.1780]) \n",
      "Test Loss tensor([0.1524, 0.0367, 0.1830, 0.1216, 0.1630, 0.1727])\n",
      "\n",
      "\n",
      "************** Batch 88 in 0.5318143367767334 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1609, 0.0360, 0.1838, 0.1206, 0.1623, 0.1699]) \n",
      "Test Loss tensor([0.1506, 0.0382, 0.1769, 0.1170, 0.1607, 0.1697])\n",
      "\n",
      "\n",
      "************** Batch 92 in 0.5485916137695312 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1464, 0.0380, 0.1850, 0.1190, 0.1621, 0.1716]) \n",
      "Test Loss tensor([0.1551, 0.0378, 0.1819, 0.1121, 0.1634, 0.1636])\n",
      "\n",
      "\n",
      "************** Batch 96 in 0.5338153839111328 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1503, 0.0372, 0.1818, 0.1097, 0.1578, 0.1654]) \n",
      "Test Loss tensor([0.1513, 0.0393, 0.1781, 0.1130, 0.1594, 0.1656])\n",
      "\n",
      "\n",
      "************** Batch 100 in 0.5425751209259033 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1495, 0.0384, 0.1757, 0.1158, 0.1493, 0.1660]) \n",
      "Test Loss tensor([0.1486, 0.0396, 0.1756, 0.1112, 0.1586, 0.1629])\n",
      "\n",
      "\n",
      "************** Batch 104 in 0.5373671054840088 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1516, 0.0380, 0.1789, 0.1078, 0.1548, 0.1626]) \n",
      "Test Loss tensor([0.1461, 0.0379, 0.1706, 0.1084, 0.1577, 0.1553])\n",
      "\n",
      "\n",
      "************** Batch 108 in 0.5422158241271973 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1525, 0.0397, 0.1657, 0.1047, 0.1554, 0.1492]) \n",
      "Test Loss tensor([0.1483, 0.0357, 0.1754, 0.1073, 0.1644, 0.1534])\n",
      "\n",
      "\n",
      "************** Batch 112 in 0.5362727642059326 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1427, 0.0355, 0.1788, 0.1141, 0.1562, 0.1536]) \n",
      "Test Loss tensor([0.1482, 0.0349, 0.1651, 0.1104, 0.1592, 0.1537])\n",
      "\n",
      "\n",
      "************** Batch 116 in 0.5433139801025391 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1487, 0.0370, 0.1641, 0.1081, 0.1517, 0.1490]) \n",
      "Test Loss tensor([0.1466, 0.0352, 0.1678, 0.1071, 0.1583, 0.1474])\n",
      "\n",
      "\n",
      "************** Batch 120 in 0.5370199680328369 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1458, 0.0372, 0.1586, 0.1022, 0.1527, 0.1524]) \n",
      "Test Loss tensor([0.1495, 0.0349, 0.1700, 0.1024, 0.1733, 0.1446])\n",
      "\n",
      "\n",
      "************** Batch 124 in 0.5461194515228271 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1533, 0.0387, 0.1773, 0.1025, 0.1743, 0.1381]) \n",
      "Test Loss tensor([0.1457, 0.0375, 0.1664, 0.1076, 0.1595, 0.1509])\n",
      "\n",
      "\n",
      "************** Batch 128 in 0.5462949275970459 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1472, 0.0336, 0.1647, 0.1082, 0.1525, 0.1522]) \n",
      "Test Loss tensor([0.1402, 0.0367, 0.1667, 0.1042, 0.1557, 0.1443])\n",
      "\n",
      "\n",
      "************** Batch 132 in 0.5477445125579834 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1333, 0.0364, 0.1659, 0.1059, 0.1549, 0.1468]) \n",
      "Test Loss tensor([0.1538, 0.0342, 0.1708, 0.1005, 0.1684, 0.1391])\n",
      "\n",
      "\n",
      "************** Batch 136 in 0.5433449745178223 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1548, 0.0373, 0.1797, 0.1084, 0.1660, 0.1411]) \n",
      "Test Loss tensor([0.1392, 0.0360, 0.1672, 0.1022, 0.1578, 0.1393])\n",
      "\n",
      "\n",
      "************** Batch 140 in 0.5386035442352295 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1433, 0.0318, 0.1685, 0.1074, 0.1520, 0.1475]) \n",
      "Test Loss tensor([0.1464, 0.0356, 0.1613, 0.1023, 0.1585, 0.1436])\n",
      "\n",
      "\n",
      "************** Batch 144 in 0.5485119819641113 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1466, 0.0308, 0.1648, 0.0984, 0.1566, 0.1436]) \n",
      "Test Loss tensor([0.1417, 0.0359, 0.1665, 0.1022, 0.1588, 0.1316])\n",
      "\n",
      "\n",
      "************** Batch 148 in 0.5421998500823975 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1495, 0.0387, 0.1676, 0.1077, 0.1641, 0.1357]) \n",
      "Test Loss tensor([0.1457, 0.0332, 0.1719, 0.0967, 0.1643, 0.1348])\n",
      "\n",
      "\n",
      "************** Batch 152 in 0.5485391616821289 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1516, 0.0377, 0.1745, 0.0988, 0.1679, 0.1362]) \n",
      "Test Loss tensor([0.1373, 0.0353, 0.1642, 0.1010, 0.1548, 0.1352])\n",
      "\n",
      "\n",
      "************** Batch 156 in 0.5413429737091064 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1384, 0.0334, 0.1694, 0.0963, 0.1729, 0.1359]) \n",
      "Test Loss tensor([0.1393, 0.0341, 0.1646, 0.0984, 0.1539, 0.1368])\n",
      "\n",
      "\n",
      "************** Batch 160 in 0.5510463714599609 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1418, 0.0343, 0.1532, 0.1040, 0.1617, 0.1259]) \n",
      "Test Loss tensor([0.1435, 0.0334, 0.1685, 0.0980, 0.1563, 0.1295])\n",
      "\n",
      "\n",
      "************** Batch 164 in 0.5349712371826172 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1353, 0.0292, 0.1674, 0.0987, 0.1561, 0.1290]) \n",
      "Test Loss tensor([0.1369, 0.0341, 0.1685, 0.0982, 0.1529, 0.1304])\n",
      "\n",
      "\n",
      "************** Batch 168 in 0.5433545112609863 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1455, 0.0381, 0.1785, 0.0988, 0.1472, 0.1357]) \n",
      "Test Loss tensor([0.1413, 0.0353, 0.1640, 0.0992, 0.1550, 0.1308])\n",
      "\n",
      "\n",
      "************** Batch 172 in 0.5375716686248779 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1417, 0.0341, 0.1566, 0.0989, 0.1581, 0.1427]) \n",
      "Test Loss tensor([0.1374, 0.0349, 0.1646, 0.0970, 0.1542, 0.1287])\n",
      "\n",
      "\n",
      "************** Batch 176 in 0.5443353652954102 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1292, 0.0351, 0.1589, 0.0980, 0.1501, 0.1285]) \n",
      "Test Loss tensor([0.1376, 0.0327, 0.1705, 0.0948, 0.1501, 0.1296])\n",
      "\n",
      "\n",
      "************** Batch 180 in 0.5400826930999756 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1443, 0.0359, 0.1714, 0.0917, 0.1526, 0.1279]) \n",
      "Test Loss tensor([0.1361, 0.0325, 0.1712, 0.0938, 0.1482, 0.1287])\n",
      "\n",
      "\n",
      "************** Batch 184 in 0.5379209518432617 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1352, 0.0317, 0.1659, 0.0942, 0.1489, 0.1182]) \n",
      "Test Loss tensor([0.1352, 0.0350, 0.1637, 0.0958, 0.1509, 0.1285])\n",
      "\n",
      "\n",
      "************** Batch 188 in 0.5438954830169678 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1344, 0.0346, 0.1618, 0.0885, 0.1435, 0.1267]) \n",
      "Test Loss tensor([0.1343, 0.0341, 0.1666, 0.0981, 0.1502, 0.1286])\n",
      "\n",
      "\n",
      "************** Batch 192 in 0.53635573387146 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1256, 0.0297, 0.1672, 0.1046, 0.1518, 0.1251]) \n",
      "Test Loss tensor([0.1380, 0.0348, 0.1665, 0.0922, 0.1486, 0.1251])\n",
      "\n",
      "\n",
      "************** Batch 196 in 0.5427963733673096 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1388, 0.0337, 0.1734, 0.0928, 0.1492, 0.1251]) \n",
      "Test Loss tensor([0.1325, 0.0338, 0.1637, 0.0918, 0.1502, 0.1241])\n",
      "\n",
      "\n",
      "************** Batch 200 in 0.5328884124755859 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1312, 0.0351, 0.1693, 0.0935, 0.1524, 0.1311]) \n",
      "Test Loss tensor([0.1320, 0.0326, 0.1661, 0.0955, 0.1504, 0.1241])\n",
      "\n",
      "\n",
      "************** Batch 204 in 0.541820764541626 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1328, 0.0331, 0.1654, 0.0933, 0.1538, 0.1265]) \n",
      "Test Loss tensor([0.1286, 0.0316, 0.1651, 0.0970, 0.1450, 0.1237])\n",
      "\n",
      "\n",
      "************** Batch 208 in 0.5393092632293701 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1354, 0.0302, 0.1610, 0.0889, 0.1406, 0.1224]) \n",
      "Test Loss tensor([0.1309, 0.0336, 0.1680, 0.0959, 0.1490, 0.1219])\n",
      "\n",
      "\n",
      "************** Batch 212 in 0.5438604354858398 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1238, 0.0289, 0.1614, 0.0905, 0.1351, 0.1237]) \n",
      "Test Loss tensor([0.1302, 0.0343, 0.1680, 0.0960, 0.1457, 0.1234])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 216 in 0.5406193733215332 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1330, 0.0352, 0.1603, 0.0949, 0.1465, 0.1233]) \n",
      "Test Loss tensor([0.1287, 0.0336, 0.1669, 0.0946, 0.1440, 0.1246])\n",
      "\n",
      "\n",
      "************** Batch 220 in 0.5381455421447754 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1322, 0.0302, 0.1657, 0.1085, 0.1490, 0.1200]) \n",
      "Test Loss tensor([0.1320, 0.0328, 0.1640, 0.0937, 0.1455, 0.1207])\n",
      "\n",
      "\n",
      "************** Batch 224 in 0.5489740371704102 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1263, 0.0344, 0.1644, 0.1026, 0.1464, 0.1155]) \n",
      "Test Loss tensor([0.1367, 0.0328, 0.1704, 0.0935, 0.1465, 0.1224])\n",
      "\n",
      "\n",
      "************** Batch 228 in 0.5288910865783691 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1343, 0.0301, 0.1668, 0.0927, 0.1463, 0.1228]) \n",
      "Test Loss tensor([0.1281, 0.0334, 0.1642, 0.0933, 0.1423, 0.1205])\n",
      "\n",
      "\n",
      "************** Batch 232 in 0.5385756492614746 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1260, 0.0346, 0.1699, 0.0978, 0.1478, 0.1224]) \n",
      "Test Loss tensor([0.1278, 0.0339, 0.1687, 0.0936, 0.1415, 0.1202])\n",
      "\n",
      "\n",
      "************** Batch 236 in 0.5338048934936523 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1339, 0.0352, 0.1673, 0.0909, 0.1460, 0.1204]) \n",
      "Test Loss tensor([0.1313, 0.0306, 0.1659, 0.0931, 0.1469, 0.1187])\n",
      "\n",
      "\n",
      "************** Batch 240 in 0.5413486957550049 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1413, 0.0352, 0.1769, 0.0962, 0.1444, 0.1207]) \n",
      "Test Loss tensor([0.1295, 0.0334, 0.1649, 0.0906, 0.1401, 0.1192])\n",
      "\n",
      "\n",
      "************** Batch 244 in 0.5379226207733154 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1341, 0.0349, 0.1577, 0.0979, 0.1391, 0.1225]) \n",
      "Test Loss tensor([0.1349, 0.0346, 0.1650, 0.0952, 0.1452, 0.1193])\n",
      "\n",
      "\n",
      "************** Batch 248 in 0.5538895130157471 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1311, 0.0338, 0.1698, 0.0941, 0.1429, 0.1177]) \n",
      "Test Loss tensor([0.1294, 0.0319, 0.1650, 0.0943, 0.1406, 0.1157])\n",
      "\n",
      "\n",
      "************** Batch 252 in 0.5323724746704102 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1227, 0.0315, 0.1665, 0.0924, 0.1322, 0.1275]) \n",
      "Test Loss tensor([0.1294, 0.0319, 0.1649, 0.0925, 0.1436, 0.1177])\n",
      "\n",
      "\n",
      "************** Batch 256 in 0.5560879707336426 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1232, 0.0352, 0.1619, 0.1010, 0.1566, 0.1140]) \n",
      "Test Loss tensor([0.1263, 0.0318, 0.1656, 0.0922, 0.1413, 0.1185])\n",
      "\n",
      "\n",
      "************** Batch 260 in 0.5375733375549316 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1333, 0.0341, 0.1595, 0.0930, 0.1446, 0.1125]) \n",
      "Test Loss tensor([0.1256, 0.0329, 0.1635, 0.0947, 0.1396, 0.1165])\n",
      "\n",
      "\n",
      "************** Batch 264 in 0.5647032260894775 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1278, 0.0383, 0.1677, 0.0869, 0.1406, 0.1126]) \n",
      "Test Loss tensor([0.1284, 0.0326, 0.1703, 0.0949, 0.1398, 0.1161])\n",
      "\n",
      "\n",
      "************** Batch 268 in 0.5623812675476074 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1237, 0.0356, 0.1621, 0.1001, 0.1426, 0.1200]) \n",
      "Test Loss tensor([0.1289, 0.0318, 0.1661, 0.0941, 0.1394, 0.1171])\n",
      "\n",
      "\n",
      "************** Batch 272 in 0.5586516857147217 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1294, 0.0370, 0.1744, 0.0954, 0.1510, 0.1089]) \n",
      "Test Loss tensor([0.1259, 0.0341, 0.1706, 0.0948, 0.1374, 0.1194])\n",
      "\n",
      "\n",
      "************** Batch 276 in 0.5493414402008057 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1258, 0.0369, 0.1704, 0.0865, 0.1452, 0.1052]) \n",
      "Test Loss tensor([0.1260, 0.0327, 0.1653, 0.0915, 0.1360, 0.1166])\n",
      "\n",
      "\n",
      "************** Batch 280 in 0.5339624881744385 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1269, 0.0352, 0.1684, 0.0884, 0.1356, 0.1126]) \n",
      "Test Loss tensor([0.1293, 0.0312, 0.1687, 0.0912, 0.1416, 0.1172])\n",
      "\n",
      "\n",
      "************** Batch 284 in 0.5509405136108398 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1309, 0.0346, 0.1702, 0.0877, 0.1549, 0.1145]) \n",
      "Test Loss tensor([0.1287, 0.0327, 0.1651, 0.0916, 0.1373, 0.1133])\n",
      "\n",
      "\n",
      "************** Batch 288 in 0.5280466079711914 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1131, 0.0266, 0.1686, 0.0968, 0.1345, 0.1112]) \n",
      "Test Loss tensor([0.1293, 0.0325, 0.1655, 0.0959, 0.1368, 0.1152])\n",
      "\n",
      "\n",
      "************** Batch 292 in 0.5452499389648438 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1300, 0.0345, 0.1628, 0.0972, 0.1352, 0.1065]) \n",
      "Test Loss tensor([0.1257, 0.0310, 0.1686, 0.0942, 0.1350, 0.1153])\n",
      "\n",
      "\n",
      "************** Batch 296 in 0.5340578556060791 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1292, 0.0333, 0.1799, 0.0915, 0.1358, 0.1115]) \n",
      "Test Loss tensor([0.1266, 0.0301, 0.1632, 0.0925, 0.1338, 0.1116])\n",
      "\n",
      "\n",
      "************** Batch 300 in 0.5996181964874268 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1233, 0.0296, 0.1677, 0.0854, 0.1339, 0.1097]) \n",
      "Test Loss tensor([0.1248, 0.0313, 0.1672, 0.0940, 0.1335, 0.1115])\n",
      "\n",
      "\n",
      "************** Batch 304 in 0.5311362743377686 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1311, 0.0351, 0.1701, 0.0943, 0.1333, 0.1101]) \n",
      "Test Loss tensor([0.1262, 0.0320, 0.1642, 0.0918, 0.1399, 0.1135])\n",
      "\n",
      "\n",
      "************** Batch 308 in 0.5477645397186279 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1306, 0.0312, 0.1747, 0.0942, 0.1367, 0.1160]) \n",
      "Test Loss tensor([0.1247, 0.0333, 0.1664, 0.0917, 0.1370, 0.1108])\n",
      "\n",
      "\n",
      "************** Batch 312 in 0.5509529113769531 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1263, 0.0369, 0.1561, 0.0880, 0.1326, 0.1126]) \n",
      "Test Loss tensor([0.1280, 0.0302, 0.1691, 0.0914, 0.1332, 0.1150])\n",
      "\n",
      "\n",
      "************** Batch 316 in 0.5405073165893555 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1367, 0.0302, 0.1637, 0.0922, 0.1357, 0.1057]) \n",
      "Test Loss tensor([0.1241, 0.0299, 0.1670, 0.0928, 0.1336, 0.1102])\n",
      "\n",
      "\n",
      "************** Batch 320 in 0.5463650226593018 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1220, 0.0337, 0.1725, 0.0962, 0.1317, 0.1030]) \n",
      "Test Loss tensor([0.1248, 0.0310, 0.1677, 0.0936, 0.1315, 0.1106])\n",
      "\n",
      "\n",
      "************** Batch 324 in 0.5374572277069092 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1266, 0.0334, 0.1668, 0.1023, 0.1380, 0.1047]) \n",
      "Test Loss tensor([0.1227, 0.0308, 0.1672, 0.0910, 0.1355, 0.1111])\n",
      "\n",
      "\n",
      "************** Batch 328 in 0.5517399311065674 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1202, 0.0259, 0.1773, 0.0952, 0.1337, 0.1124]) \n",
      "Test Loss tensor([0.1227, 0.0297, 0.1639, 0.0906, 0.1345, 0.1106])\n",
      "\n",
      "\n",
      "************** Batch 332 in 0.5426375865936279 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1266, 0.0321, 0.1594, 0.0893, 0.1308, 0.1134]) \n",
      "Test Loss tensor([0.1202, 0.0304, 0.1630, 0.0934, 0.1324, 0.1102])\n",
      "\n",
      "\n",
      "************** Batch 336 in 0.5644590854644775 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1287, 0.0364, 0.1752, 0.0881, 0.1323, 0.1093]) \n",
      "Test Loss tensor([0.1255, 0.0325, 0.1632, 0.0939, 0.1326, 0.1090])\n",
      "\n",
      "\n",
      "************** Batch 340 in 0.539945125579834 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1263, 0.0321, 0.1652, 0.0857, 0.1239, 0.1126]) \n",
      "Test Loss tensor([0.1248, 0.0309, 0.1673, 0.0930, 0.1311, 0.1089])\n",
      "\n",
      "\n",
      "************** Batch 344 in 0.5445051193237305 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1177, 0.0311, 0.1519, 0.1045, 0.1344, 0.1044]) \n",
      "Test Loss tensor([0.1235, 0.0299, 0.1648, 0.0921, 0.1275, 0.1095])\n",
      "\n",
      "\n",
      "************** Batch 348 in 0.5381820201873779 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1218, 0.0325, 0.1735, 0.0962, 0.1280, 0.1062]) \n",
      "Test Loss tensor([0.1199, 0.0300, 0.1627, 0.0900, 0.1304, 0.1113])\n",
      "\n",
      "\n",
      "************** Batch 352 in 0.5481696128845215 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1225, 0.0317, 0.1637, 0.0904, 0.1408, 0.1060]) \n",
      "Test Loss tensor([0.1224, 0.0312, 0.1641, 0.0929, 0.1295, 0.1087])\n",
      "\n",
      "\n",
      "************** Batch 356 in 0.5429365634918213 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1373, 0.0317, 0.1607, 0.0940, 0.1281, 0.1065]) \n",
      "Test Loss tensor([0.1232, 0.0293, 0.1648, 0.0917, 0.1300, 0.1073])\n",
      "\n",
      "\n",
      "************** Batch 360 in 0.5496461391448975 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1316, 0.0370, 0.1669, 0.0973, 0.1359, 0.1097]) \n",
      "Test Loss tensor([0.1230, 0.0306, 0.1617, 0.0947, 0.1295, 0.1057])\n",
      "\n",
      "\n",
      "************** Batch 364 in 0.5462822914123535 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1239, 0.0349, 0.1693, 0.1006, 0.1276, 0.1066]) \n",
      "Test Loss tensor([0.1205, 0.0294, 0.1612, 0.0917, 0.1267, 0.1076])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 368 in 0.5447835922241211 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1195, 0.0248, 0.1604, 0.0994, 0.1274, 0.1094]) \n",
      "Test Loss tensor([0.1267, 0.0309, 0.1688, 0.0932, 0.1396, 0.1052])\n",
      "\n",
      "\n",
      "************** Batch 372 in 0.5487470626831055 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1175, 0.0307, 0.1645, 0.0947, 0.1333, 0.1083]) \n",
      "Test Loss tensor([0.1210, 0.0314, 0.1664, 0.0937, 0.1280, 0.1072])\n",
      "\n",
      "\n",
      "************** Batch 376 in 0.5345792770385742 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1179, 0.0275, 0.1562, 0.0975, 0.1247, 0.1116]) \n",
      "Test Loss tensor([0.1212, 0.0304, 0.1636, 0.0949, 0.1309, 0.1078])\n",
      "\n",
      "\n",
      "************** Batch 380 in 0.5458524227142334 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1227, 0.0322, 0.1602, 0.0855, 0.1403, 0.1060]) \n",
      "Test Loss tensor([0.1240, 0.0305, 0.1689, 0.0952, 0.1276, 0.1052])\n",
      "\n",
      "\n",
      "************** Batch 384 in 0.535860538482666 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1303, 0.0314, 0.1714, 0.0908, 0.1337, 0.1044]) \n",
      "Test Loss tensor([0.1208, 0.0300, 0.1644, 0.0928, 0.1279, 0.1062])\n",
      "\n",
      "\n",
      "************** Batch 388 in 0.5558385848999023 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1186, 0.0355, 0.1599, 0.0864, 0.1289, 0.0988]) \n",
      "Test Loss tensor([0.1229, 0.0332, 0.1654, 0.0939, 0.1316, 0.1072])\n",
      "\n",
      "\n",
      "************** Batch 392 in 0.535386323928833 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1312, 0.0360, 0.1726, 0.0925, 0.1285, 0.1042]) \n",
      "Test Loss tensor([0.1200, 0.0305, 0.1652, 0.0884, 0.1232, 0.1061])\n",
      "\n",
      "\n",
      "************** Batch 396 in 0.5557818412780762 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1241, 0.0302, 0.1677, 0.0881, 0.1237, 0.0983]) \n",
      "Test Loss tensor([0.1215, 0.0308, 0.1652, 0.0920, 0.1285, 0.1058])\n",
      "\n",
      "\n",
      "************** Batch 400 in 0.5518083572387695 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1283, 0.0325, 0.1653, 0.0887, 0.1215, 0.1098]) \n",
      "Test Loss tensor([0.1199, 0.0313, 0.1658, 0.0930, 0.1273, 0.1040])\n",
      "\n",
      "\n",
      "************** Batch 404 in 0.5553641319274902 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1168, 0.0347, 0.1524, 0.0887, 0.1200, 0.1058]) \n",
      "Test Loss tensor([0.1207, 0.0310, 0.1621, 0.0935, 0.1277, 0.1049])\n",
      "\n",
      "\n",
      "************** Batch 408 in 0.5488488674163818 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1221, 0.0320, 0.1690, 0.0897, 0.1341, 0.1008]) \n",
      "Test Loss tensor([0.1216, 0.0303, 0.1668, 0.0934, 0.1259, 0.1051])\n",
      "\n",
      "\n",
      "************** Batch 412 in 0.5406832695007324 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1258, 0.0323, 0.1771, 0.0908, 0.1292, 0.1049]) \n",
      "Test Loss tensor([0.1216, 0.0298, 0.1668, 0.0924, 0.1268, 0.1042])\n",
      "\n",
      "\n",
      "************** Batch 416 in 0.5448594093322754 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1225, 0.0319, 0.1753, 0.0937, 0.1301, 0.0975]) \n",
      "Test Loss tensor([0.1183, 0.0313, 0.1636, 0.0937, 0.1280, 0.1041])\n",
      "\n",
      "\n",
      "************** Batch 420 in 0.5382273197174072 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1256, 0.0353, 0.1693, 0.0908, 0.1240, 0.1013]) \n",
      "Test Loss tensor([0.1206, 0.0318, 0.1627, 0.0914, 0.1277, 0.1032])\n",
      "\n",
      "\n",
      "************** Batch 424 in 0.5393257141113281 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1236, 0.0356, 0.1592, 0.0962, 0.1289, 0.1058]) \n",
      "Test Loss tensor([0.1245, 0.0296, 0.1663, 0.0884, 0.1285, 0.1029])\n",
      "\n",
      "\n",
      "************** Batch 428 in 0.535707950592041 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1277, 0.0252, 0.1673, 0.0901, 0.1246, 0.1023]) \n",
      "Test Loss tensor([0.1207, 0.0306, 0.1653, 0.0923, 0.1268, 0.1014])\n",
      "\n",
      "\n",
      "************** Batch 432 in 0.5553042888641357 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1264, 0.0311, 0.1659, 0.0940, 0.1184, 0.1070]) \n",
      "Test Loss tensor([0.1184, 0.0296, 0.1596, 0.0916, 0.1263, 0.1021])\n",
      "\n",
      "\n",
      "************** Batch 436 in 0.5452084541320801 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1285, 0.0291, 0.1723, 0.0918, 0.1269, 0.1078]) \n",
      "Test Loss tensor([0.1210, 0.0327, 0.1626, 0.0918, 0.1250, 0.1012])\n",
      "\n",
      "\n",
      "************** Batch 440 in 0.5478510856628418 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1222, 0.0341, 0.1829, 0.1039, 0.1176, 0.0991]) \n",
      "Test Loss tensor([0.1228, 0.0290, 0.1672, 0.0928, 0.1271, 0.1031])\n",
      "\n",
      "\n",
      "************** Batch 444 in 0.5336272716522217 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1216, 0.0348, 0.1914, 0.0926, 0.1288, 0.1045]) \n",
      "Test Loss tensor([0.1198, 0.0322, 0.1648, 0.0916, 0.1232, 0.1010])\n",
      "\n",
      "\n",
      "************** Batch 448 in 0.5446419715881348 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1240, 0.0303, 0.1573, 0.0928, 0.1215, 0.1023]) \n",
      "Test Loss tensor([0.1205, 0.0324, 0.1599, 0.0906, 0.1294, 0.1046])\n",
      "\n",
      "\n",
      "************** Batch 452 in 0.5468661785125732 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1207, 0.0362, 0.1680, 0.0877, 0.1338, 0.1046]) \n",
      "Test Loss tensor([0.1193, 0.0316, 0.1616, 0.0900, 0.1241, 0.1008])\n",
      "\n",
      "\n",
      "************** Batch 456 in 0.5352544784545898 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1259, 0.0368, 0.1560, 0.0968, 0.1164, 0.1002]) \n",
      "Test Loss tensor([0.1243, 0.0300, 0.1640, 0.0918, 0.1231, 0.1001])\n",
      "\n",
      "\n",
      "************** Batch 460 in 0.558161735534668 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1199, 0.0272, 0.1703, 0.0923, 0.1120, 0.1021]) \n",
      "Test Loss tensor([0.1200, 0.0294, 0.1639, 0.0922, 0.1223, 0.0995])\n",
      "\n",
      "\n",
      "************** Batch 464 in 0.5400528907775879 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1183, 0.0308, 0.1688, 0.1049, 0.1221, 0.0946]) \n",
      "Test Loss tensor([0.1158, 0.0304, 0.1607, 0.0941, 0.1215, 0.0990])\n",
      "\n",
      "\n",
      "************** Batch 468 in 0.5471925735473633 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1235, 0.0301, 0.1658, 0.0895, 0.1208, 0.1012]) \n",
      "Test Loss tensor([0.1190, 0.0313, 0.1613, 0.0897, 0.1206, 0.0970])\n",
      "\n",
      "\n",
      "************** Batch 472 in 0.5423614978790283 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1184, 0.0342, 0.1663, 0.0940, 0.1196, 0.0946]) \n",
      "Test Loss tensor([0.1179, 0.0305, 0.1664, 0.0928, 0.1229, 0.0960])\n",
      "\n",
      "\n",
      "************** Batch 476 in 0.5557761192321777 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1198, 0.0320, 0.1699, 0.0932, 0.1255, 0.0961]) \n",
      "Test Loss tensor([0.1198, 0.0310, 0.1636, 0.0920, 0.1220, 0.0962])\n",
      "\n",
      "\n",
      "************** Batch 480 in 0.5434362888336182 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1195, 0.0260, 0.1622, 0.0904, 0.1158, 0.0933]) \n",
      "Test Loss tensor([0.1163, 0.0299, 0.1650, 0.0922, 0.1179, 0.0987])\n",
      "\n",
      "\n",
      "************** Batch 484 in 0.5448060035705566 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1151, 0.0293, 0.1659, 0.0830, 0.1212, 0.0906]) \n",
      "Test Loss tensor([0.1201, 0.0303, 0.1634, 0.0927, 0.1188, 0.0967])\n",
      "\n",
      "\n",
      "************** Batch 488 in 0.5393857955932617 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1254, 0.0329, 0.1613, 0.0897, 0.1201, 0.0993]) \n",
      "Test Loss tensor([0.1175, 0.0297, 0.1636, 0.0922, 0.1195, 0.0943])\n",
      "\n",
      "\n",
      "************** Batch 492 in 0.5493831634521484 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1208, 0.0330, 0.1740, 0.0953, 0.1283, 0.1031]) \n",
      "Test Loss tensor([0.1201, 0.0321, 0.1660, 0.0911, 0.1166, 0.0967])\n",
      "\n",
      "\n",
      "************** Batch 496 in 0.5545041561126709 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1161, 0.0343, 0.1520, 0.0889, 0.1256, 0.0967]) \n",
      "Test Loss tensor([0.1169, 0.0304, 0.1651, 0.0931, 0.1176, 0.0980])\n",
      "\n",
      "\n",
      "************** Batch 500 in 0.5383055210113525 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1169, 0.0282, 0.1665, 0.0946, 0.1087, 0.1011]) \n",
      "Test Loss tensor([0.1141, 0.0305, 0.1592, 0.0940, 0.1203, 0.0951])\n",
      "\n",
      "\n",
      "************** Batch 504 in 0.5434117317199707 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1195, 0.0326, 0.1693, 0.0962, 0.1199, 0.0989]) \n",
      "Test Loss tensor([0.1179, 0.0305, 0.1626, 0.0909, 0.1159, 0.0959])\n",
      "\n",
      "\n",
      "************** Batch 508 in 0.5419199466705322 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1188, 0.0335, 0.1690, 0.0823, 0.1199, 0.0975]) \n",
      "Test Loss tensor([0.1174, 0.0312, 0.1632, 0.0914, 0.1161, 0.0967])\n",
      "\n",
      "\n",
      "************** Batch 512 in 0.5473859310150146 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1099, 0.0299, 0.1685, 0.0968, 0.1200, 0.0948]) \n",
      "Test Loss tensor([0.1169, 0.0289, 0.1611, 0.0920, 0.1161, 0.0953])\n",
      "\n",
      "\n",
      "************** Batch 516 in 0.5421223640441895 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1170, 0.0250, 0.1713, 0.0872, 0.1063, 0.0996]) \n",
      "Test Loss tensor([0.1173, 0.0307, 0.1623, 0.0925, 0.1156, 0.0955])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 520 in 0.5479378700256348 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1233, 0.0391, 0.1675, 0.0985, 0.1197, 0.0916]) \n",
      "Test Loss tensor([0.1169, 0.0311, 0.1624, 0.0939, 0.1162, 0.0948])\n",
      "\n",
      "\n",
      "************** Batch 524 in 0.5471928119659424 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1232, 0.0337, 0.1687, 0.0882, 0.1148, 0.0971]) \n",
      "Test Loss tensor([0.1176, 0.0312, 0.1635, 0.0929, 0.1174, 0.0938])\n",
      "\n",
      "\n",
      "************** Batch 528 in 0.5457730293273926 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1095, 0.0323, 0.1633, 0.0948, 0.1128, 0.1003]) \n",
      "Test Loss tensor([0.1177, 0.0311, 0.1614, 0.0912, 0.1152, 0.0930])\n",
      "\n",
      "\n",
      "************** Batch 532 in 0.5955703258514404 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1112, 0.0281, 0.1690, 0.0853, 0.1070, 0.1025]) \n",
      "Test Loss tensor([0.1127, 0.0298, 0.1626, 0.0912, 0.1142, 0.0946])\n",
      "\n",
      "\n",
      "************** Batch 536 in 0.5432875156402588 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1305, 0.0386, 0.1606, 0.0956, 0.1186, 0.0971]) \n",
      "Test Loss tensor([0.1164, 0.0294, 0.1617, 0.0934, 0.1140, 0.0933])\n",
      "\n",
      "\n",
      "************** Batch 540 in 0.5443623065948486 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1192, 0.0322, 0.1623, 0.0908, 0.1207, 0.0877]) \n",
      "Test Loss tensor([0.1151, 0.0297, 0.1621, 0.0933, 0.1160, 0.0934])\n",
      "\n",
      "\n",
      "************** Batch 544 in 0.5397171974182129 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1189, 0.0317, 0.1642, 0.0943, 0.1191, 0.0959]) \n",
      "Test Loss tensor([0.1173, 0.0321, 0.1624, 0.0932, 0.1162, 0.0933])\n",
      "\n",
      "\n",
      "************** Batch 548 in 0.5472266674041748 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1092, 0.0296, 0.1674, 0.0957, 0.1122, 0.0931]) \n",
      "Test Loss tensor([0.1140, 0.0304, 0.1624, 0.0926, 0.1136, 0.0923])\n",
      "\n",
      "\n",
      "************** Batch 552 in 0.5407018661499023 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1163, 0.0281, 0.1574, 0.0934, 0.1139, 0.0942]) \n",
      "Test Loss tensor([0.1139, 0.0296, 0.1623, 0.0931, 0.1176, 0.0923])\n",
      "\n",
      "\n",
      "************** Batch 556 in 0.5512704849243164 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1140, 0.0308, 0.1639, 0.0889, 0.1156, 0.0990]) \n",
      "Test Loss tensor([0.1148, 0.0307, 0.1609, 0.0906, 0.1125, 0.0921])\n",
      "\n",
      "\n",
      "************** Batch 560 in 0.5421371459960938 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1099, 0.0259, 0.1571, 0.0943, 0.1104, 0.0948]) \n",
      "Test Loss tensor([0.1168, 0.0302, 0.1604, 0.0944, 0.1130, 0.0902])\n",
      "\n",
      "\n",
      "************** Batch 564 in 0.5520448684692383 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1253, 0.0273, 0.1702, 0.0964, 0.1070, 0.0845]) \n",
      "Test Loss tensor([0.1157, 0.0293, 0.1615, 0.0894, 0.1129, 0.0919])\n",
      "\n",
      "\n",
      "************** Batch 568 in 0.5428848266601562 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1139, 0.0267, 0.1638, 0.0952, 0.1094, 0.0988]) \n",
      "Test Loss tensor([0.1189, 0.0296, 0.1631, 0.0933, 0.1143, 0.0925])\n",
      "\n",
      "\n",
      "************** Batch 572 in 0.5422375202178955 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1251, 0.0285, 0.1628, 0.0865, 0.1103, 0.0948]) \n",
      "Test Loss tensor([0.1138, 0.0309, 0.1619, 0.0912, 0.1100, 0.0930])\n",
      "\n",
      "\n",
      "************** Batch 576 in 0.5491816997528076 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1169, 0.0251, 0.1682, 0.0914, 0.1086, 0.0909]) \n",
      "Test Loss tensor([0.1168, 0.0316, 0.1569, 0.0928, 0.1146, 0.0937])\n",
      "\n",
      "\n",
      "************** Batch 580 in 0.5395078659057617 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1145, 0.0366, 0.1661, 0.0899, 0.1145, 0.0944]) \n",
      "Test Loss tensor([0.1160, 0.0312, 0.1617, 0.0915, 0.1134, 0.0894])\n",
      "\n",
      "\n",
      "************** Batch 584 in 0.5474720001220703 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1242, 0.0308, 0.1548, 0.0898, 0.1083, 0.0974]) \n",
      "Test Loss tensor([0.1166, 0.0309, 0.1641, 0.0916, 0.1130, 0.0924])\n",
      "\n",
      "\n",
      "************** Batch 588 in 0.5320899486541748 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1134, 0.0329, 0.1785, 0.0870, 0.1144, 0.0985]) \n",
      "Test Loss tensor([0.1185, 0.0323, 0.1617, 0.0931, 0.1124, 0.0914])\n",
      "\n",
      "\n",
      "************** Batch 592 in 0.5542926788330078 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1197, 0.0288, 0.1655, 0.0969, 0.1040, 0.0933]) \n",
      "Test Loss tensor([0.1151, 0.0288, 0.1615, 0.0910, 0.1110, 0.0897])\n",
      "\n",
      "\n",
      "************** Batch 596 in 0.535097599029541 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1137, 0.0313, 0.1533, 0.0914, 0.1146, 0.0928]) \n",
      "Test Loss tensor([0.1141, 0.0287, 0.1602, 0.0911, 0.1142, 0.0891])\n",
      "\n",
      "\n",
      "************** Batch 600 in 0.5478203296661377 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1139, 0.0307, 0.1645, 0.1011, 0.1100, 0.0954]) \n",
      "Test Loss tensor([0.1139, 0.0305, 0.1631, 0.0948, 0.1099, 0.0914])\n",
      "\n",
      "\n",
      "************** Batch 604 in 0.5385432243347168 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1139, 0.0331, 0.1690, 0.0913, 0.1128, 0.0837]) \n",
      "Test Loss tensor([0.1173, 0.0314, 0.1634, 0.0934, 0.1098, 0.0895])\n",
      "\n",
      "\n",
      "************** Batch 608 in 0.5432307720184326 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1056, 0.0279, 0.1590, 0.0925, 0.1098, 0.0918]) \n",
      "Test Loss tensor([0.1137, 0.0305, 0.1631, 0.0912, 0.1086, 0.0916])\n",
      "\n",
      "\n",
      "************** Batch 612 in 0.576005220413208 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1173, 0.0324, 0.1530, 0.0895, 0.1072, 0.0866]) \n",
      "Test Loss tensor([0.1133, 0.0298, 0.1610, 0.0939, 0.1100, 0.0896])\n",
      "\n",
      "\n",
      "************** Batch 616 in 0.5296905040740967 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1181, 0.0340, 0.1746, 0.0986, 0.1096, 0.0892]) \n",
      "Test Loss tensor([0.1138, 0.0311, 0.1640, 0.0924, 0.1066, 0.0894])\n",
      "\n",
      "\n",
      "************** Batch 620 in 0.5426874160766602 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1157, 0.0334, 0.1562, 0.0845, 0.1097, 0.0875]) \n",
      "Test Loss tensor([0.1118, 0.0301, 0.1591, 0.0898, 0.1088, 0.0881])\n",
      "\n",
      "\n",
      "************** Batch 624 in 0.5330004692077637 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1210, 0.0319, 0.1605, 0.0914, 0.1103, 0.0872]) \n",
      "Test Loss tensor([0.1150, 0.0322, 0.1621, 0.0904, 0.1091, 0.0891])\n",
      "\n",
      "\n",
      "************** Batch 628 in 0.5496971607208252 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1108, 0.0265, 0.1611, 0.0916, 0.1038, 0.0916]) \n",
      "Test Loss tensor([0.1149, 0.0298, 0.1623, 0.0909, 0.1085, 0.0920])\n",
      "\n",
      "\n",
      "************** Batch 632 in 0.5329675674438477 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1137, 0.0316, 0.1755, 0.0956, 0.1064, 0.0983]) \n",
      "Test Loss tensor([0.1141, 0.0299, 0.1641, 0.0893, 0.1084, 0.0896])\n",
      "\n",
      "\n",
      "************** Batch 636 in 0.5390751361846924 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1201, 0.0317, 0.1733, 0.0907, 0.1102, 0.0918]) \n",
      "Test Loss tensor([0.1131, 0.0307, 0.1648, 0.0919, 0.1085, 0.0893])\n",
      "\n",
      "\n",
      "************** Batch 640 in 0.5331258773803711 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1098, 0.0372, 0.1658, 0.0951, 0.1119, 0.0879]) \n",
      "Test Loss tensor([0.1135, 0.0307, 0.1605, 0.0914, 0.1077, 0.0860])\n",
      "\n",
      "\n",
      "************** Batch 644 in 0.5461466312408447 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1257, 0.0314, 0.1623, 0.1010, 0.1105, 0.0901]) \n",
      "Test Loss tensor([0.1132, 0.0304, 0.1617, 0.0925, 0.1073, 0.0876])\n",
      "\n",
      "\n",
      "************** Batch 648 in 0.535132884979248 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1269, 0.0314, 0.1582, 0.0913, 0.1044, 0.0783]) \n",
      "Test Loss tensor([0.1120, 0.0322, 0.1636, 0.0920, 0.1075, 0.0878])\n",
      "\n",
      "\n",
      "************** Batch 652 in 0.5332605838775635 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1082, 0.0289, 0.1539, 0.0946, 0.1067, 0.0853]) \n",
      "Test Loss tensor([0.1116, 0.0307, 0.1603, 0.0894, 0.1073, 0.0874])\n",
      "\n",
      "\n",
      "************** Batch 656 in 0.5545952320098877 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1135, 0.0260, 0.1633, 0.0961, 0.1013, 0.0928]) \n",
      "Test Loss tensor([0.1112, 0.0303, 0.1596, 0.0883, 0.1069, 0.0880])\n",
      "\n",
      "\n",
      "************** Batch 660 in 0.5349490642547607 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1073, 0.0285, 0.1616, 0.0833, 0.1097, 0.0911]) \n",
      "Test Loss tensor([0.1135, 0.0319, 0.1587, 0.0894, 0.1077, 0.0872])\n",
      "\n",
      "\n",
      "************** Batch 664 in 0.5456464290618896 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1175, 0.0354, 0.1709, 0.0930, 0.1043, 0.0918]) \n",
      "Test Loss tensor([0.1158, 0.0304, 0.1586, 0.0899, 0.1063, 0.0863])\n",
      "\n",
      "\n",
      "************** Batch 668 in 0.536684513092041 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1125, 0.0303, 0.1590, 0.0947, 0.1083, 0.0842]) \n",
      "Test Loss tensor([0.1126, 0.0297, 0.1591, 0.0919, 0.1066, 0.0859])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 672 in 0.5520679950714111 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1160, 0.0301, 0.1643, 0.0862, 0.1048, 0.0906]) \n",
      "Test Loss tensor([0.1146, 0.0306, 0.1597, 0.0904, 0.1114, 0.0869])\n",
      "\n",
      "\n",
      "************** Batch 676 in 0.5476300716400146 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1124, 0.0299, 0.1640, 0.0917, 0.1009, 0.0870]) \n",
      "Test Loss tensor([0.1170, 0.0313, 0.1618, 0.0930, 0.1099, 0.0861])\n",
      "\n",
      "\n",
      "************** Batch 680 in 0.5571057796478271 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1179, 0.0336, 0.1547, 0.0862, 0.1112, 0.0922]) \n",
      "Test Loss tensor([0.1133, 0.0303, 0.1583, 0.0882, 0.1049, 0.0866])\n",
      "\n",
      "\n",
      "************** Batch 684 in 0.5437507629394531 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1142, 0.0297, 0.1507, 0.0871, 0.1042, 0.0930]) \n",
      "Test Loss tensor([0.1143, 0.0291, 0.1621, 0.0905, 0.1082, 0.0872])\n",
      "\n",
      "\n",
      "************** Batch 688 in 0.5700051784515381 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1191, 0.0352, 0.1689, 0.0915, 0.1079, 0.0868]) \n",
      "Test Loss tensor([0.1113, 0.0292, 0.1560, 0.0895, 0.1050, 0.0857])\n",
      "\n",
      "\n",
      "************** Batch 692 in 0.5313191413879395 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1077, 0.0304, 0.1430, 0.0926, 0.1020, 0.0805]) \n",
      "Test Loss tensor([0.1131, 0.0335, 0.1587, 0.0913, 0.1092, 0.0894])\n",
      "\n",
      "\n",
      "************** Batch 696 in 0.5458564758300781 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1179, 0.0351, 0.1564, 0.0896, 0.1064, 0.0905]) \n",
      "Test Loss tensor([0.1136, 0.0310, 0.1602, 0.0898, 0.1053, 0.0854])\n",
      "\n",
      "\n",
      "************** Batch 700 in 0.5486960411071777 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1093, 0.0347, 0.1483, 0.0926, 0.1075, 0.0860]) \n",
      "Test Loss tensor([0.1151, 0.0301, 0.1616, 0.0909, 0.1085, 0.0871])\n",
      "\n",
      "\n",
      "************** Batch 704 in 0.5340042114257812 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1210, 0.0293, 0.1615, 0.0909, 0.1078, 0.0859]) \n",
      "Test Loss tensor([0.1151, 0.0318, 0.1567, 0.0902, 0.1060, 0.0840])\n",
      "\n",
      "\n",
      "************** Batch 708 in 0.5565588474273682 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1122, 0.0345, 0.1628, 0.0846, 0.1091, 0.0894]) \n",
      "Test Loss tensor([0.1123, 0.0326, 0.1628, 0.0899, 0.1032, 0.0875])\n",
      "\n",
      "\n",
      "************** Batch 712 in 0.557370662689209 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1088, 0.0307, 0.1508, 0.0828, 0.0999, 0.0838]) \n",
      "Test Loss tensor([0.1141, 0.0310, 0.1600, 0.0890, 0.1102, 0.0852])\n",
      "\n",
      "\n",
      "************** Batch 716 in 0.562471866607666 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1126, 0.0320, 0.1677, 0.0966, 0.1096, 0.0905]) \n",
      "Test Loss tensor([0.1110, 0.0325, 0.1629, 0.0932, 0.1039, 0.0847])\n",
      "\n",
      "\n",
      "************** Batch 720 in 0.5432348251342773 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1100, 0.0273, 0.1678, 0.0904, 0.1024, 0.0793]) \n",
      "Test Loss tensor([0.1121, 0.0331, 0.1603, 0.0897, 0.1052, 0.0895])\n",
      "\n",
      "\n",
      "************** Batch 724 in 0.5461888313293457 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1098, 0.0304, 0.1522, 0.0902, 0.1101, 0.0881]) \n",
      "Test Loss tensor([0.1142, 0.0302, 0.1587, 0.0912, 0.1038, 0.0829])\n",
      "\n",
      "\n",
      "************** Batch 728 in 0.5306837558746338 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1113, 0.0342, 0.1616, 0.0857, 0.1142, 0.0818]) \n",
      "Test Loss tensor([0.1112, 0.0304, 0.1565, 0.0855, 0.1045, 0.0835])\n",
      "\n",
      "\n",
      "************** Batch 732 in 0.5451054573059082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1171, 0.0352, 0.1643, 0.0892, 0.1073, 0.0887]) \n",
      "Test Loss tensor([0.1117, 0.0320, 0.1551, 0.0894, 0.1073, 0.0864])\n",
      "\n",
      "\n",
      "************** Batch 736 in 0.5388839244842529 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1205, 0.0370, 0.1667, 0.0923, 0.1026, 0.0889]) \n",
      "Test Loss tensor([0.1105, 0.0301, 0.1603, 0.0895, 0.1012, 0.0846])\n",
      "\n",
      "\n",
      "************** Batch 740 in 0.570789098739624 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1179, 0.0286, 0.1655, 0.0906, 0.0989, 0.0813]) \n",
      "Test Loss tensor([0.1152, 0.0285, 0.1598, 0.0893, 0.1131, 0.0862])\n",
      "\n",
      "\n",
      "************** Batch 744 in 0.5446853637695312 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1177, 0.0312, 0.1530, 0.0893, 0.1111, 0.0834]) \n",
      "Test Loss tensor([0.1117, 0.0321, 0.1567, 0.0885, 0.1029, 0.0834])\n",
      "\n",
      "\n",
      "************** Batch 748 in 0.5362474918365479 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1130, 0.0349, 0.1633, 0.0908, 0.1060, 0.0911]) \n",
      "Test Loss tensor([0.1151, 0.0338, 0.1576, 0.0900, 0.1057, 0.0826])\n",
      "\n",
      "\n",
      "************** Batch 752 in 0.5646319389343262 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1189, 0.0333, 0.1628, 0.0882, 0.1093, 0.0804]) \n",
      "Test Loss tensor([0.1106, 0.0300, 0.1551, 0.0889, 0.1033, 0.0831])\n",
      "\n",
      "\n",
      "************** Batch 756 in 0.5555434226989746 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1037, 0.0319, 0.1579, 0.0917, 0.0958, 0.0807]) \n",
      "Test Loss tensor([0.1139, 0.0323, 0.1612, 0.0897, 0.1028, 0.0844])\n",
      "\n",
      "\n",
      "************** Batch 760 in 0.5559654235839844 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1067, 0.0302, 0.1507, 0.0864, 0.0960, 0.0776]) \n",
      "Test Loss tensor([0.1116, 0.0327, 0.1556, 0.0866, 0.1047, 0.0827])\n",
      "\n",
      "\n",
      "************** Batch 764 in 0.5403735637664795 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1105, 0.0299, 0.1585, 0.0879, 0.1073, 0.0834]) \n",
      "Test Loss tensor([0.1111, 0.0313, 0.1616, 0.0884, 0.1001, 0.0825])\n",
      "\n",
      "\n",
      "************** Batch 768 in 0.5496232509613037 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1057, 0.0330, 0.1637, 0.0905, 0.0998, 0.0760]) \n",
      "Test Loss tensor([0.1147, 0.0306, 0.1615, 0.0884, 0.1038, 0.0829])\n",
      "\n",
      "\n",
      "************** Batch 772 in 0.5350992679595947 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1273, 0.0323, 0.1618, 0.0856, 0.1065, 0.0881]) \n",
      "Test Loss tensor([0.1112, 0.0324, 0.1576, 0.0913, 0.1024, 0.0816])\n",
      "\n",
      "\n",
      "************** Batch 776 in 0.5449800491333008 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1199, 0.0333, 0.1637, 0.0940, 0.0967, 0.0868]) \n",
      "Test Loss tensor([0.1112, 0.0322, 0.1558, 0.0875, 0.1033, 0.0824])\n",
      "\n",
      "\n",
      "************** Batch 780 in 0.5436015129089355 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1229, 0.0347, 0.1538, 0.0832, 0.1047, 0.0838]) \n",
      "Test Loss tensor([0.1104, 0.0299, 0.1585, 0.0862, 0.1024, 0.0826])\n",
      "\n",
      "\n",
      "************** Batch 784 in 0.5401637554168701 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1123, 0.0336, 0.1679, 0.0816, 0.1058, 0.0854]) \n",
      "Test Loss tensor([0.1132, 0.0294, 0.1592, 0.0864, 0.1031, 0.0828])\n",
      "\n",
      "\n",
      "************** Batch 788 in 0.5501887798309326 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1132, 0.0336, 0.1611, 0.0897, 0.1029, 0.0863]) \n",
      "Test Loss tensor([0.1110, 0.0308, 0.1539, 0.0876, 0.1012, 0.0851])\n",
      "\n",
      "\n",
      "************** Batch 792 in 0.5363342761993408 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1128, 0.0282, 0.1574, 0.0864, 0.1001, 0.0788]) \n",
      "Test Loss tensor([0.1114, 0.0335, 0.1596, 0.0921, 0.1017, 0.0815])\n",
      "\n",
      "\n",
      "************** Batch 796 in 0.5533177852630615 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1088, 0.0359, 0.1654, 0.0924, 0.0986, 0.0791]) \n",
      "Test Loss tensor([0.1141, 0.0318, 0.1568, 0.0853, 0.1031, 0.0821])\n",
      "\n",
      "\n",
      "************** Batch 800 in 0.5299694538116455 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1255, 0.0287, 0.1582, 0.0947, 0.1077, 0.0827]) \n",
      "Test Loss tensor([0.1124, 0.0331, 0.1599, 0.0896, 0.0990, 0.0819])\n",
      "\n",
      "\n",
      "************** Batch 804 in 0.550511360168457 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1124, 0.0293, 0.1547, 0.0929, 0.0977, 0.0799]) \n",
      "Test Loss tensor([0.1110, 0.0333, 0.1546, 0.0870, 0.1030, 0.0838])\n",
      "\n",
      "\n",
      "************** Batch 808 in 0.5350828170776367 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1091, 0.0317, 0.1484, 0.0843, 0.0960, 0.0793]) \n",
      "Test Loss tensor([0.1108, 0.0312, 0.1593, 0.0915, 0.0987, 0.0817])\n",
      "\n",
      "\n",
      "************** Batch 812 in 0.55043625831604 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1065, 0.0316, 0.1601, 0.0900, 0.0921, 0.0774]) \n",
      "Test Loss tensor([0.1111, 0.0324, 0.1607, 0.0887, 0.1007, 0.0813])\n",
      "\n",
      "\n",
      "************** Batch 816 in 0.5482845306396484 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1062, 0.0345, 0.1661, 0.0810, 0.1024, 0.0782]) \n",
      "Test Loss tensor([0.1079, 0.0317, 0.1557, 0.0898, 0.0984, 0.0814])\n",
      "\n",
      "\n",
      "************** Batch 820 in 0.5465233325958252 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1118, 0.0332, 0.1566, 0.0922, 0.1043, 0.0858]) \n",
      "Test Loss tensor([0.1100, 0.0314, 0.1549, 0.0892, 0.0995, 0.0843])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 824 in 0.5467476844787598 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1113, 0.0367, 0.1580, 0.0947, 0.1053, 0.0828]) \n",
      "Test Loss tensor([0.1112, 0.0318, 0.1573, 0.0896, 0.1005, 0.0807])\n",
      "\n",
      "\n",
      "************** Batch 828 in 0.5381989479064941 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1049, 0.0317, 0.1522, 0.0864, 0.1004, 0.0830]) \n",
      "Test Loss tensor([0.1121, 0.0315, 0.1606, 0.0906, 0.0989, 0.0818])\n",
      "\n",
      "\n",
      "************** Batch 832 in 0.5499377250671387 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1070, 0.0252, 0.1592, 0.0873, 0.0947, 0.0853]) \n",
      "Test Loss tensor([0.1135, 0.0343, 0.1568, 0.0890, 0.1036, 0.0832])\n",
      "\n",
      "\n",
      "************** Batch 836 in 0.5429871082305908 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1282, 0.0330, 0.1501, 0.0912, 0.0939, 0.0760]) \n",
      "Test Loss tensor([0.1086, 0.0316, 0.1533, 0.0859, 0.0993, 0.0812])\n",
      "\n",
      "\n",
      "************** Batch 840 in 0.5515596866607666 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1081, 0.0317, 0.1515, 0.0853, 0.0959, 0.0777]) \n",
      "Test Loss tensor([0.1148, 0.0307, 0.1591, 0.0877, 0.1057, 0.0828])\n",
      "\n",
      "\n",
      "************** Batch 844 in 0.5265793800354004 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1079, 0.0302, 0.1593, 0.0837, 0.1072, 0.0810]) \n",
      "Test Loss tensor([0.1089, 0.0320, 0.1545, 0.0885, 0.0983, 0.0814])\n",
      "\n",
      "\n",
      "************** Batch 848 in 0.5477414131164551 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1095, 0.0323, 0.1548, 0.0918, 0.0970, 0.0751]) \n",
      "Test Loss tensor([0.1105, 0.0338, 0.1549, 0.0893, 0.1035, 0.0825])\n",
      "\n",
      "\n",
      "************** Batch 852 in 0.5397906303405762 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1104, 0.0343, 0.1405, 0.0880, 0.0930, 0.0775]) \n",
      "Test Loss tensor([0.1068, 0.0332, 0.1586, 0.0864, 0.0989, 0.0808])\n",
      "\n",
      "\n",
      "************** Batch 856 in 0.5538434982299805 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1116, 0.0362, 0.1591, 0.0958, 0.0991, 0.0725]) \n",
      "Test Loss tensor([0.1123, 0.0306, 0.1574, 0.0888, 0.1016, 0.0798])\n",
      "\n",
      "\n",
      "************** Batch 860 in 0.5316073894500732 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1065, 0.0285, 0.1553, 0.0920, 0.1010, 0.0828]) \n",
      "Test Loss tensor([0.1068, 0.0314, 0.1541, 0.0900, 0.0988, 0.0792])\n",
      "\n",
      "\n",
      "************** Batch 864 in 0.5525264739990234 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1127, 0.0303, 0.1607, 0.0944, 0.1082, 0.0777]) \n",
      "Test Loss tensor([0.1101, 0.0324, 0.1511, 0.0890, 0.1001, 0.0793])\n",
      "\n",
      "\n",
      "************** Batch 868 in 0.5378005504608154 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1025, 0.0261, 0.1437, 0.0810, 0.1021, 0.0772]) \n",
      "Test Loss tensor([0.1097, 0.0314, 0.1560, 0.0885, 0.0997, 0.0786])\n",
      "\n",
      "\n",
      "************** Batch 872 in 0.5422132015228271 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1032, 0.0359, 0.1620, 0.0795, 0.1051, 0.0777]) \n",
      "Test Loss tensor([0.1100, 0.0333, 0.1552, 0.0886, 0.0992, 0.0764])\n",
      "\n",
      "\n",
      "************** Batch 876 in 0.532231330871582 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0798, 0.0234, 0.1165, 0.0645, 0.0764, 0.0620]) \n",
      "Test Loss tensor([0.1045, 0.0311, 0.1539, 0.0873, 0.0962, 0.0798])\n",
      "\n",
      "\n",
      "************** Batch 0 in 0.5321059226989746 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1140, 0.0307, 0.1506, 0.0904, 0.1026, 0.0781]) \n",
      "Test Loss tensor([0.1101, 0.0342, 0.1547, 0.0857, 0.0967, 0.0801])\n",
      "\n",
      "\n",
      "************** Batch 4 in 0.5517089366912842 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1122, 0.0357, 0.1530, 0.0866, 0.1002, 0.0733]) \n",
      "Test Loss tensor([0.1094, 0.0323, 0.1548, 0.0838, 0.0975, 0.0790])\n",
      "\n",
      "\n",
      "************** Batch 8 in 0.5361976623535156 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1106, 0.0344, 0.1537, 0.0843, 0.0992, 0.0797]) \n",
      "Test Loss tensor([0.1102, 0.0332, 0.1557, 0.0877, 0.0968, 0.0797])\n",
      "\n",
      "\n",
      "************** Batch 12 in 0.5551788806915283 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1203, 0.0323, 0.1592, 0.0866, 0.0947, 0.0846]) \n",
      "Test Loss tensor([0.1099, 0.0336, 0.1546, 0.0898, 0.0979, 0.0812])\n",
      "\n",
      "\n",
      "************** Batch 16 in 0.5308277606964111 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1101, 0.0300, 0.1467, 0.0846, 0.0996, 0.0825]) \n",
      "Test Loss tensor([0.1073, 0.0321, 0.1518, 0.0850, 0.0964, 0.0784])\n",
      "\n",
      "\n",
      "************** Batch 20 in 0.5743460655212402 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1072, 0.0350, 0.1630, 0.0860, 0.0884, 0.0780]) \n",
      "Test Loss tensor([0.1099, 0.0323, 0.1553, 0.0831, 0.0972, 0.0790])\n",
      "\n",
      "\n",
      "************** Batch 24 in 0.5334851741790771 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1063, 0.0322, 0.1567, 0.0839, 0.0988, 0.0826]) \n",
      "Test Loss tensor([0.1086, 0.0334, 0.1545, 0.0878, 0.0965, 0.0793])\n",
      "\n",
      "\n",
      "************** Batch 28 in 0.5385613441467285 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1139, 0.0387, 0.1531, 0.0839, 0.1020, 0.0817]) \n",
      "Test Loss tensor([0.1074, 0.0341, 0.1544, 0.0856, 0.0952, 0.0798])\n",
      "\n",
      "\n",
      "************** Batch 32 in 0.5453128814697266 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1170, 0.0312, 0.1518, 0.0992, 0.0922, 0.0781]) \n",
      "Test Loss tensor([0.1080, 0.0340, 0.1551, 0.0875, 0.0981, 0.0781])\n",
      "\n",
      "\n",
      "************** Batch 36 in 0.5366742610931396 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1085, 0.0319, 0.1536, 0.0866, 0.1006, 0.0783]) \n",
      "Test Loss tensor([0.1055, 0.0351, 0.1524, 0.0845, 0.0966, 0.0797])\n",
      "\n",
      "\n",
      "************** Batch 40 in 0.5444724559783936 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1054, 0.0351, 0.1582, 0.0850, 0.0954, 0.0725]) \n",
      "Test Loss tensor([0.1069, 0.0341, 0.1525, 0.0854, 0.0938, 0.0799])\n",
      "\n",
      "\n",
      "************** Batch 44 in 0.5498967170715332 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1049, 0.0345, 0.1518, 0.0752, 0.0986, 0.0789]) \n",
      "Test Loss tensor([0.1098, 0.0338, 0.1539, 0.0867, 0.0956, 0.0776])\n",
      "\n",
      "\n",
      "************** Batch 48 in 0.5526914596557617 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1085, 0.0293, 0.1591, 0.0791, 0.0927, 0.0785]) \n",
      "Test Loss tensor([0.1086, 0.0331, 0.1519, 0.0871, 0.0944, 0.0780])\n",
      "\n",
      "\n",
      "************** Batch 52 in 0.5570278167724609 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1069, 0.0309, 0.1545, 0.0891, 0.0922, 0.0766]) \n",
      "Test Loss tensor([0.1049, 0.0330, 0.1485, 0.0859, 0.0951, 0.0769])\n",
      "\n",
      "\n",
      "************** Batch 56 in 0.5786306858062744 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0964, 0.0331, 0.1585, 0.0888, 0.1008, 0.0814]) \n",
      "Test Loss tensor([0.1065, 0.0331, 0.1518, 0.0878, 0.0934, 0.0766])\n",
      "\n",
      "\n",
      "************** Batch 60 in 0.5331778526306152 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1029, 0.0304, 0.1504, 0.0814, 0.0956, 0.0817]) \n",
      "Test Loss tensor([0.1065, 0.0325, 0.1522, 0.0859, 0.0950, 0.0771])\n",
      "\n",
      "\n",
      "************** Batch 64 in 0.54496169090271 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1053, 0.0351, 0.1541, 0.0863, 0.0837, 0.0758]) \n",
      "Test Loss tensor([0.1043, 0.0342, 0.1500, 0.0848, 0.0938, 0.0770])\n",
      "\n",
      "\n",
      "************** Batch 68 in 0.5495946407318115 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1094, 0.0348, 0.1517, 0.0940, 0.0941, 0.0731]) \n",
      "Test Loss tensor([0.1074, 0.0344, 0.1499, 0.0842, 0.0950, 0.0776])\n",
      "\n",
      "\n",
      "************** Batch 72 in 0.5395548343658447 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0996, 0.0349, 0.1522, 0.0725, 0.0945, 0.0747]) \n",
      "Test Loss tensor([0.1083, 0.0341, 0.1559, 0.0876, 0.0957, 0.0762])\n",
      "\n",
      "\n",
      "************** Batch 76 in 0.5474207401275635 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1078, 0.0335, 0.1504, 0.0931, 0.0988, 0.0764]) \n",
      "Test Loss tensor([0.1057, 0.0337, 0.1537, 0.0858, 0.0922, 0.0763])\n",
      "\n",
      "\n",
      "************** Batch 80 in 0.530303955078125 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1144, 0.0363, 0.1449, 0.0837, 0.1018, 0.0745]) \n",
      "Test Loss tensor([0.1076, 0.0324, 0.1533, 0.0875, 0.0947, 0.0759])\n",
      "\n",
      "\n",
      "************** Batch 84 in 0.5566842555999756 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1006, 0.0340, 0.1468, 0.0828, 0.0919, 0.0770]) \n",
      "Test Loss tensor([0.1096, 0.0321, 0.1529, 0.0865, 0.0947, 0.0765])\n",
      "\n",
      "\n",
      "************** Batch 88 in 0.5409927368164062 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1088, 0.0266, 0.1469, 0.0954, 0.0981, 0.0800]) \n",
      "Test Loss tensor([0.1081, 0.0347, 0.1538, 0.0880, 0.0941, 0.0765])\n",
      "\n",
      "\n",
      "************** Batch 92 in 0.5547244548797607 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1092, 0.0340, 0.1474, 0.0882, 0.0978, 0.0723]) \n",
      "Test Loss tensor([0.1069, 0.0342, 0.1532, 0.0873, 0.0940, 0.0760])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 96 in 0.5381529331207275 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1007, 0.0303, 0.1482, 0.0925, 0.0938, 0.0759]) \n",
      "Test Loss tensor([0.1104, 0.0319, 0.1529, 0.0878, 0.0966, 0.0756])\n",
      "\n",
      "\n",
      "************** Batch 100 in 0.5588855743408203 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1079, 0.0271, 0.1599, 0.0888, 0.0908, 0.0735]) \n",
      "Test Loss tensor([0.1081, 0.0328, 0.1505, 0.0844, 0.0942, 0.0758])\n",
      "\n",
      "\n",
      "************** Batch 104 in 0.5497269630432129 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1098, 0.0347, 0.1633, 0.0865, 0.0971, 0.0753]) \n",
      "Test Loss tensor([0.1093, 0.0351, 0.1530, 0.0865, 0.0977, 0.0754])\n",
      "\n",
      "\n",
      "************** Batch 108 in 0.5508370399475098 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1089, 0.0328, 0.1452, 0.0718, 0.0984, 0.0716]) \n",
      "Test Loss tensor([0.1077, 0.0340, 0.1540, 0.0866, 0.0928, 0.0776])\n",
      "\n",
      "\n",
      "************** Batch 112 in 0.5534296035766602 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1175, 0.0393, 0.1647, 0.0886, 0.0971, 0.0797]) \n",
      "Test Loss tensor([0.1091, 0.0336, 0.1564, 0.0879, 0.0941, 0.0767])\n",
      "\n",
      "\n",
      "************** Batch 116 in 0.5366461277008057 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1123, 0.0293, 0.1495, 0.0821, 0.1011, 0.0724]) \n",
      "Test Loss tensor([0.1071, 0.0350, 0.1512, 0.0843, 0.0905, 0.0780])\n",
      "\n",
      "\n",
      "************** Batch 120 in 0.546241283416748 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1132, 0.0339, 0.1596, 0.0900, 0.0897, 0.0775]) \n",
      "Test Loss tensor([0.1089, 0.0351, 0.1538, 0.0843, 0.0927, 0.0759])\n",
      "\n",
      "\n",
      "************** Batch 124 in 0.5340588092803955 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1037, 0.0345, 0.1562, 0.0839, 0.0910, 0.0734]) \n",
      "Test Loss tensor([0.1042, 0.0340, 0.1518, 0.0876, 0.0908, 0.0754])\n",
      "\n",
      "\n",
      "************** Batch 128 in 0.555877685546875 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1089, 0.0331, 0.1448, 0.0884, 0.0950, 0.0724]) \n",
      "Test Loss tensor([0.1087, 0.0342, 0.1526, 0.0878, 0.0923, 0.0758])\n",
      "\n",
      "\n",
      "************** Batch 132 in 0.5509271621704102 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1037, 0.0331, 0.1539, 0.0859, 0.0881, 0.0800]) \n",
      "Test Loss tensor([0.1047, 0.0331, 0.1483, 0.0839, 0.0928, 0.0733])\n",
      "\n",
      "\n",
      "************** Batch 136 in 0.5618410110473633 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1073, 0.0344, 0.1426, 0.0841, 0.0883, 0.0757]) \n",
      "Test Loss tensor([0.1087, 0.0355, 0.1493, 0.0844, 0.0952, 0.0752])\n",
      "\n",
      "\n",
      "************** Batch 140 in 0.5356009006500244 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1093, 0.0357, 0.1483, 0.0844, 0.0933, 0.0758]) \n",
      "Test Loss tensor([0.1036, 0.0338, 0.1517, 0.0849, 0.0927, 0.0757])\n",
      "\n",
      "\n",
      "************** Batch 144 in 0.5500199794769287 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1079, 0.0422, 0.1564, 0.0826, 0.0923, 0.0722]) \n",
      "Test Loss tensor([0.1062, 0.0323, 0.1463, 0.0860, 0.0925, 0.0733])\n",
      "\n",
      "\n",
      "************** Batch 148 in 0.5389328002929688 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0930, 0.0312, 0.1540, 0.0825, 0.0894, 0.0736]) \n",
      "Test Loss tensor([0.1060, 0.0350, 0.1500, 0.0865, 0.0928, 0.0748])\n",
      "\n",
      "\n",
      "************** Batch 152 in 0.5521361827850342 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1029, 0.0305, 0.1552, 0.0796, 0.0975, 0.0796]) \n",
      "Test Loss tensor([0.1091, 0.0329, 0.1521, 0.0861, 0.0952, 0.0739])\n",
      "\n",
      "\n",
      "************** Batch 156 in 0.5494351387023926 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1026, 0.0354, 0.1715, 0.0907, 0.0910, 0.0733]) \n",
      "Test Loss tensor([0.1045, 0.0353, 0.1504, 0.0865, 0.0908, 0.0747])\n",
      "\n",
      "\n",
      "************** Batch 160 in 0.5453617572784424 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1062, 0.0389, 0.1366, 0.0891, 0.0905, 0.0725]) \n",
      "Test Loss tensor([0.1050, 0.0343, 0.1483, 0.0859, 0.0935, 0.0747])\n",
      "\n",
      "\n",
      "************** Batch 164 in 0.5507998466491699 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0959, 0.0335, 0.1515, 0.0792, 0.0882, 0.0804]) \n",
      "Test Loss tensor([0.1057, 0.0335, 0.1479, 0.0840, 0.0918, 0.0734])\n",
      "\n",
      "\n",
      "************** Batch 168 in 0.5479252338409424 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1032, 0.0309, 0.1521, 0.0814, 0.0934, 0.0701]) \n",
      "Test Loss tensor([0.1053, 0.0335, 0.1505, 0.0860, 0.0977, 0.0733])\n",
      "\n",
      "\n",
      "************** Batch 172 in 0.6502482891082764 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1031, 0.0312, 0.1557, 0.0845, 0.0921, 0.0780]) \n",
      "Test Loss tensor([0.1064, 0.0346, 0.1502, 0.0864, 0.0918, 0.0736])\n",
      "\n",
      "\n",
      "************** Batch 176 in 0.5513150691986084 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1143, 0.0347, 0.1598, 0.0821, 0.0958, 0.0703]) \n",
      "Test Loss tensor([0.1064, 0.0341, 0.1524, 0.0875, 0.0942, 0.0746])\n",
      "\n",
      "\n",
      "************** Batch 180 in 0.5467205047607422 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1129, 0.0356, 0.1602, 0.0835, 0.0889, 0.0809]) \n",
      "Test Loss tensor([0.1075, 0.0349, 0.1516, 0.0862, 0.0951, 0.0737])\n",
      "\n",
      "\n",
      "************** Batch 184 in 0.5794100761413574 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1058, 0.0362, 0.1595, 0.0836, 0.0983, 0.0758]) \n",
      "Test Loss tensor([0.1069, 0.0340, 0.1527, 0.0873, 0.0928, 0.0725])\n",
      "\n",
      "\n",
      "************** Batch 188 in 0.5375490188598633 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1044, 0.0338, 0.1610, 0.0869, 0.0890, 0.0706]) \n",
      "Test Loss tensor([0.1057, 0.0353, 0.1510, 0.0865, 0.0945, 0.0736])\n",
      "\n",
      "\n",
      "************** Batch 192 in 0.5543696880340576 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1120, 0.0378, 0.1487, 0.0922, 0.1037, 0.0693]) \n",
      "Test Loss tensor([0.1053, 0.0355, 0.1481, 0.0851, 0.0900, 0.0726])\n",
      "\n",
      "\n",
      "************** Batch 196 in 0.5457701683044434 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1098, 0.0340, 0.1622, 0.0976, 0.0863, 0.0725]) \n",
      "Test Loss tensor([0.1081, 0.0361, 0.1545, 0.0886, 0.0909, 0.0734])\n",
      "\n",
      "\n",
      "************** Batch 200 in 0.5653269290924072 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1089, 0.0366, 0.1462, 0.0957, 0.0962, 0.0691]) \n",
      "Test Loss tensor([0.1039, 0.0353, 0.1496, 0.0858, 0.0895, 0.0739])\n",
      "\n",
      "\n",
      "************** Batch 204 in 0.5513424873352051 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1080, 0.0340, 0.1547, 0.0896, 0.0927, 0.0767]) \n",
      "Test Loss tensor([0.1066, 0.0369, 0.1501, 0.0846, 0.0904, 0.0722])\n",
      "\n",
      "\n",
      "************** Batch 208 in 0.5502512454986572 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1009, 0.0340, 0.1608, 0.0852, 0.0870, 0.0766]) \n",
      "Test Loss tensor([0.1076, 0.0349, 0.1512, 0.0854, 0.0917, 0.0741])\n",
      "\n",
      "\n",
      "************** Batch 212 in 0.5301907062530518 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1082, 0.0366, 0.1550, 0.0830, 0.0905, 0.0778]) \n",
      "Test Loss tensor([0.1037, 0.0355, 0.1510, 0.0854, 0.0899, 0.0740])\n",
      "\n",
      "\n",
      "************** Batch 216 in 0.5602414608001709 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1030, 0.0373, 0.1502, 0.0858, 0.0948, 0.0718]) \n",
      "Test Loss tensor([0.1046, 0.0340, 0.1481, 0.0837, 0.0918, 0.0726])\n",
      "\n",
      "\n",
      "************** Batch 220 in 0.5452539920806885 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1046, 0.0309, 0.1547, 0.0897, 0.0868, 0.0685]) \n",
      "Test Loss tensor([0.1066, 0.0353, 0.1485, 0.0828, 0.0907, 0.0734])\n",
      "\n",
      "\n",
      "************** Batch 224 in 0.5461561679840088 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1067, 0.0400, 0.1448, 0.0855, 0.0906, 0.0737]) \n",
      "Test Loss tensor([0.1064, 0.0356, 0.1497, 0.0853, 0.0885, 0.0734])\n",
      "\n",
      "\n",
      "************** Batch 228 in 0.5469675064086914 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0972, 0.0250, 0.1518, 0.0741, 0.0885, 0.0682]) \n",
      "Test Loss tensor([0.1063, 0.0354, 0.1503, 0.0861, 0.0920, 0.0726])\n",
      "\n",
      "\n",
      "************** Batch 232 in 0.5589215755462646 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1047, 0.0383, 0.1459, 0.0800, 0.0897, 0.0706]) \n",
      "Test Loss tensor([0.1041, 0.0344, 0.1479, 0.0850, 0.0896, 0.0728])\n",
      "\n",
      "\n",
      "************** Batch 236 in 0.5586504936218262 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1109, 0.0405, 0.1445, 0.0851, 0.0944, 0.0668]) \n",
      "Test Loss tensor([0.1033, 0.0348, 0.1446, 0.0837, 0.0906, 0.0722])\n",
      "\n",
      "\n",
      "************** Batch 240 in 0.5346086025238037 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1014, 0.0433, 0.1468, 0.0878, 0.0918, 0.0757]) \n",
      "Test Loss tensor([0.1064, 0.0340, 0.1506, 0.0853, 0.0926, 0.0736])\n",
      "\n",
      "\n",
      "************** Batch 244 in 0.5566279888153076 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1073, 0.0368, 0.1357, 0.0836, 0.0957, 0.0722]) \n",
      "Test Loss tensor([0.1050, 0.0368, 0.1481, 0.0843, 0.0921, 0.0711])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 248 in 0.543848991394043 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1035, 0.0419, 0.1493, 0.0880, 0.0943, 0.0748]) \n",
      "Test Loss tensor([0.1032, 0.0358, 0.1489, 0.0864, 0.0911, 0.0718])\n",
      "\n",
      "\n",
      "************** Batch 252 in 0.5525448322296143 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1024, 0.0325, 0.1432, 0.0832, 0.0916, 0.0731]) \n",
      "Test Loss tensor([0.1074, 0.0357, 0.1481, 0.0843, 0.0917, 0.0720])\n",
      "\n",
      "\n",
      "************** Batch 256 in 0.532970666885376 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1065, 0.0373, 0.1515, 0.0853, 0.0922, 0.0694]) \n",
      "Test Loss tensor([0.1066, 0.0356, 0.1464, 0.0846, 0.0897, 0.0713])\n",
      "\n",
      "\n",
      "************** Batch 260 in 0.5534389019012451 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1056, 0.0296, 0.1506, 0.0849, 0.0833, 0.0703]) \n",
      "Test Loss tensor([0.1061, 0.0360, 0.1470, 0.0834, 0.0911, 0.0737])\n",
      "\n",
      "\n",
      "************** Batch 264 in 0.5603148937225342 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1048, 0.0367, 0.1454, 0.0924, 0.0891, 0.0680]) \n",
      "Test Loss tensor([0.1035, 0.0363, 0.1475, 0.0840, 0.0889, 0.0726])\n",
      "\n",
      "\n",
      "************** Batch 268 in 0.542447566986084 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1142, 0.0402, 0.1326, 0.0839, 0.0933, 0.0692]) \n",
      "Test Loss tensor([0.1033, 0.0326, 0.1475, 0.0842, 0.0913, 0.0719])\n",
      "\n",
      "\n",
      "************** Batch 272 in 0.5395352840423584 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1071, 0.0402, 0.1545, 0.0861, 0.0913, 0.0746]) \n",
      "Test Loss tensor([0.1027, 0.0352, 0.1495, 0.0821, 0.0878, 0.0744])\n",
      "\n",
      "\n",
      "************** Batch 276 in 0.5406863689422607 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1096, 0.0370, 0.1516, 0.0772, 0.0951, 0.0664]) \n",
      "Test Loss tensor([0.1047, 0.0356, 0.1451, 0.0843, 0.0910, 0.0709])\n",
      "\n",
      "\n",
      "************** Batch 280 in 0.5574226379394531 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1050, 0.0358, 0.1510, 0.0848, 0.0915, 0.0784]) \n",
      "Test Loss tensor([0.1030, 0.0345, 0.1467, 0.0842, 0.0885, 0.0704])\n",
      "\n",
      "\n",
      "************** Batch 284 in 0.5322511196136475 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0964, 0.0356, 0.1532, 0.0902, 0.0913, 0.0697]) \n",
      "Test Loss tensor([0.1049, 0.0353, 0.1471, 0.0828, 0.0910, 0.0718])\n",
      "\n",
      "\n",
      "************** Batch 288 in 0.5492007732391357 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1019, 0.0342, 0.1542, 0.0882, 0.0921, 0.0714]) \n",
      "Test Loss tensor([0.1031, 0.0361, 0.1454, 0.0834, 0.0902, 0.0715])\n",
      "\n",
      "\n",
      "************** Batch 292 in 0.5418648719787598 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1056, 0.0352, 0.1510, 0.0861, 0.0927, 0.0710]) \n",
      "Test Loss tensor([0.1021, 0.0346, 0.1452, 0.0825, 0.0895, 0.0714])\n",
      "\n",
      "\n",
      "************** Batch 296 in 0.5536820888519287 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1068, 0.0378, 0.1543, 0.0851, 0.0868, 0.0717]) \n",
      "Test Loss tensor([0.1028, 0.0338, 0.1450, 0.0809, 0.0895, 0.0727])\n",
      "\n",
      "\n",
      "************** Batch 300 in 0.5378732681274414 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1075, 0.0374, 0.1416, 0.0814, 0.0929, 0.0753]) \n",
      "Test Loss tensor([0.1041, 0.0352, 0.1479, 0.0835, 0.0909, 0.0707])\n",
      "\n",
      "\n",
      "************** Batch 304 in 0.545464038848877 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1034, 0.0375, 0.1509, 0.0777, 0.0797, 0.0723]) \n",
      "Test Loss tensor([0.1063, 0.0370, 0.1474, 0.0850, 0.0898, 0.0714])\n",
      "\n",
      "\n",
      "************** Batch 308 in 0.5426435470581055 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1021, 0.0310, 0.1464, 0.0845, 0.0953, 0.0685]) \n",
      "Test Loss tensor([0.1039, 0.0359, 0.1469, 0.0819, 0.0899, 0.0718])\n",
      "\n",
      "\n",
      "************** Batch 312 in 0.5403280258178711 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1022, 0.0345, 0.1460, 0.0770, 0.0841, 0.0719]) \n",
      "Test Loss tensor([0.1052, 0.0350, 0.1463, 0.0815, 0.0915, 0.0719])\n",
      "\n",
      "\n",
      "************** Batch 316 in 0.5476391315460205 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1036, 0.0401, 0.1393, 0.0866, 0.0948, 0.0699]) \n",
      "Test Loss tensor([0.1032, 0.0359, 0.1440, 0.0825, 0.0883, 0.0716])\n",
      "\n",
      "\n",
      "************** Batch 320 in 0.5404222011566162 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0988, 0.0408, 0.1400, 0.0889, 0.0872, 0.0694]) \n",
      "Test Loss tensor([0.1022, 0.0369, 0.1459, 0.0793, 0.0871, 0.0738])\n",
      "\n",
      "\n",
      "************** Batch 324 in 0.5772976875305176 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0958, 0.0358, 0.1384, 0.0833, 0.0855, 0.0719]) \n",
      "Test Loss tensor([0.1031, 0.0340, 0.1480, 0.0833, 0.0900, 0.0715])\n",
      "\n",
      "\n",
      "************** Batch 328 in 0.5416533946990967 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1035, 0.0356, 0.1358, 0.0747, 0.0957, 0.0721]) \n",
      "Test Loss tensor([0.1037, 0.0365, 0.1445, 0.0829, 0.0891, 0.0718])\n",
      "\n",
      "\n",
      "************** Batch 332 in 0.5562572479248047 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1066, 0.0332, 0.1435, 0.0763, 0.0870, 0.0677]) \n",
      "Test Loss tensor([0.1040, 0.0363, 0.1448, 0.0850, 0.0884, 0.0698])\n",
      "\n",
      "\n",
      "************** Batch 336 in 0.5364530086517334 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1005, 0.0366, 0.1466, 0.0770, 0.0877, 0.0711]) \n",
      "Test Loss tensor([0.1052, 0.0346, 0.1445, 0.0844, 0.0888, 0.0698])\n",
      "\n",
      "\n",
      "************** Batch 340 in 0.5551154613494873 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1072, 0.0398, 0.1438, 0.0841, 0.0879, 0.0743]) \n",
      "Test Loss tensor([0.1021, 0.0339, 0.1482, 0.0813, 0.0895, 0.0715])\n",
      "\n",
      "\n",
      "************** Batch 344 in 0.540165901184082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1095, 0.0432, 0.1471, 0.0863, 0.0899, 0.0739]) \n",
      "Test Loss tensor([0.1016, 0.0357, 0.1415, 0.0832, 0.0872, 0.0713])\n",
      "\n",
      "\n",
      "************** Batch 348 in 0.5412328243255615 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1044, 0.0345, 0.1482, 0.0845, 0.0856, 0.0709]) \n",
      "Test Loss tensor([0.1048, 0.0355, 0.1450, 0.0860, 0.0872, 0.0687])\n",
      "\n",
      "\n",
      "************** Batch 352 in 0.5580368041992188 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1037, 0.0352, 0.1468, 0.0871, 0.0906, 0.0705]) \n",
      "Test Loss tensor([0.1005, 0.0358, 0.1472, 0.0806, 0.0894, 0.0707])\n",
      "\n",
      "\n",
      "************** Batch 356 in 0.5413527488708496 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1074, 0.0334, 0.1458, 0.0796, 0.0940, 0.0716]) \n",
      "Test Loss tensor([0.1031, 0.0365, 0.1431, 0.0809, 0.0877, 0.0719])\n",
      "\n",
      "\n",
      "************** Batch 360 in 0.547137975692749 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1011, 0.0360, 0.1442, 0.0763, 0.0852, 0.0679]) \n",
      "Test Loss tensor([0.1025, 0.0377, 0.1458, 0.0834, 0.0865, 0.0716])\n",
      "\n",
      "\n",
      "************** Batch 364 in 0.5434877872467041 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1023, 0.0364, 0.1478, 0.0793, 0.0897, 0.0668]) \n",
      "Test Loss tensor([0.1023, 0.0340, 0.1457, 0.0805, 0.0893, 0.0706])\n",
      "\n",
      "\n",
      "************** Batch 368 in 0.5524048805236816 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1051, 0.0362, 0.1444, 0.0869, 0.0931, 0.0705]) \n",
      "Test Loss tensor([0.1046, 0.0369, 0.1437, 0.0812, 0.0888, 0.0734])\n",
      "\n",
      "\n",
      "************** Batch 372 in 0.5342681407928467 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1095, 0.0403, 0.1370, 0.0879, 0.0958, 0.0700]) \n",
      "Test Loss tensor([0.1026, 0.0380, 0.1436, 0.0827, 0.0898, 0.0695])\n",
      "\n",
      "\n",
      "************** Batch 376 in 0.5546557903289795 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1030, 0.0383, 0.1476, 0.0791, 0.0897, 0.0697]) \n",
      "Test Loss tensor([0.1065, 0.0352, 0.1474, 0.0814, 0.0931, 0.0704])\n",
      "\n",
      "\n",
      "************** Batch 380 in 0.5553348064422607 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1082, 0.0329, 0.1549, 0.0876, 0.0948, 0.0674]) \n",
      "Test Loss tensor([0.1020, 0.0364, 0.1465, 0.0822, 0.0861, 0.0714])\n",
      "\n",
      "\n",
      "************** Batch 384 in 0.5511293411254883 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0967, 0.0367, 0.1395, 0.0756, 0.0856, 0.0649]) \n",
      "Test Loss tensor([0.1045, 0.0370, 0.1440, 0.0821, 0.0876, 0.0717])\n",
      "\n",
      "\n",
      "************** Batch 388 in 0.5633392333984375 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1084, 0.0466, 0.1433, 0.0837, 0.0870, 0.0724]) \n",
      "Test Loss tensor([0.1058, 0.0363, 0.1422, 0.0838, 0.0910, 0.0687])\n",
      "\n",
      "\n",
      "************** Batch 392 in 0.5433731079101562 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0983, 0.0376, 0.1388, 0.0852, 0.1008, 0.0691]) \n",
      "Test Loss tensor([0.1009, 0.0373, 0.1438, 0.0800, 0.0890, 0.0695])\n",
      "\n",
      "\n",
      "************** Batch 396 in 0.5723886489868164 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0995, 0.0339, 0.1598, 0.0821, 0.0857, 0.0747]) \n",
      "Test Loss tensor([0.1046, 0.0379, 0.1429, 0.0794, 0.0905, 0.0699])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 400 in 0.5790297985076904 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1028, 0.0371, 0.1428, 0.0819, 0.0890, 0.0738]) \n",
      "Test Loss tensor([0.1046, 0.0367, 0.1441, 0.0821, 0.0867, 0.0692])\n",
      "\n",
      "\n",
      "************** Batch 404 in 0.5677106380462646 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1024, 0.0345, 0.1419, 0.0888, 0.0872, 0.0671]) \n",
      "Test Loss tensor([0.1045, 0.0374, 0.1458, 0.0830, 0.0873, 0.0692])\n",
      "\n",
      "\n",
      "************** Batch 408 in 0.5428388118743896 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0971, 0.0395, 0.1495, 0.0763, 0.0867, 0.0698]) \n",
      "Test Loss tensor([0.1046, 0.0391, 0.1444, 0.0824, 0.0893, 0.0704])\n",
      "\n",
      "\n",
      "************** Batch 412 in 0.5786697864532471 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0983, 0.0393, 0.1442, 0.0871, 0.0864, 0.0640]) \n",
      "Test Loss tensor([0.1016, 0.0362, 0.1444, 0.0804, 0.0886, 0.0687])\n",
      "\n",
      "\n",
      "************** Batch 416 in 0.5399830341339111 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0971, 0.0428, 0.1536, 0.0828, 0.0876, 0.0701]) \n",
      "Test Loss tensor([0.1024, 0.0354, 0.1401, 0.0802, 0.0889, 0.0690])\n",
      "\n",
      "\n",
      "************** Batch 420 in 0.5610244274139404 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0998, 0.0333, 0.1489, 0.0762, 0.0761, 0.0747]) \n",
      "Test Loss tensor([0.1020, 0.0370, 0.1424, 0.0819, 0.0887, 0.0681])\n",
      "\n",
      "\n",
      "************** Batch 424 in 0.5529084205627441 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0983, 0.0344, 0.1350, 0.0763, 0.0886, 0.0662]) \n",
      "Test Loss tensor([0.1039, 0.0370, 0.1393, 0.0815, 0.0877, 0.0684])\n",
      "\n",
      "\n",
      "************** Batch 428 in 0.5457613468170166 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1083, 0.0391, 0.1564, 0.0911, 0.0941, 0.0705]) \n",
      "Test Loss tensor([0.1004, 0.0357, 0.1426, 0.0821, 0.0887, 0.0680])\n",
      "\n",
      "\n",
      "************** Batch 432 in 0.5531797409057617 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1133, 0.0398, 0.1499, 0.0814, 0.0932, 0.0708]) \n",
      "Test Loss tensor([0.1012, 0.0361, 0.1428, 0.0817, 0.0875, 0.0693])\n",
      "\n",
      "\n",
      "************** Batch 436 in 0.5301022529602051 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1021, 0.0377, 0.1476, 0.0836, 0.0880, 0.0665]) \n",
      "Test Loss tensor([0.1036, 0.0384, 0.1399, 0.0811, 0.0880, 0.0686])\n",
      "\n",
      "\n",
      "************** Batch 440 in 0.5670716762542725 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1108, 0.0354, 0.1459, 0.0829, 0.0927, 0.0644]) \n",
      "Test Loss tensor([0.1040, 0.0372, 0.1465, 0.0823, 0.0896, 0.0672])\n",
      "\n",
      "\n",
      "************** Batch 444 in 0.5474686622619629 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1013, 0.0385, 0.1443, 0.0789, 0.0900, 0.0664]) \n",
      "Test Loss tensor([0.1034, 0.0359, 0.1403, 0.0813, 0.0889, 0.0695])\n",
      "\n",
      "\n",
      "************** Batch 448 in 0.5528271198272705 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1003, 0.0371, 0.1476, 0.0881, 0.0861, 0.0656]) \n",
      "Test Loss tensor([0.1032, 0.0380, 0.1410, 0.0810, 0.0860, 0.0694])\n",
      "\n",
      "\n",
      "************** Batch 452 in 0.5378220081329346 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1010, 0.0370, 0.1399, 0.0865, 0.0862, 0.0650]) \n",
      "Test Loss tensor([0.1021, 0.0367, 0.1410, 0.0826, 0.0864, 0.0696])\n",
      "\n",
      "\n",
      "************** Batch 456 in 0.5601949691772461 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0994, 0.0404, 0.1474, 0.0765, 0.0802, 0.0674]) \n",
      "Test Loss tensor([0.1045, 0.0363, 0.1391, 0.0811, 0.0887, 0.0684])\n",
      "\n",
      "\n",
      "************** Batch 460 in 0.5621480941772461 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0998, 0.0390, 0.1462, 0.0796, 0.0911, 0.0688]) \n",
      "Test Loss tensor([0.1028, 0.0370, 0.1394, 0.0822, 0.0871, 0.0690])\n",
      "\n",
      "\n",
      "************** Batch 464 in 0.5409655570983887 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1031, 0.0340, 0.1444, 0.0777, 0.0843, 0.0679]) \n",
      "Test Loss tensor([0.1026, 0.0368, 0.1390, 0.0801, 0.0852, 0.0687])\n",
      "\n",
      "\n",
      "************** Batch 468 in 0.5476090908050537 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0970, 0.0352, 0.1406, 0.0768, 0.0879, 0.0765]) \n",
      "Test Loss tensor([0.1029, 0.0376, 0.1432, 0.0800, 0.0864, 0.0688])\n",
      "\n",
      "\n",
      "************** Batch 472 in 0.55975341796875 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1018, 0.0413, 0.1425, 0.0778, 0.0898, 0.0690]) \n",
      "Test Loss tensor([0.1035, 0.0366, 0.1426, 0.0811, 0.0897, 0.0681])\n",
      "\n",
      "\n",
      "************** Batch 476 in 0.551349401473999 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1087, 0.0354, 0.1438, 0.0768, 0.0883, 0.0696]) \n",
      "Test Loss tensor([0.1001, 0.0377, 0.1408, 0.0800, 0.0853, 0.0700])\n",
      "\n",
      "\n",
      "************** Batch 480 in 0.5445630550384521 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0982, 0.0392, 0.1604, 0.0749, 0.0839, 0.0726]) \n",
      "Test Loss tensor([0.1030, 0.0359, 0.1389, 0.0835, 0.0862, 0.0675])\n",
      "\n",
      "\n",
      "************** Batch 484 in 0.5591161251068115 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0965, 0.0374, 0.1406, 0.0760, 0.0872, 0.0663]) \n",
      "Test Loss tensor([0.1011, 0.0355, 0.1443, 0.0820, 0.0871, 0.0677])\n",
      "\n",
      "\n",
      "************** Batch 488 in 0.5533111095428467 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1085, 0.0353, 0.1486, 0.0807, 0.0871, 0.0719]) \n",
      "Test Loss tensor([0.1003, 0.0362, 0.1421, 0.0782, 0.0840, 0.0699])\n",
      "\n",
      "\n",
      "************** Batch 492 in 0.5973269939422607 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0919, 0.0334, 0.1423, 0.0807, 0.0888, 0.0683]) \n",
      "Test Loss tensor([0.1033, 0.0374, 0.1392, 0.0784, 0.0869, 0.0674])\n",
      "\n",
      "\n",
      "************** Batch 496 in 0.551102876663208 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1068, 0.0411, 0.1339, 0.0873, 0.0801, 0.0726]) \n",
      "Test Loss tensor([0.1021, 0.0378, 0.1414, 0.0803, 0.0860, 0.0695])\n",
      "\n",
      "\n",
      "************** Batch 500 in 0.5645325183868408 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1069, 0.0422, 0.1505, 0.0927, 0.0897, 0.0682]) \n",
      "Test Loss tensor([0.1029, 0.0359, 0.1383, 0.0798, 0.0866, 0.0681])\n",
      "\n",
      "\n",
      "************** Batch 504 in 0.5628321170806885 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1080, 0.0385, 0.1490, 0.0845, 0.0878, 0.0661]) \n",
      "Test Loss tensor([0.1030, 0.0375, 0.1437, 0.0817, 0.0860, 0.0705])\n",
      "\n",
      "\n",
      "************** Batch 508 in 0.5412516593933105 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1058, 0.0389, 0.1495, 0.0874, 0.0849, 0.0689]) \n",
      "Test Loss tensor([0.1035, 0.0392, 0.1412, 0.0812, 0.0867, 0.0677])\n",
      "\n",
      "\n",
      "************** Batch 512 in 0.553997278213501 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1037, 0.0405, 0.1359, 0.0809, 0.0849, 0.0688]) \n",
      "Test Loss tensor([0.1058, 0.0405, 0.1411, 0.0816, 0.0869, 0.0694])\n",
      "\n",
      "\n",
      "************** Batch 516 in 0.5526330471038818 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0988, 0.0341, 0.1588, 0.0811, 0.0830, 0.0652]) \n",
      "Test Loss tensor([0.1012, 0.0373, 0.1408, 0.0817, 0.0858, 0.0655])\n",
      "\n",
      "\n",
      "************** Batch 520 in 0.5657970905303955 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1045, 0.0348, 0.1399, 0.0846, 0.0889, 0.0644]) \n",
      "Test Loss tensor([0.1025, 0.0366, 0.1408, 0.0791, 0.0932, 0.0679])\n",
      "\n",
      "\n",
      "************** Batch 524 in 0.5635592937469482 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1019, 0.0355, 0.1494, 0.0896, 0.0992, 0.0664]) \n",
      "Test Loss tensor([0.1035, 0.0395, 0.1363, 0.0795, 0.0883, 0.0685])\n",
      "\n",
      "\n",
      "************** Batch 528 in 0.5681183338165283 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0989, 0.0394, 0.1301, 0.0824, 0.0841, 0.0707]) \n",
      "Test Loss tensor([0.1002, 0.0379, 0.1373, 0.0796, 0.0864, 0.0672])\n",
      "\n",
      "\n",
      "************** Batch 532 in 0.5697088241577148 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1050, 0.0371, 0.1351, 0.0831, 0.0872, 0.0669]) \n",
      "Test Loss tensor([0.1042, 0.0385, 0.1406, 0.0805, 0.0894, 0.0688])\n",
      "\n",
      "\n",
      "************** Batch 536 in 0.5460219383239746 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1082, 0.0470, 0.1455, 0.0737, 0.0875, 0.0729]) \n",
      "Test Loss tensor([0.0996, 0.0383, 0.1391, 0.0807, 0.0860, 0.0667])\n",
      "\n",
      "\n",
      "************** Batch 540 in 0.561063289642334 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1027, 0.0379, 0.1294, 0.0803, 0.0875, 0.0621]) \n",
      "Test Loss tensor([0.1027, 0.0405, 0.1399, 0.0778, 0.0866, 0.0682])\n",
      "\n",
      "\n",
      "************** Batch 544 in 0.5450875759124756 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1005, 0.0337, 0.1303, 0.0823, 0.0856, 0.0650]) \n",
      "Test Loss tensor([0.1002, 0.0384, 0.1393, 0.0810, 0.0858, 0.0668])\n",
      "\n",
      "\n",
      "************** Batch 548 in 0.5683183670043945 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0951, 0.0377, 0.1450, 0.0729, 0.0807, 0.0725]) \n",
      "Test Loss tensor([0.1023, 0.0366, 0.1418, 0.0811, 0.0874, 0.0675])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 552 in 0.5435571670532227 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1077, 0.0407, 0.1507, 0.0797, 0.0856, 0.0713]) \n",
      "Test Loss tensor([0.1020, 0.0385, 0.1391, 0.0827, 0.0881, 0.0669])\n",
      "\n",
      "\n",
      "************** Batch 556 in 0.5624282360076904 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1014, 0.0385, 0.1340, 0.0809, 0.0910, 0.0659]) \n",
      "Test Loss tensor([0.0986, 0.0378, 0.1399, 0.0794, 0.0851, 0.0676])\n",
      "\n",
      "\n",
      "************** Batch 560 in 0.564854621887207 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1016, 0.0362, 0.1428, 0.0826, 0.0845, 0.0622]) \n",
      "Test Loss tensor([0.1009, 0.0395, 0.1403, 0.0811, 0.0874, 0.0669])\n",
      "\n",
      "\n",
      "************** Batch 564 in 0.5532312393188477 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1043, 0.0408, 0.1444, 0.0816, 0.0924, 0.0646]) \n",
      "Test Loss tensor([0.1013, 0.0383, 0.1367, 0.0766, 0.0867, 0.0666])\n",
      "\n",
      "\n",
      "************** Batch 568 in 0.5657095909118652 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1001, 0.0354, 0.1383, 0.0862, 0.0899, 0.0706]) \n",
      "Test Loss tensor([0.1023, 0.0407, 0.1365, 0.0799, 0.0874, 0.0674])\n",
      "\n",
      "\n",
      "************** Batch 572 in 0.5466866493225098 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1058, 0.0446, 0.1316, 0.0835, 0.0876, 0.0618]) \n",
      "Test Loss tensor([0.1018, 0.0394, 0.1382, 0.0798, 0.0883, 0.0652])\n",
      "\n",
      "\n",
      "************** Batch 576 in 0.5534093379974365 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1017, 0.0382, 0.1271, 0.0807, 0.0904, 0.0656]) \n",
      "Test Loss tensor([0.0987, 0.0384, 0.1389, 0.0802, 0.0858, 0.0672])\n",
      "\n",
      "\n",
      "************** Batch 580 in 0.5597476959228516 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1056, 0.0396, 0.1438, 0.0783, 0.0884, 0.0678]) \n",
      "Test Loss tensor([0.1022, 0.0389, 0.1374, 0.0834, 0.0880, 0.0696])\n",
      "\n",
      "\n",
      "************** Batch 584 in 0.5658483505249023 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1039, 0.0410, 0.1349, 0.0811, 0.0927, 0.0641]) \n",
      "Test Loss tensor([0.1007, 0.0391, 0.1378, 0.0793, 0.0868, 0.0661])\n",
      "\n",
      "\n",
      "************** Batch 588 in 0.5726933479309082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0965, 0.0368, 0.1366, 0.0791, 0.0886, 0.0654]) \n",
      "Test Loss tensor([0.1019, 0.0404, 0.1391, 0.0782, 0.0888, 0.0674])\n",
      "\n",
      "\n",
      "************** Batch 592 in 0.5711414813995361 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1083, 0.0373, 0.1288, 0.0843, 0.0863, 0.0684]) \n",
      "Test Loss tensor([0.0993, 0.0397, 0.1341, 0.0803, 0.0871, 0.0689])\n",
      "\n",
      "\n",
      "************** Batch 596 in 0.547476053237915 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1057, 0.0399, 0.1380, 0.0800, 0.0843, 0.0727]) \n",
      "Test Loss tensor([0.0997, 0.0405, 0.1344, 0.0761, 0.0867, 0.0673])\n",
      "\n",
      "\n",
      "************** Batch 600 in 0.5428962707519531 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1004, 0.0395, 0.1313, 0.0764, 0.0855, 0.0658]) \n",
      "Test Loss tensor([0.1015, 0.0400, 0.1371, 0.0790, 0.0878, 0.0648])\n",
      "\n",
      "\n",
      "************** Batch 604 in 0.5561437606811523 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1075, 0.0384, 0.1301, 0.0811, 0.0814, 0.0593]) \n",
      "Test Loss tensor([0.1006, 0.0421, 0.1343, 0.0772, 0.0833, 0.0669])\n",
      "\n",
      "\n",
      "************** Batch 608 in 0.547950029373169 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0959, 0.0419, 0.1339, 0.0751, 0.0889, 0.0607]) \n",
      "Test Loss tensor([0.1011, 0.0410, 0.1340, 0.0765, 0.0829, 0.0656])\n",
      "\n",
      "\n",
      "************** Batch 612 in 0.552809476852417 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0980, 0.0451, 0.1356, 0.0811, 0.0880, 0.0635]) \n",
      "Test Loss tensor([0.1039, 0.0413, 0.1345, 0.0776, 0.0836, 0.0664])\n",
      "\n",
      "\n",
      "************** Batch 616 in 0.5468218326568604 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1025, 0.0414, 0.1350, 0.0750, 0.0893, 0.0636]) \n",
      "Test Loss tensor([0.1010, 0.0404, 0.1351, 0.0795, 0.0842, 0.0654])\n",
      "\n",
      "\n",
      "************** Batch 620 in 0.5611796379089355 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0859, 0.0416, 0.1484, 0.0847, 0.0866, 0.0689]) \n",
      "Test Loss tensor([0.0998, 0.0418, 0.1345, 0.0804, 0.0847, 0.0673])\n",
      "\n",
      "\n",
      "************** Batch 624 in 0.5533003807067871 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0992, 0.0384, 0.1357, 0.0775, 0.0868, 0.0723]) \n",
      "Test Loss tensor([0.0980, 0.0403, 0.1316, 0.0793, 0.0837, 0.0666])\n",
      "\n",
      "\n",
      "************** Batch 628 in 0.5531134605407715 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1004, 0.0414, 0.1264, 0.0742, 0.0875, 0.0672]) \n",
      "Test Loss tensor([0.0988, 0.0405, 0.1317, 0.0778, 0.0861, 0.0650])\n",
      "\n",
      "\n",
      "************** Batch 632 in 0.556614875793457 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0963, 0.0370, 0.1276, 0.0840, 0.0825, 0.0695]) \n",
      "Test Loss tensor([0.0993, 0.0420, 0.1360, 0.0796, 0.0858, 0.0660])\n",
      "\n",
      "\n",
      "************** Batch 636 in 0.5750522613525391 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1038, 0.0401, 0.1374, 0.0849, 0.0886, 0.0649]) \n",
      "Test Loss tensor([0.1016, 0.0425, 0.1374, 0.0782, 0.0854, 0.0659])\n",
      "\n",
      "\n",
      "************** Batch 640 in 0.55503249168396 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1010, 0.0438, 0.1387, 0.0792, 0.0881, 0.0696]) \n",
      "Test Loss tensor([0.1014, 0.0420, 0.1351, 0.0776, 0.0855, 0.0667])\n",
      "\n",
      "\n",
      "************** Batch 644 in 0.5548961162567139 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0961, 0.0416, 0.1303, 0.0753, 0.0824, 0.0699]) \n",
      "Test Loss tensor([0.0994, 0.0404, 0.1325, 0.0793, 0.0864, 0.0636])\n",
      "\n",
      "\n",
      "************** Batch 648 in 0.5668160915374756 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1027, 0.0488, 0.1402, 0.0820, 0.0815, 0.0684]) \n",
      "Test Loss tensor([0.1008, 0.0424, 0.1349, 0.0768, 0.0847, 0.0666])\n",
      "\n",
      "\n",
      "************** Batch 652 in 0.5503122806549072 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1057, 0.0447, 0.1408, 0.0746, 0.0875, 0.0659]) \n",
      "Test Loss tensor([0.1007, 0.0414, 0.1298, 0.0784, 0.0850, 0.0638])\n",
      "\n",
      "\n",
      "************** Batch 656 in 0.5598747730255127 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1028, 0.0398, 0.1308, 0.0804, 0.0886, 0.0632]) \n",
      "Test Loss tensor([0.0986, 0.0408, 0.1316, 0.0782, 0.0869, 0.0637])\n",
      "\n",
      "\n",
      "************** Batch 660 in 0.543546199798584 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1023, 0.0492, 0.1310, 0.0737, 0.0825, 0.0607]) \n",
      "Test Loss tensor([0.1012, 0.0415, 0.1322, 0.0792, 0.0855, 0.0646])\n",
      "\n",
      "\n",
      "************** Batch 664 in 0.5563545227050781 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1001, 0.0429, 0.1278, 0.0728, 0.0911, 0.0582]) \n",
      "Test Loss tensor([0.0998, 0.0410, 0.1332, 0.0760, 0.0859, 0.0650])\n",
      "\n",
      "\n",
      "************** Batch 668 in 0.5599050521850586 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0976, 0.0483, 0.1283, 0.0735, 0.0859, 0.0650]) \n",
      "Test Loss tensor([0.1013, 0.0430, 0.1356, 0.0807, 0.0857, 0.0646])\n",
      "\n",
      "\n",
      "************** Batch 672 in 0.5517623424530029 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1001, 0.0425, 0.1230, 0.0841, 0.0888, 0.0668]) \n",
      "Test Loss tensor([0.1017, 0.0444, 0.1317, 0.0792, 0.0871, 0.0658])\n",
      "\n",
      "\n",
      "************** Batch 676 in 0.570347785949707 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0995, 0.0432, 0.1404, 0.0706, 0.0895, 0.0687]) \n",
      "Test Loss tensor([0.0972, 0.0411, 0.1331, 0.0776, 0.0872, 0.0658])\n",
      "\n",
      "\n",
      "************** Batch 680 in 0.5468189716339111 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0932, 0.0381, 0.1450, 0.0714, 0.0867, 0.0658]) \n",
      "Test Loss tensor([0.0994, 0.0423, 0.1294, 0.0765, 0.0879, 0.0662])\n",
      "\n",
      "\n",
      "************** Batch 684 in 0.5678644180297852 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1017, 0.0409, 0.1307, 0.0815, 0.0856, 0.0633]) \n",
      "Test Loss tensor([0.1004, 0.0434, 0.1336, 0.0809, 0.0858, 0.0650])\n",
      "\n",
      "\n",
      "************** Batch 688 in 0.5526254177093506 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0924, 0.0401, 0.1256, 0.0749, 0.0814, 0.0633]) \n",
      "Test Loss tensor([0.1028, 0.0427, 0.1319, 0.0776, 0.0871, 0.0644])\n",
      "\n",
      "\n",
      "************** Batch 692 in 0.5625865459442139 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1096, 0.0443, 0.1316, 0.0748, 0.0866, 0.0668]) \n",
      "Test Loss tensor([0.0999, 0.0426, 0.1299, 0.0798, 0.0872, 0.0647])\n",
      "\n",
      "\n",
      "************** Batch 696 in 0.5714106559753418 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1055, 0.0435, 0.1291, 0.0833, 0.0866, 0.0681]) \n",
      "Test Loss tensor([0.1003, 0.0431, 0.1308, 0.0783, 0.0839, 0.0640])\n",
      "\n",
      "\n",
      "************** Batch 700 in 0.5729415416717529 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0899, 0.0421, 0.1376, 0.0814, 0.0863, 0.0643]) \n",
      "Test Loss tensor([0.0987, 0.0393, 0.1302, 0.0768, 0.0891, 0.0646])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 704 in 0.5710694789886475 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1025, 0.0386, 0.1380, 0.0825, 0.0918, 0.0683]) \n",
      "Test Loss tensor([0.0987, 0.0420, 0.1310, 0.0784, 0.0857, 0.0645])\n",
      "\n",
      "\n",
      "************** Batch 708 in 0.5640325546264648 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0939, 0.0396, 0.1380, 0.0799, 0.0834, 0.0638]) \n",
      "Test Loss tensor([0.1001, 0.0429, 0.1296, 0.0771, 0.0861, 0.0669])\n",
      "\n",
      "\n",
      "************** Batch 712 in 0.570669412612915 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0993, 0.0440, 0.1260, 0.0741, 0.0815, 0.0689]) \n",
      "Test Loss tensor([0.0959, 0.0407, 0.1283, 0.0756, 0.0857, 0.0639])\n",
      "\n",
      "\n",
      "************** Batch 716 in 0.5609221458435059 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1041, 0.0426, 0.1249, 0.0787, 0.0846, 0.0631]) \n",
      "Test Loss tensor([0.0977, 0.0414, 0.1263, 0.0756, 0.0876, 0.0634])\n",
      "\n",
      "\n",
      "************** Batch 720 in 0.5668466091156006 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0980, 0.0420, 0.1235, 0.0771, 0.0799, 0.0617]) \n",
      "Test Loss tensor([0.0990, 0.0435, 0.1263, 0.0798, 0.0868, 0.0641])\n",
      "\n",
      "\n",
      "************** Batch 724 in 0.5663130283355713 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1088, 0.0438, 0.1306, 0.0706, 0.0909, 0.0659]) \n",
      "Test Loss tensor([0.0991, 0.0424, 0.1274, 0.0793, 0.0847, 0.0629])\n",
      "\n",
      "\n",
      "************** Batch 728 in 0.5594894886016846 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0934, 0.0466, 0.1276, 0.0768, 0.0866, 0.0647]) \n",
      "Test Loss tensor([0.1015, 0.0407, 0.1275, 0.0766, 0.0896, 0.0646])\n",
      "\n",
      "\n",
      "************** Batch 732 in 0.5651836395263672 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1043, 0.0413, 0.1283, 0.0805, 0.0819, 0.0685]) \n",
      "Test Loss tensor([0.0975, 0.0438, 0.1306, 0.0780, 0.0856, 0.0640])\n",
      "\n",
      "\n",
      "************** Batch 736 in 0.5431923866271973 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1035, 0.0431, 0.1283, 0.0771, 0.0851, 0.0659]) \n",
      "Test Loss tensor([0.0972, 0.0428, 0.1293, 0.0756, 0.0866, 0.0663])\n",
      "\n",
      "\n",
      "************** Batch 740 in 0.5710544586181641 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0999, 0.0355, 0.1256, 0.0755, 0.0876, 0.0660]) \n",
      "Test Loss tensor([0.0995, 0.0385, 0.1281, 0.0738, 0.0866, 0.0640])\n",
      "\n",
      "\n",
      "************** Batch 744 in 0.5517890453338623 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1043, 0.0463, 0.1244, 0.0761, 0.0795, 0.0623]) \n",
      "Test Loss tensor([0.0997, 0.0407, 0.1290, 0.0774, 0.0871, 0.0634])\n",
      "\n",
      "\n",
      "************** Batch 748 in 0.5672779083251953 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0995, 0.0449, 0.1338, 0.0791, 0.0878, 0.0652]) \n",
      "Test Loss tensor([0.0997, 0.0415, 0.1257, 0.0757, 0.0853, 0.0681])\n",
      "\n",
      "\n",
      "************** Batch 752 in 0.5608727931976318 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1004, 0.0440, 0.1348, 0.0756, 0.0848, 0.0654]) \n",
      "Test Loss tensor([0.0977, 0.0428, 0.1281, 0.0739, 0.0860, 0.0640])\n",
      "\n",
      "\n",
      "************** Batch 756 in 0.5589556694030762 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1027, 0.0461, 0.1397, 0.0809, 0.0917, 0.0619]) \n",
      "Test Loss tensor([0.0966, 0.0424, 0.1248, 0.0748, 0.0856, 0.0633])\n",
      "\n",
      "\n",
      "************** Batch 760 in 0.711219072341919 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1008, 0.0393, 0.1309, 0.0786, 0.0893, 0.0629]) \n",
      "Test Loss tensor([0.0991, 0.0435, 0.1254, 0.0773, 0.0885, 0.0664])\n",
      "\n",
      "\n",
      "************** Batch 764 in 0.7051413059234619 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1065, 0.0477, 0.1288, 0.0716, 0.0843, 0.0649]) \n",
      "Test Loss tensor([0.0994, 0.0430, 0.1265, 0.0773, 0.0865, 0.0629])\n",
      "\n",
      "\n",
      "************** Batch 768 in 0.7075026035308838 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1013, 0.0447, 0.1333, 0.0738, 0.0834, 0.0667]) \n",
      "Test Loss tensor([0.0988, 0.0404, 0.1292, 0.0745, 0.0869, 0.0644])\n",
      "\n",
      "\n",
      "************** Batch 772 in 0.7231345176696777 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1109, 0.0513, 0.1257, 0.0820, 0.0967, 0.0601]) \n",
      "Test Loss tensor([0.0977, 0.0413, 0.1276, 0.0757, 0.0851, 0.0642])\n",
      "\n",
      "\n",
      "************** Batch 776 in 0.7122635841369629 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1075, 0.0491, 0.1220, 0.0733, 0.0852, 0.0644]) \n",
      "Test Loss tensor([0.0985, 0.0428, 0.1278, 0.0749, 0.0858, 0.0628])\n",
      "\n",
      "\n",
      "************** Batch 780 in 0.7507288455963135 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0940, 0.0388, 0.1285, 0.0757, 0.0853, 0.0600]) \n",
      "Test Loss tensor([0.0984, 0.0417, 0.1261, 0.0748, 0.0841, 0.0637])\n",
      "\n",
      "\n",
      "************** Batch 784 in 0.6877737045288086 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1022, 0.0423, 0.1255, 0.0716, 0.0817, 0.0728]) \n",
      "Test Loss tensor([0.0980, 0.0429, 0.1310, 0.0756, 0.0856, 0.0629])\n",
      "\n",
      "\n",
      "************** Batch 788 in 0.6521217823028564 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0978, 0.0435, 0.1308, 0.0753, 0.0907, 0.0610]) \n",
      "Test Loss tensor([0.1015, 0.0421, 0.1243, 0.0746, 0.0873, 0.0652])\n",
      "\n",
      "\n",
      "************** Batch 792 in 0.7210571765899658 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0958, 0.0386, 0.1300, 0.0751, 0.0871, 0.0644]) \n",
      "Test Loss tensor([0.0990, 0.0399, 0.1271, 0.0754, 0.0849, 0.0615])\n",
      "\n",
      "\n",
      "************** Batch 796 in 0.6865737438201904 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0977, 0.0401, 0.1348, 0.0748, 0.0817, 0.0695]) \n",
      "Test Loss tensor([0.0984, 0.0409, 0.1305, 0.0767, 0.0897, 0.0622])\n",
      "\n",
      "\n",
      "************** Batch 800 in 0.6227054595947266 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0978, 0.0438, 0.1314, 0.0784, 0.0900, 0.0599]) \n",
      "Test Loss tensor([0.0990, 0.0423, 0.1248, 0.0750, 0.0866, 0.0630])\n",
      "\n",
      "\n",
      "************** Batch 804 in 0.5845112800598145 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0984, 0.0386, 0.1236, 0.0801, 0.0869, 0.0600]) \n",
      "Test Loss tensor([0.0985, 0.0398, 0.1288, 0.0757, 0.0827, 0.0643])\n",
      "\n",
      "\n",
      "************** Batch 808 in 0.6070172786712646 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0932, 0.0418, 0.1249, 0.0773, 0.0780, 0.0660]) \n",
      "Test Loss tensor([0.1014, 0.0416, 0.1303, 0.0747, 0.0908, 0.0625])\n",
      "\n",
      "\n",
      "************** Batch 812 in 0.5867369174957275 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1063, 0.0387, 0.1287, 0.0716, 0.0919, 0.0656]) \n",
      "Test Loss tensor([0.0970, 0.0406, 0.1266, 0.0753, 0.0862, 0.0629])\n",
      "\n",
      "\n",
      "************** Batch 816 in 0.6721265316009521 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0975, 0.0435, 0.1240, 0.0800, 0.0844, 0.0656]) \n",
      "Test Loss tensor([0.0980, 0.0419, 0.1222, 0.0720, 0.0866, 0.0636])\n",
      "\n",
      "\n",
      "************** Batch 820 in 0.5777790546417236 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1000, 0.0425, 0.1176, 0.0727, 0.0815, 0.0597]) \n",
      "Test Loss tensor([0.1018, 0.0403, 0.1271, 0.0733, 0.0878, 0.0637])\n",
      "\n",
      "\n",
      "************** Batch 824 in 0.6004636287689209 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1067, 0.0433, 0.1333, 0.0738, 0.0858, 0.0687]) \n",
      "Test Loss tensor([0.0972, 0.0414, 0.1269, 0.0731, 0.0853, 0.0634])\n",
      "\n",
      "\n",
      "************** Batch 828 in 0.6764395236968994 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0969, 0.0347, 0.1260, 0.0759, 0.0787, 0.0665]) \n",
      "Test Loss tensor([0.0975, 0.0420, 0.1269, 0.0765, 0.0859, 0.0623])\n",
      "\n",
      "\n",
      "************** Batch 832 in 0.8476979732513428 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0368, 0.1273, 0.0734, 0.0878, 0.0616]) \n",
      "Test Loss tensor([0.0997, 0.0400, 0.1251, 0.0715, 0.0875, 0.0637])\n",
      "\n",
      "\n",
      "************** Batch 836 in 0.7515277862548828 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1020, 0.0390, 0.1227, 0.0763, 0.0862, 0.0680]) \n",
      "Test Loss tensor([0.0976, 0.0404, 0.1267, 0.0731, 0.0849, 0.0617])\n",
      "\n",
      "\n",
      "************** Batch 840 in 0.6645786762237549 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1039, 0.0403, 0.1323, 0.0765, 0.0858, 0.0672]) \n",
      "Test Loss tensor([0.1012, 0.0415, 0.1248, 0.0736, 0.0866, 0.0630])\n",
      "\n",
      "\n",
      "************** Batch 844 in 0.6152098178863525 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0904, 0.0462, 0.1271, 0.0769, 0.0893, 0.0653]) \n",
      "Test Loss tensor([0.0988, 0.0411, 0.1259, 0.0723, 0.0843, 0.0628])\n",
      "\n",
      "\n",
      "************** Batch 848 in 0.4874916076660156 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0950, 0.0392, 0.1183, 0.0665, 0.0902, 0.0624]) \n",
      "Test Loss tensor([0.1036, 0.0404, 0.1261, 0.0760, 0.0887, 0.0614])\n",
      "\n",
      "\n",
      "************** Batch 852 in 0.630063533782959 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1028, 0.0418, 0.1323, 0.0692, 0.0915, 0.0630]) \n",
      "Test Loss tensor([0.0986, 0.0418, 0.1267, 0.0759, 0.0878, 0.0633])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 856 in 0.5696451663970947 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0991, 0.0445, 0.1204, 0.0707, 0.0858, 0.0583]) \n",
      "Test Loss tensor([0.0993, 0.0423, 0.1245, 0.0759, 0.0857, 0.0624])\n",
      "\n",
      "\n",
      "************** Batch 860 in 0.7329177856445312 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0985, 0.0392, 0.1286, 0.0729, 0.0813, 0.0607]) \n",
      "Test Loss tensor([0.1015, 0.0410, 0.1248, 0.0726, 0.0883, 0.0645])\n",
      "\n",
      "\n",
      "************** Batch 864 in 0.5893726348876953 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1025, 0.0431, 0.1148, 0.0774, 0.0860, 0.0614]) \n",
      "Test Loss tensor([0.0990, 0.0424, 0.1258, 0.0744, 0.0853, 0.0619])\n",
      "\n",
      "\n",
      "************** Batch 868 in 0.5485644340515137 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0993, 0.0441, 0.1266, 0.0659, 0.0844, 0.0650]) \n",
      "Test Loss tensor([0.1013, 0.0407, 0.1240, 0.0746, 0.0862, 0.0623])\n",
      "\n",
      "\n",
      "************** Batch 872 in 0.5763258934020996 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1023, 0.0497, 0.1298, 0.0761, 0.0882, 0.0697]) \n",
      "Test Loss tensor([0.0967, 0.0409, 0.1259, 0.0739, 0.0860, 0.0623])\n",
      "\n",
      "\n",
      "************** Batch 876 in 0.5927212238311768 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0773, 0.0357, 0.0936, 0.0525, 0.0636, 0.0431]) \n",
      "Test Loss tensor([0.0990, 0.0399, 0.1262, 0.0745, 0.0887, 0.0646])\n",
      "\n",
      "\n",
      "************** Batch 0 in 0.6515407562255859 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0907, 0.0432, 0.1260, 0.0833, 0.0872, 0.0624]) \n",
      "Test Loss tensor([0.0979, 0.0428, 0.1246, 0.0744, 0.0868, 0.0626])\n",
      "\n",
      "\n",
      "************** Batch 4 in 0.609550952911377 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1063, 0.0442, 0.1156, 0.0726, 0.0825, 0.0664]) \n",
      "Test Loss tensor([0.0945, 0.0420, 0.1229, 0.0721, 0.0851, 0.0618])\n",
      "\n",
      "\n",
      "************** Batch 8 in 0.7070722579956055 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1099, 0.0431, 0.1248, 0.0771, 0.0842, 0.0607]) \n",
      "Test Loss tensor([0.1038, 0.0417, 0.1249, 0.0733, 0.0891, 0.0602])\n",
      "\n",
      "\n",
      "************** Batch 12 in 0.6572110652923584 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0998, 0.0417, 0.1366, 0.0706, 0.0849, 0.0613]) \n",
      "Test Loss tensor([0.0983, 0.0412, 0.1235, 0.0728, 0.0865, 0.0631])\n",
      "\n",
      "\n",
      "************** Batch 16 in 0.6381728649139404 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0874, 0.0350, 0.1318, 0.0684, 0.0818, 0.0634]) \n",
      "Test Loss tensor([0.0976, 0.0411, 0.1209, 0.0728, 0.0900, 0.0663])\n",
      "\n",
      "\n",
      "************** Batch 20 in 0.6240229606628418 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1004, 0.0419, 0.1238, 0.0729, 0.0881, 0.0679]) \n",
      "Test Loss tensor([0.0972, 0.0391, 0.1250, 0.0728, 0.0894, 0.0589])\n",
      "\n",
      "\n",
      "************** Batch 24 in 0.6208767890930176 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0983, 0.0443, 0.1382, 0.0793, 0.0858, 0.0554]) \n",
      "Test Loss tensor([0.1017, 0.0415, 0.1268, 0.0726, 0.0894, 0.0609])\n",
      "\n",
      "\n",
      "************** Batch 28 in 0.7136070728302002 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0969, 0.0468, 0.1304, 0.0735, 0.0891, 0.0629]) \n",
      "Test Loss tensor([0.0988, 0.0421, 0.1231, 0.0734, 0.0866, 0.0620])\n",
      "\n",
      "\n",
      "************** Batch 32 in 0.6326613426208496 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0885, 0.0405, 0.1247, 0.0742, 0.0800, 0.0645]) \n",
      "Test Loss tensor([0.0966, 0.0416, 0.1192, 0.0719, 0.0857, 0.0602])\n",
      "\n",
      "\n",
      "************** Batch 36 in 0.6671063899993896 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1040, 0.0414, 0.1357, 0.0747, 0.0871, 0.0626]) \n",
      "Test Loss tensor([0.0952, 0.0405, 0.1203, 0.0738, 0.0876, 0.0619])\n",
      "\n",
      "\n",
      "************** Batch 40 in 0.6594300270080566 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0943, 0.0399, 0.1238, 0.0757, 0.0905, 0.0634]) \n",
      "Test Loss tensor([0.0959, 0.0419, 0.1247, 0.0728, 0.0851, 0.0604])\n",
      "\n",
      "\n",
      "************** Batch 44 in 0.6025686264038086 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0995, 0.0468, 0.1291, 0.0791, 0.0872, 0.0555]) \n",
      "Test Loss tensor([0.0952, 0.0422, 0.1220, 0.0714, 0.0861, 0.0616])\n",
      "\n",
      "\n",
      "************** Batch 48 in 0.5946593284606934 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0993, 0.0430, 0.1310, 0.0761, 0.0877, 0.0633]) \n",
      "Test Loss tensor([0.0955, 0.0402, 0.1204, 0.0713, 0.0837, 0.0614])\n",
      "\n",
      "\n",
      "************** Batch 52 in 0.5931282043457031 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1035, 0.0402, 0.1201, 0.0708, 0.0865, 0.0605]) \n",
      "Test Loss tensor([0.0953, 0.0395, 0.1210, 0.0722, 0.0846, 0.0598])\n",
      "\n",
      "\n",
      "************** Batch 56 in 0.6251459121704102 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0987, 0.0342, 0.1257, 0.0828, 0.0790, 0.0593]) \n",
      "Test Loss tensor([0.0994, 0.0418, 0.1246, 0.0750, 0.0864, 0.0593])\n",
      "\n",
      "\n",
      "************** Batch 60 in 0.7419559955596924 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1007, 0.0403, 0.1215, 0.0779, 0.0837, 0.0641]) \n",
      "Test Loss tensor([0.0973, 0.0419, 0.1219, 0.0731, 0.0843, 0.0583])\n",
      "\n",
      "\n",
      "************** Batch 64 in 0.6838593482971191 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0978, 0.0416, 0.1234, 0.0763, 0.0842, 0.0651]) \n",
      "Test Loss tensor([0.0982, 0.0393, 0.1229, 0.0710, 0.0888, 0.0603])\n",
      "\n",
      "\n",
      "************** Batch 68 in 0.6730403900146484 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0985, 0.0331, 0.1304, 0.0690, 0.0855, 0.0622]) \n",
      "Test Loss tensor([0.0971, 0.0412, 0.1251, 0.0734, 0.0838, 0.0611])\n",
      "\n",
      "\n",
      "************** Batch 72 in 0.6769521236419678 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0944, 0.0387, 0.1275, 0.0743, 0.0830, 0.0568]) \n",
      "Test Loss tensor([0.0944, 0.0366, 0.1238, 0.0725, 0.0833, 0.0608])\n",
      "\n",
      "\n",
      "************** Batch 76 in 0.6422779560089111 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0967, 0.0400, 0.1139, 0.0800, 0.0811, 0.0601]) \n",
      "Test Loss tensor([0.0975, 0.0371, 0.1221, 0.0733, 0.0871, 0.0590])\n",
      "\n",
      "\n",
      "************** Batch 80 in 0.6386501789093018 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0867, 0.0400, 0.1291, 0.0701, 0.0867, 0.0594]) \n",
      "Test Loss tensor([0.0955, 0.0392, 0.1219, 0.0721, 0.0837, 0.0611])\n",
      "\n",
      "\n",
      "************** Batch 84 in 0.6311190128326416 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0422, 0.1222, 0.0642, 0.0814, 0.0578]) \n",
      "Test Loss tensor([0.0953, 0.0391, 0.1248, 0.0724, 0.0859, 0.0611])\n",
      "\n",
      "\n",
      "************** Batch 88 in 0.6556179523468018 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0990, 0.0381, 0.1212, 0.0723, 0.0865, 0.0645]) \n",
      "Test Loss tensor([0.0969, 0.0377, 0.1242, 0.0725, 0.0842, 0.0598])\n",
      "\n",
      "\n",
      "************** Batch 92 in 0.6364998817443848 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0952, 0.0373, 0.1165, 0.0671, 0.0836, 0.0590]) \n",
      "Test Loss tensor([0.0937, 0.0360, 0.1228, 0.0708, 0.0849, 0.0597])\n",
      "\n",
      "\n",
      "************** Batch 96 in 0.6857383251190186 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0969, 0.0351, 0.1333, 0.0643, 0.0782, 0.0597]) \n",
      "Test Loss tensor([0.0976, 0.0378, 0.1228, 0.0740, 0.0849, 0.0567])\n",
      "\n",
      "\n",
      "************** Batch 100 in 0.6320128440856934 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0941, 0.0371, 0.1321, 0.0665, 0.0820, 0.0612]) \n",
      "Test Loss tensor([0.0977, 0.0376, 0.1219, 0.0714, 0.0847, 0.0600])\n",
      "\n",
      "\n",
      "************** Batch 104 in 0.6702535152435303 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1023, 0.0396, 0.1189, 0.0762, 0.0807, 0.0572]) \n",
      "Test Loss tensor([0.0951, 0.0361, 0.1244, 0.0719, 0.0886, 0.0603])\n",
      "\n",
      "\n",
      "************** Batch 108 in 0.6303701400756836 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0948, 0.0320, 0.1169, 0.0811, 0.0896, 0.0560]) \n",
      "Test Loss tensor([0.0963, 0.0368, 0.1237, 0.0750, 0.0849, 0.0578])\n",
      "\n",
      "\n",
      "************** Batch 112 in 0.6149446964263916 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1075, 0.0402, 0.1270, 0.0822, 0.0876, 0.0628]) \n",
      "Test Loss tensor([0.0966, 0.0366, 0.1231, 0.0733, 0.0844, 0.0590])\n",
      "\n",
      "\n",
      "************** Batch 116 in 0.6323111057281494 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0924, 0.0358, 0.1259, 0.0750, 0.0831, 0.0594]) \n",
      "Test Loss tensor([0.0986, 0.0344, 0.1231, 0.0717, 0.0850, 0.0589])\n",
      "\n",
      "\n",
      "************** Batch 120 in 0.6425926685333252 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1016, 0.0411, 0.1202, 0.0824, 0.0843, 0.0607]) \n",
      "Test Loss tensor([0.0967, 0.0363, 0.1239, 0.0746, 0.0857, 0.0581])\n",
      "\n",
      "\n",
      "************** Batch 124 in 0.6154406070709229 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0946, 0.0404, 0.1282, 0.0667, 0.0822, 0.0581]) \n",
      "Test Loss tensor([0.0961, 0.0366, 0.1218, 0.0739, 0.0843, 0.0570])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 128 in 0.6275901794433594 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1003, 0.0373, 0.1213, 0.0721, 0.0880, 0.0616]) \n",
      "Test Loss tensor([0.0958, 0.0361, 0.1251, 0.0724, 0.0836, 0.0586])\n",
      "\n",
      "\n",
      "************** Batch 132 in 0.6233611106872559 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0401, 0.1158, 0.0686, 0.0852, 0.0574]) \n",
      "Test Loss tensor([0.0967, 0.0349, 0.1237, 0.0755, 0.0846, 0.0585])\n",
      "\n",
      "\n",
      "************** Batch 136 in 0.6453649997711182 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0951, 0.0314, 0.1262, 0.0708, 0.0844, 0.0573]) \n",
      "Test Loss tensor([0.0970, 0.0361, 0.1229, 0.0744, 0.0849, 0.0578])\n",
      "\n",
      "\n",
      "************** Batch 140 in 0.6548182964324951 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1067, 0.0380, 0.1135, 0.0769, 0.0859, 0.0608]) \n",
      "Test Loss tensor([0.0962, 0.0348, 0.1230, 0.0715, 0.0838, 0.0573])\n",
      "\n",
      "\n",
      "************** Batch 144 in 0.638319730758667 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1047, 0.0385, 0.1240, 0.0810, 0.0901, 0.0591]) \n",
      "Test Loss tensor([0.0966, 0.0356, 0.1223, 0.0705, 0.0847, 0.0574])\n",
      "\n",
      "\n",
      "************** Batch 148 in 0.6401228904724121 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0985, 0.0387, 0.1272, 0.0688, 0.0828, 0.0553]) \n",
      "Test Loss tensor([0.0970, 0.0353, 0.1228, 0.0756, 0.0835, 0.0569])\n",
      "\n",
      "\n",
      "************** Batch 152 in 0.678790807723999 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1029, 0.0313, 0.1238, 0.0727, 0.0777, 0.0534]) \n",
      "Test Loss tensor([0.0979, 0.0369, 0.1230, 0.0749, 0.0844, 0.0587])\n",
      "\n",
      "\n",
      "************** Batch 156 in 0.6937730312347412 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1061, 0.0364, 0.1204, 0.0699, 0.0862, 0.0618]) \n",
      "Test Loss tensor([0.0984, 0.0376, 0.1239, 0.0716, 0.0849, 0.0576])\n",
      "\n",
      "\n",
      "************** Batch 160 in 0.585223913192749 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0982, 0.0350, 0.1206, 0.0801, 0.0798, 0.0556]) \n",
      "Test Loss tensor([0.0996, 0.0355, 0.1210, 0.0723, 0.0842, 0.0596])\n",
      "\n",
      "\n",
      "************** Batch 164 in 0.5936679840087891 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0967, 0.0377, 0.1228, 0.0726, 0.0842, 0.0604]) \n",
      "Test Loss tensor([0.0948, 0.0350, 0.1229, 0.0730, 0.0830, 0.0598])\n",
      "\n",
      "\n",
      "************** Batch 168 in 0.6265511512756348 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0958, 0.0381, 0.1183, 0.0743, 0.0820, 0.0568]) \n",
      "Test Loss tensor([0.0959, 0.0359, 0.1235, 0.0716, 0.0827, 0.0587])\n",
      "\n",
      "\n",
      "************** Batch 172 in 0.5947213172912598 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0975, 0.0334, 0.1259, 0.0810, 0.0838, 0.0575]) \n",
      "Test Loss tensor([0.0957, 0.0339, 0.1204, 0.0713, 0.0824, 0.0580])\n",
      "\n",
      "\n",
      "************** Batch 176 in 0.6112184524536133 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1078, 0.0429, 0.1313, 0.0821, 0.0872, 0.0588]) \n",
      "Test Loss tensor([0.0962, 0.0338, 0.1222, 0.0722, 0.0832, 0.0580])\n",
      "\n",
      "\n",
      "************** Batch 180 in 0.5506963729858398 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0955, 0.0353, 0.1256, 0.0679, 0.0856, 0.0547]) \n",
      "Test Loss tensor([0.1004, 0.0362, 0.1203, 0.0717, 0.0843, 0.0563])\n",
      "\n",
      "\n",
      "************** Batch 184 in 0.6067841053009033 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0959, 0.0322, 0.1197, 0.0886, 0.0846, 0.0513]) \n",
      "Test Loss tensor([0.0980, 0.0341, 0.1233, 0.0712, 0.0842, 0.0592])\n",
      "\n",
      "\n",
      "************** Batch 188 in 0.5606746673583984 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0968, 0.0324, 0.1095, 0.0743, 0.0761, 0.0583]) \n",
      "Test Loss tensor([0.0990, 0.0341, 0.1240, 0.0728, 0.0835, 0.0582])\n",
      "\n",
      "\n",
      "************** Batch 192 in 0.608313798904419 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0872, 0.0356, 0.1276, 0.0685, 0.0835, 0.0596]) \n",
      "Test Loss tensor([0.0991, 0.0346, 0.1238, 0.0733, 0.0842, 0.0563])\n",
      "\n",
      "\n",
      "************** Batch 196 in 0.672661304473877 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0974, 0.0396, 0.1188, 0.0758, 0.0882, 0.0536]) \n",
      "Test Loss tensor([0.0974, 0.0318, 0.1200, 0.0730, 0.0829, 0.0574])\n",
      "\n",
      "\n",
      "************** Batch 200 in 0.6160345077514648 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1010, 0.0346, 0.1167, 0.0739, 0.0832, 0.0497]) \n",
      "Test Loss tensor([0.0987, 0.0327, 0.1264, 0.0717, 0.0825, 0.0571])\n",
      "\n",
      "\n",
      "************** Batch 204 in 0.634530782699585 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0981, 0.0293, 0.1274, 0.0703, 0.0796, 0.0585]) \n",
      "Test Loss tensor([0.0979, 0.0338, 0.1237, 0.0749, 0.0823, 0.0575])\n",
      "\n",
      "\n",
      "************** Batch 208 in 0.5842876434326172 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0343, 0.1298, 0.0754, 0.0854, 0.0545]) \n",
      "Test Loss tensor([0.0977, 0.0335, 0.1222, 0.0707, 0.0838, 0.0581])\n",
      "\n",
      "\n",
      "************** Batch 212 in 0.5835728645324707 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0904, 0.0306, 0.1242, 0.0775, 0.0847, 0.0569]) \n",
      "Test Loss tensor([0.0968, 0.0337, 0.1246, 0.0741, 0.0831, 0.0557])\n",
      "\n",
      "\n",
      "************** Batch 216 in 0.5880913734436035 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1004, 0.0376, 0.1299, 0.0750, 0.0868, 0.0565]) \n",
      "Test Loss tensor([0.0998, 0.0347, 0.1220, 0.0736, 0.0829, 0.0581])\n",
      "\n",
      "\n",
      "************** Batch 220 in 0.6097455024719238 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1030, 0.0396, 0.1227, 0.0792, 0.0849, 0.0568]) \n",
      "Test Loss tensor([0.0961, 0.0317, 0.1214, 0.0731, 0.0839, 0.0553])\n",
      "\n",
      "\n",
      "************** Batch 224 in 0.6122035980224609 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0965, 0.0363, 0.1225, 0.0805, 0.0806, 0.0572]) \n",
      "Test Loss tensor([0.0966, 0.0341, 0.1198, 0.0718, 0.0843, 0.0565])\n",
      "\n",
      "\n",
      "************** Batch 228 in 0.5805544853210449 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0997, 0.0298, 0.1187, 0.0679, 0.0871, 0.0554]) \n",
      "Test Loss tensor([0.0952, 0.0330, 0.1224, 0.0711, 0.0822, 0.0574])\n",
      "\n",
      "\n",
      "************** Batch 232 in 0.5839500427246094 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1063, 0.0366, 0.1186, 0.0786, 0.0835, 0.0625]) \n",
      "Test Loss tensor([0.0967, 0.0333, 0.1243, 0.0722, 0.0806, 0.0577])\n",
      "\n",
      "\n",
      "************** Batch 236 in 0.5954954624176025 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1030, 0.0352, 0.1228, 0.0802, 0.0818, 0.0556]) \n",
      "Test Loss tensor([0.1015, 0.0324, 0.1207, 0.0735, 0.0838, 0.0567])\n",
      "\n",
      "\n",
      "************** Batch 240 in 0.6162168979644775 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0996, 0.0335, 0.1131, 0.0786, 0.0831, 0.0559]) \n",
      "Test Loss tensor([0.0949, 0.0342, 0.1218, 0.0693, 0.0841, 0.0575])\n",
      "\n",
      "\n",
      "************** Batch 244 in 0.6085681915283203 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0981, 0.0323, 0.1130, 0.0666, 0.0823, 0.0546]) \n",
      "Test Loss tensor([0.0957, 0.0325, 0.1211, 0.0711, 0.0828, 0.0567])\n",
      "\n",
      "\n",
      "************** Batch 248 in 0.5965986251831055 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0926, 0.0319, 0.1229, 0.0757, 0.0769, 0.0573]) \n",
      "Test Loss tensor([0.0967, 0.0321, 0.1217, 0.0722, 0.0837, 0.0565])\n",
      "\n",
      "\n",
      "************** Batch 252 in 0.5921609401702881 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0335, 0.1272, 0.0646, 0.0848, 0.0608]) \n",
      "Test Loss tensor([0.0946, 0.0322, 0.1215, 0.0700, 0.0826, 0.0568])\n",
      "\n",
      "\n",
      "************** Batch 256 in 0.5984704494476318 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0946, 0.0342, 0.1193, 0.0767, 0.0740, 0.0571]) \n",
      "Test Loss tensor([0.0961, 0.0314, 0.1207, 0.0707, 0.0810, 0.0568])\n",
      "\n",
      "\n",
      "************** Batch 260 in 0.6131610870361328 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0895, 0.0334, 0.1247, 0.0730, 0.0826, 0.0582]) \n",
      "Test Loss tensor([0.0960, 0.0323, 0.1221, 0.0711, 0.0815, 0.0565])\n",
      "\n",
      "\n",
      "************** Batch 264 in 0.6211538314819336 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1010, 0.0331, 0.1206, 0.0725, 0.0837, 0.0547]) \n",
      "Test Loss tensor([0.0969, 0.0321, 0.1214, 0.0712, 0.0826, 0.0567])\n",
      "\n",
      "\n",
      "************** Batch 268 in 0.5746340751647949 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0988, 0.0278, 0.1144, 0.0780, 0.0749, 0.0536]) \n",
      "Test Loss tensor([0.0981, 0.0314, 0.1204, 0.0728, 0.0831, 0.0560])\n",
      "\n",
      "\n",
      "************** Batch 272 in 0.591465950012207 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1035, 0.0315, 0.1153, 0.0793, 0.0807, 0.0557]) \n",
      "Test Loss tensor([0.0994, 0.0322, 0.1182, 0.0724, 0.0836, 0.0570])\n",
      "\n",
      "\n",
      "************** Batch 276 in 0.5965983867645264 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1010, 0.0297, 0.1217, 0.0773, 0.0791, 0.0596]) \n",
      "Test Loss tensor([0.0954, 0.0310, 0.1221, 0.0735, 0.0833, 0.0565])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 280 in 0.6001169681549072 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0985, 0.0309, 0.1285, 0.0705, 0.0871, 0.0597]) \n",
      "Test Loss tensor([0.0974, 0.0302, 0.1208, 0.0712, 0.0821, 0.0562])\n",
      "\n",
      "\n",
      "************** Batch 284 in 0.5791747570037842 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0971, 0.0298, 0.1190, 0.0825, 0.0817, 0.0556]) \n",
      "Test Loss tensor([0.0956, 0.0301, 0.1223, 0.0711, 0.0819, 0.0557])\n",
      "\n",
      "\n",
      "************** Batch 288 in 0.562849760055542 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0955, 0.0361, 0.1225, 0.0731, 0.0826, 0.0549]) \n",
      "Test Loss tensor([0.0970, 0.0325, 0.1209, 0.0733, 0.0833, 0.0549])\n",
      "\n",
      "\n",
      "************** Batch 292 in 0.5603587627410889 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0936, 0.0294, 0.1253, 0.0704, 0.0818, 0.0597]) \n",
      "Test Loss tensor([0.0983, 0.0315, 0.1218, 0.0722, 0.0832, 0.0575])\n",
      "\n",
      "\n",
      "************** Batch 296 in 0.542853832244873 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0945, 0.0302, 0.1264, 0.0735, 0.0822, 0.0613]) \n",
      "Test Loss tensor([0.0962, 0.0296, 0.1212, 0.0710, 0.0826, 0.0565])\n",
      "\n",
      "\n",
      "************** Batch 300 in 0.5682086944580078 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0983, 0.0351, 0.1289, 0.0786, 0.0841, 0.0546]) \n",
      "Test Loss tensor([0.0957, 0.0319, 0.1168, 0.0729, 0.0833, 0.0550])\n",
      "\n",
      "\n",
      "************** Batch 304 in 0.5539267063140869 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0908, 0.0314, 0.1136, 0.0679, 0.0838, 0.0525]) \n",
      "Test Loss tensor([0.0950, 0.0302, 0.1189, 0.0730, 0.0819, 0.0558])\n",
      "\n",
      "\n",
      "************** Batch 308 in 0.5628807544708252 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0919, 0.0319, 0.1190, 0.0759, 0.0850, 0.0513]) \n",
      "Test Loss tensor([0.0981, 0.0301, 0.1197, 0.0730, 0.0817, 0.0566])\n",
      "\n",
      "\n",
      "************** Batch 312 in 0.6251647472381592 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0961, 0.0337, 0.1144, 0.0686, 0.0856, 0.0524]) \n",
      "Test Loss tensor([0.0962, 0.0298, 0.1221, 0.0723, 0.0825, 0.0563])\n",
      "\n",
      "\n",
      "************** Batch 316 in 0.5835878849029541 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0973, 0.0292, 0.1260, 0.0708, 0.0827, 0.0566]) \n",
      "Test Loss tensor([0.0973, 0.0304, 0.1213, 0.0736, 0.0818, 0.0558])\n",
      "\n",
      "\n",
      "************** Batch 320 in 0.5932188034057617 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0962, 0.0309, 0.1218, 0.0765, 0.0746, 0.0588]) \n",
      "Test Loss tensor([0.0987, 0.0297, 0.1215, 0.0735, 0.0816, 0.0571])\n",
      "\n",
      "\n",
      "************** Batch 324 in 0.6104497909545898 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0962, 0.0315, 0.1370, 0.0731, 0.0822, 0.0528]) \n",
      "Test Loss tensor([0.0971, 0.0279, 0.1223, 0.0746, 0.0824, 0.0544])\n",
      "\n",
      "\n",
      "************** Batch 328 in 0.5898902416229248 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0928, 0.0305, 0.1365, 0.0696, 0.0827, 0.0565]) \n",
      "Test Loss tensor([0.0968, 0.0280, 0.1246, 0.0731, 0.0872, 0.0549])\n",
      "\n",
      "\n",
      "************** Batch 332 in 0.6122591495513916 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0849, 0.0249, 0.1229, 0.0653, 0.0817, 0.0513]) \n",
      "Test Loss tensor([0.0961, 0.0295, 0.1246, 0.0730, 0.0814, 0.0554])\n",
      "\n",
      "\n",
      "************** Batch 336 in 0.5791373252868652 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0955, 0.0320, 0.1184, 0.0702, 0.0834, 0.0600]) \n",
      "Test Loss tensor([0.0971, 0.0277, 0.1204, 0.0746, 0.0825, 0.0538])\n",
      "\n",
      "\n",
      "************** Batch 340 in 0.5623788833618164 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0972, 0.0271, 0.1301, 0.0703, 0.0872, 0.0521]) \n",
      "Test Loss tensor([0.0945, 0.0278, 0.1225, 0.0730, 0.0829, 0.0551])\n",
      "\n",
      "\n",
      "************** Batch 344 in 0.5458242893218994 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0882, 0.0254, 0.1159, 0.0731, 0.0776, 0.0552]) \n",
      "Test Loss tensor([0.0975, 0.0286, 0.1224, 0.0739, 0.0838, 0.0556])\n",
      "\n",
      "\n",
      "************** Batch 348 in 0.5547811985015869 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1052, 0.0286, 0.1228, 0.0743, 0.0839, 0.0546]) \n",
      "Test Loss tensor([0.0960, 0.0284, 0.1234, 0.0736, 0.0849, 0.0566])\n",
      "\n",
      "\n",
      "************** Batch 352 in 0.5622215270996094 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0954, 0.0296, 0.1251, 0.0703, 0.0786, 0.0589]) \n",
      "Test Loss tensor([0.0941, 0.0284, 0.1204, 0.0725, 0.0838, 0.0552])\n",
      "\n",
      "\n",
      "************** Batch 356 in 0.5545108318328857 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1004, 0.0245, 0.1228, 0.0746, 0.0826, 0.0566]) \n",
      "Test Loss tensor([0.1004, 0.0284, 0.1231, 0.0718, 0.0813, 0.0558])\n",
      "\n",
      "\n",
      "************** Batch 360 in 0.5585832595825195 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0947, 0.0290, 0.1088, 0.0744, 0.0825, 0.0551]) \n",
      "Test Loss tensor([0.0964, 0.0281, 0.1236, 0.0739, 0.0813, 0.0528])\n",
      "\n",
      "\n",
      "************** Batch 364 in 0.5581293106079102 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0986, 0.0283, 0.1248, 0.0783, 0.0874, 0.0520]) \n",
      "Test Loss tensor([0.0962, 0.0287, 0.1225, 0.0721, 0.0841, 0.0543])\n",
      "\n",
      "\n",
      "************** Batch 368 in 0.5658142566680908 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1004, 0.0352, 0.1262, 0.0778, 0.0844, 0.0505]) \n",
      "Test Loss tensor([0.0951, 0.0271, 0.1213, 0.0704, 0.0807, 0.0552])\n",
      "\n",
      "\n",
      "************** Batch 372 in 0.5494954586029053 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0922, 0.0232, 0.1272, 0.0704, 0.0800, 0.0580]) \n",
      "Test Loss tensor([0.0962, 0.0264, 0.1210, 0.0703, 0.0843, 0.0564])\n",
      "\n",
      "\n",
      "************** Batch 376 in 0.5655670166015625 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0938, 0.0310, 0.1186, 0.0768, 0.0824, 0.0464]) \n",
      "Test Loss tensor([0.0967, 0.0278, 0.1203, 0.0732, 0.0849, 0.0538])\n",
      "\n",
      "\n",
      "************** Batch 380 in 0.5479261875152588 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0959, 0.0313, 0.1197, 0.0698, 0.0870, 0.0566]) \n",
      "Test Loss tensor([0.0973, 0.0273, 0.1203, 0.0732, 0.0819, 0.0535])\n",
      "\n",
      "\n",
      "************** Batch 384 in 0.5776426792144775 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0961, 0.0253, 0.1114, 0.0682, 0.0830, 0.0540]) \n",
      "Test Loss tensor([0.1002, 0.0265, 0.1229, 0.0720, 0.0868, 0.0560])\n",
      "\n",
      "\n",
      "************** Batch 388 in 0.5829415321350098 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0899, 0.0267, 0.1173, 0.0645, 0.0813, 0.0601]) \n",
      "Test Loss tensor([0.0947, 0.0279, 0.1200, 0.0732, 0.0813, 0.0533])\n",
      "\n",
      "\n",
      "************** Batch 392 in 0.5728027820587158 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0897, 0.0285, 0.1152, 0.0709, 0.0851, 0.0542]) \n",
      "Test Loss tensor([0.0958, 0.0287, 0.1220, 0.0720, 0.0857, 0.0592])\n",
      "\n",
      "\n",
      "************** Batch 396 in 0.5726926326751709 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0929, 0.0246, 0.1178, 0.0708, 0.0799, 0.0641]) \n",
      "Test Loss tensor([0.0990, 0.0266, 0.1217, 0.0716, 0.0828, 0.0523])\n",
      "\n",
      "\n",
      "************** Batch 400 in 0.5584671497344971 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1095, 0.0288, 0.1192, 0.0628, 0.0836, 0.0540]) \n",
      "Test Loss tensor([0.1017, 0.0277, 0.1211, 0.0721, 0.0822, 0.0538])\n",
      "\n",
      "\n",
      "************** Batch 404 in 0.5714495182037354 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1054, 0.0262, 0.1129, 0.0839, 0.0839, 0.0496]) \n",
      "Test Loss tensor([0.0969, 0.0273, 0.1208, 0.0707, 0.0842, 0.0531])\n",
      "\n",
      "\n",
      "************** Batch 408 in 0.5602889060974121 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1049, 0.0292, 0.1150, 0.0699, 0.0844, 0.0510]) \n",
      "Test Loss tensor([0.0931, 0.0260, 0.1216, 0.0711, 0.0796, 0.0526])\n",
      "\n",
      "\n",
      "************** Batch 412 in 0.6023201942443848 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0259, 0.1241, 0.0713, 0.0823, 0.0520]) \n",
      "Test Loss tensor([0.0970, 0.0266, 0.1172, 0.0726, 0.0841, 0.0521])\n",
      "\n",
      "\n",
      "************** Batch 416 in 0.718942403793335 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1019, 0.0304, 0.1227, 0.0667, 0.0837, 0.0536]) \n",
      "Test Loss tensor([0.0983, 0.0269, 0.1156, 0.0736, 0.0819, 0.0520])\n",
      "\n",
      "\n",
      "************** Batch 420 in 0.6646819114685059 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0885, 0.0235, 0.1207, 0.0756, 0.0781, 0.0528]) \n",
      "Test Loss tensor([0.0953, 0.0281, 0.1207, 0.0754, 0.0850, 0.0549])\n",
      "\n",
      "\n",
      "************** Batch 424 in 0.6504387855529785 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0970, 0.0295, 0.1232, 0.0683, 0.0890, 0.0558]) \n",
      "Test Loss tensor([0.0955, 0.0253, 0.1193, 0.0719, 0.0811, 0.0524])\n",
      "\n",
      "\n",
      "************** Batch 428 in 0.6664342880249023 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1013, 0.0309, 0.1098, 0.0713, 0.0766, 0.0499]) \n",
      "Test Loss tensor([0.1043, 0.0272, 0.1197, 0.0750, 0.0845, 0.0535])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 432 in 0.6091485023498535 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0977, 0.0234, 0.1169, 0.0834, 0.0883, 0.0544]) \n",
      "Test Loss tensor([0.0946, 0.0275, 0.1189, 0.0724, 0.0856, 0.0541])\n",
      "\n",
      "\n",
      "************** Batch 436 in 0.6480510234832764 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0881, 0.0242, 0.1259, 0.0742, 0.0817, 0.0549]) \n",
      "Test Loss tensor([0.0972, 0.0257, 0.1201, 0.0726, 0.0832, 0.0519])\n",
      "\n",
      "\n",
      "************** Batch 440 in 0.6481490135192871 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1004, 0.0246, 0.1218, 0.0735, 0.0862, 0.0581]) \n",
      "Test Loss tensor([0.0990, 0.0252, 0.1219, 0.0727, 0.0884, 0.0537])\n",
      "\n",
      "\n",
      "************** Batch 444 in 0.6757040023803711 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1097, 0.0304, 0.1144, 0.0760, 0.0891, 0.0502]) \n",
      "Test Loss tensor([0.0960, 0.0248, 0.1217, 0.0725, 0.0821, 0.0514])\n",
      "\n",
      "\n",
      "************** Batch 448 in 0.630720853805542 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0993, 0.0282, 0.1205, 0.0671, 0.0810, 0.0500]) \n",
      "Test Loss tensor([0.1006, 0.0254, 0.1202, 0.0744, 0.0843, 0.0532])\n",
      "\n",
      "\n",
      "************** Batch 452 in 0.6189203262329102 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1004, 0.0229, 0.1104, 0.0760, 0.0876, 0.0491]) \n",
      "Test Loss tensor([0.0975, 0.0270, 0.1207, 0.0730, 0.0818, 0.0507])\n",
      "\n",
      "\n",
      "************** Batch 456 in 0.6193385124206543 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0886, 0.0269, 0.1278, 0.0682, 0.0848, 0.0517]) \n",
      "Test Loss tensor([0.0963, 0.0257, 0.1198, 0.0719, 0.0825, 0.0529])\n",
      "\n",
      "\n",
      "************** Batch 460 in 0.6415965557098389 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0916, 0.0260, 0.1250, 0.0735, 0.0795, 0.0596]) \n",
      "Test Loss tensor([0.0975, 0.0268, 0.1216, 0.0743, 0.0815, 0.0514])\n",
      "\n",
      "\n",
      "************** Batch 464 in 0.6128332614898682 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0960, 0.0238, 0.1250, 0.0789, 0.0843, 0.0484]) \n",
      "Test Loss tensor([0.0977, 0.0263, 0.1205, 0.0742, 0.0826, 0.0510])\n",
      "\n",
      "\n",
      "************** Batch 468 in 0.5691568851470947 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0929, 0.0215, 0.1334, 0.0616, 0.0752, 0.0508]) \n",
      "Test Loss tensor([0.0996, 0.0255, 0.1208, 0.0719, 0.0806, 0.0518])\n",
      "\n",
      "\n",
      "************** Batch 472 in 0.6185574531555176 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1003, 0.0266, 0.1103, 0.0699, 0.0780, 0.0468]) \n",
      "Test Loss tensor([0.0991, 0.0264, 0.1250, 0.0766, 0.0832, 0.0505])\n",
      "\n",
      "\n",
      "************** Batch 476 in 0.5587353706359863 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0960, 0.0252, 0.1236, 0.0787, 0.0817, 0.0548]) \n",
      "Test Loss tensor([0.0972, 0.0253, 0.1200, 0.0748, 0.0804, 0.0490])\n",
      "\n",
      "\n",
      "************** Batch 480 in 0.5758404731750488 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1000, 0.0261, 0.1270, 0.0689, 0.0757, 0.0473]) \n",
      "Test Loss tensor([0.0980, 0.0253, 0.1190, 0.0727, 0.0836, 0.0507])\n",
      "\n",
      "\n",
      "************** Batch 484 in 0.561882495880127 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0990, 0.0232, 0.1195, 0.0787, 0.0785, 0.0485]) \n",
      "Test Loss tensor([0.0957, 0.0255, 0.1182, 0.0717, 0.0810, 0.0515])\n",
      "\n",
      "\n",
      "************** Batch 488 in 0.5748767852783203 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1106, 0.0267, 0.1244, 0.0780, 0.0776, 0.0560]) \n",
      "Test Loss tensor([0.0972, 0.0250, 0.1168, 0.0717, 0.0811, 0.0510])\n",
      "\n",
      "\n",
      "************** Batch 492 in 0.5677852630615234 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0936, 0.0257, 0.1172, 0.0737, 0.0807, 0.0516]) \n",
      "Test Loss tensor([0.0959, 0.0254, 0.1197, 0.0719, 0.0830, 0.0509])\n",
      "\n",
      "\n",
      "************** Batch 496 in 0.5642240047454834 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0969, 0.0233, 0.1170, 0.0722, 0.0831, 0.0518]) \n",
      "Test Loss tensor([0.0968, 0.0263, 0.1203, 0.0712, 0.0815, 0.0506])\n",
      "\n",
      "\n",
      "************** Batch 500 in 0.5791275501251221 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0994, 0.0271, 0.1201, 0.0691, 0.0837, 0.0496]) \n",
      "Test Loss tensor([0.0946, 0.0256, 0.1185, 0.0731, 0.0808, 0.0487])\n",
      "\n",
      "\n",
      "************** Batch 504 in 0.6494910717010498 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1007, 0.0332, 0.1244, 0.0838, 0.0831, 0.0511]) \n",
      "Test Loss tensor([0.0968, 0.0265, 0.1201, 0.0730, 0.0830, 0.0491])\n",
      "\n",
      "\n",
      "************** Batch 508 in 0.6652026176452637 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0976, 0.0238, 0.1226, 0.0839, 0.0826, 0.0488]) \n",
      "Test Loss tensor([0.0984, 0.0246, 0.1205, 0.0734, 0.0806, 0.0510])\n",
      "\n",
      "\n",
      "************** Batch 512 in 0.6634299755096436 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0912, 0.0287, 0.1304, 0.0653, 0.0785, 0.0495]) \n",
      "Test Loss tensor([0.0982, 0.0256, 0.1199, 0.0715, 0.0802, 0.0500])\n",
      "\n",
      "\n",
      "************** Batch 516 in 0.6230862140655518 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0994, 0.0221, 0.1166, 0.0736, 0.0873, 0.0557]) \n",
      "Test Loss tensor([0.0995, 0.0265, 0.1215, 0.0724, 0.0823, 0.0497])\n",
      "\n",
      "\n",
      "************** Batch 520 in 0.6545314788818359 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0970, 0.0281, 0.1102, 0.0748, 0.0820, 0.0465]) \n",
      "Test Loss tensor([0.0976, 0.0253, 0.1221, 0.0724, 0.0806, 0.0499])\n",
      "\n",
      "\n",
      "************** Batch 524 in 0.6802277565002441 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1000, 0.0252, 0.1217, 0.0765, 0.0848, 0.0494]) \n",
      "Test Loss tensor([0.1012, 0.0250, 0.1202, 0.0727, 0.0848, 0.0495])\n",
      "\n",
      "\n",
      "************** Batch 528 in 0.6710240840911865 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1023, 0.0217, 0.1149, 0.0727, 0.0850, 0.0531]) \n",
      "Test Loss tensor([0.0944, 0.0235, 0.1167, 0.0718, 0.0807, 0.0491])\n",
      "\n",
      "\n",
      "************** Batch 532 in 0.6889035701751709 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1096, 0.0264, 0.1266, 0.0745, 0.0789, 0.0505]) \n",
      "Test Loss tensor([0.0944, 0.0244, 0.1218, 0.0716, 0.0790, 0.0501])\n",
      "\n",
      "\n",
      "************** Batch 536 in 0.6795663833618164 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0896, 0.0248, 0.1167, 0.0681, 0.0771, 0.0476]) \n",
      "Test Loss tensor([0.0985, 0.0247, 0.1211, 0.0745, 0.0809, 0.0512])\n",
      "\n",
      "\n",
      "************** Batch 540 in 0.6817986965179443 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0922, 0.0303, 0.1248, 0.0771, 0.0799, 0.0492]) \n",
      "Test Loss tensor([0.0981, 0.0245, 0.1192, 0.0730, 0.0785, 0.0515])\n",
      "\n",
      "\n",
      "************** Batch 544 in 0.6285281181335449 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0270, 0.1220, 0.0732, 0.0866, 0.0522]) \n",
      "Test Loss tensor([0.0952, 0.0253, 0.1179, 0.0723, 0.0805, 0.0502])\n",
      "\n",
      "\n",
      "************** Batch 548 in 0.6442165374755859 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0929, 0.0259, 0.1213, 0.0670, 0.0803, 0.0492]) \n",
      "Test Loss tensor([0.0951, 0.0246, 0.1195, 0.0706, 0.0794, 0.0510])\n",
      "\n",
      "\n",
      "************** Batch 552 in 0.6880531311035156 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0937, 0.0266, 0.1185, 0.0660, 0.0812, 0.0486]) \n",
      "Test Loss tensor([0.0989, 0.0245, 0.1185, 0.0703, 0.0820, 0.0525])\n",
      "\n",
      "\n",
      "************** Batch 556 in 0.6688418388366699 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1065, 0.0258, 0.1137, 0.0714, 0.0818, 0.0520]) \n",
      "Test Loss tensor([0.0956, 0.0258, 0.1215, 0.0710, 0.0799, 0.0496])\n",
      "\n",
      "\n",
      "************** Batch 560 in 0.6460003852844238 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0943, 0.0244, 0.1169, 0.0644, 0.0871, 0.0484]) \n",
      "Test Loss tensor([0.0964, 0.0259, 0.1179, 0.0736, 0.0802, 0.0501])\n",
      "\n",
      "\n",
      "************** Batch 564 in 0.6350634098052979 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1076, 0.0297, 0.1106, 0.0780, 0.0834, 0.0488]) \n",
      "Test Loss tensor([0.0990, 0.0253, 0.1176, 0.0710, 0.0833, 0.0485])\n",
      "\n",
      "\n",
      "************** Batch 568 in 0.6531069278717041 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0956, 0.0230, 0.1245, 0.0732, 0.0802, 0.0489]) \n",
      "Test Loss tensor([0.0970, 0.0246, 0.1176, 0.0722, 0.0800, 0.0486])\n",
      "\n",
      "\n",
      "************** Batch 572 in 0.6407928466796875 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0977, 0.0284, 0.1202, 0.0731, 0.0803, 0.0437]) \n",
      "Test Loss tensor([0.0962, 0.0237, 0.1166, 0.0721, 0.0802, 0.0488])\n",
      "\n",
      "\n",
      "************** Batch 576 in 0.6535863876342773 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0934, 0.0238, 0.1165, 0.0711, 0.0821, 0.0510]) \n",
      "Test Loss tensor([0.0982, 0.0246, 0.1216, 0.0728, 0.0823, 0.0473])\n",
      "\n",
      "\n",
      "************** Batch 580 in 0.6277205944061279 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1008, 0.0206, 0.1117, 0.0703, 0.0757, 0.0568]) \n",
      "Test Loss tensor([0.0932, 0.0243, 0.1203, 0.0707, 0.0794, 0.0491])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 584 in 0.6078188419342041 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0920, 0.0284, 0.1295, 0.0803, 0.0770, 0.0507]) \n",
      "Test Loss tensor([0.0998, 0.0242, 0.1191, 0.0754, 0.0804, 0.0478])\n",
      "\n",
      "\n",
      "************** Batch 588 in 0.639585018157959 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0986, 0.0275, 0.1250, 0.0762, 0.0736, 0.0475]) \n",
      "Test Loss tensor([0.0946, 0.0251, 0.1204, 0.0720, 0.0808, 0.0502])\n",
      "\n",
      "\n",
      "************** Batch 592 in 0.6741902828216553 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1041, 0.0285, 0.1220, 0.0768, 0.0836, 0.0488]) \n",
      "Test Loss tensor([0.0960, 0.0248, 0.1175, 0.0709, 0.0786, 0.0489])\n",
      "\n",
      "\n",
      "************** Batch 596 in 0.6004574298858643 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0994, 0.0250, 0.1245, 0.0828, 0.0809, 0.0508]) \n",
      "Test Loss tensor([0.0953, 0.0228, 0.1200, 0.0712, 0.0803, 0.0492])\n",
      "\n",
      "\n",
      "************** Batch 600 in 0.692044734954834 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1014, 0.0220, 0.1180, 0.0753, 0.0804, 0.0547]) \n",
      "Test Loss tensor([0.0970, 0.0256, 0.1183, 0.0727, 0.0806, 0.0477])\n",
      "\n",
      "\n",
      "************** Batch 604 in 0.6409401893615723 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0952, 0.0283, 0.1146, 0.0737, 0.0787, 0.0450]) \n",
      "Test Loss tensor([0.0981, 0.0254, 0.1195, 0.0731, 0.0819, 0.0484])\n",
      "\n",
      "\n",
      "************** Batch 608 in 0.6159253120422363 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1022, 0.0264, 0.1248, 0.0694, 0.0812, 0.0491]) \n",
      "Test Loss tensor([0.0977, 0.0250, 0.1162, 0.0729, 0.0834, 0.0476])\n",
      "\n",
      "\n",
      "************** Batch 612 in 0.637993574142456 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0888, 0.0185, 0.1118, 0.0800, 0.0800, 0.0491]) \n",
      "Test Loss tensor([0.0981, 0.0241, 0.1180, 0.0732, 0.0828, 0.0496])\n",
      "\n",
      "\n",
      "************** Batch 616 in 0.624384880065918 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0950, 0.0259, 0.1175, 0.0751, 0.0800, 0.0473]) \n",
      "Test Loss tensor([0.0936, 0.0235, 0.1178, 0.0688, 0.0797, 0.0491])\n",
      "\n",
      "\n",
      "************** Batch 620 in 0.6491162776947021 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0879, 0.0213, 0.1169, 0.0818, 0.0836, 0.0500]) \n",
      "Test Loss tensor([0.1008, 0.0232, 0.1174, 0.0702, 0.0850, 0.0496])\n",
      "\n",
      "\n",
      "************** Batch 624 in 0.7005612850189209 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1059, 0.0269, 0.1167, 0.0682, 0.0887, 0.0495]) \n",
      "Test Loss tensor([0.0984, 0.0246, 0.1198, 0.0707, 0.0793, 0.0488])\n",
      "\n",
      "\n",
      "************** Batch 628 in 0.6482369899749756 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1018, 0.0272, 0.1136, 0.0677, 0.0754, 0.0442]) \n",
      "Test Loss tensor([0.0959, 0.0248, 0.1172, 0.0747, 0.0830, 0.0478])\n",
      "\n",
      "\n",
      "************** Batch 632 in 0.6429154872894287 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0958, 0.0239, 0.1219, 0.0760, 0.0857, 0.0539]) \n",
      "Test Loss tensor([0.0971, 0.0240, 0.1168, 0.0707, 0.0829, 0.0494])\n",
      "\n",
      "\n",
      "************** Batch 636 in 0.6562762260437012 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1009, 0.0241, 0.1203, 0.0757, 0.0826, 0.0489]) \n",
      "Test Loss tensor([0.0942, 0.0249, 0.1198, 0.0705, 0.0792, 0.0490])\n",
      "\n",
      "\n",
      "************** Batch 640 in 0.6442162990570068 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0971, 0.0287, 0.1160, 0.0779, 0.0848, 0.0451]) \n",
      "Test Loss tensor([0.0970, 0.0241, 0.1206, 0.0721, 0.0803, 0.0505])\n",
      "\n",
      "\n",
      "************** Batch 644 in 0.6064743995666504 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0975, 0.0252, 0.1193, 0.0707, 0.0789, 0.0425]) \n",
      "Test Loss tensor([0.0952, 0.0259, 0.1194, 0.0716, 0.0787, 0.0476])\n",
      "\n",
      "\n",
      "************** Batch 648 in 0.621518611907959 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1065, 0.0251, 0.1152, 0.0857, 0.0830, 0.0430]) \n",
      "Test Loss tensor([0.0997, 0.0231, 0.1195, 0.0734, 0.0792, 0.0490])\n",
      "\n",
      "\n",
      "************** Batch 652 in 0.5960891246795654 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0961, 0.0247, 0.1271, 0.0713, 0.0812, 0.0532]) \n",
      "Test Loss tensor([0.0930, 0.0231, 0.1182, 0.0720, 0.0781, 0.0483])\n",
      "\n",
      "\n",
      "************** Batch 656 in 0.6629500389099121 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0960, 0.0206, 0.1299, 0.0720, 0.0791, 0.0496]) \n",
      "Test Loss tensor([0.0980, 0.0253, 0.1160, 0.0727, 0.0813, 0.0491])\n",
      "\n",
      "\n",
      "************** Batch 660 in 0.5946269035339355 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1006, 0.0259, 0.1140, 0.0727, 0.0837, 0.0489]) \n",
      "Test Loss tensor([0.0986, 0.0251, 0.1172, 0.0702, 0.0798, 0.0473])\n",
      "\n",
      "\n",
      "************** Batch 664 in 0.56581711769104 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0971, 0.0237, 0.1184, 0.0719, 0.0768, 0.0452]) \n",
      "Test Loss tensor([0.0955, 0.0234, 0.1186, 0.0722, 0.0815, 0.0469])\n",
      "\n",
      "\n",
      "************** Batch 668 in 0.5923771858215332 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0990, 0.0234, 0.1194, 0.0748, 0.0738, 0.0548]) \n",
      "Test Loss tensor([0.0972, 0.0263, 0.1159, 0.0726, 0.0806, 0.0482])\n",
      "\n",
      "\n",
      "************** Batch 672 in 0.5960116386413574 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0900, 0.0255, 0.1254, 0.0658, 0.0804, 0.0532]) \n",
      "Test Loss tensor([0.0990, 0.0232, 0.1184, 0.0743, 0.0816, 0.0463])\n",
      "\n",
      "\n",
      "************** Batch 676 in 0.5912718772888184 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0917, 0.0292, 0.1223, 0.0670, 0.0835, 0.0468]) \n",
      "Test Loss tensor([0.1015, 0.0243, 0.1195, 0.0714, 0.0842, 0.0501])\n",
      "\n",
      "\n",
      "************** Batch 680 in 0.5757715702056885 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0941, 0.0254, 0.1166, 0.0705, 0.0823, 0.0521]) \n",
      "Test Loss tensor([0.0973, 0.0258, 0.1140, 0.0739, 0.0846, 0.0465])\n",
      "\n",
      "\n",
      "************** Batch 684 in 0.5607903003692627 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0930, 0.0238, 0.1214, 0.0791, 0.0846, 0.0524]) \n",
      "Test Loss tensor([0.0958, 0.0249, 0.1193, 0.0723, 0.0784, 0.0478])\n",
      "\n",
      "\n",
      "************** Batch 688 in 0.5699319839477539 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0941, 0.0261, 0.1268, 0.0702, 0.0822, 0.0517]) \n",
      "Test Loss tensor([0.1005, 0.0244, 0.1183, 0.0717, 0.0807, 0.0518])\n",
      "\n",
      "\n",
      "************** Batch 692 in 0.5601058006286621 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1049, 0.0208, 0.1277, 0.0714, 0.0800, 0.0494]) \n",
      "Test Loss tensor([0.0961, 0.0243, 0.1189, 0.0721, 0.0818, 0.0485])\n",
      "\n",
      "\n",
      "************** Batch 696 in 0.5752444267272949 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0229, 0.1161, 0.0687, 0.0807, 0.0465]) \n",
      "Test Loss tensor([0.0957, 0.0250, 0.1160, 0.0726, 0.0789, 0.0481])\n",
      "\n",
      "\n",
      "************** Batch 700 in 0.5898158550262451 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0973, 0.0255, 0.1200, 0.0665, 0.0792, 0.0446]) \n",
      "Test Loss tensor([0.0960, 0.0246, 0.1189, 0.0714, 0.0814, 0.0500])\n",
      "\n",
      "\n",
      "************** Batch 704 in 0.5708968639373779 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0892, 0.0275, 0.1159, 0.0715, 0.0819, 0.0449]) \n",
      "Test Loss tensor([0.0958, 0.0239, 0.1172, 0.0723, 0.0797, 0.0468])\n",
      "\n",
      "\n",
      "************** Batch 708 in 0.5830752849578857 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0244, 0.1249, 0.0723, 0.0763, 0.0494]) \n",
      "Test Loss tensor([0.0974, 0.0234, 0.1170, 0.0726, 0.0806, 0.0493])\n",
      "\n",
      "\n",
      "************** Batch 712 in 0.5676887035369873 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0961, 0.0257, 0.1162, 0.0710, 0.0762, 0.0495]) \n",
      "Test Loss tensor([0.0967, 0.0237, 0.1185, 0.0710, 0.0791, 0.0487])\n",
      "\n",
      "\n",
      "************** Batch 716 in 0.5942714214324951 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0954, 0.0297, 0.1183, 0.0738, 0.0835, 0.0482]) \n",
      "Test Loss tensor([0.0973, 0.0242, 0.1173, 0.0730, 0.0785, 0.0482])\n",
      "\n",
      "\n",
      "************** Batch 720 in 0.5669906139373779 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0942, 0.0246, 0.1172, 0.0752, 0.0780, 0.0458]) \n",
      "Test Loss tensor([0.0961, 0.0239, 0.1177, 0.0706, 0.0786, 0.0479])\n",
      "\n",
      "\n",
      "************** Batch 724 in 0.6347556114196777 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0224, 0.1126, 0.0729, 0.0852, 0.0496]) \n",
      "Test Loss tensor([0.0961, 0.0234, 0.1206, 0.0706, 0.0780, 0.0493])\n",
      "\n",
      "\n",
      "************** Batch 728 in 0.6914238929748535 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0947, 0.0260, 0.1232, 0.0678, 0.0802, 0.0475]) \n",
      "Test Loss tensor([0.0954, 0.0238, 0.1217, 0.0709, 0.0783, 0.0503])\n",
      "\n",
      "\n",
      "************** Batch 732 in 0.6279633045196533 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1091, 0.0296, 0.1179, 0.0685, 0.0811, 0.0504]) \n",
      "Test Loss tensor([0.0972, 0.0245, 0.1160, 0.0704, 0.0793, 0.0500])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 736 in 0.6038153171539307 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1053, 0.0228, 0.1183, 0.0735, 0.0822, 0.0466]) \n",
      "Test Loss tensor([0.0958, 0.0233, 0.1192, 0.0697, 0.0788, 0.0477])\n",
      "\n",
      "\n",
      "************** Batch 740 in 0.605947732925415 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0924, 0.0265, 0.1234, 0.0687, 0.0793, 0.0459]) \n",
      "Test Loss tensor([0.0973, 0.0219, 0.1197, 0.0700, 0.0816, 0.0485])\n",
      "\n",
      "\n",
      "************** Batch 744 in 0.6679079532623291 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0951, 0.0221, 0.1108, 0.0662, 0.0810, 0.0459]) \n",
      "Test Loss tensor([0.0966, 0.0238, 0.1190, 0.0728, 0.0817, 0.0486])\n",
      "\n",
      "\n",
      "************** Batch 748 in 0.6412575244903564 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0973, 0.0244, 0.1190, 0.0684, 0.0818, 0.0476]) \n",
      "Test Loss tensor([0.0970, 0.0235, 0.1199, 0.0735, 0.0811, 0.0474])\n",
      "\n",
      "\n",
      "************** Batch 752 in 0.624464750289917 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0963, 0.0236, 0.1110, 0.0776, 0.0807, 0.0475]) \n",
      "Test Loss tensor([0.0978, 0.0237, 0.1159, 0.0697, 0.0826, 0.0462])\n",
      "\n",
      "\n",
      "************** Batch 756 in 0.6247849464416504 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0993, 0.0200, 0.1169, 0.0780, 0.0803, 0.0432]) \n",
      "Test Loss tensor([0.0963, 0.0231, 0.1184, 0.0699, 0.0806, 0.0464])\n",
      "\n",
      "\n",
      "************** Batch 760 in 0.5964295864105225 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0971, 0.0211, 0.1214, 0.0710, 0.0813, 0.0484]) \n",
      "Test Loss tensor([0.0966, 0.0231, 0.1186, 0.0721, 0.0798, 0.0491])\n",
      "\n",
      "\n",
      "************** Batch 764 in 0.6955595016479492 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0926, 0.0274, 0.1293, 0.0747, 0.0819, 0.0483]) \n",
      "Test Loss tensor([0.0944, 0.0248, 0.1206, 0.0726, 0.0775, 0.0471])\n",
      "\n",
      "\n",
      "************** Batch 768 in 0.6715912818908691 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0926, 0.0197, 0.1106, 0.0710, 0.0790, 0.0444]) \n",
      "Test Loss tensor([0.0947, 0.0241, 0.1178, 0.0697, 0.0800, 0.0486])\n",
      "\n",
      "\n",
      "************** Batch 772 in 0.666419506072998 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0999, 0.0272, 0.1255, 0.0746, 0.0813, 0.0469]) \n",
      "Test Loss tensor([0.0925, 0.0234, 0.1176, 0.0703, 0.0809, 0.0487])\n",
      "\n",
      "\n",
      "************** Batch 776 in 0.7074506282806396 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0915, 0.0280, 0.1171, 0.0733, 0.0739, 0.0443]) \n",
      "Test Loss tensor([0.0943, 0.0246, 0.1193, 0.0700, 0.0796, 0.0476])\n",
      "\n",
      "\n",
      "************** Batch 780 in 0.6345937252044678 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0926, 0.0256, 0.1187, 0.0620, 0.0814, 0.0492]) \n",
      "Test Loss tensor([0.0965, 0.0229, 0.1163, 0.0694, 0.0789, 0.0498])\n",
      "\n",
      "\n",
      "************** Batch 784 in 0.688103437423706 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1004, 0.0239, 0.1172, 0.0787, 0.0769, 0.0496]) \n",
      "Test Loss tensor([0.0984, 0.0229, 0.1159, 0.0716, 0.0781, 0.0472])\n",
      "\n",
      "\n",
      "************** Batch 788 in 0.65104079246521 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0910, 0.0203, 0.1247, 0.0653, 0.0745, 0.0446]) \n",
      "Test Loss tensor([0.0958, 0.0241, 0.1165, 0.0709, 0.0791, 0.0478])\n",
      "\n",
      "\n",
      "************** Batch 792 in 0.6127214431762695 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0952, 0.0253, 0.1204, 0.0682, 0.0799, 0.0451]) \n",
      "Test Loss tensor([0.0950, 0.0223, 0.1176, 0.0724, 0.0776, 0.0466])\n",
      "\n",
      "\n",
      "************** Batch 796 in 0.6310696601867676 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0899, 0.0256, 0.1225, 0.0857, 0.0807, 0.0537]) \n",
      "Test Loss tensor([0.0959, 0.0232, 0.1199, 0.0719, 0.0807, 0.0464])\n",
      "\n",
      "\n",
      "************** Batch 800 in 0.6302814483642578 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1033, 0.0279, 0.1186, 0.0790, 0.0737, 0.0431]) \n",
      "Test Loss tensor([0.0959, 0.0242, 0.1170, 0.0696, 0.0779, 0.0468])\n",
      "\n",
      "\n",
      "************** Batch 804 in 0.6545224189758301 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0249, 0.1143, 0.0749, 0.0805, 0.0455]) \n",
      "Test Loss tensor([0.0955, 0.0227, 0.1182, 0.0709, 0.0771, 0.0484])\n",
      "\n",
      "\n",
      "************** Batch 808 in 0.5549666881561279 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0965, 0.0204, 0.1190, 0.0693, 0.0794, 0.0444]) \n",
      "Test Loss tensor([0.0939, 0.0237, 0.1174, 0.0685, 0.0773, 0.0469])\n",
      "\n",
      "\n",
      "************** Batch 812 in 0.6272835731506348 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0884, 0.0235, 0.1200, 0.0638, 0.0821, 0.0428]) \n",
      "Test Loss tensor([0.0981, 0.0240, 0.1174, 0.0711, 0.0791, 0.0455])\n",
      "\n",
      "\n",
      "************** Batch 816 in 0.6432454586029053 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0972, 0.0231, 0.1241, 0.0712, 0.0797, 0.0405]) \n",
      "Test Loss tensor([0.0967, 0.0218, 0.1155, 0.0705, 0.0812, 0.0462])\n",
      "\n",
      "\n",
      "************** Batch 820 in 0.6533091068267822 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0973, 0.0219, 0.1187, 0.0644, 0.0794, 0.0483]) \n",
      "Test Loss tensor([0.0951, 0.0236, 0.1167, 0.0697, 0.0794, 0.0461])\n",
      "\n",
      "\n",
      "************** Batch 824 in 0.6837620735168457 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0904, 0.0204, 0.1192, 0.0737, 0.0829, 0.0494]) \n",
      "Test Loss tensor([0.0960, 0.0232, 0.1189, 0.0744, 0.0808, 0.0477])\n",
      "\n",
      "\n",
      "************** Batch 828 in 0.6693694591522217 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0230, 0.1278, 0.0692, 0.0786, 0.0433]) \n",
      "Test Loss tensor([0.0934, 0.0237, 0.1174, 0.0713, 0.0774, 0.0463])\n",
      "\n",
      "\n",
      "************** Batch 832 in 0.6559481620788574 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0958, 0.0244, 0.1196, 0.0730, 0.0739, 0.0462]) \n",
      "Test Loss tensor([0.0958, 0.0247, 0.1161, 0.0706, 0.0780, 0.0451])\n",
      "\n",
      "\n",
      "************** Batch 836 in 0.6942687034606934 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0980, 0.0201, 0.1156, 0.0781, 0.0744, 0.0421]) \n",
      "Test Loss tensor([0.0964, 0.0232, 0.1158, 0.0711, 0.0798, 0.0462])\n",
      "\n",
      "\n",
      "************** Batch 840 in 0.6876649856567383 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0984, 0.0212, 0.1215, 0.0690, 0.0789, 0.0475]) \n",
      "Test Loss tensor([0.0929, 0.0227, 0.1147, 0.0685, 0.0783, 0.0458])\n",
      "\n",
      "\n",
      "************** Batch 844 in 0.6807491779327393 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0958, 0.0236, 0.1159, 0.0728, 0.0803, 0.0423]) \n",
      "Test Loss tensor([0.0964, 0.0234, 0.1175, 0.0716, 0.0807, 0.0471])\n",
      "\n",
      "\n",
      "************** Batch 848 in 0.6640949249267578 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0998, 0.0308, 0.1154, 0.0707, 0.0818, 0.0432]) \n",
      "Test Loss tensor([0.0935, 0.0232, 0.1188, 0.0723, 0.0793, 0.0448])\n",
      "\n",
      "\n",
      "************** Batch 852 in 0.638324499130249 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0964, 0.0215, 0.1146, 0.0750, 0.0721, 0.0488]) \n",
      "Test Loss tensor([0.0992, 0.0224, 0.1190, 0.0706, 0.0767, 0.0455])\n",
      "\n",
      "\n",
      "************** Batch 856 in 0.6031157970428467 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0259, 0.1102, 0.0686, 0.0751, 0.0429]) \n",
      "Test Loss tensor([0.0963, 0.0239, 0.1192, 0.0733, 0.0771, 0.0460])\n",
      "\n",
      "\n",
      "************** Batch 860 in 0.6584048271179199 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1003, 0.0282, 0.1208, 0.0710, 0.0816, 0.0469]) \n",
      "Test Loss tensor([0.0957, 0.0234, 0.1157, 0.0706, 0.0789, 0.0451])\n",
      "\n",
      "\n",
      "************** Batch 864 in 0.7199478149414062 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0993, 0.0275, 0.1170, 0.0673, 0.0805, 0.0495]) \n",
      "Test Loss tensor([0.0942, 0.0236, 0.1168, 0.0708, 0.0773, 0.0461])\n",
      "\n",
      "\n",
      "************** Batch 868 in 0.6191325187683105 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1100, 0.0296, 0.1207, 0.0685, 0.0801, 0.0444]) \n",
      "Test Loss tensor([0.0932, 0.0223, 0.1176, 0.0689, 0.0787, 0.0461])\n",
      "\n",
      "\n",
      "************** Batch 872 in 0.6534380912780762 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1033, 0.0203, 0.1235, 0.0661, 0.0776, 0.0455]) \n",
      "Test Loss tensor([0.0923, 0.0235, 0.1174, 0.0716, 0.0755, 0.0456])\n",
      "\n",
      "\n",
      "************** Batch 876 in 0.6018426418304443 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0749, 0.0147, 0.0969, 0.0508, 0.0554, 0.0335]) \n",
      "Test Loss tensor([0.0978, 0.0226, 0.1138, 0.0708, 0.0786, 0.0441])\n",
      "\n",
      "\n",
      "************** Batch 0 in 0.6183593273162842 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0932, 0.0261, 0.1167, 0.0811, 0.0742, 0.0484]) \n",
      "Test Loss tensor([0.0951, 0.0250, 0.1172, 0.0710, 0.0780, 0.0465])\n",
      "\n",
      "\n",
      "************** Batch 4 in 0.6091234683990479 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0969, 0.0224, 0.1134, 0.0682, 0.0778, 0.0462]) \n",
      "Test Loss tensor([0.0975, 0.0232, 0.1176, 0.0699, 0.0782, 0.0458])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 8 in 0.5798180103302002 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0915, 0.0201, 0.1230, 0.0711, 0.0756, 0.0477]) \n",
      "Test Loss tensor([0.0970, 0.0222, 0.1162, 0.0694, 0.0770, 0.0471])\n",
      "\n",
      "\n",
      "************** Batch 12 in 0.5661907196044922 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0235, 0.1204, 0.0708, 0.0737, 0.0447]) \n",
      "Test Loss tensor([0.0948, 0.0222, 0.1136, 0.0709, 0.0763, 0.0453])\n",
      "\n",
      "\n",
      "************** Batch 16 in 0.5657241344451904 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0935, 0.0214, 0.1258, 0.0689, 0.0740, 0.0499]) \n",
      "Test Loss tensor([0.0971, 0.0240, 0.1164, 0.0710, 0.0784, 0.0461])\n",
      "\n",
      "\n",
      "************** Batch 20 in 0.5662422180175781 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0248, 0.1155, 0.0755, 0.0792, 0.0449]) \n",
      "Test Loss tensor([0.0945, 0.0223, 0.1165, 0.0692, 0.0773, 0.0468])\n",
      "\n",
      "\n",
      "************** Batch 24 in 0.5894591808319092 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0903, 0.0214, 0.1076, 0.0764, 0.0740, 0.0446]) \n",
      "Test Loss tensor([0.0928, 0.0227, 0.1166, 0.0706, 0.0777, 0.0462])\n",
      "\n",
      "\n",
      "************** Batch 28 in 0.5709953308105469 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0945, 0.0285, 0.1182, 0.0711, 0.0767, 0.0421]) \n",
      "Test Loss tensor([0.0943, 0.0242, 0.1186, 0.0709, 0.0775, 0.0461])\n",
      "\n",
      "\n",
      "************** Batch 32 in 0.5850627422332764 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0879, 0.0271, 0.1173, 0.0685, 0.0739, 0.0447]) \n",
      "Test Loss tensor([0.0941, 0.0230, 0.1183, 0.0701, 0.0767, 0.0452])\n",
      "\n",
      "\n",
      "************** Batch 36 in 0.5859475135803223 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0256, 0.1129, 0.0753, 0.0740, 0.0439]) \n",
      "Test Loss tensor([0.0973, 0.0226, 0.1164, 0.0704, 0.0798, 0.0472])\n",
      "\n",
      "\n",
      "************** Batch 40 in 0.5855700969696045 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0960, 0.0220, 0.1170, 0.0609, 0.0725, 0.0454]) \n",
      "Test Loss tensor([0.0928, 0.0220, 0.1185, 0.0718, 0.0795, 0.0453])\n",
      "\n",
      "\n",
      "************** Batch 44 in 0.5784912109375 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0917, 0.0251, 0.1203, 0.0723, 0.0821, 0.0426]) \n",
      "Test Loss tensor([0.0920, 0.0226, 0.1158, 0.0698, 0.0778, 0.0451])\n",
      "\n",
      "\n",
      "************** Batch 48 in 0.6111013889312744 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0990, 0.0243, 0.1248, 0.0763, 0.0799, 0.0421]) \n",
      "Test Loss tensor([0.0967, 0.0239, 0.1177, 0.0724, 0.0787, 0.0457])\n",
      "\n",
      "\n",
      "************** Batch 52 in 0.722611665725708 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1003, 0.0244, 0.1196, 0.0737, 0.0851, 0.0471]) \n",
      "Test Loss tensor([0.0957, 0.0232, 0.1162, 0.0718, 0.0779, 0.0458])\n",
      "\n",
      "\n",
      "************** Batch 56 in 0.6874127388000488 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0988, 0.0263, 0.1190, 0.0642, 0.0786, 0.0415]) \n",
      "Test Loss tensor([0.0937, 0.0238, 0.1169, 0.0709, 0.0800, 0.0486])\n",
      "\n",
      "\n",
      "************** Batch 60 in 0.6399037837982178 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1009, 0.0251, 0.1185, 0.0722, 0.0842, 0.0508]) \n",
      "Test Loss tensor([0.0940, 0.0226, 0.1181, 0.0703, 0.0768, 0.0457])\n",
      "\n",
      "\n",
      "************** Batch 64 in 0.643498420715332 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1074, 0.0249, 0.1169, 0.0734, 0.0825, 0.0454]) \n",
      "Test Loss tensor([0.0945, 0.0239, 0.1184, 0.0689, 0.0795, 0.0457])\n",
      "\n",
      "\n",
      "************** Batch 68 in 0.6231045722961426 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0984, 0.0249, 0.1200, 0.0704, 0.0764, 0.0430]) \n",
      "Test Loss tensor([0.0970, 0.0252, 0.1168, 0.0713, 0.0779, 0.0467])\n",
      "\n",
      "\n",
      "************** Batch 72 in 0.6277167797088623 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0957, 0.0263, 0.1077, 0.0803, 0.0741, 0.0455]) \n",
      "Test Loss tensor([0.0946, 0.0226, 0.1163, 0.0698, 0.0784, 0.0451])\n",
      "\n",
      "\n",
      "************** Batch 76 in 0.5822174549102783 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0947, 0.0218, 0.1171, 0.0627, 0.0751, 0.0486]) \n",
      "Test Loss tensor([0.0963, 0.0230, 0.1151, 0.0691, 0.0820, 0.0458])\n",
      "\n",
      "\n",
      "************** Batch 80 in 0.5846457481384277 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0981, 0.0263, 0.1158, 0.0791, 0.0802, 0.0443]) \n",
      "Test Loss tensor([0.0951, 0.0232, 0.1183, 0.0685, 0.0780, 0.0461])\n",
      "\n",
      "\n",
      "************** Batch 84 in 0.5742752552032471 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1014, 0.0199, 0.1296, 0.0731, 0.0756, 0.0518]) \n",
      "Test Loss tensor([0.0960, 0.0232, 0.1168, 0.0703, 0.0802, 0.0446])\n",
      "\n",
      "\n",
      "************** Batch 88 in 0.5781514644622803 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0948, 0.0245, 0.1124, 0.0693, 0.0774, 0.0431]) \n",
      "Test Loss tensor([0.0955, 0.0223, 0.1161, 0.0707, 0.0778, 0.0466])\n",
      "\n",
      "\n",
      "************** Batch 92 in 0.5635089874267578 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0981, 0.0275, 0.1133, 0.0662, 0.0775, 0.0447]) \n",
      "Test Loss tensor([0.0949, 0.0234, 0.1153, 0.0694, 0.0782, 0.0465])\n",
      "\n",
      "\n",
      "************** Batch 96 in 0.5737414360046387 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0993, 0.0261, 0.1210, 0.0701, 0.0729, 0.0470]) \n",
      "Test Loss tensor([0.0946, 0.0227, 0.1186, 0.0716, 0.0797, 0.0450])\n",
      "\n",
      "\n",
      "************** Batch 100 in 0.5639498233795166 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0967, 0.0225, 0.1232, 0.0693, 0.0799, 0.0453]) \n",
      "Test Loss tensor([0.0991, 0.0226, 0.1162, 0.0714, 0.0789, 0.0455])\n",
      "\n",
      "\n",
      "************** Batch 104 in 0.5714125633239746 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0972, 0.0235, 0.1168, 0.0645, 0.0830, 0.0463]) \n",
      "Test Loss tensor([0.0945, 0.0221, 0.1137, 0.0687, 0.0808, 0.0464])\n",
      "\n",
      "\n",
      "************** Batch 108 in 0.5995337963104248 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0901, 0.0194, 0.1191, 0.0682, 0.0787, 0.0431]) \n",
      "Test Loss tensor([0.0941, 0.0226, 0.1133, 0.0693, 0.0777, 0.0447])\n",
      "\n",
      "\n",
      "************** Batch 112 in 0.5719668865203857 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0938, 0.0229, 0.1147, 0.0735, 0.0783, 0.0422]) \n",
      "Test Loss tensor([0.0952, 0.0229, 0.1171, 0.0715, 0.0809, 0.0492])\n",
      "\n",
      "\n",
      "************** Batch 116 in 0.5753836631774902 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0911, 0.0172, 0.1091, 0.0653, 0.0772, 0.0517]) \n",
      "Test Loss tensor([0.0967, 0.0240, 0.1162, 0.0715, 0.0797, 0.0438])\n",
      "\n",
      "\n",
      "************** Batch 120 in 0.555513858795166 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0882, 0.0213, 0.1154, 0.0666, 0.0773, 0.0504]) \n",
      "Test Loss tensor([0.0974, 0.0219, 0.1158, 0.0703, 0.0822, 0.0460])\n",
      "\n",
      "\n",
      "************** Batch 124 in 0.6789898872375488 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1013, 0.0255, 0.1155, 0.0798, 0.0881, 0.0409]) \n",
      "Test Loss tensor([0.0973, 0.0222, 0.1169, 0.0708, 0.0784, 0.0463])\n",
      "\n",
      "\n",
      "************** Batch 128 in 0.6907401084899902 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1040, 0.0264, 0.1158, 0.0735, 0.0824, 0.0480]) \n",
      "Test Loss tensor([0.0948, 0.0226, 0.1164, 0.0710, 0.0761, 0.0449])\n",
      "\n",
      "\n",
      "************** Batch 132 in 0.6791753768920898 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0994, 0.0228, 0.1298, 0.0707, 0.0768, 0.0515]) \n",
      "Test Loss tensor([0.1013, 0.0240, 0.1173, 0.0705, 0.0790, 0.0465])\n",
      "\n",
      "\n",
      "************** Batch 136 in 0.6836948394775391 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0956, 0.0238, 0.1071, 0.0711, 0.0714, 0.0419]) \n",
      "Test Loss tensor([0.0931, 0.0218, 0.1154, 0.0690, 0.0784, 0.0443])\n",
      "\n",
      "\n",
      "************** Batch 140 in 0.61643385887146 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0942, 0.0267, 0.1141, 0.0694, 0.0758, 0.0479]) \n",
      "Test Loss tensor([0.0974, 0.0228, 0.1160, 0.0693, 0.0789, 0.0465])\n",
      "\n",
      "\n",
      "************** Batch 144 in 0.6585109233856201 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0973, 0.0245, 0.1182, 0.0694, 0.0810, 0.0446]) \n",
      "Test Loss tensor([0.0948, 0.0226, 0.1166, 0.0694, 0.0757, 0.0463])\n",
      "\n",
      "\n",
      "************** Batch 148 in 0.6952855587005615 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0935, 0.0234, 0.1112, 0.0755, 0.0767, 0.0475]) \n",
      "Test Loss tensor([0.0942, 0.0219, 0.1162, 0.0660, 0.0768, 0.0466])\n",
      "\n",
      "\n",
      "************** Batch 152 in 0.6790776252746582 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1010, 0.0215, 0.1208, 0.0758, 0.0778, 0.0468]) \n",
      "Test Loss tensor([0.0965, 0.0229, 0.1141, 0.0697, 0.0800, 0.0465])\n",
      "\n",
      "\n",
      "************** Batch 156 in 0.6169853210449219 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1016, 0.0231, 0.1205, 0.0755, 0.0797, 0.0384]) \n",
      "Test Loss tensor([0.0953, 0.0223, 0.1161, 0.0699, 0.0766, 0.0456])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 160 in 0.6170213222503662 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1020, 0.0190, 0.1111, 0.0730, 0.0745, 0.0413]) \n",
      "Test Loss tensor([0.0963, 0.0217, 0.1167, 0.0679, 0.0772, 0.0476])\n",
      "\n",
      "\n",
      "************** Batch 164 in 0.5985832214355469 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0961, 0.0257, 0.1242, 0.0756, 0.0781, 0.0436]) \n",
      "Test Loss tensor([0.0939, 0.0214, 0.1147, 0.0697, 0.0763, 0.0458])\n",
      "\n",
      "\n",
      "************** Batch 168 in 0.6251144409179688 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0968, 0.0252, 0.1190, 0.0680, 0.0807, 0.0441]) \n",
      "Test Loss tensor([0.0964, 0.0226, 0.1137, 0.0699, 0.0817, 0.0480])\n",
      "\n",
      "\n",
      "************** Batch 172 in 0.6123957633972168 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0939, 0.0272, 0.1263, 0.0676, 0.0835, 0.0495]) \n",
      "Test Loss tensor([0.0961, 0.0215, 0.1164, 0.0705, 0.0789, 0.0452])\n",
      "\n",
      "\n",
      "************** Batch 176 in 0.5746612548828125 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0919, 0.0227, 0.1259, 0.0653, 0.0792, 0.0520]) \n",
      "Test Loss tensor([0.0979, 0.0243, 0.1160, 0.0697, 0.0784, 0.0441])\n",
      "\n",
      "\n",
      "************** Batch 180 in 0.6829488277435303 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0950, 0.0218, 0.1175, 0.0785, 0.0820, 0.0431]) \n",
      "Test Loss tensor([0.0989, 0.0229, 0.1143, 0.0709, 0.0792, 0.0475])\n",
      "\n",
      "\n",
      "************** Batch 184 in 0.6986262798309326 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1004, 0.0213, 0.1176, 0.0751, 0.0735, 0.0427]) \n",
      "Test Loss tensor([0.0932, 0.0215, 0.1167, 0.0713, 0.0773, 0.0452])\n",
      "\n",
      "\n",
      "************** Batch 188 in 0.6452734470367432 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0954, 0.0267, 0.1194, 0.0703, 0.0790, 0.0422]) \n",
      "Test Loss tensor([0.0987, 0.0227, 0.1177, 0.0698, 0.0824, 0.0465])\n",
      "\n",
      "\n",
      "************** Batch 192 in 0.6289863586425781 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0971, 0.0207, 0.1192, 0.0691, 0.0826, 0.0494]) \n",
      "Test Loss tensor([0.0955, 0.0218, 0.1171, 0.0698, 0.0778, 0.0453])\n",
      "\n",
      "\n",
      "************** Batch 196 in 0.582625150680542 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0248, 0.1131, 0.0693, 0.0736, 0.0438]) \n",
      "Test Loss tensor([0.0939, 0.0243, 0.1157, 0.0687, 0.0834, 0.0480])\n",
      "\n",
      "\n",
      "************** Batch 200 in 0.6524758338928223 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1005, 0.0236, 0.1236, 0.0724, 0.0793, 0.0487]) \n",
      "Test Loss tensor([0.0971, 0.0225, 0.1152, 0.0682, 0.0770, 0.0434])\n",
      "\n",
      "\n",
      "************** Batch 204 in 0.620600700378418 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1043, 0.0273, 0.1199, 0.0771, 0.0797, 0.0470]) \n",
      "Test Loss tensor([0.0971, 0.0217, 0.1156, 0.0673, 0.0792, 0.0458])\n",
      "\n",
      "\n",
      "************** Batch 208 in 0.5790488719940186 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0980, 0.0281, 0.1072, 0.0686, 0.0809, 0.0455]) \n",
      "Test Loss tensor([0.0930, 0.0230, 0.1164, 0.0692, 0.0814, 0.0442])\n",
      "\n",
      "\n",
      "************** Batch 212 in 0.584770917892456 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0957, 0.0273, 0.1315, 0.0731, 0.0859, 0.0473]) \n",
      "Test Loss tensor([0.0946, 0.0222, 0.1146, 0.0702, 0.0775, 0.0438])\n",
      "\n",
      "\n",
      "************** Batch 216 in 0.6005291938781738 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0994, 0.0243, 0.1147, 0.0724, 0.0819, 0.0460]) \n",
      "Test Loss tensor([0.0946, 0.0221, 0.1158, 0.0695, 0.0814, 0.0469])\n",
      "\n",
      "\n",
      "************** Batch 220 in 0.568960428237915 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0966, 0.0191, 0.1136, 0.0652, 0.0784, 0.0520]) \n",
      "Test Loss tensor([0.0957, 0.0230, 0.1150, 0.0685, 0.0776, 0.0433])\n",
      "\n",
      "\n",
      "************** Batch 224 in 0.5658550262451172 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0880, 0.0201, 0.1122, 0.0640, 0.0742, 0.0474]) \n",
      "Test Loss tensor([0.0952, 0.0246, 0.1146, 0.0700, 0.0805, 0.0486])\n",
      "\n",
      "\n",
      "************** Batch 228 in 0.5632743835449219 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0989, 0.0258, 0.1200, 0.0778, 0.0776, 0.0458]) \n",
      "Test Loss tensor([0.0930, 0.0230, 0.1145, 0.0693, 0.0758, 0.0438])\n",
      "\n",
      "\n",
      "************** Batch 232 in 0.5839419364929199 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0958, 0.0269, 0.1048, 0.0695, 0.0799, 0.0412]) \n",
      "Test Loss tensor([0.0978, 0.0219, 0.1128, 0.0690, 0.0793, 0.0441])\n",
      "\n",
      "\n",
      "************** Batch 236 in 0.5559887886047363 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1007, 0.0236, 0.1175, 0.0807, 0.0791, 0.0436]) \n",
      "Test Loss tensor([0.0946, 0.0227, 0.1154, 0.0700, 0.0779, 0.0444])\n",
      "\n",
      "\n",
      "************** Batch 240 in 0.5824398994445801 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0985, 0.0241, 0.1210, 0.0720, 0.0768, 0.0457]) \n",
      "Test Loss tensor([0.0967, 0.0240, 0.1159, 0.0737, 0.0801, 0.0455])\n",
      "\n",
      "\n",
      "************** Batch 244 in 0.5714719295501709 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0939, 0.0238, 0.1156, 0.0688, 0.0769, 0.0501]) \n",
      "Test Loss tensor([0.0948, 0.0233, 0.1174, 0.0708, 0.0817, 0.0464])\n",
      "\n",
      "\n",
      "************** Batch 248 in 0.5946445465087891 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0954, 0.0276, 0.1211, 0.0680, 0.0835, 0.0468]) \n",
      "Test Loss tensor([0.0933, 0.0223, 0.1147, 0.0700, 0.0804, 0.0471])\n",
      "\n",
      "\n",
      "************** Batch 252 in 0.5716302394866943 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0942, 0.0208, 0.1181, 0.0742, 0.0756, 0.0440]) \n",
      "Test Loss tensor([0.1010, 0.0237, 0.1152, 0.0706, 0.0819, 0.0471])\n",
      "\n",
      "\n",
      "************** Batch 256 in 0.5571353435516357 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0995, 0.0205, 0.1164, 0.0759, 0.0829, 0.0463]) \n",
      "Test Loss tensor([0.0922, 0.0241, 0.1180, 0.0685, 0.0786, 0.0443])\n",
      "\n",
      "\n",
      "************** Batch 260 in 0.5885891914367676 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0932, 0.0243, 0.1172, 0.0697, 0.0820, 0.0451]) \n",
      "Test Loss tensor([0.1004, 0.0233, 0.1159, 0.0720, 0.0821, 0.0498])\n",
      "\n",
      "\n",
      "************** Batch 264 in 0.5543117523193359 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0954, 0.0219, 0.1149, 0.0713, 0.0750, 0.0487]) \n",
      "Test Loss tensor([0.0945, 0.0237, 0.1133, 0.0699, 0.0772, 0.0432])\n",
      "\n",
      "\n",
      "************** Batch 268 in 0.5671465396881104 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0242, 0.1180, 0.0756, 0.0799, 0.0420]) \n",
      "Test Loss tensor([0.0943, 0.0247, 0.1144, 0.0710, 0.0828, 0.0458])\n",
      "\n",
      "\n",
      "************** Batch 272 in 0.5765368938446045 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0922, 0.0241, 0.1247, 0.0699, 0.0892, 0.0484]) \n",
      "Test Loss tensor([0.0950, 0.0214, 0.1181, 0.0716, 0.0769, 0.0433])\n",
      "\n",
      "\n",
      "************** Batch 276 in 0.5629312992095947 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0954, 0.0217, 0.1111, 0.0750, 0.0737, 0.0454]) \n",
      "Test Loss tensor([0.0966, 0.0220, 0.1162, 0.0693, 0.0809, 0.0467])\n",
      "\n",
      "\n",
      "************** Batch 280 in 0.5703353881835938 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0991, 0.0245, 0.1222, 0.0698, 0.0794, 0.0475]) \n",
      "Test Loss tensor([0.0908, 0.0216, 0.1137, 0.0687, 0.0783, 0.0438])\n",
      "\n",
      "\n",
      "************** Batch 284 in 0.566777229309082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0871, 0.0204, 0.1168, 0.0699, 0.0776, 0.0499]) \n",
      "Test Loss tensor([0.0977, 0.0239, 0.1159, 0.0707, 0.0804, 0.0458])\n",
      "\n",
      "\n",
      "************** Batch 288 in 0.6011195182800293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1036, 0.0227, 0.1102, 0.0690, 0.0803, 0.0444]) \n",
      "Test Loss tensor([0.0937, 0.0229, 0.1162, 0.0692, 0.0777, 0.0462])\n",
      "\n",
      "\n",
      "************** Batch 292 in 0.5680572986602783 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0982, 0.0252, 0.1193, 0.0789, 0.0817, 0.0455]) \n",
      "Test Loss tensor([0.0968, 0.0223, 0.1149, 0.0708, 0.0799, 0.0453])\n",
      "\n",
      "\n",
      "************** Batch 296 in 0.5816822052001953 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0900, 0.0246, 0.1088, 0.0690, 0.0848, 0.0458]) \n",
      "Test Loss tensor([0.0935, 0.0219, 0.1152, 0.0700, 0.0778, 0.0433])\n",
      "\n",
      "\n",
      "************** Batch 300 in 0.5742793083190918 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0995, 0.0269, 0.1146, 0.0657, 0.0790, 0.0469]) \n",
      "Test Loss tensor([0.0949, 0.0233, 0.1134, 0.0697, 0.0798, 0.0444])\n",
      "\n",
      "\n",
      "************** Batch 304 in 0.56671142578125 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0988, 0.0185, 0.1236, 0.0648, 0.0746, 0.0473]) \n",
      "Test Loss tensor([0.0961, 0.0211, 0.1138, 0.0695, 0.0774, 0.0451])\n",
      "\n",
      "\n",
      "************** Batch 308 in 0.5874831676483154 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0970, 0.0214, 0.1122, 0.0680, 0.0791, 0.0449]) \n",
      "Test Loss tensor([0.0988, 0.0233, 0.1132, 0.0715, 0.0782, 0.0427])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 312 in 0.5685670375823975 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0937, 0.0207, 0.1209, 0.0719, 0.0765, 0.0444]) \n",
      "Test Loss tensor([0.0945, 0.0229, 0.1153, 0.0714, 0.0797, 0.0453])\n",
      "\n",
      "\n",
      "************** Batch 316 in 0.5908834934234619 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1045, 0.0243, 0.1193, 0.0751, 0.0819, 0.0415]) \n",
      "Test Loss tensor([0.0926, 0.0226, 0.1162, 0.0722, 0.0804, 0.0456])\n",
      "\n",
      "\n",
      "************** Batch 320 in 0.5678215026855469 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0935, 0.0225, 0.1146, 0.0671, 0.0776, 0.0486]) \n",
      "Test Loss tensor([0.0957, 0.0234, 0.1153, 0.0700, 0.0812, 0.0448])\n",
      "\n",
      "\n",
      "************** Batch 324 in 0.5831897258758545 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1033, 0.0187, 0.1187, 0.0706, 0.0863, 0.0516]) \n",
      "Test Loss tensor([0.0947, 0.0222, 0.1154, 0.0723, 0.0792, 0.0440])\n",
      "\n",
      "\n",
      "************** Batch 328 in 0.6186606884002686 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0972, 0.0220, 0.1182, 0.0764, 0.0822, 0.0450]) \n",
      "Test Loss tensor([0.0979, 0.0240, 0.1158, 0.0700, 0.0800, 0.0458])\n",
      "\n",
      "\n",
      "************** Batch 332 in 0.5792231559753418 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0976, 0.0208, 0.1100, 0.0687, 0.0809, 0.0418]) \n",
      "Test Loss tensor([0.0914, 0.0233, 0.1124, 0.0694, 0.0756, 0.0438])\n",
      "\n",
      "\n",
      "************** Batch 336 in 0.5972509384155273 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1024, 0.0237, 0.1103, 0.0747, 0.0797, 0.0415]) \n",
      "Test Loss tensor([0.0980, 0.0216, 0.1122, 0.0663, 0.0801, 0.0449])\n",
      "\n",
      "\n",
      "************** Batch 340 in 0.5724215507507324 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0904, 0.0245, 0.1198, 0.0651, 0.0796, 0.0473]) \n",
      "Test Loss tensor([0.0936, 0.0227, 0.1155, 0.0687, 0.0752, 0.0434])\n",
      "\n",
      "\n",
      "************** Batch 344 in 0.5896823406219482 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0881, 0.0228, 0.1111, 0.0659, 0.0763, 0.0409]) \n",
      "Test Loss tensor([0.0943, 0.0229, 0.1122, 0.0696, 0.0833, 0.0469])\n",
      "\n",
      "\n",
      "************** Batch 348 in 0.5836420059204102 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0957, 0.0244, 0.1128, 0.0721, 0.0866, 0.0480]) \n",
      "Test Loss tensor([0.0939, 0.0214, 0.1120, 0.0686, 0.0763, 0.0429])\n",
      "\n",
      "\n",
      "************** Batch 352 in 0.5652587413787842 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0874, 0.0247, 0.1129, 0.0710, 0.0774, 0.0364]) \n",
      "Test Loss tensor([0.0973, 0.0218, 0.1156, 0.0701, 0.0788, 0.0460])\n",
      "\n",
      "\n",
      "************** Batch 356 in 0.585946798324585 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0969, 0.0252, 0.1125, 0.0743, 0.0759, 0.0528]) \n",
      "Test Loss tensor([0.0937, 0.0229, 0.1141, 0.0700, 0.0792, 0.0425])\n",
      "\n",
      "\n",
      "************** Batch 360 in 0.5621950626373291 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0932, 0.0179, 0.1227, 0.0629, 0.0741, 0.0409]) \n",
      "Test Loss tensor([0.0949, 0.0219, 0.1137, 0.0719, 0.0759, 0.0429])\n",
      "\n",
      "\n",
      "************** Batch 364 in 0.5838918685913086 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1067, 0.0277, 0.1098, 0.0721, 0.0801, 0.0392]) \n",
      "Test Loss tensor([0.0935, 0.0219, 0.1121, 0.0696, 0.0780, 0.0442])\n",
      "\n",
      "\n",
      "************** Batch 368 in 0.5624673366546631 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0932, 0.0196, 0.1116, 0.0724, 0.0833, 0.0411]) \n",
      "Test Loss tensor([0.0908, 0.0234, 0.1144, 0.0714, 0.0775, 0.0425])\n",
      "\n",
      "\n",
      "************** Batch 372 in 0.5995283126831055 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0946, 0.0197, 0.1141, 0.0679, 0.0802, 0.0437]) \n",
      "Test Loss tensor([0.0984, 0.0237, 0.1136, 0.0733, 0.0770, 0.0426])\n",
      "\n",
      "\n",
      "************** Batch 376 in 0.583441972732544 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1015, 0.0253, 0.1140, 0.0709, 0.0748, 0.0406]) \n",
      "Test Loss tensor([0.0935, 0.0228, 0.1160, 0.0690, 0.0755, 0.0434])\n",
      "\n",
      "\n",
      "************** Batch 380 in 0.573472261428833 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0914, 0.0238, 0.1149, 0.0691, 0.0761, 0.0413]) \n",
      "Test Loss tensor([0.0953, 0.0218, 0.1137, 0.0696, 0.0763, 0.0436])\n",
      "\n",
      "\n",
      "************** Batch 384 in 0.5783145427703857 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0908, 0.0254, 0.1158, 0.0643, 0.0775, 0.0409]) \n",
      "Test Loss tensor([0.0950, 0.0234, 0.1137, 0.0711, 0.0756, 0.0434])\n",
      "\n",
      "\n",
      "************** Batch 388 in 0.5807797908782959 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0970, 0.0248, 0.1169, 0.0723, 0.0758, 0.0488]) \n",
      "Test Loss tensor([0.0937, 0.0219, 0.1156, 0.0698, 0.0764, 0.0440])\n",
      "\n",
      "\n",
      "************** Batch 392 in 0.5846571922302246 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0911, 0.0249, 0.1141, 0.0703, 0.0728, 0.0407]) \n",
      "Test Loss tensor([0.0933, 0.0225, 0.1144, 0.0701, 0.0758, 0.0421])\n",
      "\n",
      "\n",
      "************** Batch 396 in 0.6357111930847168 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0955, 0.0229, 0.1200, 0.0704, 0.0774, 0.0446]) \n",
      "Test Loss tensor([0.0965, 0.0224, 0.1140, 0.0704, 0.0763, 0.0434])\n",
      "\n",
      "\n",
      "************** Batch 400 in 0.5869541168212891 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0945, 0.0194, 0.1136, 0.0689, 0.0761, 0.0406]) \n",
      "Test Loss tensor([0.0933, 0.0225, 0.1152, 0.0704, 0.0759, 0.0432])\n",
      "\n",
      "\n",
      "************** Batch 404 in 0.5829274654388428 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0217, 0.1114, 0.0683, 0.0755, 0.0376]) \n",
      "Test Loss tensor([0.0901, 0.0213, 0.1154, 0.0704, 0.0778, 0.0443])\n",
      "\n",
      "\n",
      "************** Batch 408 in 0.5618083477020264 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0996, 0.0285, 0.1214, 0.0798, 0.0757, 0.0409]) \n",
      "Test Loss tensor([0.0922, 0.0224, 0.1134, 0.0698, 0.0775, 0.0411])\n",
      "\n",
      "\n",
      "************** Batch 412 in 0.578080415725708 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0859, 0.0207, 0.1243, 0.0688, 0.0721, 0.0477]) \n",
      "Test Loss tensor([0.0939, 0.0244, 0.1138, 0.0704, 0.0771, 0.0427])\n",
      "\n",
      "\n",
      "************** Batch 416 in 0.5625178813934326 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0968, 0.0236, 0.1075, 0.0772, 0.0696, 0.0408]) \n",
      "Test Loss tensor([0.0901, 0.0237, 0.1124, 0.0709, 0.0769, 0.0426])\n",
      "\n",
      "\n",
      "************** Batch 420 in 0.6067337989807129 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0856, 0.0235, 0.1138, 0.0687, 0.0797, 0.0453]) \n",
      "Test Loss tensor([0.0923, 0.0239, 0.1156, 0.0708, 0.0772, 0.0435])\n",
      "\n",
      "\n",
      "************** Batch 424 in 0.6131205558776855 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0840, 0.0194, 0.1156, 0.0734, 0.0725, 0.0499]) \n",
      "Test Loss tensor([0.0992, 0.0224, 0.1148, 0.0705, 0.0768, 0.0421])\n",
      "\n",
      "\n",
      "************** Batch 428 in 0.5789885520935059 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1055, 0.0273, 0.1068, 0.0709, 0.0769, 0.0486]) \n",
      "Test Loss tensor([0.0963, 0.0224, 0.1141, 0.0704, 0.0761, 0.0428])\n",
      "\n",
      "\n",
      "************** Batch 432 in 0.5812034606933594 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0897, 0.0230, 0.1239, 0.0736, 0.0768, 0.0445]) \n",
      "Test Loss tensor([0.0936, 0.0226, 0.1121, 0.0714, 0.0777, 0.0430])\n",
      "\n",
      "\n",
      "************** Batch 436 in 0.5623502731323242 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0882, 0.0271, 0.1116, 0.0718, 0.0718, 0.0413]) \n",
      "Test Loss tensor([0.0925, 0.0231, 0.1136, 0.0697, 0.0782, 0.0436])\n",
      "\n",
      "\n",
      "************** Batch 440 in 0.5938379764556885 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0850, 0.0236, 0.1138, 0.0609, 0.0748, 0.0436]) \n",
      "Test Loss tensor([0.0902, 0.0214, 0.1125, 0.0702, 0.0752, 0.0444])\n",
      "\n",
      "\n",
      "************** Batch 444 in 0.5869660377502441 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1009, 0.0247, 0.1154, 0.0644, 0.0806, 0.0502]) \n",
      "Test Loss tensor([0.0918, 0.0230, 0.1137, 0.0699, 0.0754, 0.0426])\n",
      "\n",
      "\n",
      "************** Batch 448 in 0.5625965595245361 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0970, 0.0199, 0.0966, 0.0681, 0.0765, 0.0418]) \n",
      "Test Loss tensor([0.0954, 0.0228, 0.1132, 0.0720, 0.0782, 0.0440])\n",
      "\n",
      "\n",
      "************** Batch 452 in 0.5828349590301514 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0918, 0.0210, 0.1093, 0.0732, 0.0809, 0.0419]) \n",
      "Test Loss tensor([0.0941, 0.0234, 0.1157, 0.0715, 0.0767, 0.0432])\n",
      "\n",
      "\n",
      "************** Batch 456 in 0.5810501575469971 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0929, 0.0265, 0.1102, 0.0722, 0.0768, 0.0467]) \n",
      "Test Loss tensor([0.0932, 0.0221, 0.1134, 0.0686, 0.0775, 0.0441])\n",
      "\n",
      "\n",
      "************** Batch 460 in 0.5815041065216064 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0866, 0.0206, 0.1131, 0.0684, 0.0768, 0.0402]) \n",
      "Test Loss tensor([0.0925, 0.0226, 0.1154, 0.0690, 0.0747, 0.0424])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 464 in 0.5578956604003906 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0907, 0.0280, 0.1113, 0.0737, 0.0726, 0.0363]) \n",
      "Test Loss tensor([0.0909, 0.0226, 0.1132, 0.0680, 0.0770, 0.0427])\n",
      "\n",
      "\n",
      "************** Batch 468 in 0.5727269649505615 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0980, 0.0250, 0.1223, 0.0777, 0.0815, 0.0430]) \n",
      "Test Loss tensor([0.0911, 0.0228, 0.1143, 0.0703, 0.0769, 0.0430])\n",
      "\n",
      "\n",
      "************** Batch 472 in 0.5843195915222168 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0929, 0.0265, 0.1103, 0.0727, 0.0734, 0.0402]) \n",
      "Test Loss tensor([0.0940, 0.0232, 0.1110, 0.0697, 0.0770, 0.0423])\n",
      "\n",
      "\n",
      "************** Batch 476 in 0.5649387836456299 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0967, 0.0223, 0.1105, 0.0752, 0.0797, 0.0403]) \n",
      "Test Loss tensor([0.0949, 0.0237, 0.1143, 0.0696, 0.0778, 0.0433])\n",
      "\n",
      "\n",
      "************** Batch 480 in 0.5805470943450928 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0890, 0.0230, 0.1234, 0.0708, 0.0692, 0.0398]) \n",
      "Test Loss tensor([0.0945, 0.0232, 0.1104, 0.0697, 0.0794, 0.0419])\n",
      "\n",
      "\n",
      "************** Batch 484 in 0.568443775177002 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0874, 0.0227, 0.1124, 0.0631, 0.0718, 0.0385]) \n",
      "Test Loss tensor([0.0928, 0.0227, 0.1112, 0.0687, 0.0762, 0.0430])\n",
      "\n",
      "\n",
      "************** Batch 488 in 0.5944969654083252 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0987, 0.0254, 0.1108, 0.0795, 0.0790, 0.0450]) \n",
      "Test Loss tensor([0.0969, 0.0236, 0.1154, 0.0696, 0.0788, 0.0476])\n",
      "\n",
      "\n",
      "************** Batch 492 in 0.5652306079864502 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0963, 0.0243, 0.1062, 0.0760, 0.0794, 0.0476]) \n",
      "Test Loss tensor([0.0938, 0.0223, 0.1139, 0.0692, 0.0772, 0.0428])\n",
      "\n",
      "\n",
      "************** Batch 496 in 0.5761158466339111 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0911, 0.0218, 0.1172, 0.0757, 0.0761, 0.0411]) \n",
      "Test Loss tensor([0.0953, 0.0237, 0.1101, 0.0719, 0.0778, 0.0420])\n",
      "\n",
      "\n",
      "************** Batch 500 in 0.575310230255127 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1007, 0.0224, 0.1139, 0.0681, 0.0771, 0.0397]) \n",
      "Test Loss tensor([0.0927, 0.0218, 0.1133, 0.0685, 0.0784, 0.0434])\n",
      "\n",
      "\n",
      "************** Batch 504 in 0.5822091102600098 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0971, 0.0240, 0.1113, 0.0687, 0.0731, 0.0454]) \n",
      "Test Loss tensor([0.0939, 0.0226, 0.1121, 0.0696, 0.0791, 0.0416])\n",
      "\n",
      "\n",
      "************** Batch 508 in 0.584308385848999 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1034, 0.0243, 0.1044, 0.0735, 0.0810, 0.0388]) \n",
      "Test Loss tensor([0.0937, 0.0225, 0.1136, 0.0699, 0.0784, 0.0456])\n",
      "\n",
      "\n",
      "************** Batch 512 in 0.5773718357086182 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0945, 0.0235, 0.1152, 0.0727, 0.0808, 0.0449]) \n",
      "Test Loss tensor([0.0931, 0.0225, 0.1111, 0.0725, 0.0763, 0.0415])\n",
      "\n",
      "\n",
      "************** Batch 516 in 0.5847094058990479 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0928, 0.0289, 0.1114, 0.0715, 0.0763, 0.0414]) \n",
      "Test Loss tensor([0.0966, 0.0218, 0.1119, 0.0695, 0.0786, 0.0427])\n",
      "\n",
      "\n",
      "************** Batch 520 in 0.581958532333374 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0914, 0.0245, 0.1159, 0.0676, 0.0871, 0.0474]) \n",
      "Test Loss tensor([0.0900, 0.0222, 0.1129, 0.0687, 0.0765, 0.0416])\n",
      "\n",
      "\n",
      "************** Batch 524 in 0.5614645481109619 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0959, 0.0212, 0.1084, 0.0712, 0.0735, 0.0409]) \n",
      "Test Loss tensor([0.0916, 0.0240, 0.1129, 0.0695, 0.0786, 0.0423])\n",
      "\n",
      "\n",
      "************** Batch 528 in 0.5766522884368896 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0220, 0.1189, 0.0732, 0.0789, 0.0472]) \n",
      "Test Loss tensor([0.0918, 0.0224, 0.1096, 0.0688, 0.0761, 0.0408])\n",
      "\n",
      "\n",
      "************** Batch 532 in 0.621995210647583 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0899, 0.0195, 0.1293, 0.0732, 0.0795, 0.0436]) \n",
      "Test Loss tensor([0.0950, 0.0232, 0.1130, 0.0673, 0.0750, 0.0429])\n",
      "\n",
      "\n",
      "************** Batch 536 in 0.6297008991241455 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1024, 0.0264, 0.1076, 0.0683, 0.0795, 0.0427]) \n",
      "Test Loss tensor([0.0906, 0.0232, 0.1118, 0.0710, 0.0750, 0.0407])\n",
      "\n",
      "\n",
      "************** Batch 540 in 0.584885835647583 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0950, 0.0224, 0.1165, 0.0642, 0.0702, 0.0388]) \n",
      "Test Loss tensor([0.0904, 0.0220, 0.1127, 0.0686, 0.0759, 0.0422])\n",
      "\n",
      "\n",
      "************** Batch 544 in 0.5787127017974854 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0884, 0.0235, 0.1110, 0.0647, 0.0720, 0.0380]) \n",
      "Test Loss tensor([0.0948, 0.0232, 0.1124, 0.0697, 0.0756, 0.0420])\n",
      "\n",
      "\n",
      "************** Batch 548 in 0.5798325538635254 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0912, 0.0253, 0.1082, 0.0717, 0.0776, 0.0466]) \n",
      "Test Loss tensor([0.0922, 0.0222, 0.1120, 0.0691, 0.0772, 0.0422])\n",
      "\n",
      "\n",
      "************** Batch 552 in 0.5716524124145508 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0974, 0.0242, 0.1099, 0.0765, 0.0746, 0.0438]) \n",
      "Test Loss tensor([0.0938, 0.0221, 0.1119, 0.0675, 0.0757, 0.0412])\n",
      "\n",
      "\n",
      "************** Batch 556 in 0.5750019550323486 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0884, 0.0203, 0.1094, 0.0700, 0.0751, 0.0381]) \n",
      "Test Loss tensor([0.0912, 0.0234, 0.1150, 0.0680, 0.0752, 0.0425])\n",
      "\n",
      "\n",
      "************** Batch 560 in 0.6047961711883545 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0948, 0.0240, 0.1007, 0.0759, 0.0760, 0.0382]) \n",
      "Test Loss tensor([0.0936, 0.0221, 0.1116, 0.0692, 0.0756, 0.0419])\n",
      "\n",
      "\n",
      "************** Batch 564 in 0.5836272239685059 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0904, 0.0209, 0.1099, 0.0751, 0.0772, 0.0449]) \n",
      "Test Loss tensor([0.0905, 0.0221, 0.1099, 0.0680, 0.0754, 0.0409])\n",
      "\n",
      "\n",
      "************** Batch 568 in 0.5822019577026367 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0848, 0.0220, 0.1147, 0.0684, 0.0762, 0.0384]) \n",
      "Test Loss tensor([0.0933, 0.0227, 0.1143, 0.0704, 0.0746, 0.0419])\n",
      "\n",
      "\n",
      "************** Batch 572 in 0.5722274780273438 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0862, 0.0214, 0.1123, 0.0650, 0.0776, 0.0428]) \n",
      "Test Loss tensor([0.0916, 0.0221, 0.1121, 0.0713, 0.0750, 0.0412])\n",
      "\n",
      "\n",
      "************** Batch 576 in 0.5852513313293457 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0967, 0.0264, 0.1092, 0.0725, 0.0804, 0.0407]) \n",
      "Test Loss tensor([0.0939, 0.0229, 0.1117, 0.0697, 0.0764, 0.0411])\n",
      "\n",
      "\n",
      "************** Batch 580 in 0.5732476711273193 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0915, 0.0255, 0.1111, 0.0641, 0.0758, 0.0394]) \n",
      "Test Loss tensor([0.0924, 0.0233, 0.1145, 0.0710, 0.0751, 0.0414])\n",
      "\n",
      "\n",
      "************** Batch 584 in 0.572676420211792 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0940, 0.0264, 0.1222, 0.0693, 0.0755, 0.0413]) \n",
      "Test Loss tensor([0.0934, 0.0225, 0.1105, 0.0703, 0.0798, 0.0432])\n",
      "\n",
      "\n",
      "************** Batch 588 in 0.6051251888275146 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0981, 0.0251, 0.1114, 0.0773, 0.0780, 0.0416]) \n",
      "Test Loss tensor([0.0931, 0.0217, 0.1113, 0.0683, 0.0754, 0.0409])\n",
      "\n",
      "\n",
      "************** Batch 592 in 0.5830533504486084 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0939, 0.0176, 0.1214, 0.0672, 0.0758, 0.0424]) \n",
      "Test Loss tensor([0.0953, 0.0228, 0.1112, 0.0711, 0.0769, 0.0419])\n",
      "\n",
      "\n",
      "************** Batch 596 in 0.5826215744018555 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0919, 0.0185, 0.1181, 0.0654, 0.0754, 0.0437]) \n",
      "Test Loss tensor([0.0906, 0.0234, 0.1150, 0.0688, 0.0737, 0.0414])\n",
      "\n",
      "\n",
      "************** Batch 600 in 0.5740053653717041 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0858, 0.0220, 0.1018, 0.0637, 0.0799, 0.0358]) \n",
      "Test Loss tensor([0.0938, 0.0243, 0.1120, 0.0691, 0.0764, 0.0426])\n",
      "\n",
      "\n",
      "************** Batch 604 in 0.5803611278533936 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0941, 0.0217, 0.1151, 0.0677, 0.0842, 0.0467]) \n",
      "Test Loss tensor([0.0932, 0.0223, 0.1093, 0.0696, 0.0761, 0.0416])\n",
      "\n",
      "\n",
      "************** Batch 608 in 0.5728228092193604 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0849, 0.0220, 0.1147, 0.0681, 0.0707, 0.0430]) \n",
      "Test Loss tensor([0.0930, 0.0222, 0.1122, 0.0690, 0.0785, 0.0424])\n",
      "\n",
      "\n",
      "************** Batch 612 in 0.574958324432373 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0920, 0.0252, 0.1129, 0.0642, 0.0809, 0.0423]) \n",
      "Test Loss tensor([0.0934, 0.0234, 0.1114, 0.0702, 0.0765, 0.0413])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 616 in 0.5919332504272461 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1003, 0.0257, 0.1105, 0.0776, 0.0783, 0.0420]) \n",
      "Test Loss tensor([0.0948, 0.0228, 0.1122, 0.0713, 0.0784, 0.0446])\n",
      "\n",
      "\n",
      "************** Batch 620 in 0.5708239078521729 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0939, 0.0260, 0.1083, 0.0737, 0.0798, 0.0397]) \n",
      "Test Loss tensor([0.0925, 0.0248, 0.1101, 0.0683, 0.0761, 0.0412])\n",
      "\n",
      "\n",
      "************** Batch 624 in 0.5829968452453613 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0969, 0.0202, 0.1125, 0.0726, 0.0794, 0.0375]) \n",
      "Test Loss tensor([0.0971, 0.0225, 0.1116, 0.0700, 0.0750, 0.0427])\n",
      "\n",
      "\n",
      "************** Batch 628 in 0.5591347217559814 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1006, 0.0277, 0.1077, 0.0775, 0.0750, 0.0341]) \n",
      "Test Loss tensor([0.0911, 0.0217, 0.1117, 0.0674, 0.0746, 0.0424])\n",
      "\n",
      "\n",
      "************** Batch 632 in 0.5862700939178467 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1009, 0.0244, 0.1094, 0.0735, 0.0773, 0.0374]) \n",
      "Test Loss tensor([0.0939, 0.0230, 0.1103, 0.0699, 0.0776, 0.0421])\n",
      "\n",
      "\n",
      "************** Batch 636 in 0.5816400051116943 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0918, 0.0220, 0.1057, 0.0681, 0.0748, 0.0442]) \n",
      "Test Loss tensor([0.0922, 0.0222, 0.1100, 0.0684, 0.0749, 0.0399])\n",
      "\n",
      "\n",
      "************** Batch 640 in 0.5776281356811523 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0853, 0.0199, 0.1067, 0.0732, 0.0706, 0.0365]) \n",
      "Test Loss tensor([0.0936, 0.0249, 0.1085, 0.0698, 0.0768, 0.0416])\n",
      "\n",
      "\n",
      "************** Batch 644 in 0.5860645771026611 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0215, 0.1041, 0.0679, 0.0732, 0.0437]) \n",
      "Test Loss tensor([0.0905, 0.0221, 0.1091, 0.0702, 0.0765, 0.0411])\n",
      "\n",
      "\n",
      "************** Batch 648 in 0.5829331874847412 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0969, 0.0246, 0.1150, 0.0714, 0.0720, 0.0402]) \n",
      "Test Loss tensor([0.0922, 0.0227, 0.1114, 0.0691, 0.0773, 0.0441])\n",
      "\n",
      "\n",
      "************** Batch 652 in 0.5755541324615479 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0936, 0.0272, 0.1149, 0.0748, 0.0744, 0.0408]) \n",
      "Test Loss tensor([0.0916, 0.0237, 0.1091, 0.0701, 0.0763, 0.0410])\n",
      "\n",
      "\n",
      "************** Batch 656 in 0.5626411437988281 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0925, 0.0224, 0.1126, 0.0731, 0.0735, 0.0449]) \n",
      "Test Loss tensor([0.0953, 0.0223, 0.1086, 0.0691, 0.0786, 0.0419])\n",
      "\n",
      "\n",
      "************** Batch 660 in 0.5801746845245361 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0915, 0.0203, 0.1122, 0.0674, 0.0763, 0.0406]) \n",
      "Test Loss tensor([0.0935, 0.0231, 0.1102, 0.0708, 0.0776, 0.0418])\n",
      "\n",
      "\n",
      "************** Batch 664 in 0.572089672088623 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0952, 0.0227, 0.1181, 0.0656, 0.0756, 0.0369]) \n",
      "Test Loss tensor([0.0929, 0.0238, 0.1104, 0.0726, 0.0785, 0.0417])\n",
      "\n",
      "\n",
      "************** Batch 668 in 0.5797898769378662 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0958, 0.0218, 0.1083, 0.0760, 0.0749, 0.0442]) \n",
      "Test Loss tensor([0.0934, 0.0233, 0.1084, 0.0675, 0.0754, 0.0411])\n",
      "\n",
      "\n",
      "************** Batch 672 in 0.598564624786377 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0990, 0.0278, 0.1106, 0.0802, 0.0734, 0.0415]) \n",
      "Test Loss tensor([0.0922, 0.0227, 0.1104, 0.0712, 0.0779, 0.0409])\n",
      "\n",
      "\n",
      "************** Batch 676 in 0.5694155693054199 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0887, 0.0251, 0.1095, 0.0715, 0.0744, 0.0395]) \n",
      "Test Loss tensor([0.0929, 0.0247, 0.1082, 0.0706, 0.0771, 0.0402])\n",
      "\n",
      "\n",
      "************** Batch 680 in 0.570725679397583 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0946, 0.0256, 0.1087, 0.0697, 0.0804, 0.0379]) \n",
      "Test Loss tensor([0.0906, 0.0224, 0.1090, 0.0687, 0.0768, 0.0408])\n",
      "\n",
      "\n",
      "************** Batch 684 in 0.5606679916381836 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0884, 0.0202, 0.1159, 0.0734, 0.0702, 0.0395]) \n",
      "Test Loss tensor([0.0927, 0.0226, 0.1082, 0.0687, 0.0771, 0.0438])\n",
      "\n",
      "\n",
      "************** Batch 688 in 0.5743207931518555 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0897, 0.0246, 0.1128, 0.0669, 0.0757, 0.0460]) \n",
      "Test Loss tensor([0.0905, 0.0226, 0.1085, 0.0679, 0.0761, 0.0413])\n",
      "\n",
      "\n",
      "************** Batch 692 in 0.5781745910644531 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0880, 0.0235, 0.1101, 0.0716, 0.0667, 0.0402]) \n",
      "Test Loss tensor([0.0934, 0.0241, 0.1097, 0.0705, 0.0780, 0.0398])\n",
      "\n",
      "\n",
      "************** Batch 696 in 0.5659291744232178 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0858, 0.0225, 0.1094, 0.0705, 0.0789, 0.0421]) \n",
      "Test Loss tensor([0.0944, 0.0240, 0.1084, 0.0690, 0.0770, 0.0405])\n",
      "\n",
      "\n",
      "************** Batch 700 in 0.5994961261749268 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0925, 0.0187, 0.1088, 0.0673, 0.0742, 0.0425]) \n",
      "Test Loss tensor([0.0934, 0.0219, 0.1106, 0.0683, 0.0765, 0.0416])\n",
      "\n",
      "\n",
      "************** Batch 704 in 0.5632283687591553 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0912, 0.0234, 0.1073, 0.0760, 0.0730, 0.0424]) \n",
      "Test Loss tensor([0.0923, 0.0237, 0.1101, 0.0706, 0.0754, 0.0415])\n",
      "\n",
      "\n",
      "************** Batch 708 in 0.5806231498718262 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0919, 0.0218, 0.1092, 0.0698, 0.0751, 0.0422]) \n",
      "Test Loss tensor([0.0908, 0.0224, 0.1081, 0.0691, 0.0765, 0.0402])\n",
      "\n",
      "\n",
      "************** Batch 712 in 0.5560152530670166 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0965, 0.0198, 0.1070, 0.0719, 0.0761, 0.0437]) \n",
      "Test Loss tensor([0.0934, 0.0223, 0.1076, 0.0703, 0.0780, 0.0403])\n",
      "\n",
      "\n",
      "************** Batch 716 in 0.573951244354248 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1032, 0.0214, 0.1175, 0.0682, 0.0807, 0.0360]) \n",
      "Test Loss tensor([0.0929, 0.0234, 0.1099, 0.0674, 0.0756, 0.0402])\n",
      "\n",
      "\n",
      "************** Batch 720 in 0.5771212577819824 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0942, 0.0217, 0.1070, 0.0700, 0.0740, 0.0401]) \n",
      "Test Loss tensor([0.0939, 0.0235, 0.1069, 0.0686, 0.0755, 0.0419])\n",
      "\n",
      "\n",
      "************** Batch 724 in 0.574894905090332 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0887, 0.0249, 0.1125, 0.0650, 0.0734, 0.0415]) \n",
      "Test Loss tensor([0.0929, 0.0235, 0.1103, 0.0698, 0.0753, 0.0414])\n",
      "\n",
      "\n",
      "************** Batch 728 in 0.5887265205383301 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0978, 0.0225, 0.1052, 0.0705, 0.0750, 0.0432]) \n",
      "Test Loss tensor([0.0943, 0.0224, 0.1106, 0.0681, 0.0766, 0.0408])\n",
      "\n",
      "\n",
      "************** Batch 732 in 0.5678870677947998 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0906, 0.0198, 0.1100, 0.0670, 0.0727, 0.0404]) \n",
      "Test Loss tensor([0.0927, 0.0222, 0.1100, 0.0697, 0.0767, 0.0407])\n",
      "\n",
      "\n",
      "************** Batch 736 in 0.5770044326782227 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0249, 0.1079, 0.0787, 0.0704, 0.0441]) \n",
      "Test Loss tensor([0.0930, 0.0237, 0.1106, 0.0699, 0.0763, 0.0406])\n",
      "\n",
      "\n",
      "************** Batch 740 in 0.5655636787414551 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0937, 0.0233, 0.1048, 0.0719, 0.0754, 0.0411]) \n",
      "Test Loss tensor([0.0890, 0.0228, 0.1069, 0.0678, 0.0739, 0.0402])\n",
      "\n",
      "\n",
      "************** Batch 744 in 0.5758082866668701 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0916, 0.0207, 0.1122, 0.0668, 0.0694, 0.0400]) \n",
      "Test Loss tensor([0.0939, 0.0223, 0.1080, 0.0680, 0.0742, 0.0416])\n",
      "\n",
      "\n",
      "************** Batch 748 in 0.586108922958374 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0915, 0.0201, 0.1103, 0.0727, 0.0697, 0.0429]) \n",
      "Test Loss tensor([0.0909, 0.0224, 0.1078, 0.0696, 0.0757, 0.0404])\n",
      "\n",
      "\n",
      "************** Batch 752 in 0.5600261688232422 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0941, 0.0222, 0.1024, 0.0704, 0.0788, 0.0390]) \n",
      "Test Loss tensor([0.0892, 0.0219, 0.1094, 0.0670, 0.0784, 0.0429])\n",
      "\n",
      "\n",
      "************** Batch 756 in 0.5841269493103027 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0912, 0.0239, 0.1052, 0.0668, 0.0771, 0.0437]) \n",
      "Test Loss tensor([0.0939, 0.0230, 0.1078, 0.0698, 0.0758, 0.0397])\n",
      "\n",
      "\n",
      "************** Batch 760 in 0.5855555534362793 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0231, 0.1120, 0.0638, 0.0735, 0.0362]) \n",
      "Test Loss tensor([0.0931, 0.0218, 0.1071, 0.0708, 0.0796, 0.0425])\n",
      "\n",
      "\n",
      "************** Batch 764 in 0.5926034450531006 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0945, 0.0171, 0.1084, 0.0661, 0.0785, 0.0413]) \n",
      "Test Loss tensor([0.0931, 0.0215, 0.1076, 0.0705, 0.0764, 0.0387])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 768 in 0.5918242931365967 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0909, 0.0250, 0.1084, 0.0661, 0.0799, 0.0382]) \n",
      "Test Loss tensor([0.0965, 0.0220, 0.1041, 0.0692, 0.0776, 0.0403])\n",
      "\n",
      "\n",
      "************** Batch 772 in 0.576369047164917 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0974, 0.0196, 0.1129, 0.0725, 0.0735, 0.0426]) \n",
      "Test Loss tensor([0.0933, 0.0224, 0.1079, 0.0685, 0.0762, 0.0405])\n",
      "\n",
      "\n",
      "************** Batch 776 in 0.6034078598022461 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0939, 0.0246, 0.1144, 0.0661, 0.0746, 0.0418]) \n",
      "Test Loss tensor([0.0945, 0.0227, 0.1085, 0.0706, 0.0761, 0.0413])\n",
      "\n",
      "\n",
      "************** Batch 780 in 0.5984082221984863 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0914, 0.0240, 0.1104, 0.0730, 0.0822, 0.0459]) \n",
      "Test Loss tensor([0.0893, 0.0226, 0.1077, 0.0674, 0.0762, 0.0403])\n",
      "\n",
      "\n",
      "************** Batch 784 in 0.6072146892547607 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0936, 0.0205, 0.1069, 0.0671, 0.0747, 0.0413]) \n",
      "Test Loss tensor([0.0939, 0.0237, 0.1084, 0.0681, 0.0767, 0.0400])\n",
      "\n",
      "\n",
      "************** Batch 788 in 0.6241359710693359 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0866, 0.0207, 0.1075, 0.0665, 0.0765, 0.0394]) \n",
      "Test Loss tensor([0.0916, 0.0224, 0.1077, 0.0688, 0.0736, 0.0403])\n",
      "\n",
      "\n",
      "************** Batch 792 in 0.5782425403594971 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0940, 0.0255, 0.1108, 0.0656, 0.0726, 0.0414]) \n",
      "Test Loss tensor([0.0956, 0.0224, 0.1046, 0.0685, 0.0748, 0.0399])\n",
      "\n",
      "\n",
      "************** Batch 796 in 0.5957794189453125 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0967, 0.0253, 0.1054, 0.0618, 0.0755, 0.0403]) \n",
      "Test Loss tensor([0.0907, 0.0224, 0.1069, 0.0692, 0.0754, 0.0400])\n",
      "\n",
      "\n",
      "************** Batch 800 in 0.5728304386138916 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0924, 0.0225, 0.1073, 0.0667, 0.0735, 0.0416]) \n",
      "Test Loss tensor([0.0931, 0.0219, 0.1074, 0.0689, 0.0760, 0.0398])\n",
      "\n",
      "\n",
      "************** Batch 804 in 0.5816457271575928 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0929, 0.0230, 0.1075, 0.0688, 0.0736, 0.0427]) \n",
      "Test Loss tensor([0.0912, 0.0219, 0.1079, 0.0701, 0.0746, 0.0391])\n",
      "\n",
      "\n",
      "************** Batch 808 in 0.5714099407196045 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0913, 0.0267, 0.1062, 0.0733, 0.0767, 0.0388]) \n",
      "Test Loss tensor([0.0916, 0.0228, 0.1075, 0.0675, 0.0753, 0.0398])\n",
      "\n",
      "\n",
      "************** Batch 812 in 0.5879666805267334 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0908, 0.0262, 0.1064, 0.0813, 0.0761, 0.0361]) \n",
      "Test Loss tensor([0.0907, 0.0220, 0.1076, 0.0693, 0.0743, 0.0402])\n",
      "\n",
      "\n",
      "************** Batch 816 in 0.6264433860778809 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0882, 0.0199, 0.1103, 0.0729, 0.0749, 0.0459]) \n",
      "Test Loss tensor([0.0919, 0.0241, 0.1071, 0.0686, 0.0753, 0.0398])\n",
      "\n",
      "\n",
      "************** Batch 820 in 0.5629737377166748 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0900, 0.0205, 0.1081, 0.0757, 0.0737, 0.0398]) \n",
      "Test Loss tensor([0.0908, 0.0232, 0.1084, 0.0688, 0.0759, 0.0410])\n",
      "\n",
      "\n",
      "************** Batch 824 in 0.594696044921875 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0953, 0.0224, 0.1085, 0.0728, 0.0788, 0.0364]) \n",
      "Test Loss tensor([0.0934, 0.0226, 0.1055, 0.0702, 0.0749, 0.0383])\n",
      "\n",
      "\n",
      "************** Batch 828 in 0.5705842971801758 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0842, 0.0210, 0.0982, 0.0751, 0.0712, 0.0402]) \n",
      "Test Loss tensor([0.0945, 0.0223, 0.1064, 0.0698, 0.0750, 0.0391])\n",
      "\n",
      "\n",
      "************** Batch 832 in 0.574955940246582 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0985, 0.0235, 0.1001, 0.0706, 0.0698, 0.0413]) \n",
      "Test Loss tensor([0.0925, 0.0237, 0.1094, 0.0692, 0.0771, 0.0407])\n",
      "\n",
      "\n",
      "************** Batch 836 in 0.5762524604797363 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1010, 0.0248, 0.1040, 0.0764, 0.0768, 0.0369]) \n",
      "Test Loss tensor([0.0924, 0.0221, 0.1075, 0.0675, 0.0741, 0.0406])\n",
      "\n",
      "\n",
      "************** Batch 840 in 0.6010704040527344 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0915, 0.0271, 0.1127, 0.0682, 0.0765, 0.0414]) \n",
      "Test Loss tensor([0.0901, 0.0220, 0.1044, 0.0675, 0.0760, 0.0401])\n",
      "\n",
      "\n",
      "************** Batch 844 in 0.6134860515594482 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0952, 0.0224, 0.0999, 0.0575, 0.0689, 0.0387]) \n",
      "Test Loss tensor([0.0928, 0.0222, 0.1084, 0.0689, 0.0758, 0.0395])\n",
      "\n",
      "\n",
      "************** Batch 848 in 0.501448392868042 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0875, 0.0214, 0.1116, 0.0640, 0.0721, 0.0348]) \n",
      "Test Loss tensor([0.0915, 0.0237, 0.1068, 0.0686, 0.0756, 0.0395])\n",
      "\n",
      "\n",
      "************** Batch 852 in 0.476823091506958 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0912, 0.0207, 0.1023, 0.0687, 0.0783, 0.0416]) \n",
      "Test Loss tensor([0.0928, 0.0234, 0.1043, 0.0667, 0.0754, 0.0392])\n",
      "\n",
      "\n",
      "************** Batch 856 in 0.5041699409484863 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0926, 0.0215, 0.1030, 0.0648, 0.0772, 0.0406]) \n",
      "Test Loss tensor([0.0917, 0.0237, 0.1050, 0.0677, 0.0760, 0.0394])\n",
      "\n",
      "\n",
      "************** Batch 860 in 0.5037553310394287 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0969, 0.0222, 0.1061, 0.0639, 0.0755, 0.0439]) \n",
      "Test Loss tensor([0.0907, 0.0215, 0.1053, 0.0687, 0.0751, 0.0396])\n",
      "\n",
      "\n",
      "************** Batch 864 in 0.541865348815918 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0911, 0.0226, 0.1133, 0.0697, 0.0751, 0.0400]) \n",
      "Test Loss tensor([0.0933, 0.0217, 0.1046, 0.0660, 0.0755, 0.0396])\n",
      "\n",
      "\n",
      "************** Batch 868 in 0.6334278583526611 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0940, 0.0234, 0.1038, 0.0709, 0.0800, 0.0319]) \n",
      "Test Loss tensor([0.0919, 0.0223, 0.1045, 0.0677, 0.0763, 0.0403])\n",
      "\n",
      "\n",
      "************** Batch 872 in 0.7197861671447754 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0863, 0.0219, 0.1020, 0.0754, 0.0708, 0.0373]) \n",
      "Test Loss tensor([0.0926, 0.0231, 0.1043, 0.0684, 0.0752, 0.0394])\n",
      "\n",
      "\n",
      "************** Batch 876 in 0.6277508735656738 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0749, 0.0192, 0.0779, 0.0460, 0.0536, 0.0305]) \n",
      "Test Loss tensor([0.0916, 0.0222, 0.1079, 0.0691, 0.0756, 0.0404])\n",
      "\n",
      "\n",
      "************** Batch 0 in 0.6991121768951416 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0957, 0.0239, 0.1043, 0.0626, 0.0809, 0.0422]) \n",
      "Test Loss tensor([0.0911, 0.0223, 0.1040, 0.0689, 0.0758, 0.0394])\n",
      "\n",
      "\n",
      "************** Batch 4 in 0.6602065563201904 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0912, 0.0272, 0.1073, 0.0680, 0.0775, 0.0353]) \n",
      "Test Loss tensor([0.0907, 0.0219, 0.1052, 0.0698, 0.0756, 0.0396])\n",
      "\n",
      "\n",
      "************** Batch 8 in 0.6598513126373291 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0895, 0.0206, 0.1025, 0.0706, 0.0725, 0.0402]) \n",
      "Test Loss tensor([0.0921, 0.0232, 0.1054, 0.0698, 0.0756, 0.0399])\n",
      "\n",
      "\n",
      "************** Batch 12 in 0.6852099895477295 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0884, 0.0224, 0.1012, 0.0709, 0.0755, 0.0418]) \n",
      "Test Loss tensor([0.0925, 0.0213, 0.1035, 0.0687, 0.0752, 0.0392])\n",
      "\n",
      "\n",
      "************** Batch 16 in 0.6760151386260986 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0974, 0.0247, 0.1037, 0.0692, 0.0758, 0.0430]) \n",
      "Test Loss tensor([0.0933, 0.0239, 0.1033, 0.0698, 0.0745, 0.0399])\n",
      "\n",
      "\n",
      "************** Batch 20 in 0.6688237190246582 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0960, 0.0227, 0.1017, 0.0622, 0.0750, 0.0394]) \n",
      "Test Loss tensor([0.0902, 0.0237, 0.1065, 0.0692, 0.0736, 0.0402])\n",
      "\n",
      "\n",
      "************** Batch 24 in 0.684471845626831 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0935, 0.0234, 0.0963, 0.0754, 0.0752, 0.0363]) \n",
      "Test Loss tensor([0.0931, 0.0224, 0.1060, 0.0690, 0.0745, 0.0404])\n",
      "\n",
      "\n",
      "************** Batch 28 in 0.6177773475646973 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0928, 0.0216, 0.1044, 0.0753, 0.0704, 0.0418]) \n",
      "Test Loss tensor([0.0917, 0.0232, 0.1054, 0.0700, 0.0752, 0.0397])\n",
      "\n",
      "\n",
      "************** Batch 32 in 0.6132781505584717 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0952, 0.0269, 0.1045, 0.0706, 0.0641, 0.0406]) \n",
      "Test Loss tensor([0.0894, 0.0227, 0.1060, 0.0681, 0.0747, 0.0399])\n",
      "\n",
      "\n",
      "************** Batch 36 in 0.6168859004974365 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0909, 0.0217, 0.1015, 0.0695, 0.0767, 0.0387]) \n",
      "Test Loss tensor([0.0932, 0.0230, 0.1054, 0.0678, 0.0757, 0.0391])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 40 in 0.6434752941131592 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0237, 0.1085, 0.0694, 0.0776, 0.0385]) \n",
      "Test Loss tensor([0.0898, 0.0226, 0.1010, 0.0696, 0.0759, 0.0387])\n",
      "\n",
      "\n",
      "************** Batch 44 in 0.621873140335083 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0930, 0.0241, 0.1097, 0.0761, 0.0689, 0.0394]) \n",
      "Test Loss tensor([0.0917, 0.0227, 0.1036, 0.0710, 0.0746, 0.0388])\n",
      "\n",
      "\n",
      "************** Batch 48 in 0.5787911415100098 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0890, 0.0261, 0.1104, 0.0714, 0.0728, 0.0439]) \n",
      "Test Loss tensor([0.0921, 0.0235, 0.1046, 0.0690, 0.0736, 0.0384])\n",
      "\n",
      "\n",
      "************** Batch 52 in 0.562464714050293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0205, 0.1076, 0.0720, 0.0784, 0.0423]) \n",
      "Test Loss tensor([0.0925, 0.0232, 0.1032, 0.0700, 0.0765, 0.0392])\n",
      "\n",
      "\n",
      "************** Batch 56 in 0.584681510925293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0858, 0.0185, 0.1079, 0.0752, 0.0789, 0.0423]) \n",
      "Test Loss tensor([0.0921, 0.0231, 0.1029, 0.0672, 0.0754, 0.0390])\n",
      "\n",
      "\n",
      "************** Batch 60 in 0.5794658660888672 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0916, 0.0213, 0.1023, 0.0691, 0.0724, 0.0411]) \n",
      "Test Loss tensor([0.0909, 0.0218, 0.1034, 0.0701, 0.0745, 0.0407])\n",
      "\n",
      "\n",
      "************** Batch 64 in 0.5721971988677979 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0902, 0.0220, 0.1027, 0.0632, 0.0796, 0.0375]) \n",
      "Test Loss tensor([0.0942, 0.0217, 0.1014, 0.0712, 0.0743, 0.0387])\n",
      "\n",
      "\n",
      "************** Batch 68 in 0.59273362159729 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0925, 0.0216, 0.0988, 0.0651, 0.0738, 0.0355]) \n",
      "Test Loss tensor([0.0906, 0.0221, 0.1025, 0.0671, 0.0770, 0.0392])\n",
      "\n",
      "\n",
      "************** Batch 72 in 0.5716111660003662 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0946, 0.0187, 0.1083, 0.0775, 0.0731, 0.0378]) \n",
      "Test Loss tensor([0.0940, 0.0228, 0.1026, 0.0683, 0.0743, 0.0391])\n",
      "\n",
      "\n",
      "************** Batch 76 in 0.5727274417877197 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0897, 0.0175, 0.0963, 0.0725, 0.0738, 0.0399]) \n",
      "Test Loss tensor([0.0896, 0.0234, 0.1038, 0.0706, 0.0762, 0.0396])\n",
      "\n",
      "\n",
      "************** Batch 80 in 0.5798931121826172 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0997, 0.0257, 0.0980, 0.0752, 0.0735, 0.0404]) \n",
      "Test Loss tensor([0.0909, 0.0220, 0.1016, 0.0698, 0.0738, 0.0397])\n",
      "\n",
      "\n",
      "************** Batch 84 in 0.5745518207550049 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0977, 0.0240, 0.0934, 0.0822, 0.0739, 0.0410]) \n",
      "Test Loss tensor([0.0921, 0.0223, 0.1043, 0.0671, 0.0762, 0.0413])\n",
      "\n",
      "\n",
      "************** Batch 88 in 0.584951639175415 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0930, 0.0207, 0.1043, 0.0738, 0.0731, 0.0403]) \n",
      "Test Loss tensor([0.0906, 0.0228, 0.0996, 0.0701, 0.0755, 0.0402])\n",
      "\n",
      "\n",
      "************** Batch 92 in 0.5705931186676025 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0968, 0.0241, 0.1018, 0.0672, 0.0789, 0.0403]) \n",
      "Test Loss tensor([0.0922, 0.0230, 0.1010, 0.0704, 0.0757, 0.0398])\n",
      "\n",
      "\n",
      "************** Batch 96 in 0.5877387523651123 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0870, 0.0253, 0.0953, 0.0793, 0.0820, 0.0321]) \n",
      "Test Loss tensor([0.0912, 0.0216, 0.1015, 0.0688, 0.0718, 0.0402])\n",
      "\n",
      "\n",
      "************** Batch 100 in 0.5616707801818848 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0209, 0.1060, 0.0686, 0.0766, 0.0409]) \n",
      "Test Loss tensor([0.0925, 0.0216, 0.1032, 0.0684, 0.0740, 0.0412])\n",
      "\n",
      "\n",
      "************** Batch 104 in 0.574688196182251 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1046, 0.0255, 0.0944, 0.0745, 0.0788, 0.0407]) \n",
      "Test Loss tensor([0.0920, 0.0215, 0.1016, 0.0682, 0.0748, 0.0398])\n",
      "\n",
      "\n",
      "************** Batch 108 in 0.5907740592956543 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0982, 0.0229, 0.0977, 0.0611, 0.0776, 0.0456]) \n",
      "Test Loss tensor([0.0915, 0.0229, 0.1035, 0.0693, 0.0754, 0.0408])\n",
      "\n",
      "\n",
      "************** Batch 112 in 0.5839557647705078 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0890, 0.0205, 0.1075, 0.0691, 0.0774, 0.0480]) \n",
      "Test Loss tensor([0.0915, 0.0232, 0.1027, 0.0677, 0.0733, 0.0393])\n",
      "\n",
      "\n",
      "************** Batch 116 in 0.6219825744628906 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0955, 0.0268, 0.0984, 0.0732, 0.0746, 0.0396]) \n",
      "Test Loss tensor([0.0938, 0.0235, 0.1010, 0.0697, 0.0736, 0.0406])\n",
      "\n",
      "\n",
      "************** Batch 120 in 0.570054292678833 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0930, 0.0234, 0.0948, 0.0656, 0.0743, 0.0387]) \n",
      "Test Loss tensor([0.0944, 0.0232, 0.1014, 0.0705, 0.0753, 0.0396])\n",
      "\n",
      "\n",
      "************** Batch 124 in 0.5740811824798584 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0818, 0.0211, 0.0974, 0.0704, 0.0763, 0.0421]) \n",
      "Test Loss tensor([0.0911, 0.0221, 0.1001, 0.0699, 0.0738, 0.0398])\n",
      "\n",
      "\n",
      "************** Batch 128 in 0.5778810977935791 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0982, 0.0265, 0.1054, 0.0742, 0.0811, 0.0411]) \n",
      "Test Loss tensor([0.0913, 0.0216, 0.1000, 0.0680, 0.0745, 0.0382])\n",
      "\n",
      "\n",
      "************** Batch 132 in 0.5858194828033447 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0879, 0.0232, 0.1040, 0.0685, 0.0736, 0.0376]) \n",
      "Test Loss tensor([0.0908, 0.0220, 0.1002, 0.0681, 0.0758, 0.0413])\n",
      "\n",
      "\n",
      "************** Batch 136 in 0.5925862789154053 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1027, 0.0216, 0.0952, 0.0776, 0.0773, 0.0445]) \n",
      "Test Loss tensor([0.0891, 0.0227, 0.0997, 0.0667, 0.0770, 0.0397])\n",
      "\n",
      "\n",
      "************** Batch 140 in 0.5753591060638428 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0972, 0.0248, 0.1049, 0.0626, 0.0722, 0.0416]) \n",
      "Test Loss tensor([0.0920, 0.0230, 0.0995, 0.0692, 0.0742, 0.0386])\n",
      "\n",
      "\n",
      "************** Batch 144 in 0.6098566055297852 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0914, 0.0193, 0.0983, 0.0645, 0.0707, 0.0375]) \n",
      "Test Loss tensor([0.0910, 0.0222, 0.0982, 0.0697, 0.0753, 0.0388])\n",
      "\n",
      "\n",
      "************** Batch 148 in 0.5703504085540771 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0238, 0.0959, 0.0759, 0.0746, 0.0376]) \n",
      "Test Loss tensor([0.0918, 0.0231, 0.0995, 0.0707, 0.0745, 0.0386])\n",
      "\n",
      "\n",
      "************** Batch 152 in 0.5817654132843018 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0884, 0.0243, 0.1068, 0.0641, 0.0746, 0.0438]) \n",
      "Test Loss tensor([0.0891, 0.0226, 0.0975, 0.0659, 0.0740, 0.0386])\n",
      "\n",
      "\n",
      "************** Batch 156 in 0.5899631977081299 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0845, 0.0237, 0.1055, 0.0659, 0.0707, 0.0406]) \n",
      "Test Loss tensor([0.0933, 0.0227, 0.0996, 0.0694, 0.0728, 0.0396])\n",
      "\n",
      "\n",
      "************** Batch 160 in 0.6150112152099609 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0847, 0.0215, 0.0934, 0.0655, 0.0692, 0.0375]) \n",
      "Test Loss tensor([0.0897, 0.0226, 0.1006, 0.0699, 0.0744, 0.0389])\n",
      "\n",
      "\n",
      "************** Batch 164 in 0.5890407562255859 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0920, 0.0244, 0.1002, 0.0701, 0.0714, 0.0401]) \n",
      "Test Loss tensor([0.0923, 0.0216, 0.0982, 0.0698, 0.0738, 0.0384])\n",
      "\n",
      "\n",
      "************** Batch 168 in 0.5824329853057861 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0892, 0.0210, 0.0926, 0.0741, 0.0717, 0.0409]) \n",
      "Test Loss tensor([0.0929, 0.0227, 0.0986, 0.0685, 0.0766, 0.0392])\n",
      "\n",
      "\n",
      "************** Batch 172 in 0.6000361442565918 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0848, 0.0193, 0.0992, 0.0628, 0.0771, 0.0414]) \n",
      "Test Loss tensor([0.0923, 0.0229, 0.0997, 0.0721, 0.0747, 0.0385])\n",
      "\n",
      "\n",
      "************** Batch 176 in 0.6235647201538086 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0920, 0.0245, 0.0891, 0.0664, 0.0756, 0.0376]) \n",
      "Test Loss tensor([0.0897, 0.0221, 0.0988, 0.0700, 0.0753, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 180 in 0.584662914276123 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0908, 0.0208, 0.0977, 0.0714, 0.0754, 0.0342]) \n",
      "Test Loss tensor([0.0922, 0.0218, 0.0973, 0.0700, 0.0763, 0.0401])\n",
      "\n",
      "\n",
      "************** Batch 184 in 0.5921792984008789 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0983, 0.0232, 0.1043, 0.0731, 0.0796, 0.0391]) \n",
      "Test Loss tensor([0.0902, 0.0224, 0.0972, 0.0699, 0.0748, 0.0372])\n",
      "\n",
      "\n",
      "************** Batch 188 in 0.579132080078125 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0891, 0.0196, 0.0935, 0.0717, 0.0728, 0.0395]) \n",
      "Test Loss tensor([0.0895, 0.0231, 0.0974, 0.0671, 0.0789, 0.0404])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 192 in 0.5848636627197266 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0883, 0.0227, 0.1014, 0.0759, 0.0771, 0.0396]) \n",
      "Test Loss tensor([0.0925, 0.0244, 0.0969, 0.0694, 0.0749, 0.0376])\n",
      "\n",
      "\n",
      "************** Batch 196 in 0.5706319808959961 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0867, 0.0227, 0.1013, 0.0701, 0.0722, 0.0373]) \n",
      "Test Loss tensor([0.0935, 0.0226, 0.0976, 0.0737, 0.0743, 0.0380])\n",
      "\n",
      "\n",
      "************** Batch 200 in 0.5973436832427979 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0860, 0.0236, 0.1061, 0.0657, 0.0687, 0.0390]) \n",
      "Test Loss tensor([0.0936, 0.0231, 0.0971, 0.0705, 0.0735, 0.0387])\n",
      "\n",
      "\n",
      "************** Batch 204 in 0.5781459808349609 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0920, 0.0258, 0.0994, 0.0643, 0.0769, 0.0371]) \n",
      "Test Loss tensor([0.0907, 0.0219, 0.0985, 0.0704, 0.0752, 0.0376])\n",
      "\n",
      "\n",
      "************** Batch 208 in 0.6057875156402588 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0221, 0.0947, 0.0726, 0.0812, 0.0329]) \n",
      "Test Loss tensor([0.0910, 0.0225, 0.0976, 0.0681, 0.0740, 0.0400])\n",
      "\n",
      "\n",
      "************** Batch 212 in 0.5990896224975586 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0929, 0.0224, 0.0988, 0.0649, 0.0777, 0.0417]) \n",
      "Test Loss tensor([0.0897, 0.0227, 0.0939, 0.0706, 0.0747, 0.0382])\n",
      "\n",
      "\n",
      "************** Batch 216 in 0.5879926681518555 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0900, 0.0238, 0.0920, 0.0638, 0.0723, 0.0398]) \n",
      "Test Loss tensor([0.0918, 0.0219, 0.0952, 0.0712, 0.0752, 0.0384])\n",
      "\n",
      "\n",
      "************** Batch 220 in 0.5919477939605713 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0888, 0.0230, 0.0977, 0.0734, 0.0766, 0.0368]) \n",
      "Test Loss tensor([0.0910, 0.0229, 0.0960, 0.0662, 0.0731, 0.0379])\n",
      "\n",
      "\n",
      "************** Batch 224 in 0.577739953994751 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0972, 0.0269, 0.1039, 0.0778, 0.0700, 0.0374]) \n",
      "Test Loss tensor([0.0916, 0.0231, 0.0981, 0.0715, 0.0741, 0.0400])\n",
      "\n",
      "\n",
      "************** Batch 228 in 0.5879642963409424 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0982, 0.0231, 0.0984, 0.0725, 0.0744, 0.0387]) \n",
      "Test Loss tensor([0.0917, 0.0230, 0.0942, 0.0687, 0.0747, 0.0391])\n",
      "\n",
      "\n",
      "************** Batch 232 in 0.6344649791717529 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0917, 0.0236, 0.0939, 0.0674, 0.0765, 0.0356]) \n",
      "Test Loss tensor([0.0938, 0.0236, 0.0973, 0.0677, 0.0732, 0.0402])\n",
      "\n",
      "\n",
      "************** Batch 236 in 0.5897364616394043 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0896, 0.0247, 0.0995, 0.0693, 0.0704, 0.0414]) \n",
      "Test Loss tensor([0.0935, 0.0224, 0.0968, 0.0706, 0.0742, 0.0385])\n",
      "\n",
      "\n",
      "************** Batch 240 in 0.5805573463439941 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0948, 0.0225, 0.0903, 0.0782, 0.0683, 0.0373]) \n",
      "Test Loss tensor([0.0878, 0.0231, 0.0955, 0.0701, 0.0741, 0.0393])\n",
      "\n",
      "\n",
      "************** Batch 244 in 0.5895571708679199 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1005, 0.0254, 0.0934, 0.0660, 0.0745, 0.0353]) \n",
      "Test Loss tensor([0.0916, 0.0225, 0.0958, 0.0706, 0.0741, 0.0388])\n",
      "\n",
      "\n",
      "************** Batch 248 in 0.5753424167633057 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0278, 0.0956, 0.0694, 0.0768, 0.0388]) \n",
      "Test Loss tensor([0.0939, 0.0222, 0.0935, 0.0677, 0.0782, 0.0401])\n",
      "\n",
      "\n",
      "************** Batch 252 in 0.6248171329498291 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0925, 0.0228, 0.0988, 0.0742, 0.0797, 0.0371]) \n",
      "Test Loss tensor([0.0903, 0.0220, 0.0942, 0.0700, 0.0747, 0.0380])\n",
      "\n",
      "\n",
      "************** Batch 256 in 0.5807003974914551 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0923, 0.0214, 0.0862, 0.0729, 0.0743, 0.0366]) \n",
      "Test Loss tensor([0.0914, 0.0227, 0.0944, 0.0684, 0.0761, 0.0381])\n",
      "\n",
      "\n",
      "************** Batch 260 in 0.5934598445892334 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0896, 0.0252, 0.0947, 0.0786, 0.0765, 0.0376]) \n",
      "Test Loss tensor([0.0926, 0.0230, 0.0958, 0.0693, 0.0737, 0.0387])\n",
      "\n",
      "\n",
      "************** Batch 264 in 0.6005067825317383 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0822, 0.0224, 0.0885, 0.0649, 0.0762, 0.0383]) \n",
      "Test Loss tensor([0.0952, 0.0228, 0.0951, 0.0705, 0.0754, 0.0396])\n",
      "\n",
      "\n",
      "************** Batch 268 in 0.5965063571929932 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0209, 0.0952, 0.0715, 0.0790, 0.0435]) \n",
      "Test Loss tensor([0.0912, 0.0229, 0.0947, 0.0698, 0.0750, 0.0384])\n",
      "\n",
      "\n",
      "************** Batch 272 in 0.6001262664794922 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0837, 0.0256, 0.0949, 0.0763, 0.0750, 0.0369]) \n",
      "Test Loss tensor([0.0929, 0.0227, 0.0936, 0.0705, 0.0767, 0.0367])\n",
      "\n",
      "\n",
      "************** Batch 276 in 0.5757911205291748 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0887, 0.0254, 0.0989, 0.0735, 0.0811, 0.0428]) \n",
      "Test Loss tensor([0.0895, 0.0223, 0.0934, 0.0680, 0.0729, 0.0402])\n",
      "\n",
      "\n",
      "************** Batch 280 in 0.596923828125 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1007, 0.0206, 0.0928, 0.0680, 0.0825, 0.0388]) \n",
      "Test Loss tensor([0.0931, 0.0233, 0.0943, 0.0694, 0.0762, 0.0392])\n",
      "\n",
      "\n",
      "************** Batch 284 in 0.5827531814575195 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0904, 0.0265, 0.0894, 0.0629, 0.0771, 0.0356]) \n",
      "Test Loss tensor([0.0924, 0.0236, 0.0950, 0.0710, 0.0759, 0.0405])\n",
      "\n",
      "\n",
      "************** Batch 288 in 0.5875334739685059 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0206, 0.0972, 0.0736, 0.0756, 0.0413]) \n",
      "Test Loss tensor([0.0903, 0.0221, 0.0939, 0.0707, 0.0740, 0.0381])\n",
      "\n",
      "\n",
      "************** Batch 292 in 0.589648962020874 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0895, 0.0229, 0.0948, 0.0689, 0.0705, 0.0413]) \n",
      "Test Loss tensor([0.0918, 0.0221, 0.0913, 0.0671, 0.0788, 0.0404])\n",
      "\n",
      "\n",
      "************** Batch 296 in 0.5726721286773682 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0914, 0.0263, 0.0948, 0.0715, 0.0802, 0.0419]) \n",
      "Test Loss tensor([0.0916, 0.0220, 0.0926, 0.0659, 0.0741, 0.0397])\n",
      "\n",
      "\n",
      "************** Batch 300 in 0.5842978954315186 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0963, 0.0267, 0.0895, 0.0667, 0.0786, 0.0402]) \n",
      "Test Loss tensor([0.0947, 0.0248, 0.0906, 0.0702, 0.0786, 0.0396])\n",
      "\n",
      "\n",
      "************** Batch 304 in 0.5711696147918701 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0939, 0.0247, 0.0894, 0.0722, 0.0822, 0.0443]) \n",
      "Test Loss tensor([0.0926, 0.0221, 0.0925, 0.0682, 0.0740, 0.0391])\n",
      "\n",
      "\n",
      "************** Batch 308 in 0.6073458194732666 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1009, 0.0229, 0.0910, 0.0678, 0.0736, 0.0391]) \n",
      "Test Loss tensor([0.0909, 0.0228, 0.0930, 0.0691, 0.0742, 0.0396])\n",
      "\n",
      "\n",
      "************** Batch 312 in 0.5953502655029297 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0923, 0.0195, 0.0877, 0.0709, 0.0778, 0.0376]) \n",
      "Test Loss tensor([0.0944, 0.0242, 0.0904, 0.0690, 0.0741, 0.0375])\n",
      "\n",
      "\n",
      "************** Batch 316 in 0.5860061645507812 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1009, 0.0236, 0.0966, 0.0789, 0.0720, 0.0442]) \n",
      "Test Loss tensor([0.0916, 0.0224, 0.0894, 0.0689, 0.0751, 0.0382])\n",
      "\n",
      "\n",
      "************** Batch 320 in 0.6077308654785156 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0911, 0.0241, 0.0926, 0.0786, 0.0771, 0.0384]) \n",
      "Test Loss tensor([0.0929, 0.0240, 0.0906, 0.0712, 0.0744, 0.0390])\n",
      "\n",
      "\n",
      "************** Batch 324 in 0.576298713684082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0870, 0.0237, 0.0924, 0.0704, 0.0682, 0.0451]) \n",
      "Test Loss tensor([0.0931, 0.0225, 0.0913, 0.0696, 0.0746, 0.0385])\n",
      "\n",
      "\n",
      "************** Batch 328 in 0.5720047950744629 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0803, 0.0211, 0.0956, 0.0724, 0.0691, 0.0392]) \n",
      "Test Loss tensor([0.0906, 0.0219, 0.0917, 0.0694, 0.0735, 0.0408])\n",
      "\n",
      "\n",
      "************** Batch 332 in 0.580974817276001 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0875, 0.0217, 0.0901, 0.0714, 0.0616, 0.0410]) \n",
      "Test Loss tensor([0.0937, 0.0245, 0.0901, 0.0700, 0.0753, 0.0383])\n",
      "\n",
      "\n",
      "************** Batch 336 in 0.5942635536193848 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0982, 0.0268, 0.0939, 0.0746, 0.0754, 0.0381]) \n",
      "Test Loss tensor([0.0969, 0.0226, 0.0905, 0.0700, 0.0762, 0.0394])\n",
      "\n",
      "\n",
      "************** Batch 340 in 0.5823206901550293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0986, 0.0250, 0.0923, 0.0706, 0.0699, 0.0366]) \n",
      "Test Loss tensor([0.0893, 0.0233, 0.0896, 0.0686, 0.0747, 0.0380])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 344 in 0.5668814182281494 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0870, 0.0263, 0.0963, 0.0630, 0.0684, 0.0446]) \n",
      "Test Loss tensor([0.0939, 0.0241, 0.0905, 0.0695, 0.0750, 0.0392])\n",
      "\n",
      "\n",
      "************** Batch 348 in 0.5856997966766357 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0228, 0.0866, 0.0641, 0.0740, 0.0325]) \n",
      "Test Loss tensor([0.0897, 0.0224, 0.0898, 0.0695, 0.0730, 0.0397])\n",
      "\n",
      "\n",
      "************** Batch 352 in 0.5790257453918457 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0842, 0.0194, 0.0916, 0.0678, 0.0734, 0.0396]) \n",
      "Test Loss tensor([0.0902, 0.0210, 0.0890, 0.0676, 0.0740, 0.0401])\n",
      "\n",
      "\n",
      "************** Batch 356 in 0.5897667407989502 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1023, 0.0237, 0.0886, 0.0771, 0.0723, 0.0395]) \n",
      "Test Loss tensor([0.0956, 0.0242, 0.0895, 0.0701, 0.0754, 0.0384])\n",
      "\n",
      "\n",
      "************** Batch 360 in 0.5911374092102051 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0990, 0.0253, 0.0932, 0.0700, 0.0745, 0.0389]) \n",
      "Test Loss tensor([0.0903, 0.0244, 0.0888, 0.0703, 0.0733, 0.0386])\n",
      "\n",
      "\n",
      "************** Batch 364 in 0.5983104705810547 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0913, 0.0255, 0.0886, 0.0630, 0.0737, 0.0390]) \n",
      "Test Loss tensor([0.0919, 0.0233, 0.0889, 0.0680, 0.0757, 0.0392])\n",
      "\n",
      "\n",
      "************** Batch 368 in 0.5917541980743408 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0924, 0.0218, 0.0904, 0.0705, 0.0768, 0.0400]) \n",
      "Test Loss tensor([0.0916, 0.0237, 0.0895, 0.0693, 0.0742, 0.0384])\n",
      "\n",
      "\n",
      "************** Batch 372 in 0.581233024597168 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0938, 0.0251, 0.0826, 0.0689, 0.0712, 0.0383]) \n",
      "Test Loss tensor([0.0914, 0.0223, 0.0870, 0.0691, 0.0737, 0.0387])\n",
      "\n",
      "\n",
      "************** Batch 376 in 0.5977187156677246 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1019, 0.0232, 0.0862, 0.0711, 0.0708, 0.0384]) \n",
      "Test Loss tensor([0.0909, 0.0235, 0.0876, 0.0693, 0.0735, 0.0381])\n",
      "\n",
      "\n",
      "************** Batch 380 in 0.5901885032653809 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0221, 0.0878, 0.0853, 0.0771, 0.0434]) \n",
      "Test Loss tensor([0.0900, 0.0241, 0.0906, 0.0695, 0.0720, 0.0378])\n",
      "\n",
      "\n",
      "************** Batch 384 in 0.6955699920654297 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0951, 0.0269, 0.0900, 0.0715, 0.0700, 0.0397]) \n",
      "Test Loss tensor([0.0897, 0.0223, 0.0885, 0.0696, 0.0735, 0.0372])\n",
      "\n",
      "\n",
      "************** Batch 388 in 0.725130558013916 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0928, 0.0242, 0.0902, 0.0733, 0.0678, 0.0396]) \n",
      "Test Loss tensor([0.0910, 0.0251, 0.0879, 0.0676, 0.0734, 0.0385])\n",
      "\n",
      "\n",
      "************** Batch 392 in 0.7158603668212891 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0972, 0.0244, 0.0849, 0.0706, 0.0789, 0.0379]) \n",
      "Test Loss tensor([0.0898, 0.0228, 0.0876, 0.0683, 0.0734, 0.0393])\n",
      "\n",
      "\n",
      "************** Batch 396 in 0.6378538608551025 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0928, 0.0259, 0.0873, 0.0705, 0.0754, 0.0452]) \n",
      "Test Loss tensor([0.0910, 0.0226, 0.0871, 0.0688, 0.0751, 0.0399])\n",
      "\n",
      "\n",
      "************** Batch 400 in 0.6963112354278564 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0992, 0.0214, 0.0884, 0.0730, 0.0707, 0.0429]) \n",
      "Test Loss tensor([0.0925, 0.0232, 0.0864, 0.0689, 0.0755, 0.0384])\n",
      "\n",
      "\n",
      "************** Batch 404 in 0.6590571403503418 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0860, 0.0242, 0.0857, 0.0684, 0.0736, 0.0398]) \n",
      "Test Loss tensor([0.0930, 0.0243, 0.0851, 0.0703, 0.0740, 0.0377])\n",
      "\n",
      "\n",
      "************** Batch 408 in 0.6121580600738525 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0976, 0.0236, 0.0911, 0.0762, 0.0730, 0.0417]) \n",
      "Test Loss tensor([0.0964, 0.0234, 0.0856, 0.0703, 0.0730, 0.0381])\n",
      "\n",
      "\n",
      "************** Batch 412 in 0.6507959365844727 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0852, 0.0246, 0.0846, 0.0694, 0.0714, 0.0361]) \n",
      "Test Loss tensor([0.0931, 0.0224, 0.0848, 0.0704, 0.0743, 0.0386])\n",
      "\n",
      "\n",
      "************** Batch 416 in 0.6318647861480713 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0868, 0.0213, 0.0838, 0.0684, 0.0730, 0.0363]) \n",
      "Test Loss tensor([0.0902, 0.0228, 0.0843, 0.0677, 0.0734, 0.0385])\n",
      "\n",
      "\n",
      "************** Batch 420 in 0.6014792919158936 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0920, 0.0243, 0.0814, 0.0708, 0.0752, 0.0433]) \n",
      "Test Loss tensor([0.0930, 0.0232, 0.0861, 0.0714, 0.0724, 0.0379])\n",
      "\n",
      "\n",
      "************** Batch 424 in 0.6915333271026611 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0895, 0.0196, 0.0868, 0.0688, 0.0721, 0.0384]) \n",
      "Test Loss tensor([0.0898, 0.0226, 0.0844, 0.0705, 0.0745, 0.0382])\n",
      "\n",
      "\n",
      "************** Batch 428 in 0.6383180618286133 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0885, 0.0252, 0.0818, 0.0687, 0.0787, 0.0354]) \n",
      "Test Loss tensor([0.0912, 0.0243, 0.0843, 0.0702, 0.0743, 0.0374])\n",
      "\n",
      "\n",
      "************** Batch 432 in 0.6331543922424316 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0903, 0.0182, 0.0855, 0.0665, 0.0706, 0.0370]) \n",
      "Test Loss tensor([0.0894, 0.0215, 0.0837, 0.0707, 0.0732, 0.0381])\n",
      "\n",
      "\n",
      "************** Batch 436 in 0.6504228115081787 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0963, 0.0241, 0.0818, 0.0732, 0.0714, 0.0378]) \n",
      "Test Loss tensor([0.0920, 0.0217, 0.0839, 0.0701, 0.0759, 0.0408])\n",
      "\n",
      "\n",
      "************** Batch 440 in 0.6570532321929932 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0922, 0.0199, 0.0830, 0.0797, 0.0775, 0.0414]) \n",
      "Test Loss tensor([0.0935, 0.0225, 0.0855, 0.0726, 0.0758, 0.0382])\n",
      "\n",
      "\n",
      "************** Batch 444 in 0.5735151767730713 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0851, 0.0284, 0.0840, 0.0711, 0.0747, 0.0395]) \n",
      "Test Loss tensor([0.0933, 0.0214, 0.0829, 0.0730, 0.0753, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 448 in 0.5954961776733398 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0217, 0.0843, 0.0692, 0.0761, 0.0360]) \n",
      "Test Loss tensor([0.0913, 0.0224, 0.0822, 0.0664, 0.0744, 0.0408])\n",
      "\n",
      "\n",
      "************** Batch 452 in 0.5840420722961426 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0974, 0.0209, 0.0841, 0.0711, 0.0803, 0.0426]) \n",
      "Test Loss tensor([0.0927, 0.0216, 0.0851, 0.0703, 0.0717, 0.0386])\n",
      "\n",
      "\n",
      "************** Batch 456 in 0.6173872947692871 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0865, 0.0244, 0.0837, 0.0638, 0.0763, 0.0373]) \n",
      "Test Loss tensor([0.0888, 0.0230, 0.0845, 0.0689, 0.0758, 0.0408])\n",
      "\n",
      "\n",
      "************** Batch 460 in 0.5992238521575928 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0899, 0.0233, 0.0828, 0.0726, 0.0756, 0.0369]) \n",
      "Test Loss tensor([0.0915, 0.0231, 0.0825, 0.0699, 0.0739, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 464 in 0.6103551387786865 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0985, 0.0257, 0.0879, 0.0719, 0.0729, 0.0382]) \n",
      "Test Loss tensor([0.0925, 0.0235, 0.0840, 0.0688, 0.0771, 0.0384])\n",
      "\n",
      "\n",
      "************** Batch 468 in 0.6051328182220459 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0942, 0.0210, 0.0806, 0.0701, 0.0774, 0.0365]) \n",
      "Test Loss tensor([0.0923, 0.0232, 0.0825, 0.0695, 0.0722, 0.0389])\n",
      "\n",
      "\n",
      "************** Batch 472 in 0.5702173709869385 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0797, 0.0266, 0.0818, 0.0730, 0.0695, 0.0387]) \n",
      "Test Loss tensor([0.0910, 0.0233, 0.0828, 0.0711, 0.0732, 0.0397])\n",
      "\n",
      "\n",
      "************** Batch 476 in 0.5960614681243896 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0920, 0.0257, 0.0809, 0.0706, 0.0726, 0.0438]) \n",
      "Test Loss tensor([0.0914, 0.0226, 0.0819, 0.0697, 0.0744, 0.0385])\n",
      "\n",
      "\n",
      "************** Batch 480 in 0.5841679573059082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0935, 0.0208, 0.0847, 0.0734, 0.0724, 0.0392]) \n",
      "Test Loss tensor([0.0915, 0.0228, 0.0804, 0.0700, 0.0752, 0.0403])\n",
      "\n",
      "\n",
      "************** Batch 484 in 0.5776166915893555 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0232, 0.0814, 0.0668, 0.0700, 0.0406]) \n",
      "Test Loss tensor([0.0900, 0.0219, 0.0807, 0.0709, 0.0744, 0.0375])\n",
      "\n",
      "\n",
      "************** Batch 488 in 0.5777461528778076 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0956, 0.0256, 0.0788, 0.0694, 0.0711, 0.0333]) \n",
      "Test Loss tensor([0.0920, 0.0227, 0.0820, 0.0700, 0.0738, 0.0377])\n",
      "\n",
      "\n",
      "************** Batch 492 in 0.5788874626159668 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0951, 0.0241, 0.0882, 0.0744, 0.0743, 0.0345]) \n",
      "Test Loss tensor([0.0890, 0.0226, 0.0796, 0.0695, 0.0743, 0.0387])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 496 in 0.5914082527160645 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0989, 0.0262, 0.0810, 0.0759, 0.0723, 0.0370]) \n",
      "Test Loss tensor([0.0958, 0.0204, 0.0801, 0.0673, 0.0752, 0.0391])\n",
      "\n",
      "\n",
      "************** Batch 500 in 0.5676460266113281 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1009, 0.0253, 0.0784, 0.0711, 0.0787, 0.0368]) \n",
      "Test Loss tensor([0.0910, 0.0223, 0.0794, 0.0690, 0.0752, 0.0391])\n",
      "\n",
      "\n",
      "************** Batch 504 in 0.5934889316558838 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0923, 0.0272, 0.0829, 0.0688, 0.0812, 0.0341]) \n",
      "Test Loss tensor([0.0900, 0.0227, 0.0813, 0.0710, 0.0747, 0.0400])\n",
      "\n",
      "\n",
      "************** Batch 508 in 0.5913283824920654 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0944, 0.0264, 0.0768, 0.0678, 0.0800, 0.0414]) \n",
      "Test Loss tensor([0.0970, 0.0220, 0.0782, 0.0702, 0.0753, 0.0388])\n",
      "\n",
      "\n",
      "************** Batch 512 in 0.5743005275726318 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0963, 0.0252, 0.0769, 0.0758, 0.0740, 0.0369]) \n",
      "Test Loss tensor([0.0895, 0.0219, 0.0787, 0.0700, 0.0749, 0.0372])\n",
      "\n",
      "\n",
      "************** Batch 516 in 0.6819825172424316 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0239, 0.0771, 0.0757, 0.0724, 0.0365]) \n",
      "Test Loss tensor([0.0910, 0.0224, 0.0793, 0.0696, 0.0767, 0.0394])\n",
      "\n",
      "\n",
      "************** Batch 520 in 0.7153475284576416 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0925, 0.0252, 0.0757, 0.0744, 0.0814, 0.0386]) \n",
      "Test Loss tensor([0.0939, 0.0228, 0.0788, 0.0711, 0.0748, 0.0376])\n",
      "\n",
      "\n",
      "************** Batch 524 in 0.6549561023712158 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0897, 0.0297, 0.0844, 0.0760, 0.0798, 0.0404]) \n",
      "Test Loss tensor([0.0924, 0.0242, 0.0794, 0.0712, 0.0770, 0.0378])\n",
      "\n",
      "\n",
      "************** Batch 528 in 0.6970582008361816 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0852, 0.0245, 0.0792, 0.0685, 0.0739, 0.0456]) \n",
      "Test Loss tensor([0.0916, 0.0230, 0.0778, 0.0697, 0.0733, 0.0380])\n",
      "\n",
      "\n",
      "************** Batch 532 in 0.6246387958526611 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0847, 0.0235, 0.0841, 0.0698, 0.0663, 0.0445]) \n",
      "Test Loss tensor([0.0953, 0.0226, 0.0778, 0.0698, 0.0747, 0.0377])\n",
      "\n",
      "\n",
      "************** Batch 536 in 0.6519689559936523 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0912, 0.0235, 0.0821, 0.0641, 0.0711, 0.0388]) \n",
      "Test Loss tensor([0.0923, 0.0225, 0.0774, 0.0704, 0.0749, 0.0371])\n",
      "\n",
      "\n",
      "************** Batch 540 in 0.6927707195281982 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0894, 0.0192, 0.0769, 0.0678, 0.0714, 0.0381]) \n",
      "Test Loss tensor([0.0905, 0.0230, 0.0781, 0.0694, 0.0727, 0.0382])\n",
      "\n",
      "\n",
      "************** Batch 544 in 0.6214399337768555 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0912, 0.0228, 0.0781, 0.0697, 0.0733, 0.0388]) \n",
      "Test Loss tensor([0.0927, 0.0257, 0.0796, 0.0680, 0.0736, 0.0382])\n",
      "\n",
      "\n",
      "************** Batch 548 in 0.6151130199432373 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0901, 0.0227, 0.0789, 0.0709, 0.0677, 0.0360]) \n",
      "Test Loss tensor([0.0910, 0.0240, 0.0770, 0.0673, 0.0762, 0.0374])\n",
      "\n",
      "\n",
      "************** Batch 552 in 0.6453759670257568 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0916, 0.0237, 0.0787, 0.0656, 0.0679, 0.0354]) \n",
      "Test Loss tensor([0.0887, 0.0230, 0.0757, 0.0689, 0.0754, 0.0377])\n",
      "\n",
      "\n",
      "************** Batch 556 in 0.5777599811553955 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0868, 0.0260, 0.0720, 0.0745, 0.0740, 0.0380]) \n",
      "Test Loss tensor([0.0889, 0.0221, 0.0760, 0.0702, 0.0724, 0.0372])\n",
      "\n",
      "\n",
      "************** Batch 560 in 0.5728669166564941 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0992, 0.0248, 0.0768, 0.0666, 0.0784, 0.0365]) \n",
      "Test Loss tensor([0.0897, 0.0236, 0.0794, 0.0701, 0.0715, 0.0404])\n",
      "\n",
      "\n",
      "************** Batch 564 in 0.5842287540435791 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0248, 0.0726, 0.0650, 0.0761, 0.0401]) \n",
      "Test Loss tensor([0.0903, 0.0232, 0.0757, 0.0705, 0.0742, 0.0367])\n",
      "\n",
      "\n",
      "************** Batch 568 in 0.5863189697265625 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1000, 0.0272, 0.0770, 0.0788, 0.0738, 0.0406]) \n",
      "Test Loss tensor([0.0912, 0.0227, 0.0740, 0.0713, 0.0741, 0.0383])\n",
      "\n",
      "\n",
      "************** Batch 572 in 0.5977554321289062 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0991, 0.0242, 0.0763, 0.0694, 0.0711, 0.0389]) \n",
      "Test Loss tensor([0.0914, 0.0225, 0.0774, 0.0705, 0.0726, 0.0378])\n",
      "\n",
      "\n",
      "************** Batch 576 in 0.5750458240509033 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0955, 0.0255, 0.0732, 0.0692, 0.0712, 0.0393]) \n",
      "Test Loss tensor([0.0924, 0.0235, 0.0759, 0.0687, 0.0749, 0.0375])\n",
      "\n",
      "\n",
      "************** Batch 580 in 0.6975061893463135 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0865, 0.0238, 0.0746, 0.0676, 0.0764, 0.0330]) \n",
      "Test Loss tensor([0.0918, 0.0237, 0.0749, 0.0720, 0.0738, 0.0379])\n",
      "\n",
      "\n",
      "************** Batch 584 in 0.6570291519165039 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0228, 0.0705, 0.0738, 0.0703, 0.0338]) \n",
      "Test Loss tensor([0.0914, 0.0214, 0.0755, 0.0717, 0.0724, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 588 in 0.6932239532470703 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0911, 0.0233, 0.0705, 0.0656, 0.0723, 0.0363]) \n",
      "Test Loss tensor([0.0896, 0.0228, 0.0747, 0.0693, 0.0737, 0.0383])\n",
      "\n",
      "\n",
      "************** Batch 592 in 0.7179992198944092 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0257, 0.0753, 0.0644, 0.0702, 0.0365]) \n",
      "Test Loss tensor([0.0887, 0.0227, 0.0744, 0.0701, 0.0733, 0.0378])\n",
      "\n",
      "\n",
      "************** Batch 596 in 0.6831040382385254 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0967, 0.0235, 0.0747, 0.0710, 0.0738, 0.0394]) \n",
      "Test Loss tensor([0.0920, 0.0223, 0.0752, 0.0717, 0.0726, 0.0385])\n",
      "\n",
      "\n",
      "************** Batch 600 in 0.6915984153747559 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0986, 0.0231, 0.0688, 0.0813, 0.0725, 0.0368]) \n",
      "Test Loss tensor([0.0952, 0.0235, 0.0750, 0.0702, 0.0735, 0.0379])\n",
      "\n",
      "\n",
      "************** Batch 604 in 0.6921982765197754 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0950, 0.0226, 0.0749, 0.0678, 0.0711, 0.0325]) \n",
      "Test Loss tensor([0.0933, 0.0249, 0.0753, 0.0698, 0.0714, 0.0384])\n",
      "\n",
      "\n",
      "************** Batch 608 in 0.6969406604766846 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1017, 0.0205, 0.0738, 0.0739, 0.0691, 0.0369]) \n",
      "Test Loss tensor([0.0892, 0.0223, 0.0749, 0.0698, 0.0740, 0.0379])\n",
      "\n",
      "\n",
      "************** Batch 612 in 0.6996610164642334 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0870, 0.0238, 0.0755, 0.0710, 0.0723, 0.0406]) \n",
      "Test Loss tensor([0.0921, 0.0231, 0.0735, 0.0711, 0.0733, 0.0366])\n",
      "\n",
      "\n",
      "************** Batch 616 in 0.7127766609191895 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0902, 0.0220, 0.0686, 0.0741, 0.0732, 0.0358]) \n",
      "Test Loss tensor([0.0911, 0.0222, 0.0734, 0.0684, 0.0721, 0.0393])\n",
      "\n",
      "\n",
      "************** Batch 620 in 0.7399375438690186 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0950, 0.0236, 0.0760, 0.0713, 0.0728, 0.0447]) \n",
      "Test Loss tensor([0.0901, 0.0225, 0.0730, 0.0687, 0.0726, 0.0375])\n",
      "\n",
      "\n",
      "************** Batch 624 in 0.6925036907196045 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0928, 0.0203, 0.0715, 0.0687, 0.0687, 0.0384]) \n",
      "Test Loss tensor([0.0914, 0.0220, 0.0753, 0.0719, 0.0736, 0.0385])\n",
      "\n",
      "\n",
      "************** Batch 628 in 0.6540873050689697 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0968, 0.0201, 0.0714, 0.0725, 0.0723, 0.0391]) \n",
      "Test Loss tensor([0.0927, 0.0227, 0.0723, 0.0687, 0.0745, 0.0381])\n",
      "\n",
      "\n",
      "************** Batch 632 in 0.6981563568115234 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0896, 0.0231, 0.0696, 0.0702, 0.0743, 0.0367]) \n",
      "Test Loss tensor([0.0949, 0.0228, 0.0723, 0.0719, 0.0754, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 636 in 0.6989827156066895 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0988, 0.0286, 0.0760, 0.0727, 0.0736, 0.0430]) \n",
      "Test Loss tensor([0.0902, 0.0244, 0.0717, 0.0700, 0.0752, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 640 in 0.733208417892456 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0939, 0.0234, 0.0705, 0.0696, 0.0697, 0.0371]) \n",
      "Test Loss tensor([0.0925, 0.0238, 0.0728, 0.0703, 0.0728, 0.0377])\n",
      "\n",
      "\n",
      "************** Batch 644 in 0.6977086067199707 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0852, 0.0205, 0.0775, 0.0729, 0.0700, 0.0384]) \n",
      "Test Loss tensor([0.0913, 0.0239, 0.0738, 0.0705, 0.0740, 0.0390])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 648 in 0.6999895572662354 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0843, 0.0220, 0.0756, 0.0694, 0.0750, 0.0417]) \n",
      "Test Loss tensor([0.0929, 0.0228, 0.0723, 0.0713, 0.0756, 0.0385])\n",
      "\n",
      "\n",
      "************** Batch 652 in 0.6951413154602051 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0904, 0.0191, 0.0689, 0.0707, 0.0691, 0.0394]) \n",
      "Test Loss tensor([0.0931, 0.0234, 0.0732, 0.0709, 0.0735, 0.0371])\n",
      "\n",
      "\n",
      "************** Batch 656 in 0.6700460910797119 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0949, 0.0223, 0.0698, 0.0686, 0.0672, 0.0392]) \n",
      "Test Loss tensor([0.0895, 0.0227, 0.0732, 0.0708, 0.0727, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 660 in 0.7313046455383301 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0935, 0.0227, 0.0708, 0.0719, 0.0707, 0.0358]) \n",
      "Test Loss tensor([0.0901, 0.0237, 0.0724, 0.0694, 0.0723, 0.0377])\n",
      "\n",
      "\n",
      "************** Batch 664 in 0.7413077354431152 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0917, 0.0240, 0.0697, 0.0672, 0.0736, 0.0335]) \n",
      "Test Loss tensor([0.0918, 0.0247, 0.0728, 0.0709, 0.0736, 0.0370])\n",
      "\n",
      "\n",
      "************** Batch 668 in 0.6317391395568848 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0226, 0.0684, 0.0692, 0.0719, 0.0348]) \n",
      "Test Loss tensor([0.0891, 0.0232, 0.0714, 0.0705, 0.0752, 0.0393])\n",
      "\n",
      "\n",
      "************** Batch 672 in 0.6429469585418701 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0950, 0.0241, 0.0704, 0.0702, 0.0751, 0.0345]) \n",
      "Test Loss tensor([0.0905, 0.0240, 0.0713, 0.0701, 0.0736, 0.0378])\n",
      "\n",
      "\n",
      "************** Batch 676 in 0.6278276443481445 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1009, 0.0260, 0.0741, 0.0701, 0.0787, 0.0352]) \n",
      "Test Loss tensor([0.0932, 0.0240, 0.0719, 0.0681, 0.0738, 0.0372])\n",
      "\n",
      "\n",
      "************** Batch 680 in 0.6286625862121582 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0903, 0.0216, 0.0677, 0.0686, 0.0708, 0.0323]) \n",
      "Test Loss tensor([0.0890, 0.0220, 0.0701, 0.0693, 0.0745, 0.0375])\n",
      "\n",
      "\n",
      "************** Batch 684 in 0.6262972354888916 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0284, 0.0729, 0.0772, 0.0787, 0.0364]) \n",
      "Test Loss tensor([0.0912, 0.0225, 0.0721, 0.0723, 0.0728, 0.0370])\n",
      "\n",
      "\n",
      "************** Batch 688 in 0.5934998989105225 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0243, 0.0707, 0.0671, 0.0728, 0.0344]) \n",
      "Test Loss tensor([0.0903, 0.0218, 0.0704, 0.0711, 0.0736, 0.0371])\n",
      "\n",
      "\n",
      "************** Batch 692 in 0.6172695159912109 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0945, 0.0228, 0.0685, 0.0639, 0.0754, 0.0319]) \n",
      "Test Loss tensor([0.0919, 0.0230, 0.0702, 0.0703, 0.0729, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 696 in 0.5839192867279053 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0804, 0.0256, 0.0708, 0.0661, 0.0734, 0.0394]) \n",
      "Test Loss tensor([0.0922, 0.0237, 0.0702, 0.0691, 0.0732, 0.0383])\n",
      "\n",
      "\n",
      "************** Batch 700 in 0.5805950164794922 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0918, 0.0243, 0.0727, 0.0714, 0.0747, 0.0376]) \n",
      "Test Loss tensor([0.0918, 0.0227, 0.0700, 0.0702, 0.0738, 0.0371])\n",
      "\n",
      "\n",
      "************** Batch 704 in 0.5925891399383545 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1017, 0.0231, 0.0713, 0.0722, 0.0788, 0.0342]) \n",
      "Test Loss tensor([0.0925, 0.0239, 0.0713, 0.0697, 0.0724, 0.0390])\n",
      "\n",
      "\n",
      "************** Batch 708 in 0.5849604606628418 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0907, 0.0247, 0.0712, 0.0643, 0.0713, 0.0390]) \n",
      "Test Loss tensor([0.0903, 0.0231, 0.0680, 0.0690, 0.0745, 0.0369])\n",
      "\n",
      "\n",
      "************** Batch 712 in 0.6031808853149414 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0815, 0.0255, 0.0666, 0.0731, 0.0735, 0.0375]) \n",
      "Test Loss tensor([0.0903, 0.0228, 0.0684, 0.0728, 0.0728, 0.0367])\n",
      "\n",
      "\n",
      "************** Batch 716 in 0.6013193130493164 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0813, 0.0237, 0.0694, 0.0701, 0.0733, 0.0362]) \n",
      "Test Loss tensor([0.0937, 0.0236, 0.0689, 0.0716, 0.0745, 0.0375])\n",
      "\n",
      "\n",
      "************** Batch 720 in 0.6067450046539307 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0893, 0.0228, 0.0692, 0.0715, 0.0719, 0.0375]) \n",
      "Test Loss tensor([0.0924, 0.0218, 0.0695, 0.0709, 0.0747, 0.0399])\n",
      "\n",
      "\n",
      "************** Batch 724 in 0.5936233997344971 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0924, 0.0228, 0.0680, 0.0727, 0.0713, 0.0387]) \n",
      "Test Loss tensor([0.0898, 0.0221, 0.0672, 0.0711, 0.0734, 0.0363])\n",
      "\n",
      "\n",
      "************** Batch 728 in 0.5733747482299805 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0916, 0.0202, 0.0677, 0.0702, 0.0740, 0.0332]) \n",
      "Test Loss tensor([0.0915, 0.0218, 0.0674, 0.0718, 0.0739, 0.0374])\n",
      "\n",
      "\n",
      "************** Batch 732 in 0.5918827056884766 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0911, 0.0262, 0.0648, 0.0717, 0.0717, 0.0429]) \n",
      "Test Loss tensor([0.0942, 0.0233, 0.0688, 0.0697, 0.0757, 0.0390])\n",
      "\n",
      "\n",
      "************** Batch 736 in 0.5867130756378174 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0226, 0.0665, 0.0726, 0.0751, 0.0315]) \n",
      "Test Loss tensor([0.0928, 0.0229, 0.0700, 0.0714, 0.0726, 0.0376])\n",
      "\n",
      "\n",
      "************** Batch 740 in 0.5781552791595459 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0884, 0.0260, 0.0688, 0.0727, 0.0706, 0.0360]) \n",
      "Test Loss tensor([0.0920, 0.0239, 0.0671, 0.0677, 0.0766, 0.0427])\n",
      "\n",
      "\n",
      "************** Batch 744 in 0.6031434535980225 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0944, 0.0269, 0.0675, 0.0823, 0.0805, 0.0428]) \n",
      "Test Loss tensor([0.0917, 0.0232, 0.0702, 0.0721, 0.0728, 0.0374])\n",
      "\n",
      "\n",
      "************** Batch 748 in 0.5743930339813232 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0964, 0.0197, 0.0692, 0.0792, 0.0730, 0.0311]) \n",
      "Test Loss tensor([0.0942, 0.0222, 0.0674, 0.0708, 0.0736, 0.0379])\n",
      "\n",
      "\n",
      "************** Batch 752 in 0.5922048091888428 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0973, 0.0255, 0.0661, 0.0676, 0.0721, 0.0381]) \n",
      "Test Loss tensor([0.0942, 0.0233, 0.0665, 0.0702, 0.0761, 0.0377])\n",
      "\n",
      "\n",
      "************** Batch 756 in 0.586559534072876 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0940, 0.0246, 0.0676, 0.0728, 0.0713, 0.0340]) \n",
      "Test Loss tensor([0.0923, 0.0225, 0.0663, 0.0710, 0.0730, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 760 in 0.5773482322692871 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0892, 0.0260, 0.0659, 0.0734, 0.0723, 0.0370]) \n",
      "Test Loss tensor([0.0902, 0.0234, 0.0663, 0.0695, 0.0756, 0.0386])\n",
      "\n",
      "\n",
      "************** Batch 764 in 0.5954525470733643 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1007, 0.0228, 0.0625, 0.0731, 0.0733, 0.0393]) \n",
      "Test Loss tensor([0.0902, 0.0222, 0.0644, 0.0690, 0.0734, 0.0360])\n",
      "\n",
      "\n",
      "************** Batch 768 in 0.5824942588806152 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0919, 0.0288, 0.0665, 0.0802, 0.0760, 0.0362]) \n",
      "Test Loss tensor([0.0942, 0.0237, 0.0676, 0.0736, 0.0730, 0.0377])\n",
      "\n",
      "\n",
      "************** Batch 772 in 0.6020424365997314 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0908, 0.0235, 0.0679, 0.0688, 0.0775, 0.0402]) \n",
      "Test Loss tensor([0.0933, 0.0229, 0.0666, 0.0730, 0.0720, 0.0367])\n",
      "\n",
      "\n",
      "************** Batch 776 in 0.5743317604064941 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0928, 0.0218, 0.0624, 0.0761, 0.0761, 0.0370]) \n",
      "Test Loss tensor([0.0905, 0.0233, 0.0666, 0.0708, 0.0732, 0.0365])\n",
      "\n",
      "\n",
      "************** Batch 780 in 0.5847957134246826 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0904, 0.0247, 0.0653, 0.0710, 0.0711, 0.0399]) \n",
      "Test Loss tensor([0.0905, 0.0221, 0.0652, 0.0687, 0.0743, 0.0389])\n",
      "\n",
      "\n",
      "************** Batch 784 in 0.5955443382263184 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0997, 0.0246, 0.0648, 0.0733, 0.0726, 0.0380]) \n",
      "Test Loss tensor([0.0901, 0.0230, 0.0658, 0.0726, 0.0721, 0.0367])\n",
      "\n",
      "\n",
      "************** Batch 788 in 0.5877256393432617 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0922, 0.0210, 0.0626, 0.0729, 0.0709, 0.0344]) \n",
      "Test Loss tensor([0.0907, 0.0227, 0.0656, 0.0695, 0.0722, 0.0372])\n",
      "\n",
      "\n",
      "************** Batch 792 in 0.592158317565918 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0929, 0.0235, 0.0644, 0.0732, 0.0707, 0.0403]) \n",
      "Test Loss tensor([0.0884, 0.0223, 0.0644, 0.0701, 0.0722, 0.0370])\n",
      "\n",
      "\n",
      "************** Batch 796 in 0.5879907608032227 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0955, 0.0260, 0.0599, 0.0745, 0.0731, 0.0393]) \n",
      "Test Loss tensor([0.0903, 0.0234, 0.0663, 0.0687, 0.0718, 0.0375])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 800 in 0.605797529220581 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0926, 0.0248, 0.0668, 0.0734, 0.0689, 0.0320]) \n",
      "Test Loss tensor([0.0911, 0.0228, 0.0651, 0.0702, 0.0731, 0.0369])\n",
      "\n",
      "\n",
      "************** Batch 804 in 0.6025645732879639 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0864, 0.0235, 0.0663, 0.0748, 0.0760, 0.0396]) \n",
      "Test Loss tensor([0.0903, 0.0222, 0.0642, 0.0692, 0.0733, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 808 in 0.5780396461486816 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0900, 0.0222, 0.0673, 0.0726, 0.0711, 0.0374]) \n",
      "Test Loss tensor([0.0909, 0.0222, 0.0634, 0.0695, 0.0738, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 812 in 0.5955057144165039 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0834, 0.0244, 0.0582, 0.0710, 0.0697, 0.0372]) \n",
      "Test Loss tensor([0.0945, 0.0235, 0.0651, 0.0707, 0.0741, 0.0378])\n",
      "\n",
      "\n",
      "************** Batch 816 in 0.5710477828979492 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1008, 0.0212, 0.0613, 0.0706, 0.0709, 0.0345]) \n",
      "Test Loss tensor([0.0922, 0.0241, 0.0646, 0.0719, 0.0748, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 820 in 0.6004948616027832 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0926, 0.0190, 0.0675, 0.0801, 0.0699, 0.0414]) \n",
      "Test Loss tensor([0.0916, 0.0232, 0.0642, 0.0703, 0.0753, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 824 in 0.6084365844726562 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0999, 0.0230, 0.0642, 0.0695, 0.0784, 0.0342]) \n",
      "Test Loss tensor([0.0902, 0.0220, 0.0651, 0.0714, 0.0740, 0.0369])\n",
      "\n",
      "\n",
      "************** Batch 828 in 0.604426383972168 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0942, 0.0265, 0.0663, 0.0677, 0.0790, 0.0366]) \n",
      "Test Loss tensor([0.0946, 0.0234, 0.0644, 0.0696, 0.0725, 0.0370])\n",
      "\n",
      "\n",
      "************** Batch 832 in 0.5962061882019043 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0175, 0.0623, 0.0717, 0.0708, 0.0350]) \n",
      "Test Loss tensor([0.0910, 0.0260, 0.0654, 0.0713, 0.0713, 0.0376])\n",
      "\n",
      "\n",
      "************** Batch 836 in 0.57779860496521 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0867, 0.0218, 0.0670, 0.0651, 0.0746, 0.0417]) \n",
      "Test Loss tensor([0.0929, 0.0235, 0.0644, 0.0700, 0.0738, 0.0367])\n",
      "\n",
      "\n",
      "************** Batch 840 in 0.5903499126434326 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0794, 0.0167, 0.0643, 0.0679, 0.0733, 0.0329]) \n",
      "Test Loss tensor([0.0930, 0.0227, 0.0638, 0.0729, 0.0753, 0.0376])\n",
      "\n",
      "\n",
      "************** Batch 844 in 0.5801267623901367 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0936, 0.0194, 0.0625, 0.0747, 0.0682, 0.0415]) \n",
      "Test Loss tensor([0.0898, 0.0228, 0.0648, 0.0681, 0.0733, 0.0374])\n",
      "\n",
      "\n",
      "************** Batch 848 in 0.5808796882629395 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0923, 0.0211, 0.0640, 0.0658, 0.0713, 0.0379]) \n",
      "Test Loss tensor([0.0907, 0.0221, 0.0643, 0.0720, 0.0745, 0.0361])\n",
      "\n",
      "\n",
      "************** Batch 852 in 0.5859735012054443 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0960, 0.0285, 0.0661, 0.0791, 0.0755, 0.0388]) \n",
      "Test Loss tensor([0.0885, 0.0220, 0.0634, 0.0702, 0.0719, 0.0378])\n",
      "\n",
      "\n",
      "************** Batch 856 in 0.6170032024383545 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0932, 0.0223, 0.0680, 0.0668, 0.0746, 0.0362]) \n",
      "Test Loss tensor([0.0913, 0.0225, 0.0631, 0.0720, 0.0729, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 860 in 0.7147002220153809 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0936, 0.0209, 0.0641, 0.0695, 0.0687, 0.0365]) \n",
      "Test Loss tensor([0.0914, 0.0236, 0.0613, 0.0709, 0.0727, 0.0366])\n",
      "\n",
      "\n",
      "************** Batch 864 in 0.6772415637969971 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0858, 0.0213, 0.0641, 0.0760, 0.0724, 0.0368]) \n",
      "Test Loss tensor([0.0921, 0.0220, 0.0635, 0.0706, 0.0730, 0.0366])\n",
      "\n",
      "\n",
      "************** Batch 868 in 0.6389131546020508 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0833, 0.0216, 0.0590, 0.0780, 0.0733, 0.0371]) \n",
      "Test Loss tensor([0.0923, 0.0234, 0.0633, 0.0693, 0.0718, 0.0365])\n",
      "\n",
      "\n",
      "************** Batch 872 in 0.6535983085632324 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0855, 0.0217, 0.0634, 0.0790, 0.0728, 0.0384]) \n",
      "Test Loss tensor([0.0884, 0.0230, 0.0621, 0.0695, 0.0730, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 876 in 0.657189130783081 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0685, 0.0140, 0.0490, 0.0530, 0.0533, 0.0255]) \n",
      "Test Loss tensor([0.0890, 0.0225, 0.0633, 0.0684, 0.0725, 0.0366])\n",
      "\n",
      "\n",
      "************** Batch 0 in 0.6665496826171875 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0940, 0.0240, 0.0645, 0.0712, 0.0702, 0.0343]) \n",
      "Test Loss tensor([0.0897, 0.0237, 0.0623, 0.0684, 0.0722, 0.0369])\n",
      "\n",
      "\n",
      "************** Batch 4 in 0.6771438121795654 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0955, 0.0276, 0.0607, 0.0702, 0.0714, 0.0347]) \n",
      "Test Loss tensor([0.0927, 0.0223, 0.0617, 0.0713, 0.0735, 0.0367])\n",
      "\n",
      "\n",
      "************** Batch 8 in 0.7051174640655518 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0900, 0.0240, 0.0641, 0.0755, 0.0728, 0.0386]) \n",
      "Test Loss tensor([0.0888, 0.0222, 0.0624, 0.0719, 0.0736, 0.0375])\n",
      "\n",
      "\n",
      "************** Batch 12 in 0.6357307434082031 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0877, 0.0190, 0.0652, 0.0674, 0.0786, 0.0365]) \n",
      "Test Loss tensor([0.0918, 0.0228, 0.0612, 0.0699, 0.0741, 0.0364])\n",
      "\n",
      "\n",
      "************** Batch 16 in 0.6205673217773438 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0963, 0.0219, 0.0621, 0.0725, 0.0753, 0.0354]) \n",
      "Test Loss tensor([0.0944, 0.0232, 0.0632, 0.0717, 0.0730, 0.0366])\n",
      "\n",
      "\n",
      "************** Batch 20 in 0.6424386501312256 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0844, 0.0233, 0.0653, 0.0686, 0.0673, 0.0392]) \n",
      "Test Loss tensor([0.0894, 0.0234, 0.0617, 0.0695, 0.0736, 0.0371])\n",
      "\n",
      "\n",
      "************** Batch 24 in 0.6631569862365723 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0821, 0.0244, 0.0610, 0.0692, 0.0659, 0.0349]) \n",
      "Test Loss tensor([0.0886, 0.0239, 0.0629, 0.0726, 0.0740, 0.0361])\n",
      "\n",
      "\n",
      "************** Batch 28 in 0.7020049095153809 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0212, 0.0602, 0.0628, 0.0713, 0.0366]) \n",
      "Test Loss tensor([0.0900, 0.0224, 0.0641, 0.0699, 0.0741, 0.0378])\n",
      "\n",
      "\n",
      "************** Batch 32 in 0.6837787628173828 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0912, 0.0235, 0.0649, 0.0641, 0.0705, 0.0365]) \n",
      "Test Loss tensor([0.0923, 0.0220, 0.0615, 0.0695, 0.0731, 0.0359])\n",
      "\n",
      "\n",
      "************** Batch 36 in 0.6630995273590088 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0884, 0.0185, 0.0572, 0.0726, 0.0741, 0.0373]) \n",
      "Test Loss tensor([0.0912, 0.0223, 0.0618, 0.0718, 0.0745, 0.0366])\n",
      "\n",
      "\n",
      "************** Batch 40 in 0.6731019020080566 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0869, 0.0246, 0.0598, 0.0752, 0.0727, 0.0358]) \n",
      "Test Loss tensor([0.0915, 0.0227, 0.0612, 0.0705, 0.0722, 0.0364])\n",
      "\n",
      "\n",
      "************** Batch 44 in 0.6462109088897705 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0960, 0.0245, 0.0618, 0.0751, 0.0772, 0.0379]) \n",
      "Test Loss tensor([0.0929, 0.0226, 0.0604, 0.0709, 0.0742, 0.0362])\n",
      "\n",
      "\n",
      "************** Batch 48 in 0.6122915744781494 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0990, 0.0262, 0.0629, 0.0730, 0.0649, 0.0362]) \n",
      "Test Loss tensor([0.0917, 0.0217, 0.0611, 0.0696, 0.0741, 0.0367])\n",
      "\n",
      "\n",
      "************** Batch 52 in 0.6467328071594238 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0924, 0.0236, 0.0614, 0.0717, 0.0668, 0.0369]) \n",
      "Test Loss tensor([0.0937, 0.0231, 0.0622, 0.0682, 0.0764, 0.0390])\n",
      "\n",
      "\n",
      "************** Batch 56 in 0.6521110534667969 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0901, 0.0246, 0.0628, 0.0715, 0.0772, 0.0359]) \n",
      "Test Loss tensor([0.0897, 0.0227, 0.0601, 0.0683, 0.0733, 0.0371])\n",
      "\n",
      "\n",
      "************** Batch 60 in 0.6990501880645752 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0801, 0.0263, 0.0587, 0.0584, 0.0698, 0.0432]) \n",
      "Test Loss tensor([0.0902, 0.0229, 0.0600, 0.0687, 0.0770, 0.0386])\n",
      "\n",
      "\n",
      "************** Batch 64 in 0.7188527584075928 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0906, 0.0204, 0.0584, 0.0641, 0.0740, 0.0372]) \n",
      "Test Loss tensor([0.0931, 0.0227, 0.0589, 0.0687, 0.0731, 0.0363])\n",
      "\n",
      "\n",
      "************** Batch 68 in 0.6974835395812988 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0958, 0.0270, 0.0537, 0.0747, 0.0691, 0.0321]) \n",
      "Test Loss tensor([0.0922, 0.0227, 0.0615, 0.0711, 0.0727, 0.0389])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 72 in 0.6062521934509277 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0900, 0.0216, 0.0591, 0.0707, 0.0724, 0.0409]) \n",
      "Test Loss tensor([0.0930, 0.0222, 0.0611, 0.0690, 0.0742, 0.0376])\n",
      "\n",
      "\n",
      "************** Batch 76 in 0.6374781131744385 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0943, 0.0206, 0.0652, 0.0714, 0.0704, 0.0361]) \n",
      "Test Loss tensor([0.0904, 0.0235, 0.0599, 0.0682, 0.0717, 0.0362])\n",
      "\n",
      "\n",
      "************** Batch 80 in 0.6373028755187988 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0327, 0.0565, 0.0729, 0.0701, 0.0337]) \n",
      "Test Loss tensor([0.0925, 0.0229, 0.0595, 0.0702, 0.0742, 0.0379])\n",
      "\n",
      "\n",
      "************** Batch 84 in 0.7082312107086182 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0924, 0.0263, 0.0565, 0.0677, 0.0649, 0.0385]) \n",
      "Test Loss tensor([0.0916, 0.0235, 0.0611, 0.0688, 0.0723, 0.0384])\n",
      "\n",
      "\n",
      "************** Batch 88 in 0.714911699295044 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0883, 0.0258, 0.0611, 0.0703, 0.0731, 0.0364]) \n",
      "Test Loss tensor([0.0911, 0.0242, 0.0585, 0.0667, 0.0721, 0.0387])\n",
      "\n",
      "\n",
      "************** Batch 92 in 0.653059720993042 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1021, 0.0226, 0.0608, 0.0679, 0.0754, 0.0434]) \n",
      "Test Loss tensor([0.0938, 0.0234, 0.0599, 0.0673, 0.0758, 0.0379])\n",
      "\n",
      "\n",
      "************** Batch 96 in 0.6078052520751953 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0972, 0.0257, 0.0561, 0.0732, 0.0732, 0.0377]) \n",
      "Test Loss tensor([0.0915, 0.0235, 0.0608, 0.0699, 0.0732, 0.0356])\n",
      "\n",
      "\n",
      "************** Batch 100 in 0.6333208084106445 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0939, 0.0248, 0.0593, 0.0739, 0.0657, 0.0372]) \n",
      "Test Loss tensor([0.0925, 0.0226, 0.0602, 0.0705, 0.0756, 0.0382])\n",
      "\n",
      "\n",
      "************** Batch 104 in 0.6544384956359863 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0200, 0.0540, 0.0702, 0.0798, 0.0349]) \n",
      "Test Loss tensor([0.0929, 0.0223, 0.0598, 0.0691, 0.0736, 0.0372])\n",
      "\n",
      "\n",
      "************** Batch 108 in 0.623546838760376 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0861, 0.0221, 0.0570, 0.0711, 0.0743, 0.0360]) \n",
      "Test Loss tensor([0.0926, 0.0227, 0.0603, 0.0704, 0.0742, 0.0379])\n",
      "\n",
      "\n",
      "************** Batch 112 in 0.617027997970581 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0220, 0.0589, 0.0682, 0.0750, 0.0445]) \n",
      "Test Loss tensor([0.0927, 0.0229, 0.0600, 0.0717, 0.0751, 0.0400])\n",
      "\n",
      "\n",
      "************** Batch 116 in 0.6206998825073242 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0887, 0.0200, 0.0614, 0.0687, 0.0739, 0.0333]) \n",
      "Test Loss tensor([0.0910, 0.0229, 0.0588, 0.0690, 0.0747, 0.0366])\n",
      "\n",
      "\n",
      "************** Batch 120 in 0.5823357105255127 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0906, 0.0216, 0.0573, 0.0753, 0.0747, 0.0408]) \n",
      "Test Loss tensor([0.0944, 0.0225, 0.0594, 0.0717, 0.0736, 0.0369])\n",
      "\n",
      "\n",
      "************** Batch 124 in 0.633263349533081 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0919, 0.0270, 0.0552, 0.0708, 0.0730, 0.0393]) \n",
      "Test Loss tensor([0.0932, 0.0239, 0.0580, 0.0699, 0.0712, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 128 in 0.6521894931793213 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0913, 0.0240, 0.0623, 0.0664, 0.0703, 0.0414]) \n",
      "Test Loss tensor([0.0913, 0.0218, 0.0586, 0.0721, 0.0735, 0.0371])\n",
      "\n",
      "\n",
      "************** Batch 132 in 0.6279399394989014 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0916, 0.0204, 0.0582, 0.0657, 0.0706, 0.0368]) \n",
      "Test Loss tensor([0.0907, 0.0220, 0.0577, 0.0713, 0.0711, 0.0379])\n",
      "\n",
      "\n",
      "************** Batch 136 in 0.6223368644714355 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0799, 0.0233, 0.0569, 0.0659, 0.0752, 0.0351]) \n",
      "Test Loss tensor([0.0922, 0.0240, 0.0584, 0.0719, 0.0759, 0.0393])\n",
      "\n",
      "\n",
      "************** Batch 140 in 0.6653714179992676 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0904, 0.0212, 0.0597, 0.0694, 0.0790, 0.0402]) \n",
      "Test Loss tensor([0.0938, 0.0240, 0.0567, 0.0698, 0.0745, 0.0372])\n",
      "\n",
      "\n",
      "************** Batch 144 in 0.6711175441741943 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0959, 0.0225, 0.0548, 0.0729, 0.0753, 0.0366]) \n",
      "Test Loss tensor([0.0921, 0.0231, 0.0575, 0.0696, 0.0730, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 148 in 0.6522526741027832 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0932, 0.0179, 0.0528, 0.0716, 0.0649, 0.0356]) \n",
      "Test Loss tensor([0.0954, 0.0232, 0.0574, 0.0696, 0.0786, 0.0405])\n",
      "\n",
      "\n",
      "************** Batch 152 in 0.6103270053863525 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0991, 0.0247, 0.0545, 0.0689, 0.0755, 0.0442]) \n",
      "Test Loss tensor([0.0938, 0.0241, 0.0584, 0.0713, 0.0741, 0.0363])\n",
      "\n",
      "\n",
      "************** Batch 156 in 0.6397538185119629 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0959, 0.0284, 0.0601, 0.0776, 0.0731, 0.0380]) \n",
      "Test Loss tensor([0.0912, 0.0242, 0.0574, 0.0693, 0.0769, 0.0411])\n",
      "\n",
      "\n",
      "************** Batch 160 in 0.6098301410675049 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0997, 0.0234, 0.0587, 0.0728, 0.0770, 0.0432]) \n",
      "Test Loss tensor([0.0922, 0.0223, 0.0592, 0.0695, 0.0714, 0.0370])\n",
      "\n",
      "\n",
      "************** Batch 164 in 0.6450145244598389 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0955, 0.0238, 0.0592, 0.0702, 0.0703, 0.0335]) \n",
      "Test Loss tensor([0.0976, 0.0243, 0.0583, 0.0712, 0.0744, 0.0381])\n",
      "\n",
      "\n",
      "************** Batch 168 in 0.6151583194732666 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0959, 0.0186, 0.0560, 0.0702, 0.0738, 0.0378]) \n",
      "Test Loss tensor([0.0908, 0.0229, 0.0587, 0.0722, 0.0737, 0.0374])\n",
      "\n",
      "\n",
      "************** Batch 172 in 0.5788888931274414 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0902, 0.0258, 0.0546, 0.0729, 0.0765, 0.0368]) \n",
      "Test Loss tensor([0.0922, 0.0240, 0.0584, 0.0700, 0.0746, 0.0364])\n",
      "\n",
      "\n",
      "************** Batch 176 in 0.5812644958496094 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0867, 0.0213, 0.0584, 0.0644, 0.0656, 0.0367]) \n",
      "Test Loss tensor([0.0918, 0.0225, 0.0550, 0.0690, 0.0776, 0.0406])\n",
      "\n",
      "\n",
      "************** Batch 180 in 0.5967819690704346 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0883, 0.0215, 0.0579, 0.0766, 0.0764, 0.0387]) \n",
      "Test Loss tensor([0.0904, 0.0234, 0.0583, 0.0700, 0.0748, 0.0381])\n",
      "\n",
      "\n",
      "************** Batch 184 in 0.5855348110198975 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0832, 0.0259, 0.0606, 0.0779, 0.0726, 0.0364]) \n",
      "Test Loss tensor([0.0945, 0.0224, 0.0580, 0.0697, 0.0774, 0.0404])\n",
      "\n",
      "\n",
      "************** Batch 188 in 0.5987212657928467 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0947, 0.0240, 0.0578, 0.0705, 0.0769, 0.0444]) \n",
      "Test Loss tensor([0.0883, 0.0226, 0.0564, 0.0678, 0.0716, 0.0364])\n",
      "\n",
      "\n",
      "************** Batch 192 in 0.577277421951294 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0994, 0.0257, 0.0554, 0.0736, 0.0726, 0.0390]) \n",
      "Test Loss tensor([0.0991, 0.0241, 0.0609, 0.0711, 0.0790, 0.0406])\n",
      "\n",
      "\n",
      "************** Batch 196 in 0.6008703708648682 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0918, 0.0225, 0.0556, 0.0704, 0.0830, 0.0385]) \n",
      "Test Loss tensor([0.0897, 0.0223, 0.0553, 0.0702, 0.0752, 0.0377])\n",
      "\n",
      "\n",
      "************** Batch 200 in 0.5791819095611572 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0947, 0.0283, 0.0562, 0.0740, 0.0740, 0.0313]) \n",
      "Test Loss tensor([0.0896, 0.0236, 0.0565, 0.0704, 0.0810, 0.0408])\n",
      "\n",
      "\n",
      "************** Batch 204 in 0.5970773696899414 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0957, 0.0269, 0.0582, 0.0694, 0.0778, 0.0360]) \n",
      "Test Loss tensor([0.0955, 0.0238, 0.0567, 0.0706, 0.0742, 0.0374])\n",
      "\n",
      "\n",
      "************** Batch 208 in 0.5850741863250732 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0985, 0.0218, 0.0587, 0.0750, 0.0707, 0.0367]) \n",
      "Test Loss tensor([0.0925, 0.0226, 0.0561, 0.0721, 0.0730, 0.0387])\n",
      "\n",
      "\n",
      "************** Batch 212 in 0.5857598781585693 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0877, 0.0258, 0.0550, 0.0713, 0.0690, 0.0401]) \n",
      "Test Loss tensor([0.0932, 0.0231, 0.0553, 0.0703, 0.0740, 0.0365])\n",
      "\n",
      "\n",
      "************** Batch 216 in 0.5937397480010986 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1028, 0.0258, 0.0554, 0.0723, 0.0733, 0.0365]) \n",
      "Test Loss tensor([0.0931, 0.0233, 0.0570, 0.0703, 0.0735, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 220 in 0.5798041820526123 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1078, 0.0232, 0.0559, 0.0765, 0.0761, 0.0331]) \n",
      "Test Loss tensor([0.0915, 0.0241, 0.0562, 0.0712, 0.0730, 0.0387])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 224 in 0.6088743209838867 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0243, 0.0547, 0.0667, 0.0737, 0.0404]) \n",
      "Test Loss tensor([0.0914, 0.0227, 0.0573, 0.0707, 0.0751, 0.0372])\n",
      "\n",
      "\n",
      "************** Batch 228 in 0.5925350189208984 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0917, 0.0228, 0.0559, 0.0696, 0.0768, 0.0351]) \n",
      "Test Loss tensor([0.0914, 0.0241, 0.0576, 0.0715, 0.0740, 0.0403])\n",
      "\n",
      "\n",
      "************** Batch 232 in 0.5813140869140625 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0960, 0.0199, 0.0579, 0.0729, 0.0783, 0.0398]) \n",
      "Test Loss tensor([0.0849, 0.0226, 0.0567, 0.0706, 0.0722, 0.0385])\n",
      "\n",
      "\n",
      "************** Batch 236 in 0.5900485515594482 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0982, 0.0232, 0.0649, 0.0751, 0.0769, 0.0393]) \n",
      "Test Loss tensor([0.0974, 0.0220, 0.0574, 0.0730, 0.0764, 0.0385])\n",
      "\n",
      "\n",
      "************** Batch 240 in 0.5756180286407471 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0969, 0.0210, 0.0567, 0.0775, 0.0752, 0.0367]) \n",
      "Test Loss tensor([0.0909, 0.0229, 0.0562, 0.0707, 0.0719, 0.0364])\n",
      "\n",
      "\n",
      "************** Batch 244 in 0.59403395652771 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0922, 0.0239, 0.0537, 0.0732, 0.0701, 0.0389]) \n",
      "Test Loss tensor([0.0900, 0.0221, 0.0577, 0.0715, 0.0742, 0.0401])\n",
      "\n",
      "\n",
      "************** Batch 248 in 0.5887601375579834 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0978, 0.0223, 0.0553, 0.0634, 0.0751, 0.0332]) \n",
      "Test Loss tensor([0.0893, 0.0226, 0.0564, 0.0713, 0.0714, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 252 in 0.705366849899292 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0949, 0.0250, 0.0522, 0.0641, 0.0754, 0.0346]) \n",
      "Test Loss tensor([0.0918, 0.0238, 0.0556, 0.0721, 0.0732, 0.0376])\n",
      "\n",
      "\n",
      "************** Batch 256 in 0.6499843597412109 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0940, 0.0234, 0.0570, 0.0719, 0.0695, 0.0417]) \n",
      "Test Loss tensor([0.0904, 0.0221, 0.0559, 0.0713, 0.0733, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 260 in 0.622429370880127 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0920, 0.0252, 0.0576, 0.0671, 0.0731, 0.0325]) \n",
      "Test Loss tensor([0.0908, 0.0231, 0.0548, 0.0713, 0.0746, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 264 in 0.6179311275482178 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0973, 0.0216, 0.0544, 0.0624, 0.0732, 0.0382]) \n",
      "Test Loss tensor([0.0918, 0.0228, 0.0556, 0.0710, 0.0719, 0.0371])\n",
      "\n",
      "\n",
      "************** Batch 268 in 0.6307733058929443 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0903, 0.0236, 0.0537, 0.0718, 0.0729, 0.0378]) \n",
      "Test Loss tensor([0.0912, 0.0221, 0.0536, 0.0698, 0.0734, 0.0365])\n",
      "\n",
      "\n",
      "************** Batch 272 in 0.6110668182373047 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0919, 0.0234, 0.0572, 0.0696, 0.0752, 0.0411]) \n",
      "Test Loss tensor([0.0900, 0.0230, 0.0558, 0.0699, 0.0730, 0.0369])\n",
      "\n",
      "\n",
      "************** Batch 276 in 0.6104934215545654 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0217, 0.0553, 0.0784, 0.0683, 0.0400]) \n",
      "Test Loss tensor([0.0905, 0.0221, 0.0553, 0.0686, 0.0714, 0.0366])\n",
      "\n",
      "\n",
      "************** Batch 280 in 0.5855128765106201 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0970, 0.0189, 0.0550, 0.0772, 0.0696, 0.0330]) \n",
      "Test Loss tensor([0.0907, 0.0224, 0.0553, 0.0697, 0.0721, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 284 in 0.5786774158477783 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0951, 0.0242, 0.0560, 0.0720, 0.0740, 0.0348]) \n",
      "Test Loss tensor([0.0915, 0.0229, 0.0549, 0.0684, 0.0745, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 288 in 0.6009643077850342 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0841, 0.0245, 0.0565, 0.0694, 0.0709, 0.0391]) \n",
      "Test Loss tensor([0.0908, 0.0218, 0.0549, 0.0714, 0.0734, 0.0386])\n",
      "\n",
      "\n",
      "************** Batch 292 in 0.5838935375213623 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0893, 0.0275, 0.0536, 0.0663, 0.0717, 0.0382]) \n",
      "Test Loss tensor([0.0890, 0.0236, 0.0552, 0.0721, 0.0726, 0.0357])\n",
      "\n",
      "\n",
      "************** Batch 296 in 0.5950312614440918 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0941, 0.0240, 0.0553, 0.0711, 0.0741, 0.0369]) \n",
      "Test Loss tensor([0.0927, 0.0226, 0.0551, 0.0684, 0.0722, 0.0355])\n",
      "\n",
      "\n",
      "************** Batch 300 in 0.5903546810150146 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0977, 0.0235, 0.0495, 0.0780, 0.0744, 0.0329]) \n",
      "Test Loss tensor([0.0899, 0.0230, 0.0560, 0.0692, 0.0710, 0.0363])\n",
      "\n",
      "\n",
      "************** Batch 304 in 0.6063516139984131 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0911, 0.0225, 0.0525, 0.0814, 0.0697, 0.0380]) \n",
      "Test Loss tensor([0.0912, 0.0232, 0.0534, 0.0688, 0.0719, 0.0366])\n",
      "\n",
      "\n",
      "************** Batch 308 in 0.6239500045776367 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0860, 0.0197, 0.0534, 0.0628, 0.0732, 0.0312]) \n",
      "Test Loss tensor([0.0886, 0.0232, 0.0547, 0.0690, 0.0714, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 312 in 0.585097074508667 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0871, 0.0216, 0.0526, 0.0765, 0.0732, 0.0381]) \n",
      "Test Loss tensor([0.0888, 0.0229, 0.0541, 0.0705, 0.0716, 0.0350])\n",
      "\n",
      "\n",
      "************** Batch 316 in 0.5989170074462891 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0952, 0.0249, 0.0520, 0.0701, 0.0686, 0.0371]) \n",
      "Test Loss tensor([0.0919, 0.0236, 0.0548, 0.0684, 0.0701, 0.0360])\n",
      "\n",
      "\n",
      "************** Batch 320 in 0.6100635528564453 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0891, 0.0237, 0.0524, 0.0729, 0.0755, 0.0339]) \n",
      "Test Loss tensor([0.0902, 0.0204, 0.0540, 0.0688, 0.0723, 0.0362])\n",
      "\n",
      "\n",
      "************** Batch 324 in 0.5856809616088867 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0955, 0.0274, 0.0531, 0.0813, 0.0705, 0.0353]) \n",
      "Test Loss tensor([0.0899, 0.0230, 0.0543, 0.0695, 0.0730, 0.0364])\n",
      "\n",
      "\n",
      "************** Batch 328 in 0.590174674987793 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0923, 0.0201, 0.0581, 0.0668, 0.0695, 0.0366]) \n",
      "Test Loss tensor([0.0886, 0.0224, 0.0544, 0.0697, 0.0699, 0.0355])\n",
      "\n",
      "\n",
      "************** Batch 332 in 0.5870897769927979 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0961, 0.0195, 0.0578, 0.0677, 0.0705, 0.0378]) \n",
      "Test Loss tensor([0.0900, 0.0222, 0.0535, 0.0689, 0.0713, 0.0372])\n",
      "\n",
      "\n",
      "************** Batch 336 in 0.5895326137542725 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0862, 0.0255, 0.0566, 0.0687, 0.0697, 0.0403]) \n",
      "Test Loss tensor([0.0901, 0.0231, 0.0541, 0.0695, 0.0700, 0.0372])\n",
      "\n",
      "\n",
      "************** Batch 340 in 0.5936555862426758 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0948, 0.0197, 0.0537, 0.0687, 0.0692, 0.0364]) \n",
      "Test Loss tensor([0.0905, 0.0223, 0.0522, 0.0702, 0.0717, 0.0362])\n",
      "\n",
      "\n",
      "************** Batch 344 in 0.586878776550293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1020, 0.0242, 0.0530, 0.0741, 0.0682, 0.0366]) \n",
      "Test Loss tensor([0.0923, 0.0235, 0.0529, 0.0709, 0.0728, 0.0364])\n",
      "\n",
      "\n",
      "************** Batch 348 in 0.5968482494354248 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0888, 0.0245, 0.0534, 0.0803, 0.0766, 0.0412]) \n",
      "Test Loss tensor([0.0908, 0.0233, 0.0534, 0.0687, 0.0726, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 352 in 0.5821137428283691 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0880, 0.0205, 0.0578, 0.0698, 0.0755, 0.0363]) \n",
      "Test Loss tensor([0.0906, 0.0221, 0.0520, 0.0699, 0.0753, 0.0361])\n",
      "\n",
      "\n",
      "************** Batch 356 in 0.5908129215240479 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0979, 0.0262, 0.0555, 0.0694, 0.0783, 0.0354]) \n",
      "Test Loss tensor([0.0896, 0.0230, 0.0537, 0.0717, 0.0708, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 360 in 0.5979564189910889 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0937, 0.0232, 0.0560, 0.0694, 0.0718, 0.0381]) \n",
      "Test Loss tensor([0.0882, 0.0228, 0.0523, 0.0679, 0.0719, 0.0377])\n",
      "\n",
      "\n",
      "************** Batch 364 in 0.7253592014312744 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0220, 0.0541, 0.0795, 0.0724, 0.0366]) \n",
      "Test Loss tensor([0.0916, 0.0229, 0.0543, 0.0693, 0.0750, 0.0377])\n",
      "\n",
      "\n",
      "************** Batch 368 in 0.682633638381958 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0903, 0.0236, 0.0476, 0.0655, 0.0739, 0.0379]) \n",
      "Test Loss tensor([0.0914, 0.0226, 0.0526, 0.0718, 0.0712, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 372 in 0.6846230030059814 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0942, 0.0279, 0.0515, 0.0743, 0.0752, 0.0350]) \n",
      "Test Loss tensor([0.0909, 0.0234, 0.0527, 0.0706, 0.0736, 0.0362])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 376 in 0.6823723316192627 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0896, 0.0223, 0.0495, 0.0669, 0.0677, 0.0343]) \n",
      "Test Loss tensor([0.0918, 0.0235, 0.0541, 0.0699, 0.0705, 0.0371])\n",
      "\n",
      "\n",
      "************** Batch 380 in 0.6604170799255371 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0940, 0.0223, 0.0535, 0.0693, 0.0700, 0.0327]) \n",
      "Test Loss tensor([0.0913, 0.0208, 0.0534, 0.0688, 0.0723, 0.0397])\n",
      "\n",
      "\n",
      "************** Batch 384 in 0.6423957347869873 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0857, 0.0257, 0.0532, 0.0718, 0.0701, 0.0338]) \n",
      "Test Loss tensor([0.0900, 0.0223, 0.0528, 0.0711, 0.0733, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 388 in 0.68581223487854 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0918, 0.0236, 0.0544, 0.0662, 0.0722, 0.0331]) \n",
      "Test Loss tensor([0.0890, 0.0239, 0.0537, 0.0697, 0.0715, 0.0366])\n",
      "\n",
      "\n",
      "************** Batch 392 in 0.6401851177215576 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0956, 0.0248, 0.0496, 0.0798, 0.0713, 0.0294]) \n",
      "Test Loss tensor([0.0925, 0.0226, 0.0537, 0.0704, 0.0758, 0.0389])\n",
      "\n",
      "\n",
      "************** Batch 396 in 0.67547607421875 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0878, 0.0270, 0.0496, 0.0678, 0.0744, 0.0401]) \n",
      "Test Loss tensor([0.0887, 0.0224, 0.0524, 0.0693, 0.0691, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 400 in 0.6542496681213379 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0922, 0.0231, 0.0502, 0.0649, 0.0682, 0.0328]) \n",
      "Test Loss tensor([0.0896, 0.0221, 0.0530, 0.0708, 0.0721, 0.0382])\n",
      "\n",
      "\n",
      "************** Batch 404 in 0.6791481971740723 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0883, 0.0226, 0.0558, 0.0674, 0.0732, 0.0410]) \n",
      "Test Loss tensor([0.0924, 0.0226, 0.0531, 0.0691, 0.0725, 0.0365])\n",
      "\n",
      "\n",
      "************** Batch 408 in 0.6534664630889893 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0881, 0.0235, 0.0551, 0.0713, 0.0678, 0.0383]) \n",
      "Test Loss tensor([0.0889, 0.0220, 0.0533, 0.0686, 0.0697, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 412 in 0.6448187828063965 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0950, 0.0246, 0.0504, 0.0741, 0.0720, 0.0354]) \n",
      "Test Loss tensor([0.0912, 0.0232, 0.0521, 0.0678, 0.0725, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 416 in 0.5979218482971191 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0860, 0.0224, 0.0556, 0.0816, 0.0756, 0.0363]) \n",
      "Test Loss tensor([0.0890, 0.0229, 0.0532, 0.0702, 0.0709, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 420 in 0.5817375183105469 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0883, 0.0281, 0.0549, 0.0700, 0.0670, 0.0398]) \n",
      "Test Loss tensor([0.0911, 0.0234, 0.0520, 0.0685, 0.0728, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 424 in 0.5825161933898926 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0858, 0.0240, 0.0572, 0.0651, 0.0671, 0.0345]) \n",
      "Test Loss tensor([0.0896, 0.0225, 0.0519, 0.0698, 0.0708, 0.0367])\n",
      "\n",
      "\n",
      "************** Batch 428 in 0.6524438858032227 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0935, 0.0210, 0.0528, 0.0733, 0.0694, 0.0374]) \n",
      "Test Loss tensor([0.0889, 0.0215, 0.0517, 0.0701, 0.0716, 0.0356])\n",
      "\n",
      "\n",
      "************** Batch 432 in 0.7290420532226562 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0787, 0.0263, 0.0517, 0.0700, 0.0758, 0.0350]) \n",
      "Test Loss tensor([0.0941, 0.0238, 0.0522, 0.0717, 0.0718, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 436 in 0.6685566902160645 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0918, 0.0204, 0.0515, 0.0749, 0.0748, 0.0374]) \n",
      "Test Loss tensor([0.0889, 0.0234, 0.0513, 0.0691, 0.0727, 0.0354])\n",
      "\n",
      "\n",
      "************** Batch 440 in 0.6262280941009521 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0959, 0.0220, 0.0503, 0.0725, 0.0709, 0.0388]) \n",
      "Test Loss tensor([0.0889, 0.0229, 0.0518, 0.0692, 0.0742, 0.0366])\n",
      "\n",
      "\n",
      "************** Batch 444 in 0.5978944301605225 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0885, 0.0253, 0.0510, 0.0763, 0.0721, 0.0335]) \n",
      "Test Loss tensor([0.0910, 0.0229, 0.0522, 0.0685, 0.0702, 0.0360])\n",
      "\n",
      "\n",
      "************** Batch 448 in 0.6312475204467773 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0905, 0.0235, 0.0496, 0.0660, 0.0705, 0.0364]) \n",
      "Test Loss tensor([0.0869, 0.0237, 0.0528, 0.0692, 0.0706, 0.0351])\n",
      "\n",
      "\n",
      "************** Batch 452 in 0.6781251430511475 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0910, 0.0256, 0.0488, 0.0725, 0.0709, 0.0363]) \n",
      "Test Loss tensor([0.0918, 0.0223, 0.0509, 0.0698, 0.0712, 0.0351])\n",
      "\n",
      "\n",
      "************** Batch 456 in 0.6858479976654053 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0894, 0.0234, 0.0496, 0.0718, 0.0661, 0.0380]) \n",
      "Test Loss tensor([0.0896, 0.0221, 0.0494, 0.0712, 0.0724, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 460 in 0.6623613834381104 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1003, 0.0251, 0.0508, 0.0683, 0.0714, 0.0363]) \n",
      "Test Loss tensor([0.0922, 0.0220, 0.0534, 0.0699, 0.0708, 0.0362])\n",
      "\n",
      "\n",
      "************** Batch 464 in 0.6487312316894531 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0944, 0.0286, 0.0502, 0.0697, 0.0708, 0.0390]) \n",
      "Test Loss tensor([0.0906, 0.0227, 0.0516, 0.0704, 0.0688, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 468 in 0.615330696105957 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0851, 0.0217, 0.0492, 0.0609, 0.0713, 0.0320]) \n",
      "Test Loss tensor([0.0928, 0.0233, 0.0525, 0.0698, 0.0712, 0.0361])\n",
      "\n",
      "\n",
      "************** Batch 472 in 0.6213259696960449 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0936, 0.0249, 0.0556, 0.0715, 0.0776, 0.0368]) \n",
      "Test Loss tensor([0.0883, 0.0216, 0.0506, 0.0686, 0.0716, 0.0363])\n",
      "\n",
      "\n",
      "************** Batch 476 in 0.698617696762085 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0873, 0.0255, 0.0502, 0.0709, 0.0729, 0.0323]) \n",
      "Test Loss tensor([0.0933, 0.0237, 0.0521, 0.0707, 0.0719, 0.0360])\n",
      "\n",
      "\n",
      "************** Batch 480 in 0.5781822204589844 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0976, 0.0266, 0.0488, 0.0695, 0.0688, 0.0381]) \n",
      "Test Loss tensor([0.0914, 0.0229, 0.0504, 0.0693, 0.0744, 0.0381])\n",
      "\n",
      "\n",
      "************** Batch 484 in 0.5924277305603027 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0886, 0.0239, 0.0525, 0.0770, 0.0722, 0.0377]) \n",
      "Test Loss tensor([0.0867, 0.0224, 0.0510, 0.0684, 0.0712, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 488 in 0.5689859390258789 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0937, 0.0237, 0.0532, 0.0618, 0.0682, 0.0316]) \n",
      "Test Loss tensor([0.0915, 0.0224, 0.0540, 0.0704, 0.0717, 0.0399])\n",
      "\n",
      "\n",
      "************** Batch 492 in 0.5870859622955322 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0966, 0.0236, 0.0532, 0.0750, 0.0722, 0.0392]) \n",
      "Test Loss tensor([0.0922, 0.0239, 0.0513, 0.0709, 0.0715, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 496 in 0.5749125480651855 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0960, 0.0235, 0.0498, 0.0651, 0.0703, 0.0347]) \n",
      "Test Loss tensor([0.0913, 0.0227, 0.0511, 0.0703, 0.0740, 0.0365])\n",
      "\n",
      "\n",
      "************** Batch 500 in 0.5863635540008545 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0975, 0.0272, 0.0509, 0.0718, 0.0698, 0.0404]) \n",
      "Test Loss tensor([0.0906, 0.0224, 0.0506, 0.0705, 0.0722, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 504 in 0.597937822341919 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0945, 0.0246, 0.0498, 0.0788, 0.0733, 0.0337]) \n",
      "Test Loss tensor([0.0889, 0.0228, 0.0512, 0.0697, 0.0710, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 508 in 0.5754435062408447 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0925, 0.0150, 0.0476, 0.0661, 0.0722, 0.0380]) \n",
      "Test Loss tensor([0.0905, 0.0236, 0.0515, 0.0695, 0.0722, 0.0388])\n",
      "\n",
      "\n",
      "************** Batch 512 in 0.5926837921142578 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0915, 0.0226, 0.0494, 0.0731, 0.0687, 0.0387]) \n",
      "Test Loss tensor([0.0879, 0.0239, 0.0507, 0.0682, 0.0723, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 516 in 0.5776598453521729 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0849, 0.0282, 0.0491, 0.0690, 0.0692, 0.0351]) \n",
      "Test Loss tensor([0.0904, 0.0229, 0.0523, 0.0712, 0.0710, 0.0364])\n",
      "\n",
      "\n",
      "************** Batch 520 in 0.5862045288085938 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0830, 0.0238, 0.0531, 0.0656, 0.0718, 0.0359]) \n",
      "Test Loss tensor([0.0897, 0.0226, 0.0515, 0.0692, 0.0713, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 524 in 0.5760784149169922 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0905, 0.0214, 0.0469, 0.0640, 0.0664, 0.0363]) \n",
      "Test Loss tensor([0.0876, 0.0222, 0.0510, 0.0699, 0.0722, 0.0337])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 528 in 0.5888452529907227 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0930, 0.0225, 0.0482, 0.0677, 0.0695, 0.0349]) \n",
      "Test Loss tensor([0.0857, 0.0217, 0.0515, 0.0683, 0.0708, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 532 in 0.6813449859619141 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0848, 0.0236, 0.0510, 0.0655, 0.0748, 0.0322]) \n",
      "Test Loss tensor([0.0905, 0.0232, 0.0523, 0.0706, 0.0698, 0.0355])\n",
      "\n",
      "\n",
      "************** Batch 536 in 0.6924881935119629 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0985, 0.0221, 0.0496, 0.0682, 0.0690, 0.0385]) \n",
      "Test Loss tensor([0.0905, 0.0236, 0.0506, 0.0690, 0.0703, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 540 in 0.6404092311859131 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0880, 0.0208, 0.0535, 0.0658, 0.0696, 0.0303]) \n",
      "Test Loss tensor([0.0858, 0.0227, 0.0510, 0.0693, 0.0708, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 544 in 0.6356089115142822 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0982, 0.0181, 0.0477, 0.0687, 0.0680, 0.0327]) \n",
      "Test Loss tensor([0.0897, 0.0226, 0.0493, 0.0708, 0.0714, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 548 in 0.62496018409729 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0897, 0.0219, 0.0466, 0.0697, 0.0701, 0.0319]) \n",
      "Test Loss tensor([0.0888, 0.0217, 0.0511, 0.0690, 0.0698, 0.0364])\n",
      "\n",
      "\n",
      "************** Batch 552 in 0.6405942440032959 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0893, 0.0204, 0.0510, 0.0712, 0.0703, 0.0392]) \n",
      "Test Loss tensor([0.0888, 0.0236, 0.0511, 0.0702, 0.0714, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 556 in 0.6547768115997314 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0859, 0.0209, 0.0522, 0.0730, 0.0699, 0.0377]) \n",
      "Test Loss tensor([0.0888, 0.0231, 0.0508, 0.0686, 0.0715, 0.0362])\n",
      "\n",
      "\n",
      "************** Batch 560 in 0.6260931491851807 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0867, 0.0256, 0.0521, 0.0711, 0.0782, 0.0418]) \n",
      "Test Loss tensor([0.0873, 0.0232, 0.0487, 0.0668, 0.0723, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 564 in 0.6836552619934082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0814, 0.0222, 0.0496, 0.0646, 0.0710, 0.0298]) \n",
      "Test Loss tensor([0.0886, 0.0228, 0.0494, 0.0714, 0.0703, 0.0354])\n",
      "\n",
      "\n",
      "************** Batch 568 in 0.6274018287658691 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0919, 0.0216, 0.0485, 0.0703, 0.0689, 0.0368]) \n",
      "Test Loss tensor([0.0892, 0.0230, 0.0485, 0.0683, 0.0716, 0.0350])\n",
      "\n",
      "\n",
      "************** Batch 572 in 0.6263506412506104 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0952, 0.0268, 0.0505, 0.0665, 0.0756, 0.0344]) \n",
      "Test Loss tensor([0.0923, 0.0222, 0.0493, 0.0676, 0.0707, 0.0359])\n",
      "\n",
      "\n",
      "************** Batch 576 in 0.6320023536682129 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0821, 0.0245, 0.0453, 0.0729, 0.0698, 0.0337]) \n",
      "Test Loss tensor([0.0909, 0.0209, 0.0490, 0.0669, 0.0711, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 580 in 0.6679267883300781 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0978, 0.0210, 0.0460, 0.0679, 0.0711, 0.0392]) \n",
      "Test Loss tensor([0.0886, 0.0222, 0.0493, 0.0685, 0.0713, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 584 in 0.5935869216918945 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0862, 0.0240, 0.0490, 0.0724, 0.0743, 0.0357]) \n",
      "Test Loss tensor([0.0879, 0.0227, 0.0496, 0.0680, 0.0696, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 588 in 0.5849368572235107 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0936, 0.0247, 0.0461, 0.0724, 0.0731, 0.0343]) \n",
      "Test Loss tensor([0.0906, 0.0232, 0.0497, 0.0694, 0.0702, 0.0363])\n",
      "\n",
      "\n",
      "************** Batch 592 in 0.5709018707275391 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0894, 0.0235, 0.0489, 0.0733, 0.0684, 0.0445]) \n",
      "Test Loss tensor([0.0917, 0.0236, 0.0491, 0.0693, 0.0690, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 596 in 0.5919828414916992 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0919, 0.0261, 0.0471, 0.0735, 0.0707, 0.0323]) \n",
      "Test Loss tensor([0.0910, 0.0237, 0.0497, 0.0696, 0.0723, 0.0357])\n",
      "\n",
      "\n",
      "************** Batch 600 in 0.5666723251342773 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0927, 0.0280, 0.0491, 0.0751, 0.0760, 0.0318]) \n",
      "Test Loss tensor([0.0879, 0.0230, 0.0477, 0.0713, 0.0709, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 604 in 0.5964713096618652 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0946, 0.0231, 0.0466, 0.0728, 0.0700, 0.0381]) \n",
      "Test Loss tensor([0.0881, 0.0221, 0.0480, 0.0690, 0.0707, 0.0348])\n",
      "\n",
      "\n",
      "************** Batch 608 in 0.5791690349578857 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0923, 0.0241, 0.0470, 0.0687, 0.0619, 0.0352]) \n",
      "Test Loss tensor([0.0897, 0.0229, 0.0492, 0.0697, 0.0704, 0.0350])\n",
      "\n",
      "\n",
      "************** Batch 612 in 0.5858922004699707 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0859, 0.0250, 0.0546, 0.0698, 0.0707, 0.0382]) \n",
      "Test Loss tensor([0.0901, 0.0218, 0.0491, 0.0693, 0.0692, 0.0361])\n",
      "\n",
      "\n",
      "************** Batch 616 in 0.5873932838439941 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0824, 0.0215, 0.0503, 0.0643, 0.0660, 0.0363]) \n",
      "Test Loss tensor([0.0901, 0.0235, 0.0474, 0.0705, 0.0700, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 620 in 0.5779848098754883 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0907, 0.0217, 0.0475, 0.0803, 0.0722, 0.0327]) \n",
      "Test Loss tensor([0.0910, 0.0231, 0.0485, 0.0704, 0.0708, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 624 in 0.5885756015777588 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0959, 0.0243, 0.0441, 0.0780, 0.0694, 0.0362]) \n",
      "Test Loss tensor([0.0891, 0.0233, 0.0508, 0.0692, 0.0706, 0.0372])\n",
      "\n",
      "\n",
      "************** Batch 628 in 0.6053507328033447 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0946, 0.0189, 0.0465, 0.0636, 0.0676, 0.0362]) \n",
      "Test Loss tensor([0.0889, 0.0229, 0.0503, 0.0684, 0.0690, 0.0363])\n",
      "\n",
      "\n",
      "************** Batch 632 in 0.5844199657440186 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0988, 0.0211, 0.0476, 0.0719, 0.0711, 0.0363]) \n",
      "Test Loss tensor([0.0896, 0.0233, 0.0489, 0.0686, 0.0715, 0.0363])\n",
      "\n",
      "\n",
      "************** Batch 636 in 0.5891728401184082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0838, 0.0219, 0.0550, 0.0673, 0.0692, 0.0351]) \n",
      "Test Loss tensor([0.0906, 0.0231, 0.0491, 0.0706, 0.0712, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 640 in 0.590660810470581 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0961, 0.0261, 0.0513, 0.0722, 0.0685, 0.0332]) \n",
      "Test Loss tensor([0.0878, 0.0228, 0.0495, 0.0674, 0.0700, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 644 in 0.5991220474243164 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0245, 0.0477, 0.0793, 0.0716, 0.0340]) \n",
      "Test Loss tensor([0.0892, 0.0231, 0.0503, 0.0692, 0.0694, 0.0350])\n",
      "\n",
      "\n",
      "************** Batch 648 in 0.5900702476501465 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0857, 0.0214, 0.0520, 0.0616, 0.0688, 0.0371]) \n",
      "Test Loss tensor([0.0910, 0.0230, 0.0502, 0.0702, 0.0698, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 652 in 0.5728065967559814 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0884, 0.0229, 0.0471, 0.0703, 0.0639, 0.0335]) \n",
      "Test Loss tensor([0.0877, 0.0218, 0.0475, 0.0685, 0.0709, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 656 in 0.5931193828582764 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0820, 0.0222, 0.0470, 0.0644, 0.0771, 0.0338]) \n",
      "Test Loss tensor([0.0876, 0.0221, 0.0492, 0.0677, 0.0694, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 660 in 0.5819604396820068 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0886, 0.0251, 0.0478, 0.0750, 0.0731, 0.0323]) \n",
      "Test Loss tensor([0.0912, 0.0228, 0.0500, 0.0679, 0.0717, 0.0355])\n",
      "\n",
      "\n",
      "************** Batch 664 in 0.593447208404541 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0939, 0.0257, 0.0469, 0.0618, 0.0726, 0.0331]) \n",
      "Test Loss tensor([0.0897, 0.0222, 0.0489, 0.0682, 0.0697, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 668 in 0.6122870445251465 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0874, 0.0231, 0.0518, 0.0752, 0.0718, 0.0355]) \n",
      "Test Loss tensor([0.0890, 0.0219, 0.0495, 0.0675, 0.0700, 0.0356])\n",
      "\n",
      "\n",
      "************** Batch 672 in 0.5739951133728027 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0781, 0.0237, 0.0486, 0.0591, 0.0760, 0.0325]) \n",
      "Test Loss tensor([0.0911, 0.0231, 0.0517, 0.0689, 0.0730, 0.0367])\n",
      "\n",
      "\n",
      "************** Batch 676 in 0.6006357669830322 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0966, 0.0292, 0.0468, 0.0744, 0.0742, 0.0367]) \n",
      "Test Loss tensor([0.0878, 0.0230, 0.0489, 0.0689, 0.0703, 0.0347])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 680 in 0.5684170722961426 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0910, 0.0206, 0.0520, 0.0745, 0.0745, 0.0351]) \n",
      "Test Loss tensor([0.0901, 0.0239, 0.0499, 0.0692, 0.0733, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 684 in 0.5857710838317871 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0860, 0.0205, 0.0495, 0.0687, 0.0729, 0.0413]) \n",
      "Test Loss tensor([0.0872, 0.0228, 0.0481, 0.0667, 0.0695, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 688 in 0.5835952758789062 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0916, 0.0226, 0.0489, 0.0693, 0.0652, 0.0337]) \n",
      "Test Loss tensor([0.0947, 0.0227, 0.0491, 0.0696, 0.0747, 0.0384])\n",
      "\n",
      "\n",
      "************** Batch 692 in 0.573359489440918 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0997, 0.0190, 0.0504, 0.0619, 0.0721, 0.0371]) \n",
      "Test Loss tensor([0.0873, 0.0224, 0.0487, 0.0706, 0.0723, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 696 in 0.602308988571167 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0859, 0.0257, 0.0481, 0.0700, 0.0755, 0.0376]) \n",
      "Test Loss tensor([0.0900, 0.0236, 0.0497, 0.0690, 0.0730, 0.0387])\n",
      "\n",
      "\n",
      "************** Batch 700 in 0.5775196552276611 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0836, 0.0225, 0.0486, 0.0666, 0.0728, 0.0383]) \n",
      "Test Loss tensor([0.0954, 0.0232, 0.0468, 0.0683, 0.0728, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 704 in 0.5871057510375977 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0985, 0.0282, 0.0453, 0.0709, 0.0710, 0.0359]) \n",
      "Test Loss tensor([0.0876, 0.0224, 0.0471, 0.0686, 0.0710, 0.0348])\n",
      "\n",
      "\n",
      "************** Batch 708 in 0.579697847366333 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0934, 0.0231, 0.0472, 0.0654, 0.0661, 0.0317]) \n",
      "Test Loss tensor([0.0876, 0.0227, 0.0476, 0.0673, 0.0715, 0.0356])\n",
      "\n",
      "\n",
      "************** Batch 712 in 0.5908050537109375 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0945, 0.0238, 0.0485, 0.0780, 0.0732, 0.0333]) \n",
      "Test Loss tensor([0.0905, 0.0230, 0.0481, 0.0703, 0.0697, 0.0350])\n",
      "\n",
      "\n",
      "************** Batch 716 in 0.5900723934173584 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0878, 0.0187, 0.0488, 0.0728, 0.0706, 0.0336]) \n",
      "Test Loss tensor([0.0898, 0.0232, 0.0474, 0.0680, 0.0707, 0.0354])\n",
      "\n",
      "\n",
      "************** Batch 720 in 0.5668840408325195 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0969, 0.0204, 0.0470, 0.0752, 0.0707, 0.0381]) \n",
      "Test Loss tensor([0.0889, 0.0229, 0.0463, 0.0671, 0.0704, 0.0341])\n",
      "\n",
      "\n",
      "************** Batch 724 in 0.5992472171783447 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0835, 0.0182, 0.0483, 0.0624, 0.0697, 0.0340]) \n",
      "Test Loss tensor([0.0868, 0.0222, 0.0470, 0.0679, 0.0672, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 728 in 0.5749521255493164 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0877, 0.0185, 0.0456, 0.0658, 0.0649, 0.0350]) \n",
      "Test Loss tensor([0.0911, 0.0228, 0.0468, 0.0696, 0.0691, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 732 in 0.585106611251831 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0992, 0.0221, 0.0524, 0.0795, 0.0668, 0.0374]) \n",
      "Test Loss tensor([0.0879, 0.0228, 0.0477, 0.0683, 0.0706, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 736 in 0.5789916515350342 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0854, 0.0241, 0.0468, 0.0683, 0.0693, 0.0348]) \n",
      "Test Loss tensor([0.0893, 0.0233, 0.0480, 0.0679, 0.0709, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 740 in 0.5744085311889648 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0890, 0.0285, 0.0477, 0.0770, 0.0687, 0.0338]) \n",
      "Test Loss tensor([0.0881, 0.0234, 0.0492, 0.0681, 0.0695, 0.0357])\n",
      "\n",
      "\n",
      "************** Batch 744 in 0.5813632011413574 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0932, 0.0275, 0.0439, 0.0732, 0.0721, 0.0321]) \n",
      "Test Loss tensor([0.0903, 0.0242, 0.0478, 0.0692, 0.0705, 0.0359])\n",
      "\n",
      "\n",
      "************** Batch 748 in 0.5797936916351318 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0912, 0.0214, 0.0472, 0.0702, 0.0674, 0.0325]) \n",
      "Test Loss tensor([0.0869, 0.0221, 0.0474, 0.0668, 0.0679, 0.0360])\n",
      "\n",
      "\n",
      "************** Batch 752 in 0.5971670150756836 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0930, 0.0231, 0.0493, 0.0684, 0.0715, 0.0346]) \n",
      "Test Loss tensor([0.0874, 0.0233, 0.0486, 0.0698, 0.0686, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 756 in 0.5832939147949219 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0891, 0.0260, 0.0456, 0.0684, 0.0669, 0.0350]) \n",
      "Test Loss tensor([0.0898, 0.0246, 0.0460, 0.0695, 0.0716, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 760 in 0.5936720371246338 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0923, 0.0212, 0.0468, 0.0658, 0.0750, 0.0341]) \n",
      "Test Loss tensor([0.0885, 0.0233, 0.0480, 0.0683, 0.0691, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 764 in 0.5897672176361084 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0836, 0.0244, 0.0469, 0.0670, 0.0689, 0.0357]) \n",
      "Test Loss tensor([0.0891, 0.0229, 0.0459, 0.0688, 0.0686, 0.0360])\n",
      "\n",
      "\n",
      "************** Batch 768 in 0.5741949081420898 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0867, 0.0227, 0.0501, 0.0605, 0.0760, 0.0386]) \n",
      "Test Loss tensor([0.0903, 0.0228, 0.0470, 0.0687, 0.0705, 0.0361])\n",
      "\n",
      "\n",
      "************** Batch 772 in 0.5903201103210449 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0886, 0.0246, 0.0461, 0.0680, 0.0719, 0.0339]) \n",
      "Test Loss tensor([0.0905, 0.0235, 0.0480, 0.0681, 0.0722, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 776 in 0.5748648643493652 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0835, 0.0223, 0.0490, 0.0645, 0.0758, 0.0375]) \n",
      "Test Loss tensor([0.0877, 0.0226, 0.0471, 0.0681, 0.0701, 0.0351])\n",
      "\n",
      "\n",
      "************** Batch 780 in 0.6063973903656006 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0809, 0.0205, 0.0449, 0.0616, 0.0709, 0.0355]) \n",
      "Test Loss tensor([0.0894, 0.0232, 0.0465, 0.0686, 0.0682, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 784 in 0.5744071006774902 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0885, 0.0225, 0.0449, 0.0672, 0.0751, 0.0289]) \n",
      "Test Loss tensor([0.0886, 0.0228, 0.0469, 0.0683, 0.0714, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 788 in 0.589076042175293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0903, 0.0231, 0.0488, 0.0711, 0.0773, 0.0310]) \n",
      "Test Loss tensor([0.0917, 0.0236, 0.0475, 0.0678, 0.0684, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 792 in 0.5848453044891357 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0910, 0.0206, 0.0463, 0.0739, 0.0681, 0.0377]) \n",
      "Test Loss tensor([0.0877, 0.0220, 0.0467, 0.0681, 0.0690, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 796 in 0.5669593811035156 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0877, 0.0223, 0.0453, 0.0685, 0.0752, 0.0354]) \n",
      "Test Loss tensor([0.0898, 0.0234, 0.0489, 0.0693, 0.0698, 0.0357])\n",
      "\n",
      "\n",
      "************** Batch 800 in 0.5851175785064697 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0968, 0.0257, 0.0428, 0.0733, 0.0720, 0.0394]) \n",
      "Test Loss tensor([0.0864, 0.0231, 0.0467, 0.0685, 0.0697, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 804 in 0.5815310478210449 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0863, 0.0256, 0.0486, 0.0650, 0.0689, 0.0346]) \n",
      "Test Loss tensor([0.0901, 0.0225, 0.0447, 0.0698, 0.0701, 0.0341])\n",
      "\n",
      "\n",
      "************** Batch 808 in 0.6033451557159424 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0871, 0.0248, 0.0455, 0.0635, 0.0673, 0.0393]) \n",
      "Test Loss tensor([0.0903, 0.0238, 0.0454, 0.0687, 0.0718, 0.0362])\n",
      "\n",
      "\n",
      "************** Batch 812 in 0.5969321727752686 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0903, 0.0166, 0.0497, 0.0710, 0.0698, 0.0334]) \n",
      "Test Loss tensor([0.0898, 0.0232, 0.0453, 0.0694, 0.0690, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 816 in 0.615635871887207 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0963, 0.0184, 0.0466, 0.0700, 0.0690, 0.0344]) \n",
      "Test Loss tensor([0.0886, 0.0226, 0.0472, 0.0668, 0.0697, 0.0364])\n",
      "\n",
      "\n",
      "************** Batch 820 in 0.598259449005127 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0880, 0.0218, 0.0517, 0.0714, 0.0706, 0.0339]) \n",
      "Test Loss tensor([0.0873, 0.0217, 0.0466, 0.0695, 0.0682, 0.0356])\n",
      "\n",
      "\n",
      "************** Batch 824 in 0.5761499404907227 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0888, 0.0185, 0.0409, 0.0702, 0.0653, 0.0335]) \n",
      "Test Loss tensor([0.0887, 0.0231, 0.0457, 0.0697, 0.0687, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 828 in 0.5875842571258545 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0891, 0.0246, 0.0411, 0.0660, 0.0704, 0.0338]) \n",
      "Test Loss tensor([0.0873, 0.0232, 0.0463, 0.0679, 0.0726, 0.0360])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 832 in 0.5948300361633301 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0858, 0.0246, 0.0423, 0.0668, 0.0695, 0.0372]) \n",
      "Test Loss tensor([0.0881, 0.0222, 0.0479, 0.0704, 0.0691, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 836 in 0.5861501693725586 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0908, 0.0255, 0.0488, 0.0749, 0.0684, 0.0317]) \n",
      "Test Loss tensor([0.0892, 0.0214, 0.0462, 0.0669, 0.0696, 0.0351])\n",
      "\n",
      "\n",
      "************** Batch 840 in 0.6146228313446045 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0936, 0.0265, 0.0449, 0.0754, 0.0728, 0.0337]) \n",
      "Test Loss tensor([0.0896, 0.0223, 0.0477, 0.0694, 0.0687, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 844 in 0.6090216636657715 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0992, 0.0224, 0.0442, 0.0786, 0.0721, 0.0333]) \n",
      "Test Loss tensor([0.0850, 0.0224, 0.0474, 0.0660, 0.0688, 0.0350])\n",
      "\n",
      "\n",
      "************** Batch 848 in 0.6053528785705566 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0839, 0.0206, 0.0417, 0.0691, 0.0697, 0.0317]) \n",
      "Test Loss tensor([0.0878, 0.0227, 0.0451, 0.0679, 0.0696, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 852 in 0.6248197555541992 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0934, 0.0214, 0.0459, 0.0679, 0.0646, 0.0360]) \n",
      "Test Loss tensor([0.0879, 0.0221, 0.0458, 0.0689, 0.0694, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 856 in 0.5783035755157471 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0865, 0.0228, 0.0481, 0.0681, 0.0672, 0.0336]) \n",
      "Test Loss tensor([0.0892, 0.0236, 0.0468, 0.0671, 0.0699, 0.0357])\n",
      "\n",
      "\n",
      "************** Batch 860 in 0.6291220188140869 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0874, 0.0212, 0.0424, 0.0626, 0.0636, 0.0367]) \n",
      "Test Loss tensor([0.0893, 0.0238, 0.0463, 0.0676, 0.0701, 0.0360])\n",
      "\n",
      "\n",
      "************** Batch 864 in 0.5759837627410889 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0833, 0.0213, 0.0460, 0.0636, 0.0673, 0.0366]) \n",
      "Test Loss tensor([0.0874, 0.0235, 0.0471, 0.0674, 0.0701, 0.0370])\n",
      "\n",
      "\n",
      "************** Batch 868 in 0.5932636260986328 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0922, 0.0210, 0.0437, 0.0733, 0.0678, 0.0360]) \n",
      "Test Loss tensor([0.0881, 0.0226, 0.0461, 0.0699, 0.0703, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 872 in 0.6069884300231934 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0876, 0.0218, 0.0453, 0.0634, 0.0687, 0.0340]) \n",
      "Test Loss tensor([0.0912, 0.0227, 0.0470, 0.0680, 0.0698, 0.0365])\n",
      "\n",
      "\n",
      "************** Batch 876 in 0.5525023937225342 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0667, 0.0108, 0.0337, 0.0533, 0.0503, 0.0250]) \n",
      "Test Loss tensor([0.0879, 0.0220, 0.0467, 0.0671, 0.0679, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 0 in 0.5946455001831055 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0884, 0.0252, 0.0457, 0.0679, 0.0677, 0.0306]) \n",
      "Test Loss tensor([0.0909, 0.0226, 0.0472, 0.0677, 0.0709, 0.0362])\n",
      "\n",
      "\n",
      "************** Batch 4 in 0.5837175846099854 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0906, 0.0231, 0.0439, 0.0756, 0.0676, 0.0328]) \n",
      "Test Loss tensor([0.0876, 0.0206, 0.0456, 0.0659, 0.0689, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 8 in 0.5987873077392578 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0872, 0.0229, 0.0445, 0.0673, 0.0716, 0.0328]) \n",
      "Test Loss tensor([0.0893, 0.0231, 0.0454, 0.0672, 0.0689, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 12 in 0.5908253192901611 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0888, 0.0220, 0.0497, 0.0692, 0.0696, 0.0398]) \n",
      "Test Loss tensor([0.0905, 0.0228, 0.0459, 0.0647, 0.0703, 0.0355])\n",
      "\n",
      "\n",
      "************** Batch 16 in 0.5741870403289795 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0876, 0.0196, 0.0460, 0.0630, 0.0682, 0.0405]) \n",
      "Test Loss tensor([0.0906, 0.0243, 0.0445, 0.0677, 0.0711, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 20 in 0.6001789569854736 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0958, 0.0271, 0.0472, 0.0721, 0.0644, 0.0344]) \n",
      "Test Loss tensor([0.0880, 0.0231, 0.0447, 0.0690, 0.0727, 0.0362])\n",
      "\n",
      "\n",
      "************** Batch 24 in 0.5841407775878906 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0899, 0.0220, 0.0443, 0.0686, 0.0679, 0.0320]) \n",
      "Test Loss tensor([0.0900, 0.0222, 0.0447, 0.0663, 0.0701, 0.0341])\n",
      "\n",
      "\n",
      "************** Batch 28 in 0.7049531936645508 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0935, 0.0258, 0.0483, 0.0755, 0.0741, 0.0357]) \n",
      "Test Loss tensor([0.0910, 0.0229, 0.0450, 0.0683, 0.0690, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 32 in 0.7079384326934814 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0908, 0.0230, 0.0450, 0.0706, 0.0665, 0.0367]) \n",
      "Test Loss tensor([0.0886, 0.0233, 0.0446, 0.0668, 0.0690, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 36 in 0.6766567230224609 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0867, 0.0250, 0.0501, 0.0687, 0.0706, 0.0353]) \n",
      "Test Loss tensor([0.0878, 0.0224, 0.0455, 0.0657, 0.0694, 0.0350])\n",
      "\n",
      "\n",
      "************** Batch 40 in 0.6983568668365479 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0821, 0.0235, 0.0463, 0.0652, 0.0703, 0.0320]) \n",
      "Test Loss tensor([0.0906, 0.0226, 0.0467, 0.0697, 0.0699, 0.0394])\n",
      "\n",
      "\n",
      "************** Batch 44 in 0.7159006595611572 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0935, 0.0280, 0.0459, 0.0649, 0.0697, 0.0343]) \n",
      "Test Loss tensor([0.0879, 0.0228, 0.0448, 0.0689, 0.0716, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 48 in 0.6421849727630615 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0873, 0.0242, 0.0424, 0.0687, 0.0719, 0.0353]) \n",
      "Test Loss tensor([0.0905, 0.0231, 0.0462, 0.0694, 0.0697, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 52 in 0.6462206840515137 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0992, 0.0230, 0.0443, 0.0687, 0.0672, 0.0333]) \n",
      "Test Loss tensor([0.0871, 0.0223, 0.0437, 0.0659, 0.0692, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 56 in 0.6561558246612549 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0978, 0.0263, 0.0446, 0.0667, 0.0701, 0.0412]) \n",
      "Test Loss tensor([0.0950, 0.0232, 0.0468, 0.0702, 0.0689, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 60 in 0.6338281631469727 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0942, 0.0229, 0.0448, 0.0653, 0.0684, 0.0355]) \n",
      "Test Loss tensor([0.0870, 0.0221, 0.0449, 0.0686, 0.0688, 0.0348])\n",
      "\n",
      "\n",
      "************** Batch 64 in 0.6005525588989258 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0949, 0.0221, 0.0440, 0.0640, 0.0634, 0.0331]) \n",
      "Test Loss tensor([0.0873, 0.0226, 0.0439, 0.0705, 0.0724, 0.0351])\n",
      "\n",
      "\n",
      "************** Batch 68 in 0.59220290184021 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0213, 0.0475, 0.0799, 0.0769, 0.0385]) \n",
      "Test Loss tensor([0.0891, 0.0235, 0.0444, 0.0666, 0.0694, 0.0364])\n",
      "\n",
      "\n",
      "************** Batch 72 in 0.5921106338500977 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0907, 0.0191, 0.0434, 0.0747, 0.0739, 0.0367]) \n",
      "Test Loss tensor([0.0899, 0.0230, 0.0445, 0.0690, 0.0722, 0.0360])\n",
      "\n",
      "\n",
      "************** Batch 76 in 0.6107425689697266 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0938, 0.0240, 0.0420, 0.0784, 0.0740, 0.0381]) \n",
      "Test Loss tensor([0.0936, 0.0238, 0.0460, 0.0686, 0.0704, 0.0359])\n",
      "\n",
      "\n",
      "************** Batch 80 in 0.5897326469421387 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0935, 0.0196, 0.0436, 0.0688, 0.0696, 0.0320]) \n",
      "Test Loss tensor([0.0885, 0.0230, 0.0437, 0.0673, 0.0703, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 84 in 0.5853407382965088 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0940, 0.0257, 0.0410, 0.0750, 0.0698, 0.0379]) \n",
      "Test Loss tensor([0.0896, 0.0231, 0.0467, 0.0690, 0.0740, 0.0373])\n",
      "\n",
      "\n",
      "************** Batch 88 in 0.5825304985046387 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0882, 0.0188, 0.0484, 0.0696, 0.0688, 0.0381]) \n",
      "Test Loss tensor([0.0858, 0.0221, 0.0450, 0.0650, 0.0692, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 92 in 0.5894017219543457 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0886, 0.0195, 0.0477, 0.0611, 0.0715, 0.0382]) \n",
      "Test Loss tensor([0.0866, 0.0216, 0.0442, 0.0672, 0.0732, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 96 in 0.6026022434234619 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0908, 0.0287, 0.0508, 0.0682, 0.0714, 0.0394]) \n",
      "Test Loss tensor([0.0888, 0.0223, 0.0442, 0.0671, 0.0696, 0.0355])\n",
      "\n",
      "\n",
      "************** Batch 100 in 0.5741069316864014 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0858, 0.0244, 0.0429, 0.0725, 0.0699, 0.0339]) \n",
      "Test Loss tensor([0.0923, 0.0223, 0.0459, 0.0674, 0.0716, 0.0359])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 104 in 0.598682165145874 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0890, 0.0253, 0.0483, 0.0737, 0.0686, 0.0318]) \n",
      "Test Loss tensor([0.0882, 0.0228, 0.0443, 0.0650, 0.0682, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 108 in 0.5959343910217285 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0221, 0.0410, 0.0716, 0.0706, 0.0336]) \n",
      "Test Loss tensor([0.0928, 0.0235, 0.0474, 0.0690, 0.0705, 0.0377])\n",
      "\n",
      "\n",
      "************** Batch 112 in 0.5861861705780029 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0900, 0.0231, 0.0427, 0.0692, 0.0757, 0.0342]) \n",
      "Test Loss tensor([0.0890, 0.0223, 0.0442, 0.0648, 0.0693, 0.0351])\n",
      "\n",
      "\n",
      "************** Batch 116 in 0.5963332653045654 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0895, 0.0237, 0.0439, 0.0693, 0.0736, 0.0306]) \n",
      "Test Loss tensor([0.0892, 0.0225, 0.0449, 0.0669, 0.0726, 0.0361])\n",
      "\n",
      "\n",
      "************** Batch 120 in 0.583305835723877 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0965, 0.0262, 0.0467, 0.0684, 0.0736, 0.0348]) \n",
      "Test Loss tensor([0.0901, 0.0234, 0.0453, 0.0693, 0.0706, 0.0371])\n",
      "\n",
      "\n",
      "************** Batch 124 in 0.6505684852600098 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0899, 0.0213, 0.0472, 0.0634, 0.0685, 0.0355]) \n",
      "Test Loss tensor([0.0895, 0.0229, 0.0461, 0.0667, 0.0688, 0.0369])\n",
      "\n",
      "\n",
      "************** Batch 128 in 0.7432906627655029 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0846, 0.0222, 0.0453, 0.0747, 0.0691, 0.0349]) \n",
      "Test Loss tensor([0.0872, 0.0233, 0.0468, 0.0672, 0.0692, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 132 in 0.7018907070159912 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0832, 0.0239, 0.0450, 0.0690, 0.0705, 0.0359]) \n",
      "Test Loss tensor([0.0905, 0.0220, 0.0449, 0.0694, 0.0685, 0.0341])\n",
      "\n",
      "\n",
      "************** Batch 136 in 0.6734075546264648 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0938, 0.0220, 0.0428, 0.0654, 0.0702, 0.0350]) \n",
      "Test Loss tensor([0.0899, 0.0235, 0.0446, 0.0657, 0.0697, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 140 in 0.5766565799713135 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0916, 0.0257, 0.0495, 0.0700, 0.0668, 0.0353]) \n",
      "Test Loss tensor([0.0902, 0.0220, 0.0447, 0.0673, 0.0688, 0.0348])\n",
      "\n",
      "\n",
      "************** Batch 144 in 0.5943002700805664 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0872, 0.0250, 0.0455, 0.0696, 0.0670, 0.0322]) \n",
      "Test Loss tensor([0.0873, 0.0222, 0.0450, 0.0662, 0.0699, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 148 in 0.6036837100982666 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0928, 0.0231, 0.0445, 0.0681, 0.0671, 0.0314]) \n",
      "Test Loss tensor([0.0865, 0.0226, 0.0450, 0.0682, 0.0684, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 152 in 0.5835309028625488 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0837, 0.0245, 0.0468, 0.0657, 0.0672, 0.0326]) \n",
      "Test Loss tensor([0.0878, 0.0225, 0.0439, 0.0685, 0.0702, 0.0320])\n",
      "\n",
      "\n",
      "************** Batch 156 in 0.5970828533172607 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0774, 0.0236, 0.0469, 0.0671, 0.0709, 0.0330]) \n",
      "Test Loss tensor([0.0900, 0.0233, 0.0441, 0.0686, 0.0683, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 160 in 0.6152589321136475 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0262, 0.0450, 0.0675, 0.0681, 0.0361]) \n",
      "Test Loss tensor([0.0881, 0.0217, 0.0447, 0.0660, 0.0684, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 164 in 0.5816583633422852 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0935, 0.0254, 0.0429, 0.0682, 0.0675, 0.0328]) \n",
      "Test Loss tensor([0.0860, 0.0229, 0.0452, 0.0673, 0.0672, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 168 in 0.5940141677856445 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0224, 0.0388, 0.0734, 0.0699, 0.0358]) \n",
      "Test Loss tensor([0.0886, 0.0219, 0.0441, 0.0686, 0.0686, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 172 in 0.5708003044128418 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0917, 0.0225, 0.0452, 0.0690, 0.0658, 0.0353]) \n",
      "Test Loss tensor([0.0839, 0.0214, 0.0451, 0.0667, 0.0686, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 176 in 0.5973806381225586 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0935, 0.0198, 0.0427, 0.0571, 0.0717, 0.0373]) \n",
      "Test Loss tensor([0.0879, 0.0216, 0.0441, 0.0692, 0.0717, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 180 in 0.6088707447052002 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0853, 0.0262, 0.0484, 0.0669, 0.0710, 0.0392]) \n",
      "Test Loss tensor([0.0895, 0.0219, 0.0457, 0.0693, 0.0690, 0.0354])\n",
      "\n",
      "\n",
      "************** Batch 184 in 0.5860610008239746 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0820, 0.0244, 0.0402, 0.0678, 0.0692, 0.0310]) \n",
      "Test Loss tensor([0.0895, 0.0237, 0.0456, 0.0691, 0.0683, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 188 in 0.6245207786560059 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0838, 0.0215, 0.0467, 0.0621, 0.0715, 0.0370]) \n",
      "Test Loss tensor([0.0889, 0.0234, 0.0435, 0.0697, 0.0692, 0.0355])\n",
      "\n",
      "\n",
      "************** Batch 192 in 0.5670957565307617 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0965, 0.0250, 0.0462, 0.0677, 0.0703, 0.0342]) \n",
      "Test Loss tensor([0.0871, 0.0222, 0.0447, 0.0661, 0.0681, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 196 in 0.602358341217041 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0865, 0.0222, 0.0413, 0.0678, 0.0715, 0.0360]) \n",
      "Test Loss tensor([0.0892, 0.0230, 0.0431, 0.0677, 0.0687, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 200 in 0.5810518264770508 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0876, 0.0206, 0.0480, 0.0727, 0.0692, 0.0348]) \n",
      "Test Loss tensor([0.0857, 0.0218, 0.0448, 0.0694, 0.0681, 0.0341])\n",
      "\n",
      "\n",
      "************** Batch 204 in 0.5407230854034424 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0871, 0.0204, 0.0374, 0.0676, 0.0649, 0.0392]) \n",
      "Test Loss tensor([0.0891, 0.0233, 0.0434, 0.0670, 0.0680, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 208 in 0.6047871112823486 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1005, 0.0222, 0.0405, 0.0741, 0.0628, 0.0322]) \n",
      "Test Loss tensor([0.0889, 0.0228, 0.0436, 0.0690, 0.0677, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 212 in 0.6060471534729004 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0849, 0.0220, 0.0421, 0.0616, 0.0616, 0.0313]) \n",
      "Test Loss tensor([0.0872, 0.0221, 0.0443, 0.0666, 0.0679, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 216 in 0.595923662185669 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0880, 0.0252, 0.0449, 0.0727, 0.0648, 0.0359]) \n",
      "Test Loss tensor([0.0876, 0.0226, 0.0440, 0.0663, 0.0672, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 220 in 0.5610959529876709 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0870, 0.0212, 0.0474, 0.0672, 0.0689, 0.0305]) \n",
      "Test Loss tensor([0.0859, 0.0229, 0.0444, 0.0676, 0.0673, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 224 in 0.5672252178192139 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0856, 0.0243, 0.0424, 0.0657, 0.0643, 0.0338]) \n",
      "Test Loss tensor([0.0886, 0.0229, 0.0434, 0.0701, 0.0666, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 228 in 0.5752098560333252 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0940, 0.0314, 0.0448, 0.0721, 0.0758, 0.0368]) \n",
      "Test Loss tensor([0.0901, 0.0221, 0.0428, 0.0692, 0.0677, 0.0332])\n",
      "\n",
      "\n",
      "************** Batch 232 in 0.5481116771697998 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0880, 0.0236, 0.0446, 0.0570, 0.0709, 0.0359]) \n",
      "Test Loss tensor([0.0890, 0.0237, 0.0436, 0.0658, 0.0689, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 236 in 0.5572621822357178 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0873, 0.0242, 0.0449, 0.0724, 0.0731, 0.0313]) \n",
      "Test Loss tensor([0.0877, 0.0235, 0.0423, 0.0679, 0.0683, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 240 in 0.5363545417785645 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0868, 0.0227, 0.0385, 0.0708, 0.0631, 0.0273]) \n",
      "Test Loss tensor([0.0867, 0.0232, 0.0432, 0.0660, 0.0692, 0.0354])\n",
      "\n",
      "\n",
      "************** Batch 244 in 0.563312292098999 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0912, 0.0297, 0.0413, 0.0697, 0.0685, 0.0383]) \n",
      "Test Loss tensor([0.0880, 0.0236, 0.0432, 0.0672, 0.0713, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 248 in 0.5660324096679688 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0905, 0.0241, 0.0406, 0.0756, 0.0691, 0.0306]) \n",
      "Test Loss tensor([0.0861, 0.0236, 0.0429, 0.0675, 0.0677, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 252 in 0.5832645893096924 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0952, 0.0207, 0.0422, 0.0656, 0.0664, 0.0329]) \n",
      "Test Loss tensor([0.0859, 0.0219, 0.0442, 0.0670, 0.0684, 0.0343])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 256 in 0.6090097427368164 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0887, 0.0193, 0.0458, 0.0685, 0.0700, 0.0363]) \n",
      "Test Loss tensor([0.0888, 0.0225, 0.0430, 0.0666, 0.0683, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 260 in 0.5985162258148193 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0866, 0.0240, 0.0433, 0.0636, 0.0581, 0.0368]) \n",
      "Test Loss tensor([0.0879, 0.0238, 0.0444, 0.0650, 0.0676, 0.0351])\n",
      "\n",
      "\n",
      "************** Batch 264 in 0.6008882522583008 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0990, 0.0243, 0.0371, 0.0696, 0.0643, 0.0347]) \n",
      "Test Loss tensor([0.0873, 0.0230, 0.0435, 0.0656, 0.0691, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 268 in 0.5635876655578613 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0919, 0.0236, 0.0382, 0.0674, 0.0726, 0.0324]) \n",
      "Test Loss tensor([0.0875, 0.0244, 0.0434, 0.0675, 0.0689, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 272 in 0.5772285461425781 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0238, 0.0417, 0.0662, 0.0702, 0.0358]) \n",
      "Test Loss tensor([0.0887, 0.0238, 0.0443, 0.0649, 0.0686, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 276 in 0.6157443523406982 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0229, 0.0446, 0.0661, 0.0658, 0.0334]) \n",
      "Test Loss tensor([0.0893, 0.0235, 0.0441, 0.0655, 0.0692, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 280 in 0.5636718273162842 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0917, 0.0248, 0.0427, 0.0676, 0.0696, 0.0367]) \n",
      "Test Loss tensor([0.0881, 0.0222, 0.0436, 0.0651, 0.0677, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 284 in 0.5739691257476807 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0879, 0.0222, 0.0446, 0.0601, 0.0690, 0.0319]) \n",
      "Test Loss tensor([0.0858, 0.0229, 0.0446, 0.0676, 0.0670, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 288 in 0.5466537475585938 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0821, 0.0245, 0.0420, 0.0689, 0.0696, 0.0332]) \n",
      "Test Loss tensor([0.0895, 0.0229, 0.0416, 0.0662, 0.0708, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 292 in 0.5780758857727051 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0851, 0.0260, 0.0486, 0.0651, 0.0682, 0.0372]) \n",
      "Test Loss tensor([0.0884, 0.0229, 0.0443, 0.0652, 0.0695, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 296 in 0.5937139987945557 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0852, 0.0250, 0.0455, 0.0678, 0.0698, 0.0349]) \n",
      "Test Loss tensor([0.0869, 0.0213, 0.0436, 0.0666, 0.0680, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 300 in 0.595947265625 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0886, 0.0173, 0.0464, 0.0758, 0.0632, 0.0332]) \n",
      "Test Loss tensor([0.0874, 0.0233, 0.0437, 0.0679, 0.0678, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 304 in 0.5799427032470703 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0917, 0.0235, 0.0442, 0.0664, 0.0727, 0.0348]) \n",
      "Test Loss tensor([0.0886, 0.0215, 0.0426, 0.0672, 0.0691, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 308 in 0.5460186004638672 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0916, 0.0228, 0.0438, 0.0729, 0.0690, 0.0319]) \n",
      "Test Loss tensor([0.0890, 0.0218, 0.0421, 0.0703, 0.0679, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 312 in 0.6519162654876709 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0834, 0.0221, 0.0407, 0.0679, 0.0640, 0.0323]) \n",
      "Test Loss tensor([0.0902, 0.0233, 0.0433, 0.0672, 0.0708, 0.0356])\n",
      "\n",
      "\n",
      "************** Batch 316 in 0.667266845703125 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0978, 0.0265, 0.0446, 0.0676, 0.0706, 0.0371]) \n",
      "Test Loss tensor([0.0882, 0.0230, 0.0435, 0.0696, 0.0674, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 320 in 0.5580077171325684 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0874, 0.0225, 0.0427, 0.0651, 0.0622, 0.0353]) \n",
      "Test Loss tensor([0.0853, 0.0218, 0.0428, 0.0657, 0.0681, 0.0359])\n",
      "\n",
      "\n",
      "************** Batch 324 in 0.6163866519927979 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0866, 0.0234, 0.0436, 0.0719, 0.0659, 0.0357]) \n",
      "Test Loss tensor([0.0893, 0.0231, 0.0421, 0.0700, 0.0704, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 328 in 0.5755045413970947 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0970, 0.0237, 0.0455, 0.0663, 0.0700, 0.0343]) \n",
      "Test Loss tensor([0.0876, 0.0226, 0.0423, 0.0664, 0.0688, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 332 in 0.5932438373565674 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0831, 0.0199, 0.0391, 0.0697, 0.0700, 0.0303]) \n",
      "Test Loss tensor([0.0888, 0.0210, 0.0440, 0.0694, 0.0700, 0.0348])\n",
      "\n",
      "\n",
      "************** Batch 336 in 0.5550312995910645 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0880, 0.0216, 0.0444, 0.0646, 0.0724, 0.0358]) \n",
      "Test Loss tensor([0.0892, 0.0226, 0.0423, 0.0678, 0.0685, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 340 in 0.5561351776123047 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0814, 0.0217, 0.0427, 0.0727, 0.0684, 0.0341]) \n",
      "Test Loss tensor([0.0889, 0.0218, 0.0422, 0.0689, 0.0696, 0.0350])\n",
      "\n",
      "\n",
      "************** Batch 344 in 0.5712425708770752 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0830, 0.0185, 0.0384, 0.0590, 0.0665, 0.0394]) \n",
      "Test Loss tensor([0.0926, 0.0234, 0.0431, 0.0677, 0.0709, 0.0353])\n",
      "\n",
      "\n",
      "************** Batch 348 in 0.574786901473999 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0859, 0.0208, 0.0451, 0.0688, 0.0726, 0.0353]) \n",
      "Test Loss tensor([0.0862, 0.0239, 0.0428, 0.0685, 0.0682, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 352 in 0.5615057945251465 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0883, 0.0215, 0.0489, 0.0688, 0.0640, 0.0341]) \n",
      "Test Loss tensor([0.0866, 0.0226, 0.0430, 0.0673, 0.0697, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 356 in 0.5387306213378906 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0910, 0.0247, 0.0440, 0.0740, 0.0715, 0.0338]) \n",
      "Test Loss tensor([0.0886, 0.0235, 0.0438, 0.0675, 0.0686, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 360 in 0.5713744163513184 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0894, 0.0249, 0.0415, 0.0652, 0.0671, 0.0340]) \n",
      "Test Loss tensor([0.0877, 0.0229, 0.0429, 0.0680, 0.0674, 0.0336])\n",
      "\n",
      "\n",
      "************** Batch 364 in 0.5416407585144043 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0877, 0.0248, 0.0481, 0.0669, 0.0686, 0.0368]) \n",
      "Test Loss tensor([0.0874, 0.0224, 0.0441, 0.0655, 0.0676, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 368 in 0.5641162395477295 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0974, 0.0287, 0.0386, 0.0707, 0.0701, 0.0321]) \n",
      "Test Loss tensor([0.0884, 0.0229, 0.0430, 0.0667, 0.0683, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 372 in 0.5460977554321289 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0922, 0.0231, 0.0427, 0.0623, 0.0660, 0.0308]) \n",
      "Test Loss tensor([0.0889, 0.0227, 0.0419, 0.0665, 0.0676, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 376 in 0.5452096462249756 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0817, 0.0235, 0.0396, 0.0650, 0.0629, 0.0370]) \n",
      "Test Loss tensor([0.0861, 0.0217, 0.0420, 0.0664, 0.0678, 0.0339])\n",
      "\n",
      "\n",
      "************** Batch 380 in 0.5881867408752441 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0864, 0.0259, 0.0481, 0.0710, 0.0694, 0.0334]) \n",
      "Test Loss tensor([0.0874, 0.0218, 0.0426, 0.0661, 0.0694, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 384 in 0.5887014865875244 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0913, 0.0217, 0.0460, 0.0707, 0.0658, 0.0373]) \n",
      "Test Loss tensor([0.0872, 0.0228, 0.0420, 0.0659, 0.0688, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 388 in 0.5848219394683838 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0934, 0.0239, 0.0418, 0.0593, 0.0723, 0.0344]) \n",
      "Test Loss tensor([0.0890, 0.0242, 0.0427, 0.0672, 0.0672, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 392 in 0.5884702205657959 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0901, 0.0220, 0.0433, 0.0704, 0.0724, 0.0312]) \n",
      "Test Loss tensor([0.0875, 0.0218, 0.0417, 0.0660, 0.0689, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 396 in 0.5687727928161621 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0851, 0.0235, 0.0401, 0.0681, 0.0686, 0.0329]) \n",
      "Test Loss tensor([0.0862, 0.0229, 0.0424, 0.0648, 0.0689, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 400 in 0.5576205253601074 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0860, 0.0266, 0.0437, 0.0645, 0.0667, 0.0325]) \n",
      "Test Loss tensor([0.0857, 0.0230, 0.0420, 0.0650, 0.0697, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 404 in 0.5483160018920898 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0829, 0.0218, 0.0398, 0.0659, 0.0631, 0.0351]) \n",
      "Test Loss tensor([0.0860, 0.0233, 0.0432, 0.0661, 0.0679, 0.0351])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 408 in 0.5700855255126953 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0928, 0.0199, 0.0420, 0.0650, 0.0704, 0.0330]) \n",
      "Test Loss tensor([0.0932, 0.0226, 0.0446, 0.0685, 0.0699, 0.0360])\n",
      "\n",
      "\n",
      "************** Batch 412 in 0.5661442279815674 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0983, 0.0246, 0.0431, 0.0661, 0.0730, 0.0358]) \n",
      "Test Loss tensor([0.0897, 0.0230, 0.0436, 0.0681, 0.0673, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 416 in 0.5583570003509521 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0860, 0.0225, 0.0435, 0.0666, 0.0674, 0.0310]) \n",
      "Test Loss tensor([0.0888, 0.0220, 0.0424, 0.0660, 0.0697, 0.0341])\n",
      "\n",
      "\n",
      "************** Batch 420 in 0.5477340221405029 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0980, 0.0252, 0.0418, 0.0667, 0.0652, 0.0309]) \n",
      "Test Loss tensor([0.0839, 0.0225, 0.0427, 0.0660, 0.0682, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 424 in 0.5749082565307617 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0908, 0.0248, 0.0493, 0.0670, 0.0719, 0.0353]) \n",
      "Test Loss tensor([0.0854, 0.0218, 0.0410, 0.0652, 0.0686, 0.0348])\n",
      "\n",
      "\n",
      "************** Batch 428 in 0.6191415786743164 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0849, 0.0211, 0.0405, 0.0596, 0.0646, 0.0334]) \n",
      "Test Loss tensor([0.0883, 0.0238, 0.0434, 0.0665, 0.0684, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 432 in 0.6407554149627686 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0814, 0.0204, 0.0459, 0.0738, 0.0676, 0.0333]) \n",
      "Test Loss tensor([0.0892, 0.0231, 0.0420, 0.0671, 0.0688, 0.0322])\n",
      "\n",
      "\n",
      "************** Batch 436 in 0.6353023052215576 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0880, 0.0172, 0.0438, 0.0674, 0.0658, 0.0322]) \n",
      "Test Loss tensor([0.0880, 0.0223, 0.0426, 0.0688, 0.0697, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 440 in 0.5895586013793945 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0937, 0.0254, 0.0394, 0.0686, 0.0710, 0.0343]) \n",
      "Test Loss tensor([0.0893, 0.0224, 0.0440, 0.0670, 0.0668, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 444 in 0.6188468933105469 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0863, 0.0173, 0.0407, 0.0613, 0.0693, 0.0317]) \n",
      "Test Loss tensor([0.0859, 0.0230, 0.0421, 0.0654, 0.0678, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 448 in 0.6282315254211426 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0984, 0.0254, 0.0422, 0.0655, 0.0729, 0.0377]) \n",
      "Test Loss tensor([0.0854, 0.0229, 0.0422, 0.0671, 0.0668, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 452 in 0.6060445308685303 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0919, 0.0270, 0.0404, 0.0668, 0.0696, 0.0333]) \n",
      "Test Loss tensor([0.0865, 0.0221, 0.0428, 0.0659, 0.0680, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 456 in 0.605675220489502 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0885, 0.0260, 0.0442, 0.0626, 0.0685, 0.0346]) \n",
      "Test Loss tensor([0.0891, 0.0227, 0.0410, 0.0673, 0.0692, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 460 in 0.620227575302124 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0923, 0.0236, 0.0424, 0.0664, 0.0714, 0.0344]) \n",
      "Test Loss tensor([0.0867, 0.0221, 0.0421, 0.0661, 0.0689, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 464 in 0.6136536598205566 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0844, 0.0223, 0.0419, 0.0637, 0.0652, 0.0302]) \n",
      "Test Loss tensor([0.0866, 0.0232, 0.0434, 0.0678, 0.0689, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 468 in 0.5938117504119873 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0838, 0.0178, 0.0396, 0.0615, 0.0649, 0.0346]) \n",
      "Test Loss tensor([0.0849, 0.0229, 0.0406, 0.0664, 0.0689, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 472 in 0.5904603004455566 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0763, 0.0182, 0.0406, 0.0681, 0.0645, 0.0339]) \n",
      "Test Loss tensor([0.0891, 0.0240, 0.0418, 0.0670, 0.0662, 0.0350])\n",
      "\n",
      "\n",
      "************** Batch 476 in 0.594017744064331 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0839, 0.0257, 0.0455, 0.0689, 0.0674, 0.0364]) \n",
      "Test Loss tensor([0.0851, 0.0231, 0.0418, 0.0667, 0.0688, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 480 in 0.5931479930877686 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0938, 0.0282, 0.0415, 0.0638, 0.0679, 0.0376]) \n",
      "Test Loss tensor([0.0865, 0.0226, 0.0420, 0.0654, 0.0670, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 484 in 0.6093852519989014 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0890, 0.0209, 0.0371, 0.0702, 0.0682, 0.0315]) \n",
      "Test Loss tensor([0.0871, 0.0220, 0.0428, 0.0680, 0.0674, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 488 in 0.6282491683959961 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0779, 0.0264, 0.0464, 0.0584, 0.0720, 0.0359]) \n",
      "Test Loss tensor([0.0875, 0.0217, 0.0423, 0.0677, 0.0684, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 492 in 0.5810792446136475 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0837, 0.0237, 0.0435, 0.0695, 0.0666, 0.0275]) \n",
      "Test Loss tensor([0.0863, 0.0235, 0.0423, 0.0668, 0.0683, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 496 in 0.5679023265838623 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0932, 0.0277, 0.0392, 0.0671, 0.0628, 0.0324]) \n",
      "Test Loss tensor([0.0853, 0.0223, 0.0418, 0.0668, 0.0687, 0.0332])\n",
      "\n",
      "\n",
      "************** Batch 500 in 0.530397891998291 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0833, 0.0207, 0.0430, 0.0624, 0.0675, 0.0352]) \n",
      "Test Loss tensor([0.0854, 0.0227, 0.0439, 0.0675, 0.0682, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 504 in 0.5429177284240723 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0913, 0.0213, 0.0387, 0.0604, 0.0662, 0.0332]) \n",
      "Test Loss tensor([0.0882, 0.0223, 0.0429, 0.0649, 0.0666, 0.0341])\n",
      "\n",
      "\n",
      "************** Batch 508 in 0.5294373035430908 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0829, 0.0181, 0.0401, 0.0650, 0.0648, 0.0355]) \n",
      "Test Loss tensor([0.0862, 0.0219, 0.0421, 0.0681, 0.0680, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 512 in 0.536231517791748 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0777, 0.0255, 0.0420, 0.0618, 0.0699, 0.0345]) \n",
      "Test Loss tensor([0.0892, 0.0216, 0.0430, 0.0677, 0.0694, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 516 in 0.5871269702911377 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0878, 0.0210, 0.0464, 0.0706, 0.0695, 0.0371]) \n",
      "Test Loss tensor([0.0855, 0.0224, 0.0416, 0.0689, 0.0678, 0.0328])\n",
      "\n",
      "\n",
      "************** Batch 520 in 0.5671396255493164 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0852, 0.0243, 0.0408, 0.0679, 0.0663, 0.0301]) \n",
      "Test Loss tensor([0.0884, 0.0232, 0.0419, 0.0670, 0.0645, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 524 in 0.5779500007629395 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0823, 0.0246, 0.0414, 0.0679, 0.0670, 0.0300]) \n",
      "Test Loss tensor([0.0878, 0.0227, 0.0408, 0.0656, 0.0684, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 528 in 0.5530154705047607 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0962, 0.0261, 0.0393, 0.0634, 0.0679, 0.0314]) \n",
      "Test Loss tensor([0.0843, 0.0218, 0.0411, 0.0660, 0.0672, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 532 in 0.6408867835998535 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0831, 0.0233, 0.0448, 0.0652, 0.0681, 0.0344]) \n",
      "Test Loss tensor([0.0879, 0.0233, 0.0420, 0.0671, 0.0663, 0.0350])\n",
      "\n",
      "\n",
      "************** Batch 536 in 0.8475041389465332 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0819, 0.0202, 0.0389, 0.0665, 0.0599, 0.0379]) \n",
      "Test Loss tensor([0.0862, 0.0232, 0.0430, 0.0667, 0.0667, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 540 in 0.7563445568084717 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0882, 0.0263, 0.0450, 0.0678, 0.0579, 0.0314]) \n",
      "Test Loss tensor([0.0885, 0.0221, 0.0415, 0.0666, 0.0692, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 544 in 0.895622730255127 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0888, 0.0218, 0.0382, 0.0661, 0.0676, 0.0340]) \n",
      "Test Loss tensor([0.0877, 0.0214, 0.0398, 0.0664, 0.0672, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 548 in 1.0116238594055176 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0876, 0.0189, 0.0450, 0.0707, 0.0666, 0.0332]) \n",
      "Test Loss tensor([0.0879, 0.0220, 0.0408, 0.0650, 0.0672, 0.0336])\n",
      "\n",
      "\n",
      "************** Batch 552 in 0.8069977760314941 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0807, 0.0238, 0.0393, 0.0696, 0.0628, 0.0319]) \n",
      "Test Loss tensor([0.0862, 0.0228, 0.0420, 0.0671, 0.0674, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 556 in 0.7320160865783691 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0906, 0.0216, 0.0435, 0.0681, 0.0710, 0.0325]) \n",
      "Test Loss tensor([0.0873, 0.0227, 0.0410, 0.0658, 0.0681, 0.0342])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 560 in 0.7511448860168457 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0858, 0.0219, 0.0392, 0.0680, 0.0749, 0.0326]) \n",
      "Test Loss tensor([0.0852, 0.0222, 0.0408, 0.0630, 0.0687, 0.0329])\n",
      "\n",
      "\n",
      "************** Batch 564 in 0.7005031108856201 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0833, 0.0220, 0.0431, 0.0657, 0.0660, 0.0327]) \n",
      "Test Loss tensor([0.0860, 0.0214, 0.0430, 0.0668, 0.0655, 0.0339])\n",
      "\n",
      "\n",
      "************** Batch 568 in 0.6656126976013184 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0863, 0.0251, 0.0357, 0.0661, 0.0679, 0.0322]) \n",
      "Test Loss tensor([0.0891, 0.0220, 0.0400, 0.0661, 0.0680, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 572 in 0.677154541015625 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0911, 0.0222, 0.0392, 0.0647, 0.0680, 0.0333]) \n",
      "Test Loss tensor([0.0840, 0.0219, 0.0415, 0.0653, 0.0661, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 576 in 0.6617763042449951 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0989, 0.0297, 0.0445, 0.0684, 0.0690, 0.0333]) \n",
      "Test Loss tensor([0.0865, 0.0219, 0.0424, 0.0669, 0.0677, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 580 in 0.6572620868682861 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0846, 0.0245, 0.0419, 0.0699, 0.0671, 0.0353]) \n",
      "Test Loss tensor([0.0872, 0.0232, 0.0422, 0.0661, 0.0659, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 584 in 0.6374964714050293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0936, 0.0230, 0.0428, 0.0641, 0.0728, 0.0325]) \n",
      "Test Loss tensor([0.0873, 0.0228, 0.0427, 0.0668, 0.0700, 0.0363])\n",
      "\n",
      "\n",
      "************** Batch 588 in 0.6096823215484619 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0876, 0.0252, 0.0430, 0.0669, 0.0688, 0.0421]) \n",
      "Test Loss tensor([0.0874, 0.0235, 0.0426, 0.0661, 0.0676, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 592 in 0.6459751129150391 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0917, 0.0218, 0.0410, 0.0731, 0.0664, 0.0301]) \n",
      "Test Loss tensor([0.0869, 0.0234, 0.0426, 0.0631, 0.0679, 0.0350])\n",
      "\n",
      "\n",
      "************** Batch 596 in 0.6189391613006592 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0843, 0.0233, 0.0390, 0.0716, 0.0738, 0.0320]) \n",
      "Test Loss tensor([0.0862, 0.0214, 0.0411, 0.0661, 0.0673, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 600 in 0.587449312210083 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0915, 0.0214, 0.0379, 0.0617, 0.0642, 0.0365]) \n",
      "Test Loss tensor([0.0862, 0.0228, 0.0418, 0.0666, 0.0671, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 604 in 0.5989627838134766 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0923, 0.0229, 0.0399, 0.0654, 0.0669, 0.0306]) \n",
      "Test Loss tensor([0.0855, 0.0226, 0.0411, 0.0656, 0.0683, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 608 in 0.592961311340332 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0908, 0.0257, 0.0464, 0.0706, 0.0696, 0.0353]) \n",
      "Test Loss tensor([0.0843, 0.0225, 0.0426, 0.0665, 0.0658, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 612 in 0.6127204895019531 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0783, 0.0160, 0.0418, 0.0627, 0.0600, 0.0347]) \n",
      "Test Loss tensor([0.0859, 0.0222, 0.0427, 0.0653, 0.0693, 0.0354])\n",
      "\n",
      "\n",
      "************** Batch 616 in 0.6290709972381592 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0777, 0.0201, 0.0436, 0.0738, 0.0748, 0.0364]) \n",
      "Test Loss tensor([0.0865, 0.0231, 0.0409, 0.0650, 0.0663, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 620 in 0.590447187423706 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0837, 0.0200, 0.0411, 0.0631, 0.0672, 0.0355]) \n",
      "Test Loss tensor([0.0879, 0.0232, 0.0414, 0.0665, 0.0683, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 624 in 0.6189825534820557 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0977, 0.0261, 0.0383, 0.0742, 0.0666, 0.0353]) \n",
      "Test Loss tensor([0.0871, 0.0239, 0.0404, 0.0667, 0.0689, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 628 in 0.5884120464324951 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0851, 0.0203, 0.0383, 0.0663, 0.0708, 0.0350]) \n",
      "Test Loss tensor([0.0855, 0.0221, 0.0410, 0.0664, 0.0676, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 632 in 0.5936002731323242 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0888, 0.0238, 0.0397, 0.0661, 0.0640, 0.0325]) \n",
      "Test Loss tensor([0.0892, 0.0243, 0.0421, 0.0651, 0.0690, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 636 in 0.6130311489105225 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0867, 0.0179, 0.0414, 0.0599, 0.0671, 0.0405]) \n",
      "Test Loss tensor([0.0846, 0.0219, 0.0413, 0.0653, 0.0665, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 640 in 0.6189224720001221 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0943, 0.0201, 0.0383, 0.0730, 0.0665, 0.0351]) \n",
      "Test Loss tensor([0.0864, 0.0228, 0.0417, 0.0648, 0.0661, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 644 in 0.6465985774993896 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0908, 0.0293, 0.0424, 0.0740, 0.0724, 0.0379]) \n",
      "Test Loss tensor([0.0880, 0.0230, 0.0409, 0.0664, 0.0705, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 648 in 0.5912516117095947 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0817, 0.0257, 0.0408, 0.0655, 0.0720, 0.0341]) \n",
      "Test Loss tensor([0.0893, 0.0231, 0.0410, 0.0662, 0.0696, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 652 in 0.5998873710632324 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0846, 0.0217, 0.0399, 0.0611, 0.0683, 0.0333]) \n",
      "Test Loss tensor([0.0869, 0.0226, 0.0414, 0.0649, 0.0659, 0.0360])\n",
      "\n",
      "\n",
      "************** Batch 656 in 0.6176893711090088 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0827, 0.0169, 0.0443, 0.0733, 0.0649, 0.0371]) \n",
      "Test Loss tensor([0.0849, 0.0234, 0.0412, 0.0667, 0.0667, 0.0327])\n",
      "\n",
      "\n",
      "************** Batch 660 in 0.5863404273986816 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0925, 0.0246, 0.0450, 0.0652, 0.0690, 0.0352]) \n",
      "Test Loss tensor([0.0895, 0.0236, 0.0415, 0.0670, 0.0670, 0.0359])\n",
      "\n",
      "\n",
      "************** Batch 664 in 0.6154062747955322 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0887, 0.0253, 0.0403, 0.0688, 0.0686, 0.0340]) \n",
      "Test Loss tensor([0.0882, 0.0237, 0.0413, 0.0637, 0.0675, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 668 in 0.6959619522094727 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0891, 0.0235, 0.0436, 0.0671, 0.0659, 0.0337]) \n",
      "Test Loss tensor([0.0897, 0.0222, 0.0412, 0.0666, 0.0687, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 672 in 0.7392287254333496 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0841, 0.0207, 0.0365, 0.0626, 0.0706, 0.0302]) \n",
      "Test Loss tensor([0.0867, 0.0221, 0.0414, 0.0652, 0.0678, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 676 in 0.6699130535125732 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0868, 0.0221, 0.0343, 0.0605, 0.0655, 0.0311]) \n",
      "Test Loss tensor([0.0855, 0.0228, 0.0414, 0.0647, 0.0696, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 680 in 0.6321027278900146 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0790, 0.0217, 0.0396, 0.0650, 0.0683, 0.0322]) \n",
      "Test Loss tensor([0.0883, 0.0236, 0.0446, 0.0643, 0.0682, 0.0360])\n",
      "\n",
      "\n",
      "************** Batch 684 in 0.6344196796417236 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0906, 0.0206, 0.0434, 0.0650, 0.0688, 0.0344]) \n",
      "Test Loss tensor([0.0881, 0.0214, 0.0402, 0.0660, 0.0688, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 688 in 0.6796784400939941 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0899, 0.0251, 0.0380, 0.0665, 0.0652, 0.0310]) \n",
      "Test Loss tensor([0.0911, 0.0234, 0.0408, 0.0650, 0.0703, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 692 in 0.6249294281005859 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0944, 0.0232, 0.0410, 0.0725, 0.0691, 0.0331]) \n",
      "Test Loss tensor([0.0897, 0.0229, 0.0413, 0.0650, 0.0681, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 696 in 0.6375243663787842 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0832, 0.0223, 0.0418, 0.0670, 0.0707, 0.0326]) \n",
      "Test Loss tensor([0.0876, 0.0232, 0.0411, 0.0649, 0.0686, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 700 in 0.6190929412841797 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0829, 0.0222, 0.0432, 0.0660, 0.0693, 0.0335]) \n",
      "Test Loss tensor([0.0844, 0.0211, 0.0417, 0.0637, 0.0667, 0.0351])\n",
      "\n",
      "\n",
      "************** Batch 704 in 0.6034634113311768 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0941, 0.0200, 0.0366, 0.0734, 0.0665, 0.0334]) \n",
      "Test Loss tensor([0.0840, 0.0226, 0.0423, 0.0655, 0.0661, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 708 in 0.5992522239685059 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0956, 0.0243, 0.0406, 0.0704, 0.0711, 0.0361]) \n",
      "Test Loss tensor([0.0914, 0.0230, 0.0423, 0.0643, 0.0692, 0.0353])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 712 in 0.603358268737793 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0963, 0.0184, 0.0404, 0.0652, 0.0678, 0.0332]) \n",
      "Test Loss tensor([0.0873, 0.0223, 0.0408, 0.0669, 0.0683, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 716 in 0.5953183174133301 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0853, 0.0258, 0.0424, 0.0669, 0.0677, 0.0400]) \n",
      "Test Loss tensor([0.0854, 0.0221, 0.0410, 0.0663, 0.0660, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 720 in 0.6179239749908447 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0829, 0.0208, 0.0414, 0.0605, 0.0614, 0.0341]) \n",
      "Test Loss tensor([0.0872, 0.0221, 0.0419, 0.0646, 0.0684, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 724 in 0.5881593227386475 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0839, 0.0228, 0.0403, 0.0590, 0.0649, 0.0370]) \n",
      "Test Loss tensor([0.0867, 0.0224, 0.0410, 0.0658, 0.0661, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 728 in 0.6146273612976074 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0940, 0.0235, 0.0413, 0.0682, 0.0711, 0.0317]) \n",
      "Test Loss tensor([0.0908, 0.0231, 0.0419, 0.0666, 0.0657, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 732 in 0.6084442138671875 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0832, 0.0269, 0.0433, 0.0702, 0.0718, 0.0326]) \n",
      "Test Loss tensor([0.0847, 0.0231, 0.0427, 0.0668, 0.0687, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 736 in 0.5934765338897705 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0909, 0.0236, 0.0431, 0.0672, 0.0687, 0.0306]) \n",
      "Test Loss tensor([0.0855, 0.0222, 0.0399, 0.0643, 0.0672, 0.0329])\n",
      "\n",
      "\n",
      "************** Batch 740 in 0.6328639984130859 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0894, 0.0232, 0.0372, 0.0731, 0.0620, 0.0312]) \n",
      "Test Loss tensor([0.0862, 0.0226, 0.0407, 0.0647, 0.0665, 0.0328])\n",
      "\n",
      "\n",
      "************** Batch 744 in 0.6029503345489502 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0899, 0.0203, 0.0395, 0.0553, 0.0672, 0.0361]) \n",
      "Test Loss tensor([0.0857, 0.0235, 0.0410, 0.0658, 0.0665, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 748 in 0.6412694454193115 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0796, 0.0188, 0.0368, 0.0634, 0.0639, 0.0326]) \n",
      "Test Loss tensor([0.0864, 0.0226, 0.0406, 0.0656, 0.0677, 0.0322])\n",
      "\n",
      "\n",
      "************** Batch 752 in 0.6146082878112793 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0886, 0.0212, 0.0387, 0.0621, 0.0622, 0.0358]) \n",
      "Test Loss tensor([0.0844, 0.0213, 0.0408, 0.0641, 0.0669, 0.0339])\n",
      "\n",
      "\n",
      "************** Batch 756 in 0.5877506732940674 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0872, 0.0216, 0.0439, 0.0613, 0.0712, 0.0303]) \n",
      "Test Loss tensor([0.0852, 0.0232, 0.0384, 0.0650, 0.0678, 0.0339])\n",
      "\n",
      "\n",
      "************** Batch 760 in 0.6138737201690674 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0863, 0.0203, 0.0389, 0.0671, 0.0709, 0.0365]) \n",
      "Test Loss tensor([0.0829, 0.0212, 0.0404, 0.0663, 0.0657, 0.0328])\n",
      "\n",
      "\n",
      "************** Batch 764 in 0.6053881645202637 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0947, 0.0244, 0.0432, 0.0705, 0.0665, 0.0291]) \n",
      "Test Loss tensor([0.0845, 0.0221, 0.0417, 0.0655, 0.0680, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 768 in 0.5940582752227783 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0876, 0.0262, 0.0427, 0.0679, 0.0684, 0.0324]) \n",
      "Test Loss tensor([0.0870, 0.0221, 0.0412, 0.0674, 0.0654, 0.0336])\n",
      "\n",
      "\n",
      "************** Batch 772 in 0.6152997016906738 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0895, 0.0245, 0.0423, 0.0653, 0.0662, 0.0374]) \n",
      "Test Loss tensor([0.0866, 0.0225, 0.0403, 0.0652, 0.0674, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 776 in 0.6049392223358154 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0843, 0.0239, 0.0412, 0.0652, 0.0664, 0.0314]) \n",
      "Test Loss tensor([0.0879, 0.0220, 0.0405, 0.0663, 0.0668, 0.0355])\n",
      "\n",
      "\n",
      "************** Batch 780 in 0.6101465225219727 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0806, 0.0210, 0.0423, 0.0630, 0.0712, 0.0368]) \n",
      "Test Loss tensor([0.0822, 0.0217, 0.0410, 0.0641, 0.0647, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 784 in 0.5967411994934082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0929, 0.0222, 0.0405, 0.0640, 0.0704, 0.0351]) \n",
      "Test Loss tensor([0.0861, 0.0222, 0.0403, 0.0651, 0.0698, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 788 in 0.575810432434082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0838, 0.0207, 0.0383, 0.0647, 0.0678, 0.0314]) \n",
      "Test Loss tensor([0.0863, 0.0220, 0.0402, 0.0656, 0.0685, 0.0339])\n",
      "\n",
      "\n",
      "************** Batch 792 in 0.631828784942627 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0784, 0.0232, 0.0417, 0.0662, 0.0645, 0.0345]) \n",
      "Test Loss tensor([0.0873, 0.0228, 0.0404, 0.0643, 0.0677, 0.0351])\n",
      "\n",
      "\n",
      "************** Batch 796 in 0.608867883682251 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0876, 0.0227, 0.0353, 0.0651, 0.0648, 0.0372]) \n",
      "Test Loss tensor([0.0850, 0.0227, 0.0406, 0.0649, 0.0657, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 800 in 0.615715742111206 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0852, 0.0227, 0.0394, 0.0668, 0.0718, 0.0345]) \n",
      "Test Loss tensor([0.0876, 0.0231, 0.0400, 0.0653, 0.0671, 0.0336])\n",
      "\n",
      "\n",
      "************** Batch 804 in 0.6055080890655518 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0926, 0.0230, 0.0381, 0.0669, 0.0711, 0.0310]) \n",
      "Test Loss tensor([0.0883, 0.0212, 0.0393, 0.0662, 0.0668, 0.0328])\n",
      "\n",
      "\n",
      "************** Batch 808 in 0.5866520404815674 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0866, 0.0207, 0.0403, 0.0598, 0.0677, 0.0307]) \n",
      "Test Loss tensor([0.0868, 0.0212, 0.0399, 0.0647, 0.0669, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 812 in 0.6039345264434814 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0850, 0.0238, 0.0365, 0.0613, 0.0720, 0.0340]) \n",
      "Test Loss tensor([0.0854, 0.0229, 0.0412, 0.0655, 0.0659, 0.0327])\n",
      "\n",
      "\n",
      "************** Batch 816 in 0.600067138671875 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0910, 0.0213, 0.0373, 0.0662, 0.0684, 0.0331]) \n",
      "Test Loss tensor([0.0829, 0.0218, 0.0395, 0.0650, 0.0663, 0.0328])\n",
      "\n",
      "\n",
      "************** Batch 820 in 0.614661455154419 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0863, 0.0197, 0.0397, 0.0582, 0.0732, 0.0317]) \n",
      "Test Loss tensor([0.0867, 0.0211, 0.0385, 0.0659, 0.0667, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 824 in 0.6050992012023926 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0836, 0.0212, 0.0405, 0.0610, 0.0720, 0.0334]) \n",
      "Test Loss tensor([0.0842, 0.0213, 0.0407, 0.0667, 0.0653, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 828 in 0.5985312461853027 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0883, 0.0230, 0.0393, 0.0667, 0.0669, 0.0329]) \n",
      "Test Loss tensor([0.0877, 0.0230, 0.0401, 0.0654, 0.0650, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 832 in 0.6052930355072021 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0832, 0.0235, 0.0380, 0.0688, 0.0683, 0.0297]) \n",
      "Test Loss tensor([0.0833, 0.0225, 0.0409, 0.0654, 0.0659, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 836 in 0.6354506015777588 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0959, 0.0264, 0.0447, 0.0759, 0.0686, 0.0419]) \n",
      "Test Loss tensor([0.0860, 0.0218, 0.0406, 0.0646, 0.0681, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 840 in 0.6051125526428223 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0854, 0.0222, 0.0371, 0.0688, 0.0693, 0.0319]) \n",
      "Test Loss tensor([0.0857, 0.0225, 0.0382, 0.0646, 0.0683, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 844 in 0.6084446907043457 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0851, 0.0208, 0.0405, 0.0614, 0.0611, 0.0356]) \n",
      "Test Loss tensor([0.0866, 0.0230, 0.0399, 0.0667, 0.0685, 0.0348])\n",
      "\n",
      "\n",
      "************** Batch 848 in 0.6044254302978516 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0821, 0.0248, 0.0406, 0.0667, 0.0686, 0.0379]) \n",
      "Test Loss tensor([0.0836, 0.0229, 0.0396, 0.0655, 0.0637, 0.0322])\n",
      "\n",
      "\n",
      "************** Batch 852 in 0.6104433536529541 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0878, 0.0239, 0.0370, 0.0695, 0.0634, 0.0363]) \n",
      "Test Loss tensor([0.0895, 0.0222, 0.0396, 0.0653, 0.0682, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 856 in 0.6168951988220215 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0865, 0.0232, 0.0405, 0.0609, 0.0665, 0.0354]) \n",
      "Test Loss tensor([0.0859, 0.0231, 0.0402, 0.0635, 0.0679, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 860 in 0.5929365158081055 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0830, 0.0280, 0.0392, 0.0727, 0.0670, 0.0363]) \n",
      "Test Loss tensor([0.0847, 0.0225, 0.0393, 0.0656, 0.0668, 0.0340])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 864 in 0.6427228450775146 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0836, 0.0190, 0.0387, 0.0668, 0.0636, 0.0310]) \n",
      "Test Loss tensor([0.0904, 0.0214, 0.0402, 0.0677, 0.0723, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 868 in 0.6292505264282227 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0888, 0.0178, 0.0375, 0.0692, 0.0714, 0.0373]) \n",
      "Test Loss tensor([0.0836, 0.0224, 0.0399, 0.0648, 0.0664, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 872 in 0.6016325950622559 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0830, 0.0238, 0.0401, 0.0645, 0.0674, 0.0328]) \n",
      "Test Loss tensor([0.0884, 0.0228, 0.0397, 0.0652, 0.0701, 0.0370])\n",
      "\n",
      "\n",
      "************** Batch 876 in 0.5799517631530762 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0695, 0.0190, 0.0296, 0.0481, 0.0506, 0.0237]) \n",
      "Test Loss tensor([0.0873, 0.0223, 0.0401, 0.0646, 0.0669, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 0 in 0.5907180309295654 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0873, 0.0214, 0.0440, 0.0667, 0.0616, 0.0319]) \n",
      "Test Loss tensor([0.0901, 0.0234, 0.0426, 0.0635, 0.0705, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 4 in 0.6139054298400879 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0831, 0.0219, 0.0451, 0.0691, 0.0658, 0.0372]) \n",
      "Test Loss tensor([0.0832, 0.0230, 0.0410, 0.0654, 0.0689, 0.0339])\n",
      "\n",
      "\n",
      "************** Batch 8 in 0.6246116161346436 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0888, 0.0229, 0.0347, 0.0652, 0.0683, 0.0304]) \n",
      "Test Loss tensor([0.0862, 0.0226, 0.0390, 0.0649, 0.0714, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 12 in 0.5904791355133057 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0842, 0.0222, 0.0432, 0.0670, 0.0706, 0.0285]) \n",
      "Test Loss tensor([0.0851, 0.0216, 0.0395, 0.0654, 0.0685, 0.0341])\n",
      "\n",
      "\n",
      "************** Batch 16 in 0.6198897361755371 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0220, 0.0397, 0.0716, 0.0637, 0.0328]) \n",
      "Test Loss tensor([0.0871, 0.0218, 0.0399, 0.0650, 0.0676, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 20 in 0.5894322395324707 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0814, 0.0186, 0.0388, 0.0637, 0.0696, 0.0358]) \n",
      "Test Loss tensor([0.0860, 0.0236, 0.0420, 0.0630, 0.0658, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 24 in 0.6002171039581299 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0802, 0.0228, 0.0406, 0.0634, 0.0683, 0.0380]) \n",
      "Test Loss tensor([0.0851, 0.0217, 0.0405, 0.0646, 0.0663, 0.0323])\n",
      "\n",
      "\n",
      "************** Batch 28 in 0.6096277236938477 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0814, 0.0229, 0.0424, 0.0656, 0.0648, 0.0331]) \n",
      "Test Loss tensor([0.0868, 0.0210, 0.0418, 0.0644, 0.0662, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 32 in 0.5931625366210938 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0961, 0.0232, 0.0427, 0.0706, 0.0740, 0.0325]) \n",
      "Test Loss tensor([0.0844, 0.0223, 0.0388, 0.0636, 0.0675, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 36 in 0.6031265258789062 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0803, 0.0263, 0.0387, 0.0667, 0.0656, 0.0329]) \n",
      "Test Loss tensor([0.0868, 0.0223, 0.0390, 0.0644, 0.0674, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 40 in 0.7712347507476807 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0870, 0.0257, 0.0412, 0.0689, 0.0680, 0.0367]) \n",
      "Test Loss tensor([0.0868, 0.0211, 0.0383, 0.0636, 0.0693, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 44 in 0.7526955604553223 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0841, 0.0236, 0.0384, 0.0619, 0.0682, 0.0339]) \n",
      "Test Loss tensor([0.0871, 0.0214, 0.0391, 0.0655, 0.0669, 0.0329])\n",
      "\n",
      "\n",
      "************** Batch 48 in 0.7119734287261963 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0878, 0.0249, 0.0375, 0.0700, 0.0680, 0.0353]) \n",
      "Test Loss tensor([0.0864, 0.0222, 0.0396, 0.0643, 0.0677, 0.0327])\n",
      "\n",
      "\n",
      "************** Batch 52 in 0.6496155261993408 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0842, 0.0204, 0.0444, 0.0612, 0.0575, 0.0355]) \n",
      "Test Loss tensor([0.0851, 0.0224, 0.0415, 0.0672, 0.0678, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 56 in 0.6405210494995117 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0882, 0.0217, 0.0363, 0.0651, 0.0708, 0.0372]) \n",
      "Test Loss tensor([0.0858, 0.0217, 0.0399, 0.0655, 0.0661, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 60 in 0.6917300224304199 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0865, 0.0215, 0.0378, 0.0626, 0.0664, 0.0342]) \n",
      "Test Loss tensor([0.0892, 0.0236, 0.0388, 0.0659, 0.0684, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 64 in 0.7129051685333252 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0868, 0.0221, 0.0428, 0.0632, 0.0664, 0.0329]) \n",
      "Test Loss tensor([0.0860, 0.0232, 0.0397, 0.0642, 0.0664, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 68 in 0.6865825653076172 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0879, 0.0271, 0.0387, 0.0698, 0.0686, 0.0301]) \n",
      "Test Loss tensor([0.0854, 0.0226, 0.0387, 0.0633, 0.0669, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 72 in 0.6804358959197998 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0925, 0.0268, 0.0400, 0.0649, 0.0724, 0.0325]) \n",
      "Test Loss tensor([0.0825, 0.0214, 0.0396, 0.0652, 0.0663, 0.0325])\n",
      "\n",
      "\n",
      "************** Batch 76 in 0.6600277423858643 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0912, 0.0216, 0.0336, 0.0683, 0.0648, 0.0312]) \n",
      "Test Loss tensor([0.0871, 0.0231, 0.0398, 0.0659, 0.0670, 0.0339])\n",
      "\n",
      "\n",
      "************** Batch 80 in 0.6568825244903564 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0890, 0.0202, 0.0379, 0.0611, 0.0659, 0.0278]) \n",
      "Test Loss tensor([0.0844, 0.0222, 0.0392, 0.0635, 0.0672, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 84 in 0.6101758480072021 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0882, 0.0276, 0.0398, 0.0633, 0.0666, 0.0327]) \n",
      "Test Loss tensor([0.0857, 0.0239, 0.0401, 0.0649, 0.0671, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 88 in 0.6047954559326172 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0884, 0.0238, 0.0406, 0.0671, 0.0641, 0.0352]) \n",
      "Test Loss tensor([0.0851, 0.0212, 0.0412, 0.0626, 0.0668, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 92 in 0.6068956851959229 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0867, 0.0226, 0.0385, 0.0635, 0.0696, 0.0313]) \n",
      "Test Loss tensor([0.0850, 0.0218, 0.0395, 0.0657, 0.0663, 0.0322])\n",
      "\n",
      "\n",
      "************** Batch 96 in 0.6009311676025391 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0870, 0.0211, 0.0418, 0.0656, 0.0632, 0.0379]) \n",
      "Test Loss tensor([0.0841, 0.0215, 0.0390, 0.0642, 0.0670, 0.0327])\n",
      "\n",
      "\n",
      "************** Batch 100 in 0.5820677280426025 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0817, 0.0219, 0.0422, 0.0587, 0.0658, 0.0306]) \n",
      "Test Loss tensor([0.0852, 0.0205, 0.0413, 0.0637, 0.0677, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 104 in 0.6062986850738525 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0825, 0.0263, 0.0407, 0.0649, 0.0627, 0.0376]) \n",
      "Test Loss tensor([0.0843, 0.0213, 0.0401, 0.0637, 0.0646, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 108 in 0.6960132122039795 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0859, 0.0230, 0.0362, 0.0613, 0.0665, 0.0324]) \n",
      "Test Loss tensor([0.0825, 0.0214, 0.0417, 0.0652, 0.0671, 0.0336])\n",
      "\n",
      "\n",
      "************** Batch 112 in 0.6481673717498779 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0855, 0.0248, 0.0433, 0.0725, 0.0699, 0.0326]) \n",
      "Test Loss tensor([0.0846, 0.0216, 0.0388, 0.0656, 0.0670, 0.0323])\n",
      "\n",
      "\n",
      "************** Batch 116 in 0.6925981044769287 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0881, 0.0227, 0.0392, 0.0616, 0.0665, 0.0312]) \n",
      "Test Loss tensor([0.0860, 0.0218, 0.0400, 0.0645, 0.0663, 0.0325])\n",
      "\n",
      "\n",
      "************** Batch 120 in 0.6836574077606201 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0791, 0.0246, 0.0385, 0.0622, 0.0666, 0.0290]) \n",
      "Test Loss tensor([0.0841, 0.0220, 0.0390, 0.0623, 0.0660, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 124 in 0.657944917678833 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0904, 0.0223, 0.0398, 0.0642, 0.0662, 0.0283]) \n",
      "Test Loss tensor([0.0886, 0.0221, 0.0397, 0.0646, 0.0701, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 128 in 0.6127398014068604 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0866, 0.0180, 0.0402, 0.0624, 0.0666, 0.0367]) \n",
      "Test Loss tensor([0.0849, 0.0222, 0.0398, 0.0646, 0.0671, 0.0350])\n",
      "\n",
      "\n",
      "************** Batch 132 in 0.6094527244567871 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0920, 0.0212, 0.0384, 0.0646, 0.0695, 0.0344]) \n",
      "Test Loss tensor([0.0846, 0.0218, 0.0411, 0.0651, 0.0654, 0.0345])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 136 in 0.5864651203155518 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0854, 0.0229, 0.0367, 0.0594, 0.0626, 0.0338]) \n",
      "Test Loss tensor([0.0888, 0.0218, 0.0393, 0.0640, 0.0681, 0.0336])\n",
      "\n",
      "\n",
      "************** Batch 140 in 0.62119460105896 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0856, 0.0229, 0.0386, 0.0707, 0.0688, 0.0345]) \n",
      "Test Loss tensor([0.0857, 0.0225, 0.0399, 0.0652, 0.0668, 0.0328])\n",
      "\n",
      "\n",
      "************** Batch 144 in 0.6029033660888672 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0927, 0.0224, 0.0424, 0.0652, 0.0684, 0.0303]) \n",
      "Test Loss tensor([0.0890, 0.0224, 0.0402, 0.0667, 0.0739, 0.0369])\n",
      "\n",
      "\n",
      "************** Batch 148 in 0.5918421745300293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0874, 0.0238, 0.0346, 0.0684, 0.0760, 0.0316]) \n",
      "Test Loss tensor([0.0857, 0.0220, 0.0388, 0.0658, 0.0664, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 152 in 0.5910232067108154 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0862, 0.0238, 0.0400, 0.0665, 0.0597, 0.0331]) \n",
      "Test Loss tensor([0.0881, 0.0225, 0.0404, 0.0637, 0.0693, 0.0386])\n",
      "\n",
      "\n",
      "************** Batch 156 in 0.5786774158477783 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0195, 0.0394, 0.0640, 0.0670, 0.0415]) \n",
      "Test Loss tensor([0.0876, 0.0221, 0.0395, 0.0655, 0.0653, 0.0328])\n",
      "\n",
      "\n",
      "************** Batch 160 in 0.6231250762939453 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0874, 0.0258, 0.0434, 0.0621, 0.0659, 0.0355]) \n",
      "Test Loss tensor([0.0869, 0.0221, 0.0394, 0.0644, 0.0678, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 164 in 0.5885624885559082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0856, 0.0202, 0.0376, 0.0633, 0.0616, 0.0317]) \n",
      "Test Loss tensor([0.0870, 0.0212, 0.0396, 0.0652, 0.0681, 0.0339])\n",
      "\n",
      "\n",
      "************** Batch 168 in 0.5987222194671631 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0779, 0.0226, 0.0379, 0.0617, 0.0707, 0.0314]) \n",
      "Test Loss tensor([0.0878, 0.0233, 0.0398, 0.0642, 0.0661, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 172 in 0.6169941425323486 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0968, 0.0240, 0.0361, 0.0640, 0.0689, 0.0315]) \n",
      "Test Loss tensor([0.0886, 0.0226, 0.0409, 0.0656, 0.0685, 0.0381])\n",
      "\n",
      "\n",
      "************** Batch 176 in 0.5806379318237305 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0858, 0.0232, 0.0445, 0.0705, 0.0656, 0.0387]) \n",
      "Test Loss tensor([0.0834, 0.0223, 0.0392, 0.0638, 0.0659, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 180 in 0.6088137626647949 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0821, 0.0213, 0.0403, 0.0690, 0.0647, 0.0331]) \n",
      "Test Loss tensor([0.0860, 0.0218, 0.0396, 0.0649, 0.0679, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 184 in 0.583181619644165 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0863, 0.0215, 0.0386, 0.0702, 0.0688, 0.0341]) \n",
      "Test Loss tensor([0.0849, 0.0215, 0.0378, 0.0636, 0.0677, 0.0332])\n",
      "\n",
      "\n",
      "************** Batch 188 in 0.6002950668334961 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0929, 0.0218, 0.0369, 0.0701, 0.0679, 0.0382]) \n",
      "Test Loss tensor([0.0869, 0.0215, 0.0391, 0.0656, 0.0667, 0.0328])\n",
      "\n",
      "\n",
      "************** Batch 192 in 0.5993392467498779 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0909, 0.0216, 0.0363, 0.0652, 0.0667, 0.0382]) \n",
      "Test Loss tensor([0.0879, 0.0227, 0.0411, 0.0643, 0.0680, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 196 in 0.5921781063079834 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0827, 0.0209, 0.0365, 0.0640, 0.0640, 0.0376]) \n",
      "Test Loss tensor([0.0868, 0.0224, 0.0392, 0.0649, 0.0654, 0.0328])\n",
      "\n",
      "\n",
      "************** Batch 200 in 0.6285970211029053 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0818, 0.0212, 0.0384, 0.0620, 0.0673, 0.0331]) \n",
      "Test Loss tensor([0.0885, 0.0211, 0.0389, 0.0629, 0.0697, 0.0352])\n",
      "\n",
      "\n",
      "************** Batch 204 in 0.5867934226989746 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0785, 0.0187, 0.0379, 0.0594, 0.0652, 0.0331]) \n",
      "Test Loss tensor([0.0875, 0.0236, 0.0385, 0.0669, 0.0665, 0.0321])\n",
      "\n",
      "\n",
      "************** Batch 208 in 0.596543550491333 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0216, 0.0374, 0.0704, 0.0621, 0.0346]) \n",
      "Test Loss tensor([0.0878, 0.0222, 0.0394, 0.0644, 0.0665, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 212 in 0.5945186614990234 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0269, 0.0427, 0.0620, 0.0673, 0.0343]) \n",
      "Test Loss tensor([0.0863, 0.0220, 0.0383, 0.0655, 0.0658, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 216 in 0.5947556495666504 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0790, 0.0209, 0.0377, 0.0556, 0.0660, 0.0301]) \n",
      "Test Loss tensor([0.0883, 0.0223, 0.0392, 0.0641, 0.0675, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 220 in 0.6155178546905518 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0809, 0.0204, 0.0448, 0.0646, 0.0605, 0.0307]) \n",
      "Test Loss tensor([0.0837, 0.0208, 0.0382, 0.0624, 0.0658, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 224 in 0.5948913097381592 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0829, 0.0193, 0.0382, 0.0649, 0.0673, 0.0363]) \n",
      "Test Loss tensor([0.0854, 0.0226, 0.0393, 0.0639, 0.0668, 0.0356])\n",
      "\n",
      "\n",
      "************** Batch 228 in 0.6090354919433594 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0809, 0.0207, 0.0383, 0.0608, 0.0660, 0.0372]) \n",
      "Test Loss tensor([0.0856, 0.0235, 0.0383, 0.0628, 0.0663, 0.0326])\n",
      "\n",
      "\n",
      "************** Batch 232 in 0.6029837131500244 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0904, 0.0226, 0.0401, 0.0675, 0.0665, 0.0347]) \n",
      "Test Loss tensor([0.0904, 0.0220, 0.0393, 0.0665, 0.0675, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 236 in 0.5968632698059082 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0928, 0.0266, 0.0362, 0.0604, 0.0648, 0.0336]) \n",
      "Test Loss tensor([0.0868, 0.0239, 0.0401, 0.0638, 0.0655, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 240 in 0.6248555183410645 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0843, 0.0225, 0.0423, 0.0734, 0.0679, 0.0332]) \n",
      "Test Loss tensor([0.0885, 0.0224, 0.0398, 0.0653, 0.0677, 0.0327])\n",
      "\n",
      "\n",
      "************** Batch 244 in 0.593207836151123 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0829, 0.0231, 0.0399, 0.0666, 0.0755, 0.0335]) \n",
      "Test Loss tensor([0.0841, 0.0231, 0.0388, 0.0639, 0.0643, 0.0348])\n",
      "\n",
      "\n",
      "************** Batch 248 in 0.6015996932983398 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0851, 0.0244, 0.0365, 0.0596, 0.0669, 0.0323]) \n",
      "Test Loss tensor([0.0867, 0.0222, 0.0395, 0.0648, 0.0666, 0.0361])\n",
      "\n",
      "\n",
      "************** Batch 252 in 0.6213870048522949 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0856, 0.0249, 0.0370, 0.0680, 0.0658, 0.0438]) \n",
      "Test Loss tensor([0.0905, 0.0234, 0.0404, 0.0631, 0.0686, 0.0332])\n",
      "\n",
      "\n",
      "************** Batch 256 in 0.5939364433288574 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0911, 0.0204, 0.0387, 0.0621, 0.0684, 0.0324]) \n",
      "Test Loss tensor([0.0867, 0.0229, 0.0384, 0.0628, 0.0670, 0.0332])\n",
      "\n",
      "\n",
      "************** Batch 260 in 0.6128485202789307 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0853, 0.0228, 0.0421, 0.0619, 0.0639, 0.0327]) \n",
      "Test Loss tensor([0.0951, 0.0205, 0.0407, 0.0648, 0.0720, 0.0381])\n",
      "\n",
      "\n",
      "************** Batch 264 in 0.6014008522033691 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0909, 0.0211, 0.0387, 0.0615, 0.0716, 0.0414]) \n",
      "Test Loss tensor([0.0869, 0.0238, 0.0390, 0.0650, 0.0668, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 268 in 0.6025042533874512 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0839, 0.0235, 0.0399, 0.0612, 0.0683, 0.0315]) \n",
      "Test Loss tensor([0.0870, 0.0218, 0.0389, 0.0642, 0.0694, 0.0399])\n",
      "\n",
      "\n",
      "************** Batch 272 in 0.6077113151550293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0900, 0.0209, 0.0395, 0.0604, 0.0720, 0.0411]) \n",
      "Test Loss tensor([0.0862, 0.0218, 0.0391, 0.0640, 0.0663, 0.0332])\n",
      "\n",
      "\n",
      "************** Batch 276 in 0.5849685668945312 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.1017, 0.0190, 0.0395, 0.0749, 0.0654, 0.0358]) \n",
      "Test Loss tensor([0.0854, 0.0217, 0.0383, 0.0634, 0.0657, 0.0332])\n",
      "\n",
      "\n",
      "************** Batch 280 in 0.6253824234008789 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0880, 0.0227, 0.0390, 0.0666, 0.0730, 0.0305]) \n",
      "Test Loss tensor([0.0890, 0.0238, 0.0387, 0.0628, 0.0669, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 284 in 0.5874838829040527 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0217, 0.0387, 0.0633, 0.0678, 0.0344]) \n",
      "Test Loss tensor([0.0839, 0.0224, 0.0397, 0.0649, 0.0639, 0.0340])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 288 in 0.6004149913787842 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0840, 0.0190, 0.0362, 0.0669, 0.0676, 0.0319]) \n",
      "Test Loss tensor([0.0840, 0.0233, 0.0392, 0.0617, 0.0657, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 292 in 0.6061084270477295 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0869, 0.0226, 0.0381, 0.0602, 0.0606, 0.0362]) \n",
      "Test Loss tensor([0.0872, 0.0229, 0.0406, 0.0638, 0.0646, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 296 in 0.586146354675293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0814, 0.0202, 0.0377, 0.0710, 0.0649, 0.0316]) \n",
      "Test Loss tensor([0.0876, 0.0235, 0.0397, 0.0632, 0.0650, 0.0326])\n",
      "\n",
      "\n",
      "************** Batch 300 in 0.6011943817138672 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0852, 0.0235, 0.0379, 0.0669, 0.0662, 0.0347]) \n",
      "Test Loss tensor([0.0875, 0.0221, 0.0387, 0.0638, 0.0661, 0.0332])\n",
      "\n",
      "\n",
      "************** Batch 304 in 0.5909368991851807 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0934, 0.0251, 0.0405, 0.0662, 0.0715, 0.0309]) \n",
      "Test Loss tensor([0.0849, 0.0215, 0.0387, 0.0639, 0.0655, 0.0332])\n",
      "\n",
      "\n",
      "************** Batch 308 in 0.6818177700042725 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0871, 0.0255, 0.0431, 0.0652, 0.0671, 0.0326]) \n",
      "Test Loss tensor([0.0851, 0.0218, 0.0387, 0.0641, 0.0656, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 312 in 0.6191117763519287 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0827, 0.0231, 0.0372, 0.0600, 0.0674, 0.0357]) \n",
      "Test Loss tensor([0.0837, 0.0228, 0.0402, 0.0646, 0.0659, 0.0329])\n",
      "\n",
      "\n",
      "************** Batch 316 in 0.5926671028137207 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0854, 0.0220, 0.0380, 0.0643, 0.0688, 0.0353]) \n",
      "Test Loss tensor([0.0852, 0.0208, 0.0394, 0.0640, 0.0671, 0.0341])\n",
      "\n",
      "\n",
      "************** Batch 320 in 0.5984325408935547 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0868, 0.0265, 0.0342, 0.0581, 0.0654, 0.0336]) \n",
      "Test Loss tensor([0.0855, 0.0223, 0.0395, 0.0654, 0.0646, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 324 in 0.6069645881652832 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0879, 0.0245, 0.0400, 0.0707, 0.0664, 0.0306]) \n",
      "Test Loss tensor([0.0858, 0.0226, 0.0388, 0.0657, 0.0650, 0.0326])\n",
      "\n",
      "\n",
      "************** Batch 328 in 0.5901927947998047 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0848, 0.0234, 0.0353, 0.0612, 0.0657, 0.0293]) \n",
      "Test Loss tensor([0.0873, 0.0227, 0.0405, 0.0670, 0.0678, 0.0341])\n",
      "\n",
      "\n",
      "************** Batch 332 in 0.5999369621276855 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0830, 0.0240, 0.0389, 0.0611, 0.0698, 0.0354]) \n",
      "Test Loss tensor([0.0837, 0.0216, 0.0381, 0.0656, 0.0671, 0.0325])\n",
      "\n",
      "\n",
      "************** Batch 336 in 0.6277616024017334 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0861, 0.0227, 0.0380, 0.0636, 0.0679, 0.0316]) \n",
      "Test Loss tensor([0.0884, 0.0225, 0.0373, 0.0658, 0.0667, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 340 in 0.6150784492492676 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0826, 0.0252, 0.0413, 0.0658, 0.0628, 0.0364]) \n",
      "Test Loss tensor([0.0863, 0.0222, 0.0390, 0.0650, 0.0664, 0.0326])\n",
      "\n",
      "\n",
      "************** Batch 344 in 0.6023988723754883 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0901, 0.0221, 0.0379, 0.0648, 0.0684, 0.0304]) \n",
      "Test Loss tensor([0.0849, 0.0233, 0.0387, 0.0646, 0.0658, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 348 in 0.590172529220581 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0802, 0.0229, 0.0395, 0.0625, 0.0675, 0.0330]) \n",
      "Test Loss tensor([0.0848, 0.0234, 0.0393, 0.0644, 0.0683, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 352 in 0.5983567237854004 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0901, 0.0266, 0.0364, 0.0710, 0.0701, 0.0314]) \n",
      "Test Loss tensor([0.0845, 0.0217, 0.0396, 0.0639, 0.0661, 0.0325])\n",
      "\n",
      "\n",
      "************** Batch 356 in 0.584658145904541 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0917, 0.0261, 0.0392, 0.0579, 0.0663, 0.0331]) \n",
      "Test Loss tensor([0.0852, 0.0215, 0.0387, 0.0631, 0.0660, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 360 in 0.6051995754241943 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0884, 0.0217, 0.0365, 0.0663, 0.0661, 0.0349]) \n",
      "Test Loss tensor([0.0836, 0.0228, 0.0394, 0.0630, 0.0654, 0.0322])\n",
      "\n",
      "\n",
      "************** Batch 364 in 0.6078410148620605 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0948, 0.0217, 0.0390, 0.0721, 0.0675, 0.0332]) \n",
      "Test Loss tensor([0.0851, 0.0230, 0.0392, 0.0639, 0.0655, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 368 in 0.5980277061462402 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0801, 0.0222, 0.0427, 0.0726, 0.0666, 0.0349]) \n",
      "Test Loss tensor([0.0839, 0.0217, 0.0392, 0.0626, 0.0650, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 372 in 0.6403310298919678 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0846, 0.0225, 0.0396, 0.0648, 0.0609, 0.0316]) \n",
      "Test Loss tensor([0.0847, 0.0230, 0.0384, 0.0631, 0.0676, 0.0348])\n",
      "\n",
      "\n",
      "************** Batch 376 in 0.6294455528259277 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0836, 0.0192, 0.0376, 0.0624, 0.0619, 0.0377]) \n",
      "Test Loss tensor([0.0844, 0.0236, 0.0381, 0.0623, 0.0663, 0.0327])\n",
      "\n",
      "\n",
      "************** Batch 380 in 0.613994836807251 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0888, 0.0236, 0.0373, 0.0585, 0.0640, 0.0336]) \n",
      "Test Loss tensor([0.0852, 0.0222, 0.0394, 0.0651, 0.0651, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 384 in 0.6100025177001953 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0766, 0.0254, 0.0438, 0.0600, 0.0633, 0.0341]) \n",
      "Test Loss tensor([0.0869, 0.0221, 0.0391, 0.0633, 0.0659, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 388 in 0.613025426864624 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0841, 0.0245, 0.0406, 0.0652, 0.0639, 0.0317]) \n",
      "Test Loss tensor([0.0841, 0.0227, 0.0395, 0.0628, 0.0658, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 392 in 0.617945671081543 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0955, 0.0227, 0.0368, 0.0673, 0.0676, 0.0314]) \n",
      "Test Loss tensor([0.0861, 0.0229, 0.0391, 0.0637, 0.0674, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 396 in 0.617304801940918 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0780, 0.0173, 0.0373, 0.0582, 0.0632, 0.0351]) \n",
      "Test Loss tensor([0.0869, 0.0211, 0.0380, 0.0637, 0.0655, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 400 in 0.5917873382568359 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0825, 0.0233, 0.0383, 0.0673, 0.0675, 0.0290]) \n",
      "Test Loss tensor([0.0873, 0.0240, 0.0378, 0.0624, 0.0650, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 404 in 0.6348106861114502 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0823, 0.0262, 0.0377, 0.0665, 0.0638, 0.0327]) \n",
      "Test Loss tensor([0.0856, 0.0236, 0.0383, 0.0642, 0.0669, 0.0321])\n",
      "\n",
      "\n",
      "************** Batch 408 in 0.5994930267333984 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0888, 0.0277, 0.0360, 0.0649, 0.0611, 0.0354]) \n",
      "Test Loss tensor([0.0840, 0.0232, 0.0377, 0.0639, 0.0656, 0.0325])\n",
      "\n",
      "\n",
      "************** Batch 412 in 0.603262186050415 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0857, 0.0187, 0.0370, 0.0629, 0.0648, 0.0317]) \n",
      "Test Loss tensor([0.0840, 0.0220, 0.0387, 0.0640, 0.0651, 0.0323])\n",
      "\n",
      "\n",
      "************** Batch 416 in 0.6142313480377197 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0781, 0.0212, 0.0368, 0.0607, 0.0660, 0.0295]) \n",
      "Test Loss tensor([0.0849, 0.0231, 0.0376, 0.0649, 0.0670, 0.0322])\n",
      "\n",
      "\n",
      "************** Batch 420 in 0.5854825973510742 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0890, 0.0220, 0.0421, 0.0581, 0.0659, 0.0375]) \n",
      "Test Loss tensor([0.0833, 0.0208, 0.0386, 0.0630, 0.0653, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 424 in 0.6146578788757324 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0843, 0.0215, 0.0353, 0.0666, 0.0661, 0.0310]) \n",
      "Test Loss tensor([0.0861, 0.0219, 0.0380, 0.0650, 0.0652, 0.0322])\n",
      "\n",
      "\n",
      "************** Batch 428 in 0.5883080959320068 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0909, 0.0283, 0.0380, 0.0668, 0.0697, 0.0290]) \n",
      "Test Loss tensor([0.0836, 0.0236, 0.0380, 0.0625, 0.0651, 0.0323])\n",
      "\n",
      "\n",
      "************** Batch 432 in 0.6290907859802246 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0901, 0.0223, 0.0389, 0.0689, 0.0688, 0.0291]) \n",
      "Test Loss tensor([0.0828, 0.0211, 0.0382, 0.0640, 0.0652, 0.0323])\n",
      "\n",
      "\n",
      "************** Batch 436 in 0.6157829761505127 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0237, 0.0396, 0.0665, 0.0679, 0.0340]) \n",
      "Test Loss tensor([0.0853, 0.0224, 0.0377, 0.0640, 0.0659, 0.0337])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 440 in 0.6024527549743652 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0905, 0.0211, 0.0385, 0.0652, 0.0642, 0.0354]) \n",
      "Test Loss tensor([0.0883, 0.0226, 0.0371, 0.0635, 0.0687, 0.0329])\n",
      "\n",
      "\n",
      "************** Batch 444 in 0.5982396602630615 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0869, 0.0291, 0.0385, 0.0646, 0.0630, 0.0367]) \n",
      "Test Loss tensor([0.0866, 0.0222, 0.0374, 0.0634, 0.0651, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 448 in 0.5905976295471191 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0839, 0.0250, 0.0354, 0.0653, 0.0657, 0.0334]) \n",
      "Test Loss tensor([0.0854, 0.0220, 0.0401, 0.0620, 0.0641, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 452 in 0.604987621307373 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0822, 0.0246, 0.0379, 0.0579, 0.0633, 0.0297]) \n",
      "Test Loss tensor([0.0847, 0.0227, 0.0370, 0.0634, 0.0658, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 456 in 0.6045718193054199 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0887, 0.0233, 0.0344, 0.0699, 0.0656, 0.0293]) \n",
      "Test Loss tensor([0.0844, 0.0217, 0.0375, 0.0623, 0.0666, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 460 in 0.599780797958374 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0196, 0.0368, 0.0628, 0.0625, 0.0371]) \n",
      "Test Loss tensor([0.0822, 0.0216, 0.0380, 0.0638, 0.0645, 0.0319])\n",
      "\n",
      "\n",
      "************** Batch 464 in 0.6042685508728027 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0758, 0.0204, 0.0403, 0.0691, 0.0641, 0.0345]) \n",
      "Test Loss tensor([0.0839, 0.0208, 0.0391, 0.0636, 0.0649, 0.0341])\n",
      "\n",
      "\n",
      "************** Batch 468 in 0.5938265323638916 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0819, 0.0230, 0.0401, 0.0643, 0.0630, 0.0323]) \n",
      "Test Loss tensor([0.0825, 0.0218, 0.0387, 0.0640, 0.0636, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 472 in 0.6027932167053223 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0869, 0.0219, 0.0395, 0.0569, 0.0675, 0.0339]) \n",
      "Test Loss tensor([0.0852, 0.0220, 0.0376, 0.0626, 0.0644, 0.0332])\n",
      "\n",
      "\n",
      "************** Batch 476 in 0.6104419231414795 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0877, 0.0220, 0.0383, 0.0627, 0.0653, 0.0345]) \n",
      "Test Loss tensor([0.0858, 0.0224, 0.0378, 0.0635, 0.0675, 0.0326])\n",
      "\n",
      "\n",
      "************** Batch 480 in 0.5951204299926758 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0932, 0.0259, 0.0425, 0.0695, 0.0681, 0.0321]) \n",
      "Test Loss tensor([0.0864, 0.0224, 0.0384, 0.0651, 0.0643, 0.0346])\n",
      "\n",
      "\n",
      "************** Batch 484 in 0.6056385040283203 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0811, 0.0216, 0.0381, 0.0644, 0.0645, 0.0313]) \n",
      "Test Loss tensor([0.0856, 0.0222, 0.0385, 0.0624, 0.0664, 0.0370])\n",
      "\n",
      "\n",
      "************** Batch 488 in 0.6089866161346436 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0776, 0.0206, 0.0384, 0.0644, 0.0649, 0.0348]) \n",
      "Test Loss tensor([0.0864, 0.0228, 0.0385, 0.0664, 0.0657, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 492 in 0.5936987400054932 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0840, 0.0214, 0.0380, 0.0702, 0.0641, 0.0379]) \n",
      "Test Loss tensor([0.0855, 0.0225, 0.0385, 0.0636, 0.0648, 0.0320])\n",
      "\n",
      "\n",
      "************** Batch 496 in 0.6166555881500244 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0895, 0.0221, 0.0393, 0.0600, 0.0609, 0.0358]) \n",
      "Test Loss tensor([0.0872, 0.0216, 0.0391, 0.0632, 0.0658, 0.0349])\n",
      "\n",
      "\n",
      "************** Batch 500 in 0.5892531871795654 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0858, 0.0214, 0.0377, 0.0630, 0.0692, 0.0336]) \n",
      "Test Loss tensor([0.0855, 0.0224, 0.0383, 0.0622, 0.0647, 0.0339])\n",
      "\n",
      "\n",
      "************** Batch 504 in 0.6217622756958008 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0859, 0.0190, 0.0364, 0.0592, 0.0627, 0.0337]) \n",
      "Test Loss tensor([0.0848, 0.0219, 0.0398, 0.0626, 0.0651, 0.0369])\n",
      "\n",
      "\n",
      "************** Batch 508 in 0.6204097270965576 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0832, 0.0204, 0.0383, 0.0643, 0.0662, 0.0360]) \n",
      "Test Loss tensor([0.0876, 0.0222, 0.0381, 0.0613, 0.0655, 0.0319])\n",
      "\n",
      "\n",
      "************** Batch 512 in 0.6026008129119873 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0944, 0.0225, 0.0413, 0.0644, 0.0670, 0.0329]) \n",
      "Test Loss tensor([0.0874, 0.0221, 0.0385, 0.0628, 0.0677, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 516 in 0.6193282604217529 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0831, 0.0220, 0.0337, 0.0658, 0.0678, 0.0328]) \n",
      "Test Loss tensor([0.0881, 0.0223, 0.0378, 0.0630, 0.0652, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 520 in 0.5994677543640137 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0838, 0.0228, 0.0406, 0.0676, 0.0650, 0.0316]) \n",
      "Test Loss tensor([0.0858, 0.0228, 0.0374, 0.0617, 0.0654, 0.0319])\n",
      "\n",
      "\n",
      "************** Batch 524 in 0.6100568771362305 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0933, 0.0231, 0.0365, 0.0647, 0.0691, 0.0362]) \n",
      "Test Loss tensor([0.0860, 0.0225, 0.0384, 0.0641, 0.0655, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 528 in 0.6075718402862549 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0828, 0.0214, 0.0392, 0.0604, 0.0624, 0.0301]) \n",
      "Test Loss tensor([0.0853, 0.0220, 0.0366, 0.0609, 0.0649, 0.0328])\n",
      "\n",
      "\n",
      "************** Batch 532 in 0.5898783206939697 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0808, 0.0254, 0.0374, 0.0590, 0.0657, 0.0375]) \n",
      "Test Loss tensor([0.0805, 0.0212, 0.0400, 0.0647, 0.0644, 0.0336])\n",
      "\n",
      "\n",
      "************** Batch 536 in 0.613377571105957 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0932, 0.0198, 0.0383, 0.0639, 0.0607, 0.0333]) \n",
      "Test Loss tensor([0.0846, 0.0213, 0.0379, 0.0609, 0.0644, 0.0320])\n",
      "\n",
      "\n",
      "************** Batch 540 in 0.5942292213439941 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0889, 0.0183, 0.0339, 0.0601, 0.0661, 0.0317]) \n",
      "Test Loss tensor([0.0858, 0.0233, 0.0377, 0.0645, 0.0667, 0.0344])\n",
      "\n",
      "\n",
      "************** Batch 544 in 0.6066079139709473 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0836, 0.0207, 0.0412, 0.0643, 0.0624, 0.0368]) \n",
      "Test Loss tensor([0.0813, 0.0210, 0.0390, 0.0638, 0.0649, 0.0322])\n",
      "\n",
      "\n",
      "************** Batch 548 in 0.5995171070098877 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0849, 0.0234, 0.0390, 0.0631, 0.0618, 0.0312]) \n",
      "Test Loss tensor([0.0868, 0.0222, 0.0381, 0.0626, 0.0659, 0.0320])\n",
      "\n",
      "\n",
      "************** Batch 552 in 0.6063423156738281 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0832, 0.0199, 0.0361, 0.0682, 0.0666, 0.0314]) \n",
      "Test Loss tensor([0.0839, 0.0218, 0.0366, 0.0645, 0.0636, 0.0329])\n",
      "\n",
      "\n",
      "************** Batch 556 in 0.6134006977081299 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0851, 0.0229, 0.0378, 0.0627, 0.0685, 0.0348]) \n",
      "Test Loss tensor([0.0850, 0.0217, 0.0393, 0.0627, 0.0647, 0.0332])\n",
      "\n",
      "\n",
      "************** Batch 560 in 0.5802874565124512 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0807, 0.0236, 0.0347, 0.0617, 0.0630, 0.0342]) \n",
      "Test Loss tensor([0.0860, 0.0215, 0.0399, 0.0633, 0.0662, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 564 in 0.6297619342803955 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0911, 0.0244, 0.0391, 0.0612, 0.0733, 0.0340]) \n",
      "Test Loss tensor([0.0843, 0.0213, 0.0399, 0.0618, 0.0646, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 568 in 0.6493663787841797 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0816, 0.0201, 0.0411, 0.0665, 0.0610, 0.0342]) \n",
      "Test Loss tensor([0.0853, 0.0221, 0.0401, 0.0655, 0.0659, 0.0339])\n",
      "\n",
      "\n",
      "************** Batch 572 in 0.5907576084136963 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0887, 0.0191, 0.0389, 0.0621, 0.0673, 0.0352]) \n",
      "Test Loss tensor([0.0849, 0.0222, 0.0387, 0.0626, 0.0650, 0.0323])\n",
      "\n",
      "\n",
      "************** Batch 576 in 0.6033580303192139 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0987, 0.0238, 0.0384, 0.0721, 0.0652, 0.0322]) \n",
      "Test Loss tensor([0.0888, 0.0217, 0.0378, 0.0626, 0.0673, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 580 in 0.5903043746948242 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0839, 0.0217, 0.0352, 0.0628, 0.0674, 0.0338]) \n",
      "Test Loss tensor([0.0843, 0.0221, 0.0394, 0.0634, 0.0671, 0.0345])\n",
      "\n",
      "\n",
      "************** Batch 584 in 0.592156171798706 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0885, 0.0229, 0.0430, 0.0601, 0.0675, 0.0337]) \n",
      "Test Loss tensor([0.0828, 0.0201, 0.0396, 0.0643, 0.0648, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 588 in 0.6019761562347412 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0845, 0.0244, 0.0456, 0.0628, 0.0673, 0.0325]) \n",
      "Test Loss tensor([0.0852, 0.0224, 0.0403, 0.0630, 0.0639, 0.0349])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 592 in 0.5859720706939697 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0813, 0.0262, 0.0383, 0.0621, 0.0684, 0.0343]) \n",
      "Test Loss tensor([0.0863, 0.0221, 0.0370, 0.0649, 0.0639, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 596 in 0.6256988048553467 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0853, 0.0215, 0.0433, 0.0558, 0.0643, 0.0307]) \n",
      "Test Loss tensor([0.0845, 0.0229, 0.0395, 0.0642, 0.0647, 0.0339])\n",
      "\n",
      "\n",
      "************** Batch 600 in 0.5903255939483643 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0816, 0.0210, 0.0425, 0.0652, 0.0647, 0.0401]) \n",
      "Test Loss tensor([0.0830, 0.0215, 0.0378, 0.0632, 0.0652, 0.0319])\n",
      "\n",
      "\n",
      "************** Batch 604 in 0.5976760387420654 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0817, 0.0214, 0.0362, 0.0691, 0.0663, 0.0300]) \n",
      "Test Loss tensor([0.0853, 0.0237, 0.0379, 0.0647, 0.0683, 0.0325])\n",
      "\n",
      "\n",
      "************** Batch 608 in 0.6022346019744873 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0824, 0.0239, 0.0348, 0.0600, 0.0667, 0.0343]) \n",
      "Test Loss tensor([0.0850, 0.0217, 0.0399, 0.0626, 0.0626, 0.0368])\n",
      "\n",
      "\n",
      "************** Batch 612 in 0.5955300331115723 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0846, 0.0238, 0.0352, 0.0604, 0.0611, 0.0390]) \n",
      "Test Loss tensor([0.0860, 0.0231, 0.0382, 0.0633, 0.0659, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 616 in 0.603830099105835 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0811, 0.0185, 0.0387, 0.0652, 0.0676, 0.0336]) \n",
      "Test Loss tensor([0.0842, 0.0212, 0.0365, 0.0625, 0.0678, 0.0315])\n",
      "\n",
      "\n",
      "************** Batch 620 in 0.595144510269165 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0863, 0.0242, 0.0376, 0.0564, 0.0694, 0.0317]) \n",
      "Test Loss tensor([0.0839, 0.0213, 0.0379, 0.0628, 0.0629, 0.0313])\n",
      "\n",
      "\n",
      "************** Batch 624 in 0.6740384101867676 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0846, 0.0256, 0.0338, 0.0675, 0.0643, 0.0301]) \n",
      "Test Loss tensor([0.0866, 0.0231, 0.0373, 0.0635, 0.0686, 0.0347])\n",
      "\n",
      "\n",
      "************** Batch 628 in 0.6055529117584229 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0839, 0.0272, 0.0420, 0.0670, 0.0691, 0.0360]) \n",
      "Test Loss tensor([0.0886, 0.0236, 0.0367, 0.0636, 0.0660, 0.0325])\n",
      "\n",
      "\n",
      "************** Batch 632 in 0.5913112163543701 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0934, 0.0255, 0.0387, 0.0608, 0.0644, 0.0339]) \n",
      "Test Loss tensor([0.0846, 0.0228, 0.0372, 0.0621, 0.0653, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 636 in 0.6168844699859619 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0747, 0.0232, 0.0391, 0.0653, 0.0646, 0.0331]) \n",
      "Test Loss tensor([0.0864, 0.0232, 0.0388, 0.0654, 0.0662, 0.0321])\n",
      "\n",
      "\n",
      "************** Batch 640 in 0.6357629299163818 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0822, 0.0228, 0.0399, 0.0667, 0.0676, 0.0353]) \n",
      "Test Loss tensor([0.0843, 0.0226, 0.0391, 0.0641, 0.0657, 0.0323])\n",
      "\n",
      "\n",
      "************** Batch 644 in 0.5982835292816162 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0882, 0.0275, 0.0392, 0.0637, 0.0670, 0.0294]) \n",
      "Test Loss tensor([0.0833, 0.0217, 0.0383, 0.0625, 0.0676, 0.0342])\n",
      "\n",
      "\n",
      "************** Batch 648 in 0.6056201457977295 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0874, 0.0223, 0.0405, 0.0605, 0.0694, 0.0295]) \n",
      "Test Loss tensor([0.0823, 0.0221, 0.0400, 0.0623, 0.0635, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 652 in 0.6405785083770752 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0875, 0.0228, 0.0335, 0.0610, 0.0599, 0.0330]) \n",
      "Test Loss tensor([0.0842, 0.0224, 0.0372, 0.0644, 0.0643, 0.0320])\n",
      "\n",
      "\n",
      "************** Batch 656 in 0.6032030582427979 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0851, 0.0241, 0.0343, 0.0634, 0.0682, 0.0302]) \n",
      "Test Loss tensor([0.0838, 0.0225, 0.0369, 0.0633, 0.0649, 0.0325])\n",
      "\n",
      "\n",
      "************** Batch 660 in 0.6010448932647705 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0883, 0.0201, 0.0354, 0.0657, 0.0598, 0.0315]) \n",
      "Test Loss tensor([0.0850, 0.0218, 0.0395, 0.0645, 0.0641, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 664 in 0.5871214866638184 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0845, 0.0247, 0.0392, 0.0664, 0.0630, 0.0332]) \n",
      "Test Loss tensor([0.0841, 0.0230, 0.0384, 0.0636, 0.0655, 0.0327])\n",
      "\n",
      "\n",
      "************** Batch 668 in 0.6044528484344482 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0886, 0.0211, 0.0426, 0.0674, 0.0630, 0.0318]) \n",
      "Test Loss tensor([0.0840, 0.0217, 0.0374, 0.0640, 0.0662, 0.0337])\n",
      "\n",
      "\n",
      "************** Batch 672 in 0.5899271965026855 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0845, 0.0198, 0.0357, 0.0616, 0.0593, 0.0405]) \n",
      "Test Loss tensor([0.0840, 0.0218, 0.0379, 0.0623, 0.0641, 0.0328])\n",
      "\n",
      "\n",
      "************** Batch 676 in 0.6221563816070557 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0885, 0.0215, 0.0411, 0.0708, 0.0636, 0.0364]) \n",
      "Test Loss tensor([0.0855, 0.0223, 0.0364, 0.0638, 0.0661, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 680 in 0.6112446784973145 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0886, 0.0208, 0.0412, 0.0602, 0.0665, 0.0326]) \n",
      "Test Loss tensor([0.0841, 0.0232, 0.0390, 0.0640, 0.0639, 0.0333])\n",
      "\n",
      "\n",
      "************** Batch 684 in 0.594022274017334 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0896, 0.0259, 0.0348, 0.0572, 0.0642, 0.0302]) \n",
      "Test Loss tensor([0.0852, 0.0230, 0.0381, 0.0627, 0.0651, 0.0318])\n",
      "\n",
      "\n",
      "************** Batch 688 in 0.6067502498626709 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0866, 0.0230, 0.0321, 0.0714, 0.0638, 0.0310]) \n",
      "Test Loss tensor([0.0844, 0.0219, 0.0378, 0.0634, 0.0652, 0.0323])\n",
      "\n",
      "\n",
      "************** Batch 692 in 0.592210054397583 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0873, 0.0219, 0.0364, 0.0609, 0.0653, 0.0333]) \n",
      "Test Loss tensor([0.0862, 0.0213, 0.0383, 0.0621, 0.0649, 0.0327])\n",
      "\n",
      "\n",
      "************** Batch 696 in 0.6273646354675293 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0791, 0.0248, 0.0400, 0.0625, 0.0674, 0.0362]) \n",
      "Test Loss tensor([0.0848, 0.0211, 0.0388, 0.0644, 0.0635, 0.0327])\n",
      "\n",
      "\n",
      "************** Batch 700 in 0.606595516204834 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0959, 0.0241, 0.0365, 0.0702, 0.0662, 0.0340]) \n",
      "Test Loss tensor([0.0850, 0.0228, 0.0382, 0.0637, 0.0652, 0.0316])\n",
      "\n",
      "\n",
      "************** Batch 704 in 0.6307284832000732 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0882, 0.0220, 0.0423, 0.0640, 0.0622, 0.0327]) \n",
      "Test Loss tensor([0.0831, 0.0208, 0.0393, 0.0620, 0.0644, 0.0343])\n",
      "\n",
      "\n",
      "************** Batch 708 in 0.6247425079345703 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0832, 0.0208, 0.0351, 0.0540, 0.0642, 0.0287]) \n",
      "Test Loss tensor([0.0841, 0.0224, 0.0384, 0.0633, 0.0636, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 712 in 0.6002922058105469 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0856, 0.0225, 0.0401, 0.0626, 0.0691, 0.0335]) \n",
      "Test Loss tensor([0.0845, 0.0219, 0.0377, 0.0649, 0.0639, 0.0325])\n",
      "\n",
      "\n",
      "************** Batch 716 in 0.5938677787780762 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0888, 0.0215, 0.0346, 0.0610, 0.0631, 0.0358]) \n",
      "Test Loss tensor([0.0859, 0.0215, 0.0378, 0.0653, 0.0650, 0.0327])\n",
      "\n",
      "\n",
      "************** Batch 720 in 0.6072959899902344 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0843, 0.0301, 0.0387, 0.0634, 0.0667, 0.0324]) \n",
      "Test Loss tensor([0.0821, 0.0224, 0.0384, 0.0629, 0.0649, 0.0306])\n",
      "\n",
      "\n",
      "************** Batch 724 in 0.599647045135498 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0915, 0.0197, 0.0376, 0.0591, 0.0675, 0.0343]) \n",
      "Test Loss tensor([0.0837, 0.0226, 0.0389, 0.0625, 0.0619, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 728 in 0.6153624057769775 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0895, 0.0252, 0.0375, 0.0616, 0.0650, 0.0302]) \n",
      "Test Loss tensor([0.0839, 0.0223, 0.0376, 0.0634, 0.0635, 0.0321])\n",
      "\n",
      "\n",
      "************** Batch 732 in 0.6243722438812256 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0874, 0.0215, 0.0361, 0.0614, 0.0601, 0.0334]) \n",
      "Test Loss tensor([0.0850, 0.0228, 0.0377, 0.0626, 0.0647, 0.0326])\n",
      "\n",
      "\n",
      "************** Batch 736 in 0.5944573879241943 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0863, 0.0244, 0.0362, 0.0604, 0.0655, 0.0321]) \n",
      "Test Loss tensor([0.0814, 0.0219, 0.0381, 0.0608, 0.0639, 0.0325])\n",
      "\n",
      "\n",
      "************** Batch 740 in 0.6088311672210693 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0196, 0.0393, 0.0669, 0.0623, 0.0287]) \n",
      "Test Loss tensor([0.0839, 0.0218, 0.0373, 0.0666, 0.0630, 0.0329])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Batch 744 in 0.5841748714447021 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0756, 0.0228, 0.0351, 0.0561, 0.0672, 0.0307]) \n",
      "Test Loss tensor([0.0840, 0.0218, 0.0376, 0.0617, 0.0643, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 748 in 0.612682580947876 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0786, 0.0199, 0.0362, 0.0660, 0.0661, 0.0352]) \n",
      "Test Loss tensor([0.0842, 0.0212, 0.0372, 0.0621, 0.0642, 0.0320])\n",
      "\n",
      "\n",
      "************** Batch 752 in 0.5984852313995361 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0772, 0.0186, 0.0354, 0.0622, 0.0649, 0.0352]) \n",
      "Test Loss tensor([0.0855, 0.0227, 0.0382, 0.0644, 0.0652, 0.0332])\n",
      "\n",
      "\n",
      "************** Batch 756 in 0.5749771595001221 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0795, 0.0235, 0.0425, 0.0634, 0.0617, 0.0329]) \n",
      "Test Loss tensor([0.0863, 0.0229, 0.0375, 0.0616, 0.0652, 0.0317])\n",
      "\n",
      "\n",
      "************** Batch 760 in 0.6471207141876221 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0869, 0.0275, 0.0357, 0.0577, 0.0677, 0.0349]) \n",
      "Test Loss tensor([0.0844, 0.0217, 0.0375, 0.0609, 0.0641, 0.0316])\n",
      "\n",
      "\n",
      "************** Batch 764 in 0.60196852684021 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0816, 0.0225, 0.0412, 0.0570, 0.0650, 0.0316]) \n",
      "Test Loss tensor([0.0865, 0.0216, 0.0378, 0.0633, 0.0643, 0.0330])\n",
      "\n",
      "\n",
      "************** Batch 768 in 0.6055898666381836 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0854, 0.0232, 0.0364, 0.0675, 0.0676, 0.0277]) \n",
      "Test Loss tensor([0.0827, 0.0227, 0.0383, 0.0617, 0.0639, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 772 in 0.6349585056304932 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0809, 0.0201, 0.0362, 0.0621, 0.0643, 0.0352]) \n",
      "Test Loss tensor([0.0830, 0.0216, 0.0370, 0.0633, 0.0671, 0.0358])\n",
      "\n",
      "\n",
      "************** Batch 776 in 0.5918035507202148 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0882, 0.0198, 0.0397, 0.0583, 0.0696, 0.0347]) \n",
      "Test Loss tensor([0.0871, 0.0227, 0.0381, 0.0618, 0.0677, 0.0335])\n",
      "\n",
      "\n",
      "************** Batch 780 in 0.6025247573852539 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0885, 0.0231, 0.0360, 0.0629, 0.0677, 0.0323]) \n",
      "Test Loss tensor([0.0858, 0.0225, 0.0371, 0.0637, 0.0655, 0.0324])\n",
      "\n",
      "\n",
      "************** Batch 784 in 0.6105194091796875 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0861, 0.0212, 0.0393, 0.0723, 0.0668, 0.0277]) \n",
      "Test Loss tensor([0.0856, 0.0225, 0.0373, 0.0607, 0.0678, 0.0327])\n",
      "\n",
      "\n",
      "************** Batch 788 in 0.609426736831665 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0927, 0.0194, 0.0386, 0.0677, 0.0679, 0.0303]) \n",
      "Test Loss tensor([0.0825, 0.0228, 0.0380, 0.0628, 0.0635, 0.0323])\n",
      "\n",
      "\n",
      "************** Batch 792 in 0.6047155857086182 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0850, 0.0237, 0.0392, 0.0609, 0.0649, 0.0357]) \n",
      "Test Loss tensor([0.0853, 0.0232, 0.0377, 0.0617, 0.0647, 0.0348])\n",
      "\n",
      "\n",
      "************** Batch 796 in 0.5910899639129639 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0877, 0.0209, 0.0400, 0.0631, 0.0637, 0.0305]) \n",
      "Test Loss tensor([0.0822, 0.0206, 0.0373, 0.0624, 0.0637, 0.0321])\n",
      "\n",
      "\n",
      "************** Batch 800 in 0.599719762802124 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0908, 0.0210, 0.0336, 0.0608, 0.0656, 0.0276]) \n",
      "Test Loss tensor([0.0853, 0.0207, 0.0377, 0.0646, 0.0640, 0.0323])\n",
      "\n",
      "\n",
      "************** Batch 804 in 0.5874810218811035 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0881, 0.0204, 0.0415, 0.0613, 0.0659, 0.0332]) \n",
      "Test Loss tensor([0.0856, 0.0217, 0.0384, 0.0623, 0.0660, 0.0315])\n",
      "\n",
      "\n",
      "************** Batch 808 in 0.6045489311218262 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0852, 0.0228, 0.0344, 0.0641, 0.0599, 0.0332]) \n",
      "Test Loss tensor([0.0848, 0.0244, 0.0384, 0.0636, 0.0651, 0.0321])\n",
      "\n",
      "\n",
      "************** Batch 812 in 0.6329679489135742 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0894, 0.0259, 0.0346, 0.0692, 0.0677, 0.0314]) \n",
      "Test Loss tensor([0.0847, 0.0213, 0.0375, 0.0644, 0.0638, 0.0331])\n",
      "\n",
      "\n",
      "************** Batch 816 in 0.5987954139709473 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0848, 0.0233, 0.0344, 0.0651, 0.0658, 0.0329]) \n",
      "Test Loss tensor([0.0846, 0.0220, 0.0374, 0.0598, 0.0649, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 820 in 0.6092486381530762 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0877, 0.0186, 0.0364, 0.0622, 0.0661, 0.0295]) \n",
      "Test Loss tensor([0.0833, 0.0219, 0.0379, 0.0623, 0.0640, 0.0327])\n",
      "\n",
      "\n",
      "************** Batch 824 in 0.6006076335906982 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0922, 0.0258, 0.0385, 0.0674, 0.0676, 0.0296]) \n",
      "Test Loss tensor([0.0839, 0.0227, 0.0378, 0.0614, 0.0645, 0.0321])\n",
      "\n",
      "\n",
      "************** Batch 828 in 0.6194896697998047 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0915, 0.0245, 0.0355, 0.0635, 0.0649, 0.0360]) \n",
      "Test Loss tensor([0.0858, 0.0218, 0.0380, 0.0627, 0.0660, 0.0322])\n",
      "\n",
      "\n",
      "************** Batch 832 in 0.7562458515167236 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0797, 0.0219, 0.0288, 0.0669, 0.0599, 0.0324]) \n",
      "Test Loss tensor([0.0835, 0.0209, 0.0373, 0.0624, 0.0635, 0.0340])\n",
      "\n",
      "\n",
      "************** Batch 836 in 0.7364425659179688 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0898, 0.0237, 0.0396, 0.0584, 0.0626, 0.0326]) \n",
      "Test Loss tensor([0.0849, 0.0215, 0.0378, 0.0610, 0.0629, 0.0328])\n",
      "\n",
      "\n",
      "************** Batch 840 in 0.6968944072723389 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0860, 0.0203, 0.0371, 0.0596, 0.0652, 0.0338]) \n",
      "Test Loss tensor([0.0821, 0.0219, 0.0393, 0.0629, 0.0632, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 844 in 0.7060096263885498 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0837, 0.0215, 0.0420, 0.0611, 0.0685, 0.0363]) \n",
      "Test Loss tensor([0.0846, 0.0225, 0.0385, 0.0620, 0.0631, 0.0329])\n",
      "\n",
      "\n",
      "************** Batch 848 in 0.7214417457580566 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0856, 0.0225, 0.0341, 0.0658, 0.0658, 0.0357]) \n",
      "Test Loss tensor([0.0819, 0.0223, 0.0381, 0.0622, 0.0630, 0.0334])\n",
      "\n",
      "\n",
      "************** Batch 852 in 0.6726219654083252 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0878, 0.0229, 0.0332, 0.0654, 0.0594, 0.0276]) \n",
      "Test Loss tensor([0.0804, 0.0215, 0.0382, 0.0608, 0.0628, 0.0321])\n",
      "\n",
      "\n",
      "************** Batch 856 in 0.6922988891601562 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0770, 0.0188, 0.0373, 0.0636, 0.0602, 0.0371]) \n",
      "Test Loss tensor([0.0833, 0.0221, 0.0391, 0.0616, 0.0635, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 860 in 0.679936408996582 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0801, 0.0237, 0.0375, 0.0586, 0.0572, 0.0311]) \n",
      "Test Loss tensor([0.0831, 0.0208, 0.0369, 0.0612, 0.0630, 0.0338])\n",
      "\n",
      "\n",
      "************** Batch 864 in 0.6462116241455078 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0886, 0.0231, 0.0340, 0.0630, 0.0648, 0.0337]) \n",
      "Test Loss tensor([0.0843, 0.0219, 0.0363, 0.0618, 0.0654, 0.0319])\n",
      "\n",
      "\n",
      "************** Batch 868 in 0.6032958030700684 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0878, 0.0222, 0.0363, 0.0585, 0.0690, 0.0283]) \n",
      "Test Loss tensor([0.0849, 0.0227, 0.0376, 0.0611, 0.0635, 0.0323])\n",
      "\n",
      "\n",
      "************** Batch 872 in 0.6113262176513672 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0931, 0.0235, 0.0337, 0.0630, 0.0678, 0.0293]) \n",
      "Test Loss tensor([0.0826, 0.0221, 0.0372, 0.0642, 0.0642, 0.0319])\n",
      "\n",
      "\n",
      "************** Batch 876 in 0.5761103630065918 **************\n",
      "\n",
      "Training Idx 0 \n",
      "Train Loss tensor([0.0678, 0.0153, 0.0274, 0.0498, 0.0473, 0.0272]) \n",
      "Test Loss tensor([0.0821, 0.0212, 0.0373, 0.0609, 0.0642, 0.0355])\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainLosses = {}\n",
    "testLosses = {}\n",
    "validLosses = {}\n",
    "trainingIdxs = []\n",
    "validationIdxs = []\n",
    "\n",
    "inputNetworkGradients = []\n",
    "messageNetworkGradients = []\n",
    "updateNetworkGradients = []\n",
    "outputNetworkGradients = []\n",
    "\n",
    "\n",
    "for morphIdx in range(7):\n",
    "    trainLosses[morphIdx] = []\n",
    "    testLosses[morphIdx] = []\n",
    "    validLosses[morphIdx] = []\n",
    "\n",
    "for index in [0]:\n",
    "    \n",
    "    inputNetwork = Network(inputSize, stateSize, hidden_sizes, batch_size, with_batch_norm)\n",
    "    messageNetwork = Network(stateSize + 1, messageSize, hidden_sizes, batch_size, with_batch_norm, nn.Tanh)\n",
    "    updateNetwork = Network(stateSize + messageSize, stateSize, hidden_sizes, batch_size, with_batch_norm)\n",
    "    outputNetwork = Network(stateSize, outputSize, hidden_sizes, batch_size, with_batch_norm, nn.Tanh)\n",
    "\n",
    "    gnn = GraphNeuralNetwork(inputNetwork, messageNetwork, updateNetwork, outputNetwork, numMessagePassingIterations).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(itertools.chain(inputNetwork.parameters(), messageNetwork.parameters(), updateNetwork.parameters(), outputNetwork.parameters())\n",
    "                           , lr, weight_decay=1e-5)\n",
    "    \n",
    "    trainingIdxs = [index]\n",
    "    \n",
    "    for epoch in range(10):\n",
    "\n",
    "        for morphIdx in trainingIdxs:\n",
    "            permutation = np.random.permutation(X_train[morphIdx].shape[0])\n",
    "            X_train[morphIdx] = X_train[morphIdx][permutation]\n",
    "            Y_train[morphIdx] = Y_train[morphIdx][permutation]\n",
    "\n",
    "        stepLoss = None\n",
    "        graphs = []\n",
    "        numAggregatedBatches = 0\n",
    "\n",
    "        for batch in range(0, numTrainingBatches, numBatchesPerTrainingStep):\n",
    "\n",
    "            inputNetwork.train()\n",
    "            messageNetwork.train()\n",
    "            updateNetwork.train()\n",
    "            outputNetwork.train()\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            for morphIdx in trainingIdxs:\n",
    "                numNodes = (X_train[morphIdx].shape[1] - 6) // 2\n",
    "                trainLosses[morphIdx].append(torch.zeros(numNodes))\n",
    "\n",
    "            for batchOffset in range(numBatchesPerTrainingStep):\n",
    "\n",
    "                if batch + batchOffset >= numTrainingBatches:\n",
    "                    break\n",
    "\n",
    "                for morphIdx in trainingIdxs:\n",
    "                    graphs.append(env[morphIdx].get_graph()._get_dgl_graph())\n",
    "                    x = X_train[morphIdx][(batch+batchOffset) * batch_size:(batch+batchOffset+1)*batch_size].to(device)\n",
    "                    y = Y_train[morphIdx][(batch+batchOffset) * batch_size:(batch+batchOffset+1)*batch_size].to(device)\n",
    "\n",
    "                    y_hat = gnn.forward(graphs[-1], x)\n",
    "\n",
    "                    loss_tmp = criterion(y, y_hat).mean(dim=0)\n",
    "\n",
    "                    trainLosses[morphIdx][-1] += loss_tmp.cpu().detach() / numBatchesPerTrainingStep\n",
    "\n",
    "                    if stepLoss is None:\n",
    "                        stepLoss = loss_tmp.mean()\n",
    "\n",
    "                    else:\n",
    "                        stepLoss += loss_tmp.mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            stepLoss.backward()\n",
    "\n",
    "\n",
    "            s = 0\n",
    "            for parameter in inputNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            inputNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0\n",
    "            for parameter in messageNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            messageNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0        \n",
    "            for parameter in updateNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            updateNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0        \n",
    "            for parameter in outputNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            outputNetworkGradients.append(s.item())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            stepLoss = None\n",
    "            graphs = []\n",
    "\n",
    "            inputNetwork.eval()\n",
    "            messageNetwork.eval()\n",
    "            updateNetwork.eval()\n",
    "            outputNetwork.eval()\n",
    "\n",
    "            numBatchesForExectution = 50\n",
    "            for morphIdx in trainingIdxs:\n",
    "                numNodes = (X_train[morphIdx].shape[1] - 6) // 2\n",
    "                testLosses[morphIdx].append(torch.zeros(numNodes))\n",
    "                for batch_ in np.random.choice(np.arange(numTestingBatches-1), numBatchesForExectution):\n",
    "                    g = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                    x = X_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "                    y = Y_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "                    y_hat = gnn.forward(g, x)\n",
    "                    loss = criterion(y, y_hat).mean(dim=0)\n",
    "                    testLosses[morphIdx][-1] += loss.cpu().detach()\n",
    "                testLosses[morphIdx][-1] /= numBatchesForExectution\n",
    "\n",
    "            for morphIdx in validationIdxs:\n",
    "                numNodes = (X_train[morphIdx].shape[1] // - 6) // 2\n",
    "                validLosses[morphIdx].append(torch.zeros(numNodes))\n",
    "                for batch_ in np.random.choice(np.arange(numTestingBatches-1), numBatchesForExectution):\n",
    "\n",
    "                    g = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                    x = X_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "                    y = Y_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "\n",
    "                    y_hat = gnn.forward(g, x)\n",
    "                    loss = criterion(y, y_hat).mean(dim=0)\n",
    "\n",
    "                    validLosses[morphIdx][-1] += loss.cpu().detach()\n",
    "                validLosses[morphIdx][-1] /= numBatchesForExectution\n",
    "\n",
    "            print('\\n************** Batch {} in {} **************\\n'.format(batch, time.time() - t0))\n",
    "            for morphIdx in trainingIdxs:\n",
    "                print('Training Idx {} \\nTrain Loss {} \\nTest Loss {}\\n'.format(morphIdx, trainLosses[morphIdx][-1], testLosses[morphIdx][-1]))\n",
    "            for morphIdx in validationIdxs:\n",
    "                print('Valid Idx {} | Loss {}'.format(morphIdx, validLosses[morphIdx][-1]))\n",
    "                \n",
    "    lossArr = torch.stack(testLosses[index]).T\n",
    "    fig, ax = plt.subplots(1, sharex=True)\n",
    "    for i in range(lossArr.shape[0]):\n",
    "        ax.plot(range(lossArr.shape[1]), torch.log10(lossArr[i]))\n",
    "    plt.legend(range(lossArr.shape[0]))\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.grid()\n",
    "    plt.ylabel('Smooth L1 Loss')\n",
    "    plt.title('Per Node Loss Morphology {}'.format(index))\n",
    "    plt.savefig('xv-per-node-loss-{}.jpg'.format(morphIdx))\n",
    "    plt.show()\n",
    "\n",
    "#             if batch % 20 ==0:\n",
    "#                 print('Gradients: Input {} | Message {} | Update {} | Output {}'.format(\n",
    "#                     inputNetworkGradients[-1], messageNetworkGradients[-1], updateNetworkGradients[-1], outputNetworkGradients[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '../models/mix/'\n",
    "\n",
    "testLossesArr = np.array(testLosses)\n",
    "trainLossesArr = np.array(trainLosses)\n",
    "validLossesArr = np.array(validLosses)\n",
    "learningRatesArr = np.array(learningRates)\n",
    "\n",
    "np.save(prefix + 'testLosses', testLossesArr)\n",
    "np.save(prefix + 'trainLosses', trainLossesArr)\n",
    "np.save(prefix + 'validLosses', validLossesArr)\n",
    "np.save(prefix + 'learningRates', learningRatesArr)\n",
    "\n",
    "# torch.save(inputNetwork, prefix + 'inputNetwork.pt')\n",
    "# torch.save(outputNetwork, prefix + 'outputNetwork.pt')\n",
    "# torch.save(messageNetwork, prefix + 'messageNetwork.pt')\n",
    "# torch.save(updateNetwork, prefix + 'updateNetwork.pt')\n",
    "\n",
    "fig, ax = plt.subplots(1, sharex=True)\n",
    "ax.plot(range(len(testLossesArr)), testLossesArr)\n",
    "ax.plot(range(len(trainLossesArr)), trainLossesArr, 'b')\n",
    "ax.plot(range(len(validLossesArr)), validLossesArr)\n",
    "ax.set(ylabel='Smooth L1 Loss')\n",
    "# ax[1].plot(range(len(learningRates)), learningRatesArr)\n",
    "# ax[1].set(ylabel='Learning Rate')\n",
    "ax.legend([\"Testing\", \"Training\", \"Valid\"])\n",
    "plt.xlabel('Batch')\n",
    "plt.savefig(prefix + 'valid-0.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '../models/mix/'\n",
    "\n",
    "inputNetwork = torch.load(prefix + 'inputNetwork.pt')\n",
    "messageNetwork = torch.load(prefix + 'messageNetwork.pt')\n",
    "updateNetwork = torch.load(prefix + 'updateNetwork.pt')\n",
    "outputNetwork = torch.load(prefix + 'outputNetwork.pt')\n",
    "\n",
    "gnn = GraphNeuralNetwork(inputNetwork, messageNetwork, updateNetwork, outputNetwork, numMessagePassingIterations=3).to(device)\n",
    "\n",
    "testLoss = 0\n",
    "for morphIdx in [0]:\n",
    "    \n",
    "    for batch in range(numTestingBatches):\n",
    "\n",
    "        g = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "        x = X_test[morphIdx][batch * batch_size:(batch+1)*batch_size].to(device)\n",
    "        y = Y_test[morphIdx][batch * batch_size:(batch+1)*batch_size].to(device)\n",
    "\n",
    "        y_hat = gnn.forward(g, x)\n",
    "        loss = criterion(y, y_hat)\n",
    "        testLoss += loss.item()\n",
    "        \n",
    "print(testLoss / X_test[morphIdx].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
