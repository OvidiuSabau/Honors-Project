{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dda1e128-0977-4954-9c4d-6d111739620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1dacbd82-c4b1-43d1-b7ff-fa4717bd0d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.autograd\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.markers as markers\n",
    "import pybullet as p \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import dgl\n",
    "import morphsim as m\n",
    "from graphenvs import HalfCheetahGraphEnv\n",
    "import itertools\n",
    "import queue\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7902809-4054-48a5-b605-78264c0e8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkInverseDynamics(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        hidden_sizes,\n",
    "        with_batch_norm=False,\n",
    "        activation=None\n",
    "    ):\n",
    "        super(NetworkInverseDynamics, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        if with_batch_norm:\n",
    "            self.layers.append(nn.LayerNorm(normalized_shape=(hidden_sizes[0])))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            if with_batch_norm:\n",
    "                self.layers.append(nn.LayerNorm(normalized_shape=(hidden_sizes[i+1])))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], self.output_size))\n",
    "        \n",
    "        if activation is not None:\n",
    "            self.layers.append(activation())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7b13c22-6b04-48fd-9cda-79d281a11cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNInverseDynamics(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputNetwork,\n",
    "        messageNetwork,\n",
    "        updateNetwork,\n",
    "        outputNetwork,\n",
    "        numMessagePassingIterations,\n",
    "        withInputNetwork = True\n",
    "    ):\n",
    "        \n",
    "        super(GNNInverseDynamics, self).__init__()\n",
    "                \n",
    "        self.inputNetwork = inputNetwork\n",
    "        self.messageNetwork = messageNetwork\n",
    "        self.updateNetwork = updateNetwork\n",
    "        self.outputNetwork = outputNetwork\n",
    "        \n",
    "        self.numMessagePassingIterations = numMessagePassingIterations\n",
    "        self.withInputNetwork = withInputNetwork\n",
    "        \n",
    "    def inputFunction(self, nodes):\n",
    "        return {'state' : self.inputNetwork(nodes.data['input'])}\n",
    "    \n",
    "    def messageFunction(self, edges):\n",
    "        \n",
    "        batchSize = edges.src['state'].shape[1]\n",
    "        edgeData = edges.data['feature'].repeat(batchSize, 1).T.unsqueeze(-1)\n",
    "        nodeInput = edges.src['input']\n",
    "        \n",
    "        return {'m' : self.messageNetwork(torch.cat((edges.src['state'], edgeData, nodeInput), -1))}\n",
    "    \n",
    "    def updateFunction(self, nodes):\n",
    "        return {'state': self.updateNetwork(torch.cat((nodes.data['m_hat'], nodes.data['state']), -1))}\n",
    "    \n",
    "    def outputFunction(self, nodes):\n",
    "        \n",
    "        return {'output': self.outputNetwork(nodes.data['state'])}\n",
    "\n",
    "\n",
    "    def forward(self, graph, state):\n",
    "        \n",
    "        self.update_states_in_graph(graph, state)\n",
    "        \n",
    "        if self.withInputNetwork:\n",
    "            graph.apply_nodes(self.inputFunction)\n",
    "        \n",
    "        for messagePassingIteration in range(self.numMessagePassingIterations):\n",
    "            graph.update_all(self.messageFunction, dgl.function.mean('m', 'm_hat'), self.updateFunction)\n",
    "        \n",
    "        graph.apply_nodes(self.outputFunction)\n",
    "        \n",
    "        output = graph.ndata['output']\n",
    "        output = output.squeeze(-1).mean(0)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    def update_states_in_graph(self, graph, state):\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        numGraphFeature = 6\n",
    "        numGlobalStateInformation = 5\n",
    "        numLocalStateInformation = 2\n",
    "        numStateVar = state.shape[1] // 2\n",
    "        globalInformation = torch.cat((state[:, 0:5], state[:, numStateVar:numStateVar+5]), -1)\n",
    "        \n",
    "        numNodes = (numStateVar - 5) // 2\n",
    "\n",
    "        nodeData = torch.empty((numNodes, state.shape[0], numGraphFeature + 2 * numGlobalStateInformation + 2 * numLocalStateInformation)).to(device)\n",
    "        for nodeIdx in range(numNodes):\n",
    "\n",
    "            # Assign global features from graph\n",
    "            nodeData[nodeIdx, :, :6] = graph.ndata['feature'][nodeIdx]\n",
    "            # Assign local state information\n",
    "            nodeData[nodeIdx, :, 16] = state[:, 5 + nodeIdx]\n",
    "            nodeData[nodeIdx, :, 17] = state[:, 5 + numNodes + nodeIdx]\n",
    "            nodeData[nodeIdx, :, 18] = state[:, numStateVar + 5 + nodeIdx]\n",
    "            nodeData[nodeIdx, :, 19] = state[:, numStateVar + 5 + numNodes + nodeIdx]\n",
    "\n",
    "        # Assdign global state information\n",
    "        nodeData[:, :, 6:16] = globalInformation\n",
    "        \n",
    "        if self.withInputNetwork:\n",
    "            graph.ndata['input'] = nodeData        \n",
    "        \n",
    "        else:\n",
    "            graph.ndata['state'] = nodeData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46d808bf-b65b-4a87-9be6-dfa016b585f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkAutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        hidden_sizes,\n",
    "        batch_size=256, # Needed only for batch norm\n",
    "        with_batch_norm=False,\n",
    "        activation=None\n",
    "    ):\n",
    "        super(NetworkAutoEncoder, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        if with_batch_norm:\n",
    "#             self.layers.append(nn.BatchNorm1d(batch_size))\n",
    "            self.layers.append(nn.LayerNorm(normalized_shape=(hidden_sizes[0])))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            if with_batch_norm:\n",
    "#                 self.layers.append(nn.BatchNorm1d(batch_size))\n",
    "                self.layers.append(nn.LayerNorm(normalized_shape=(hidden_sizes[i+1])))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], self.output_size))\n",
    "        \n",
    "        if activation is not None:\n",
    "            self.layers.append(activation())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8eef77e3-523a-4d58-8336-09a22ac2b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralNetworkAutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            inputNetwork,\n",
    "            messageNetwork,\n",
    "            updateNetwork,\n",
    "            outputNetwork,\n",
    "            numMessagePassingIterations,\n",
    "            encoder=True\n",
    "    ):\n",
    "\n",
    "        super(GraphNeuralNetworkAutoEncoder, self).__init__()\n",
    "\n",
    "        self.inputNetwork = inputNetwork\n",
    "        self.messageNetwork = messageNetwork\n",
    "        self.updateNetwork = updateNetwork\n",
    "        self.outputNetwork = outputNetwork\n",
    "\n",
    "        self.numMessagePassingIterations = numMessagePassingIterations\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def inputFunction(self, nodes):\n",
    "        return {'state': self.inputNetwork(nodes.data['input'])}\n",
    "\n",
    "    def messageFunction(self, edges):\n",
    "\n",
    "        batchSize = edges.src['state'].shape[1]\n",
    "        edgeData = edges.data['feature'].repeat(batchSize, 1).T.unsqueeze(-1)\n",
    "        nodeInput = edges.src['input']\n",
    "\n",
    "        #         print(edges.src['state'].shape)\n",
    "        #         print(nodeInput.shape)\n",
    "        return {'m': self.messageNetwork(torch.cat((edges.src['state'], edgeData, nodeInput), -1))}\n",
    "\n",
    "    def updateFunction(self, nodes):\n",
    "        return {'state': self.updateNetwork(torch.cat((nodes.data['m_hat'], nodes.data['state']), -1))}\n",
    "\n",
    "    def outputFunction(self, nodes):\n",
    "\n",
    "        #         numNodes, batchSize, stateSize = graph.ndata['state'].shape\n",
    "        #         return self.outputNetwork.forward(graph.ndata['state'])\n",
    "        return {'output': self.outputNetwork(nodes.data['state'])}\n",
    "\n",
    "    def forward(self, graph, state):\n",
    "\n",
    "        self.update_states_in_graph(graph, state)\n",
    "\n",
    "        graph.apply_nodes(self.inputFunction)\n",
    "\n",
    "        for messagePassingIteration in range(self.numMessagePassingIterations):\n",
    "            graph.update_all(self.messageFunction, dgl.function.max('m', 'm_hat'), self.updateFunction)\n",
    "\n",
    "        graph.apply_nodes(self.outputFunction)\n",
    "\n",
    "        output = graph.ndata['output']\n",
    "\n",
    "        if self.encoder:\n",
    "            output = F.normalize(output, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def update_states_in_graph(self, graph, state):\n",
    "\n",
    "        if self.encoder:\n",
    "            if len(state.shape) == 1:\n",
    "                state = state.unsqueeze(0)\n",
    "\n",
    "            numGraphFeature = 6\n",
    "            numGlobalStateInformation = 5\n",
    "            numLocalStateInformation = 2\n",
    "            numStateVar = state.shape[1]\n",
    "            globalInformation = state[:, 0:5]\n",
    "            batch_size = state.shape[0]\n",
    "            numNodes = (numStateVar - 5) // 2\n",
    "\n",
    "            nodeData = torch.empty(\n",
    "                (numNodes, batch_size, numGraphFeature + numGlobalStateInformation + numLocalStateInformation)).to(\n",
    "                device)\n",
    "\n",
    "            nodeData[:, :, 0:numGlobalStateInformation] = globalInformation\n",
    "            for nodeIdx in range(numNodes):\n",
    "                # Assign local state information\n",
    "                nodeData[nodeIdx, :, numGlobalStateInformation] = state[:, 5 + nodeIdx]\n",
    "                nodeData[nodeIdx, :, numGlobalStateInformation + 1] = state[:, 5 + numNodes + nodeIdx]\n",
    "                # Assign global features from graph\n",
    "                nodeData[nodeIdx, :, numGlobalStateInformation + 2: numGlobalStateInformation + 2 + numGraphFeature] = \\\n",
    "                graph.ndata['feature'][nodeIdx]\n",
    "\n",
    "            graph.ndata['input'] = nodeData\n",
    "\n",
    "        else:\n",
    "            numNodes, batchSize, inputSize = state.shape\n",
    "            nodeData = torch.empty((numNodes, batchSize, inputSize + 6)).to(device)\n",
    "            nodeData[:, :, :inputSize] = state\n",
    "            nodeData[:, :, inputSize: inputSize + 6] = graph.ndata['feature'].unsqueeze(dim=1).repeat_interleave(\n",
    "                batchSize, dim=1)\n",
    "            #             for nodeIdx in range(numNodes):\n",
    "            #                 nodeData[nodeIdx, :, inputSize : inputSize + 6] = graph.ndata['feature'][nodeIdx]\n",
    "\n",
    "            graph.ndata['input'] = nodeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a4f0e25-c6a4-4da7-88b6-4687efa53cad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ovi/anaconda3/envs/honors-project/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "NoneType: None\n"
     ]
    }
   ],
   "source": [
    "states = {}\n",
    "actions = {}\n",
    "rewards = {}\n",
    "next_states = {}\n",
    "dones = {}\n",
    "env = {}\n",
    "\n",
    "for morphIdx in [5]:\n",
    "\n",
    "    prefix = '../datasets/{}/'.format(morphIdx)\n",
    "    \n",
    "    states[morphIdx] = np.load(prefix + 'states_array.npy')\n",
    "    actions[morphIdx] = np.load(prefix + 'actions_array.npy')\n",
    "    rewards[morphIdx] = np.load(prefix + 'rewards_array.npy')\n",
    "    next_states[morphIdx] = np.load(prefix + 'next_states_array.npy')\n",
    "    dones[morphIdx] = np.load(prefix + 'dones_array.npy')\n",
    "    \n",
    "    env[morphIdx] = HalfCheetahGraphEnv(None)\n",
    "    env[morphIdx].set_morphology(morphIdx)\n",
    "    env[morphIdx].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98478513-7dda-4f69-b016-8cccd53b222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewGraph(env, morphIdx):\n",
    "    return env[morphIdx].get_graph()._get_dgl_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78f6a479-536e-4a71-ba83-fb400692ff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [256, 256]\n",
    "\n",
    "inputSize = 13\n",
    "stateSize = 64\n",
    "messageSize = 64\n",
    "latentSize = 3\n",
    "numMessagePassingIterations = 6\n",
    "batch_size = 1024\n",
    "numBatchesPerTrainingStep = 1\n",
    "minRandomDistance = 1\n",
    "maxSequentialDistance = 0.04\n",
    "with_batch_norm = True\n",
    "\n",
    "# # Encoder Networks\n",
    "encoderInputNetwork = NetworkAutoEncoder(inputSize, stateSize, hidden_sizes, with_batch_norm=with_batch_norm)\n",
    "encoderMessageNetwork = NetworkAutoEncoder(stateSize + inputSize + 1, messageSize, hidden_sizes, with_batch_norm=with_batch_norm, activation=nn.Tanh)\n",
    "encoderUpdateNetwork = NetworkAutoEncoder(stateSize + messageSize, stateSize, hidden_sizes, with_batch_norm=with_batch_norm)\n",
    "encoderOutputNetwork = NetworkAutoEncoder(stateSize, latentSize, hidden_sizes, with_batch_norm=with_batch_norm)\n",
    "encoderGNN = GraphNeuralNetworkAutoEncoder(encoderInputNetwork, encoderMessageNetwork, encoderUpdateNetwork, encoderOutputNetwork, numMessagePassingIterations, encoder=True).to(device)\n",
    "\n",
    "# # Decoder Networks\n",
    "decoderInputNetwork = NetworkAutoEncoder(latentSize + 6, stateSize, hidden_sizes, with_batch_norm=with_batch_norm)\n",
    "decoderMessageNetwork = NetworkAutoEncoder(stateSize + latentSize + 7, messageSize, hidden_sizes, with_batch_norm=with_batch_norm, activation=nn.Tanh)\n",
    "decoderUpdateNetwork = NetworkAutoEncoder(stateSize + messageSize, stateSize, hidden_sizes, with_batch_norm=with_batch_norm)\n",
    "decoderOutputNetwork = NetworkAutoEncoder(stateSize, 7, hidden_sizes, with_batch_norm=with_batch_norm)\n",
    "decoderGNN = GraphNeuralNetworkAutoEncoder(decoderInputNetwork, decoderMessageNetwork, decoderUpdateNetwork, decoderOutputNetwork, numMessagePassingIterations, encoder=False).to(device)\n",
    "\n",
    "# encoderGNN.load_state_dict(torch.load('../models/new/4-latent-single-GNN-AutoEncoder/5/0.0-1.5/encoderGNN.pt'))\n",
    "# decoderGNN.load_state_dict(torch.load('../models/new/4-latent-single-GNN-AutoEncoder/5/0.0-1.5/decoderGNN.pt'))\n",
    "\n",
    "print(encoderGNN.load_state_dict(torch.load('../models/new/3-latent-single-GNN-AutoEncoder/5/0.4-1.33/' + 'encoderGNN.pt')))\n",
    "print(decoderGNN.load_state_dict(torch.load('../models/new/3-latent-single-GNN-AutoEncoder/5/0.4-1.33/' + 'decoderGNN.pt')))\n",
    "\n",
    "# print(encoderGNN.load_state_dict(torch.load('../models/new/3-latent-single-GNN-AutoEncoder/5/no-contrastive/' + 'encoderGNN.pt')))\n",
    "# print(decoderGNN.load_state_dict(torch.load('../models/new/3-latent-single-GNN-AutoEncoder/5/no-contrastive/' + 'decoderGNN.pt')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ebebf44-efd1-4ced-b5bf-e5a030b3b2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_sizes = [256, 256]\n",
    "\n",
    "inputSize = 20\n",
    "stateSize = 64\n",
    "messageSize = 64\n",
    "outputSize = 1\n",
    "numMessagePassingIterations = 6\n",
    "batch_size = 2048\n",
    "with_batch_norm = True\n",
    "numBatchesPerTrainingStep = 1\n",
    "\n",
    "inputNetwork = NetworkInverseDynamics(inputSize, stateSize, hidden_sizes, with_batch_norm)\n",
    "messageNetwork = NetworkInverseDynamics(stateSize + inputSize + 1, messageSize, hidden_sizes, with_batch_norm, nn.Tanh)\n",
    "updateNetwork = NetworkInverseDynamics(stateSize + messageSize, stateSize, hidden_sizes, with_batch_norm)\n",
    "outputNetwork = NetworkInverseDynamics(stateSize, outputSize, hidden_sizes, with_batch_norm, nn.Sigmoid)\n",
    "\n",
    "inverseDynamics = GNNInverseDynamics(inputNetwork, messageNetwork, updateNetwork, outputNetwork, numMessagePassingIterations).to(device)\n",
    "inverseDynamics.load_state_dict(torch.load('mixed-delta-validTransition.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e1f20ec3-a505-4520-966e-d3c69a92d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructStateFromGraph(graph_data):\n",
    "    num_nodes, batch_size, data_size = graph_data.shape\n",
    "    output = torch.empty((batch_size, 5 + 2 * num_nodes))\n",
    "    output[:, :5] = graph_data[:, :, :5].mean(dim=0)\n",
    "    output[:, 5 : 5+num_nodes] = graph_data[:, :, 5].T\n",
    "    output[:, 5+num_nodes:] = graph_data[:, :, 6].T\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9df96933-98d3-4935-972c-76e270ecba35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached iteration 1000 with 687 states\n",
      "Reached iteration 2000 with 762 states\n",
      "Reached iteration 3000 with 779 states\n",
      "Reached iteration 4000 with 1233 states\n",
      "Reached iteration 5000 with 1247 states\n",
      "Reached iteration 6000 with 1419 states\n",
      "Reached iteration 7000 with 1438 states\n",
      "1313.729305267334\n"
     ]
    }
   ],
   "source": [
    "saved_states = []\n",
    "saved_encodings = []\n",
    "adjacency_list = []\n",
    "\n",
    "morphIdx = 5\n",
    "alpha = 0.4\n",
    "num_offsets = 2048\n",
    "transition_threshold = 0.9\n",
    "num_samples = 10000\n",
    "\n",
    "# Just to get things started\n",
    "random_idx = np.random.choice(int(1e6))\n",
    "random_state = torch.from_numpy(states[morphIdx][random_idx]).unsqueeze(0)\n",
    "g = getNewGraph(env, morphIdx)\n",
    "random_encoding = encoderGNN(g, random_state)[:, 0].detach().cpu()\n",
    "saved_states.append(random_state)\n",
    "saved_encodings.append(random_encoding)\n",
    "adjacency_list.append([])\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    i = 0\n",
    "    #     for i in range(50000):\n",
    "    while len(saved_states) < 200000:\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print('Reached iteration {} with {} states'.format(i, len(saved_states)))\n",
    "            \n",
    "        # np array of size batch: min(len(saved_states), num_samples) \n",
    "        random_subset_indeces = np.random.choice(len(saved_states), size=min(len(saved_states), num_samples))\n",
    "        # tensor of shape (batch, 19)\n",
    "        random_subset_states = [saved_states[i] for i in random_subset_indeces]\n",
    "        # tensor of shape (batch, 7, 3)\n",
    "        random_subset_encodings = torch.stack([saved_encodings[i] for i in random_subset_indeces]).to(device)\n",
    "        # tensor of shape (7, batch, 3)\n",
    "        random_subset_encodings = random_subset_encodings.transpose(0, 1)\n",
    "        \n",
    "        # tensor of shape (batch)\n",
    "        distances = torch.cdist(random_subset_encodings, random_subset_encodings).sum(dim=0).sum(dim=-1)\n",
    "        largest_distance_idx = torch.argmax(distances)\n",
    "        \n",
    "        # tensor of shape (batch, 7, 3)\n",
    "        random_subset_encodings = random_subset_encodings.transpose(0, 1)\n",
    "        current_encoding = random_subset_encodings[largest_distance_idx]\n",
    "        current_state = random_subset_states[largest_distance_idx]\n",
    "                            \n",
    "        offsets = torch.normal(0, 1, (current_encoding.size(0), num_offsets, current_encoding.size(1))).to(device)\n",
    "        offsets = F.normalize(offsets, dim=-1)\n",
    "        offsets *= alpha\n",
    "#         offsets *= np.random.uniform(low=0, high=alpha, size=num_offsets)[..., np.newaxis][np.newaxis, ...]\n",
    "\n",
    "        # Convert coordinates from sphere into global ones, and project onto original sphere\n",
    "        new_encodings = current_encoding.unsqueeze(1) - offsets\n",
    "        new_encodings = F.normalize(new_encodings, dim=-1)\n",
    "        new_encodings = new_encodings.to(device)\n",
    "\n",
    "        g = getNewGraph(env, morphIdx)\n",
    "        new_states = decoderGNN(g, new_encodings)\n",
    "        new_states = reconstructStateFromGraph(new_states)\n",
    "        transitions = current_state.repeat(num_offsets, 2)\n",
    "        transitions [:, transitions.size(1) // 2:] -= new_states\n",
    "\n",
    "        g = getNewGraph(env, morphIdx)\n",
    "        probabilities = inverseDynamics(g, transitions).cpu()\n",
    "        \n",
    "        valid_transition_indeces = probabilities > transition_threshold\n",
    "\n",
    "        kept_states = new_states[valid_transition_indeces]\n",
    "        kept_encodings = new_encodings[:, valid_transition_indeces]\n",
    "        \n",
    "        largest_distance_idx_global = random_subset_indeces[largest_distance_idx]\n",
    "        adjacency_list[largest_distance_idx_global].extend(range(len(saved_states), len(saved_states) + kept_states.size(0)))\n",
    "        \n",
    "        for new_idx in range(kept_states.size(0)):\n",
    "            saved_states.append(kept_states[new_idx].cpu())\n",
    "            saved_encodings.append(kept_encodings[:, new_idx].cpu())\n",
    "            adjacency_list.append([])\n",
    "        \n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b7e39349-d187-4d7c-b5f0-e4deb2e0e939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2002, 7, 3])\n"
     ]
    }
   ],
   "source": [
    "saved_encodings = torch.stack(saved_encodings)\n",
    "print(saved_encodings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f3a96aa-6c12-4ff4-b8a9-579b5ddd810a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2],\n",
       " [30, 31, 32, 33, 34, 35, 36, 37],\n",
       " [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [22, 23, 24, 25, 26, 27, 28, 29, 126, 127, 128, 129, 130, 131, 132, 133, 134],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [138, 174],\n",
       " [139, 173, 175, 176],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [38],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [39],\n",
       " [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [64, 65],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [846, 847, 848, 888, 901, 902, 903],\n",
       " [],\n",
       " [],\n",
       " [60, 61, 62, 63],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [67],\n",
       " [],\n",
       " [],\n",
       " [66],\n",
       " [],\n",
       " [68, 69, 70, 71],\n",
       " [],\n",
       " [],\n",
       " [73, 74],\n",
       " [],\n",
       " [72],\n",
       " [],\n",
       " [],\n",
       " [75, 76, 77],\n",
       " [78, 79, 80],\n",
       " [],\n",
       " [],\n",
       " [87],\n",
       " [],\n",
       " [81, 82, 83, 84, 85, 86],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [88, 89, 90],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [91, 92, 93, 94, 95, 96, 97, 98],\n",
       " [],\n",
       " [],\n",
       " [99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125],\n",
       " [],\n",
       " [],\n",
       " [850,\n",
       "  851,\n",
       "  852,\n",
       "  853,\n",
       "  854,\n",
       "  855,\n",
       "  856,\n",
       "  857,\n",
       "  858,\n",
       "  859,\n",
       "  860,\n",
       "  861,\n",
       "  862,\n",
       "  863,\n",
       "  864,\n",
       "  865,\n",
       "  866,\n",
       "  867,\n",
       "  868,\n",
       "  869,\n",
       "  870,\n",
       "  871,\n",
       "  872,\n",
       "  873,\n",
       "  874,\n",
       "  875],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172],\n",
       " [],\n",
       " [135, 136, 137],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [177, 178, 180],\n",
       " [],\n",
       " [],\n",
       " [317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [179],\n",
       " [],\n",
       " [],\n",
       " [181, 182],\n",
       " [183, 184, 185, 186, 187],\n",
       " [],\n",
       " [],\n",
       " [188, 189, 190, 191, 192, 193, 194, 195],\n",
       " [],\n",
       " [196, 197, 198, 199, 200, 203, 204, 205, 206, 209],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [201, 202, 207, 208],\n",
       " [210, 211, 212, 213, 214, 215],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [216, 217, 218],\n",
       " [],\n",
       " [],\n",
       " [219],\n",
       " [],\n",
       " [220, 221, 222, 223, 224],\n",
       " [],\n",
       " [231, 232],\n",
       " [],\n",
       " [],\n",
       " [225, 226, 227, 228, 229],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [230],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243],\n",
       " [],\n",
       " [],\n",
       " [590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [244, 245, 246, 247, 248, 249, 250, 251],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [252, 253, 254, 255],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [411, 412, 413],\n",
       " [],\n",
       " [256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [277, 278, 279, 280],\n",
       " [276],\n",
       " [],\n",
       " [],\n",
       " [274, 275],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [586, 587],\n",
       " [],\n",
       " [],\n",
       " [281, 282, 283, 284, 285, 286, 287, 288, 289],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [290, 291],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [304, 305, 306, 307, 308, 309],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [310, 311, 312, 313, 314, 315],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [316],\n",
       " [],\n",
       " [344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [368],\n",
       " [],\n",
       " [357, 358, 359, 366],\n",
       " [],\n",
       " [],\n",
       " [649],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [360, 361, 362, 363, 364, 365, 404, 405, 406, 407, 408, 409],\n",
       " [367],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [756, 757],\n",
       " [402, 403, 652, 653, 654, 819, 820, 821, 822],\n",
       " [448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [759,\n",
       "  760,\n",
       "  761,\n",
       "  762,\n",
       "  763,\n",
       "  764,\n",
       "  765,\n",
       "  766,\n",
       "  767,\n",
       "  768,\n",
       "  769,\n",
       "  770,\n",
       "  771,\n",
       "  772,\n",
       "  773,\n",
       "  774,\n",
       "  775,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  780,\n",
       "  781,\n",
       "  782,\n",
       "  783,\n",
       "  784,\n",
       "  785,\n",
       "  786,\n",
       "  787,\n",
       "  788,\n",
       "  789,\n",
       "  790,\n",
       "  791,\n",
       "  792,\n",
       "  793,\n",
       "  794,\n",
       "  795,\n",
       "  796,\n",
       "  797,\n",
       "  798,\n",
       "  799,\n",
       "  800,\n",
       "  801,\n",
       "  802,\n",
       "  803,\n",
       "  804,\n",
       "  805,\n",
       "  806,\n",
       "  807,\n",
       "  808,\n",
       "  809,\n",
       "  810,\n",
       "  811,\n",
       "  812,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  816,\n",
       "  817],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [414, 415, 416, 417, 418],\n",
       " [410],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [436, 441, 443, 444, 447, 460, 477, 504, 588, 589],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [419, 420, 421, 422, 423, 427, 428, 433, 434, 435],\n",
       " [],\n",
       " [424, 425, 426],\n",
       " [429, 430, 431, 432],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [437,\n",
       "  438,\n",
       "  445,\n",
       "  446,\n",
       "  461,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  478,\n",
       "  479,\n",
       "  480,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  650,\n",
       "  651],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [439, 440, 442],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [474, 475, 476],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [462, 463, 464, 465, 466, 467, 468, 469, 470],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  519,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  584,\n",
       "  585],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [646, 647, 648],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [924,\n",
       "  925,\n",
       "  926,\n",
       "  927,\n",
       "  928,\n",
       "  929,\n",
       "  930,\n",
       "  931,\n",
       "  932,\n",
       "  933,\n",
       "  934,\n",
       "  935,\n",
       "  936,\n",
       "  937,\n",
       "  938,\n",
       "  939,\n",
       "  940,\n",
       "  941,\n",
       "  942,\n",
       "  943,\n",
       "  944],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [603, 604, 605, 606, 607, 608],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [609, 610, 611, 612, 613, 614],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [626,\n",
       "  627,\n",
       "  628,\n",
       "  629,\n",
       "  630,\n",
       "  631,\n",
       "  632,\n",
       "  633,\n",
       "  634,\n",
       "  635,\n",
       "  636,\n",
       "  637,\n",
       "  638,\n",
       "  639,\n",
       "  640,\n",
       "  641,\n",
       "  642,\n",
       "  643,\n",
       "  644,\n",
       "  645],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [655,\n",
       "  656,\n",
       "  657,\n",
       "  658,\n",
       "  659,\n",
       "  660,\n",
       "  661,\n",
       "  662,\n",
       "  663,\n",
       "  664,\n",
       "  665,\n",
       "  666,\n",
       "  667,\n",
       "  668,\n",
       "  669,\n",
       "  670,\n",
       "  671,\n",
       "  672,\n",
       "  673,\n",
       "  674,\n",
       "  675,\n",
       "  676,\n",
       "  677,\n",
       "  678,\n",
       "  679,\n",
       "  680,\n",
       "  681,\n",
       "  682,\n",
       "  683,\n",
       "  684,\n",
       "  685,\n",
       "  686,\n",
       "  687,\n",
       "  688,\n",
       "  689,\n",
       "  690,\n",
       "  691,\n",
       "  692,\n",
       "  693,\n",
       "  694,\n",
       "  695,\n",
       "  696,\n",
       "  697,\n",
       "  698,\n",
       "  699,\n",
       "  700,\n",
       "  701,\n",
       "  702,\n",
       "  703,\n",
       "  704,\n",
       "  705,\n",
       "  706,\n",
       "  707,\n",
       "  708,\n",
       "  709,\n",
       "  710,\n",
       "  711,\n",
       "  712,\n",
       "  713,\n",
       "  714,\n",
       "  715,\n",
       "  716,\n",
       "  717,\n",
       "  718,\n",
       "  719,\n",
       "  720,\n",
       "  721,\n",
       "  722,\n",
       "  723,\n",
       "  724,\n",
       "  725,\n",
       "  726,\n",
       "  727,\n",
       "  728,\n",
       "  729,\n",
       "  730,\n",
       "  731,\n",
       "  732,\n",
       "  733,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  737,\n",
       "  738,\n",
       "  739,\n",
       "  740,\n",
       "  741,\n",
       "  742,\n",
       "  743,\n",
       "  744,\n",
       "  745,\n",
       "  746,\n",
       "  747,\n",
       "  748,\n",
       "  749,\n",
       "  750,\n",
       "  751,\n",
       "  752,\n",
       "  753,\n",
       "  754,\n",
       "  755],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [979,\n",
       "  980,\n",
       "  981,\n",
       "  982,\n",
       "  983,\n",
       "  984,\n",
       "  985,\n",
       "  986,\n",
       "  987,\n",
       "  988,\n",
       "  989,\n",
       "  990,\n",
       "  991,\n",
       "  992,\n",
       "  993,\n",
       "  994],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [758, 818, 921],\n",
       " [876, 877, 878, 879],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [898],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [837, 838, 839, 840, 841, 842, 843],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [844, 845, 880, 881, 882, 883, 884, 885, 886, 887],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [849],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [900],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [912],\n",
       " [892, 893, 894, 896],\n",
       " [],\n",
       " [889,\n",
       "  890,\n",
       "  891,\n",
       "  904,\n",
       "  905,\n",
       "  906,\n",
       "  907,\n",
       "  908,\n",
       "  909,\n",
       "  910,\n",
       "  911,\n",
       "  963,\n",
       "  964,\n",
       "  965,\n",
       "  966,\n",
       "  967,\n",
       "  968],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [895, 897, 913],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [899, 920],\n",
       " [],\n",
       " [],\n",
       " [914, 915, 916, 917, 918, 919],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [961, 962],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [997],\n",
       " [922, 923],\n",
       " [],\n",
       " [],\n",
       " [945, 946, 957, 958, 959, 960, 995, 996, 1002, 1003],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [947, 948, 949, 950, 951, 952, 953],\n",
       " [954, 955, 956],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [969],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [970, 971, 972, 973, 974, 975, 976, 977],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [978, 1001],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [998, 999, 1000],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ...]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4ee5dcc6-90f1-4487-920b-79e98f793c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4bf3c9ac0742808da5435f67d3d348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "x = saved_encodings[:, 0, 0].cpu().numpy()\n",
    "y = saved_encodings[:, 0, 1].cpu().numpy()\n",
    "z = saved_encodings[:, 0, 2].cpu().numpy()\n",
    "\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.scatter3D(x, y, z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813cb76d-7910-4a8c-9cc4-546f3dbd1b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
