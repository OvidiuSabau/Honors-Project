{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.autograd\n",
    "import os\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet as p \n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import pybullet \n",
    "import pybullet_envs.gym_pendulum_envs \n",
    "import pybullet_envs.gym_locomotion_envs\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import dgl\n",
    "import morphsim as m\n",
    "from graphenvs import HalfCheetahGraphEnv\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        hidden_sizes,\n",
    "        batch_size=256, # Needed only for batch norm\n",
    "        with_batch_norm=False,\n",
    "        activation=None\n",
    "    ):\n",
    "        super(Network, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        if with_batch_norm:\n",
    "#             self.layers.append(nn.BatchNorm1d(batch_size))\n",
    "            self.layers.append(nn.LayerNorm(normalized_shape=(hidden_sizes[0])))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            if with_batch_norm:\n",
    "#                 self.layers.append(nn.BatchNorm1d(batch_size))\n",
    "                self.layers.append(nn.LayerNorm(normalized_shape=(hidden_sizes[i+1])))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], self.output_size))\n",
    "        \n",
    "        if activation is not None:\n",
    "            self.layers.append(activation())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputNetwork,\n",
    "        messageNetwork,\n",
    "        updateNetwork,\n",
    "        outputNetwork,\n",
    "        numMessagePassingIterations,\n",
    "        encoder = True\n",
    "    ):\n",
    "        \n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "                \n",
    "        self.inputNetwork = inputNetwork\n",
    "        self.messageNetwork = messageNetwork\n",
    "        self.updateNetwork = updateNetwork\n",
    "        self.outputNetwork = outputNetwork\n",
    "        \n",
    "        self.numMessagePassingIterations = numMessagePassingIterations\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    def inputFunction(self, nodes):\n",
    "        return {'state' : self.inputNetwork(nodes.data['input'])}\n",
    "    \n",
    "    def messageFunction(self, edges):\n",
    "        \n",
    "        batchSize = edges.src['state'].shape[1]\n",
    "        edgeData = edges.data['feature'].repeat(batchSize, 1).T.unsqueeze(-1)\n",
    "        nodeInput = edges.src['input']\n",
    "        \n",
    "#         print(edges.src['state'].shape)\n",
    "#         print(nodeInput.shape)\n",
    "        return {'m' : self.messageNetwork(torch.cat((edges.src['state'], edgeData, nodeInput), -1))}\n",
    "        \n",
    "\n",
    "    def updateFunction(self, nodes):\n",
    "        return {'state': self.updateNetwork(torch.cat((nodes.data['m_hat'], nodes.data['state']), -1))}\n",
    "    \n",
    "    def outputFunction(self, nodes):\n",
    "        \n",
    "#         numNodes, batchSize, stateSize = graph.ndata['state'].shape\n",
    "#         return self.outputNetwork.forward(graph.ndata['state'])\n",
    "        return {'output': self.outputNetwork(nodes.data['state'])}\n",
    "\n",
    "\n",
    "    def forward(self, graph, state):\n",
    "        \n",
    "        self.update_states_in_graph(graph, state)\n",
    "        \n",
    "        graph.apply_nodes(self.inputFunction)\n",
    "        \n",
    "        for messagePassingIteration in range(self.numMessagePassingIterations):\n",
    "            graph.update_all(self.messageFunction, dgl.function.max('m', 'm_hat'), self.updateFunction)\n",
    "        \n",
    "        graph.apply_nodes(self.outputFunction)\n",
    "        \n",
    "        output = graph.ndata['output']\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def update_states_in_graph(self, graph, state):\n",
    "        \n",
    "        if self.encoder:\n",
    "            if len(state.shape) == 1:\n",
    "                state = state.unsqueeze(0)\n",
    "\n",
    "            numGraphFeature = 6\n",
    "            numGlobalStateInformation = 5\n",
    "            numLocalStateInformation = 2\n",
    "            numStateVar = state.shape[1]\n",
    "            globalInformation = state[:, 0:5]\n",
    "            batch_size = state.shape[0]\n",
    "            numNodes = (numStateVar - 5) // 2\n",
    "\n",
    "            nodeData = torch.empty((numNodes, batch_size, numGraphFeature + numGlobalStateInformation + numLocalStateInformation)).to(device)\n",
    "\n",
    "            nodeData[:, :, 0:numGlobalStateInformation] = globalInformation            \n",
    "            for nodeIdx in range(numNodes):\n",
    "                # Assign local state information\n",
    "                nodeData[nodeIdx, :, numGlobalStateInformation] = state[:, 5 + nodeIdx]\n",
    "                nodeData[nodeIdx, :, numGlobalStateInformation + 1] = state[:, 5 + numNodes + nodeIdx]\n",
    "                # Assign global features from graph\n",
    "                nodeData[nodeIdx, :, numGlobalStateInformation + 2 : numGlobalStateInformation + 2 + numGraphFeature] = graph.ndata['feature'][nodeIdx]\n",
    "\n",
    "            graph.ndata['input'] = nodeData\n",
    "        \n",
    "        else:\n",
    "            numNodes, batchSize, inputSize = state.shape\n",
    "            nodeData = torch.empty((numNodes, batchSize, inputSize + 6)).to(device)\n",
    "            nodeData[:, :, :inputSize] = state\n",
    "            nodeData[:, :, inputSize : inputSize + 6] = graph.ndata['feature'].unsqueeze(dim=1).repeat_interleave(batchSize, dim=1)\n",
    "#             for nodeIdx in range(numNodes):\n",
    "#                 nodeData[nodeIdx, :, inputSize : inputSize + 6] = graph.ndata['feature'][nodeIdx]\n",
    "            \n",
    "            graph.ndata['input'] = nodeData\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ovi/anaconda3/envs/honors-project/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    }
   ],
   "source": [
    "states = {}\n",
    "actions = {}\n",
    "rewards = {}\n",
    "next_states = {}\n",
    "dones = {}\n",
    "env = {}\n",
    "\n",
    "for morphIdx in range(7):\n",
    "\n",
    "    prefix = '../datasets/{}/'.format(morphIdx)\n",
    "    \n",
    "    states[morphIdx] = np.load(prefix + 'states_array.npy')\n",
    "    actions[morphIdx] = np.load(prefix + 'actions_array.npy')\n",
    "    rewards[morphIdx] = np.load(prefix + 'rewards_array.npy')\n",
    "    next_states[morphIdx] = np.load(prefix + 'next_states_array.npy')\n",
    "    dones[morphIdx] = np.load(prefix + 'dones_array.npy')\n",
    "    \n",
    "    env[morphIdx] = HalfCheetahGraphEnv(None)\n",
    "    env[morphIdx].set_morphology(morphIdx)\n",
    "    env[morphIdx].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = {}\n",
    "X_train = {}\n",
    "Y_test = {}\n",
    "Y_train = {}\n",
    "\n",
    "for morphIdx in range(7):\n",
    "    X = states[morphIdx]\n",
    "    Y = next_states[morphIdx]\n",
    "    permutation = np.random.permutation(X.shape[0])\n",
    "    X = X[permutation]\n",
    "    X_test[morphIdx] = torch.from_numpy(X[:100000]).float()\n",
    "    X_train[morphIdx] = torch.from_numpy(X[100000:]).float()\n",
    "    Y = Y[permutation]\n",
    "    Y_test[morphIdx] = torch.from_numpy(Y[:100000]).float()\n",
    "    Y_train[morphIdx] = torch.from_numpy(Y[100000:]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_sizes = [256, 256]\n",
    "\n",
    "inputSize = 13\n",
    "stateSize = 64\n",
    "messageSize = 64\n",
    "latentSize = 2\n",
    "numMessagePassingIterations = 4\n",
    "batch_size = 1024\n",
    "numBatchesPerTrainingStep = 1\n",
    "minDistanceSeqAndRand = 0.1\n",
    "with_batch_norm = True\n",
    "\n",
    "# # Encoder Networks \n",
    "encoderInputNetwork = Network(inputSize, stateSize, hidden_sizes, with_batch_norm=with_batch_norm)\n",
    "encoderMessageNetwork = Network(stateSize + inputSize + 1, messageSize, hidden_sizes, with_batch_norm=with_batch_norm, activation=nn.Tanh)\n",
    "encoderUpdateNetwork = Network(stateSize + messageSize, stateSize, hidden_sizes, with_batch_norm=with_batch_norm)\n",
    "encoderOutputNetwork = Network(stateSize, latentSize, hidden_sizes, with_batch_norm=with_batch_norm, activation=nn.Tanh)\n",
    "encoderGNN = GraphNeuralNetwork(encoderInputNetwork, encoderMessageNetwork, encoderUpdateNetwork, encoderOutputNetwork, numMessagePassingIterations, encoder=True).to(device)\n",
    "\n",
    "# # Decoder Networks\n",
    "decoderInputNetwork = Network(latentSize + 6, stateSize, hidden_sizes, with_batch_norm=with_batch_norm)\n",
    "decoderMessageNetwork = Network(stateSize + latentSize + 7, messageSize, hidden_sizes, with_batch_norm=with_batch_norm, activation=nn.Tanh)\n",
    "decoderUpdateNetwork = Network(stateSize + messageSize, stateSize, hidden_sizes, with_batch_norm=with_batch_norm)\n",
    "decoderOutputNetwork = Network(stateSize, 7, hidden_sizes, with_batch_norm=with_batch_norm)\n",
    "decoderGNN = GraphNeuralNetwork(decoderInputNetwork, decoderMessageNetwork, decoderUpdateNetwork, decoderOutputNetwork, numMessagePassingIterations, encoder=False).to(device)\n",
    "\n",
    "# Optimizer\n",
    "lr =  1e-3\n",
    "optimizer = optim.Adam(itertools.chain(\n",
    "                    encoderInputNetwork.parameters(), encoderMessageNetwork.parameters(), \n",
    "                    encoderUpdateNetwork.parameters(), encoderOutputNetwork.parameters(),\n",
    "                    decoderInputNetwork.parameters(), decoderMessageNetwork.parameters(), \n",
    "                    decoderUpdateNetwork.parameters(), decoderOutputNetwork.parameters()),\n",
    "                    lr, weight_decay=0)\n",
    "\n",
    "lr_lambda = lambda epoch: 0.7\n",
    "lr_scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda)\n",
    "criterion  = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrainingBatches = int(np.ceil(X_train[0].shape[0] / batch_size))\n",
    "numTestingBatches = int(np.ceil(X_test[0].shape[0] / batch_size))\n",
    "\n",
    "\n",
    "zeroTensor = torch.zeros([1]).to(device)\n",
    "trainLosses = {}\n",
    "testLosses = {}\n",
    "validLosses = {}\n",
    "trainingIdxs = [0,1,2,3,4,5,6]\n",
    "validationIdxs = []\n",
    "\n",
    "encoderInputNetworkGradients = []\n",
    "encoderMessageNetworkGradients = []\n",
    "encoderUpdateNetworkGradients = []\n",
    "encoderOutputNetworkGradients = []\n",
    "decoderInputNetworkGradients = []\n",
    "decoderMessageNetworkGradients = []\n",
    "decoderUpdateNetworkGradients = []\n",
    "decoderOutputNetworkGradients = []\n",
    "\n",
    "for morphIdx in range(7):\n",
    "    trainLosses[morphIdx] = []\n",
    "    testLosses[morphIdx] = []\n",
    "    validLosses[morphIdx] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoderGNN.load_state_dict(torch.load('encoderGNN-normalized-time-contrastive.pt'))\n",
    "decoderGNN.load_state_dict(torch.load('decoderGNN-normalized-time-contrastive.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0\n",
      "Batch 0 in 1.8s\n",
      "Idx 0 | Train 0.281 : 0.092\n",
      "Idx 1 | Train 0.222 : 0.107\n",
      "Idx 2 | Train 0.716 : 0.12\n",
      "Idx 3 | Train 0.242 : 0.085\n",
      "Idx 4 | Train 0.579 : 0.109\n",
      "Idx 5 | Train 0.595 : 0.099\n",
      "Idx 6 | Train 0.428 : 0.085\n",
      "Gradients: Encoder Input -2.18 | Encoder Message -1.69 | Encoder  Update -1.61 | Encoder Output -0.68\n",
      "Gradients: Decoder Input -1.96 | Decoder Message -1.71 | Decoder  Update -1.54 | Decoder Output -1.37\n",
      "\n",
      "Batch 200 in 1.8s\n",
      "Idx 0 | Train 0.252 : 0.09\n",
      "Idx 1 | Train 0.17 : 0.108\n",
      "Idx 2 | Train 0.676 : 0.104\n",
      "Idx 3 | Train 0.229 : 0.076\n",
      "Idx 4 | Train 0.228 : 0.115\n",
      "Idx 5 | Train 0.563 : 0.107\n",
      "Idx 6 | Train 0.401 : 0.086\n",
      "Gradients: Encoder Input -2.41 | Encoder Message -1.95 | Encoder  Update -1.92 | Encoder Output -1.47\n",
      "Gradients: Decoder Input -2.17 | Decoder Message -1.9 | Decoder  Update -1.68 | Decoder Output -1.51\n",
      "\n",
      "Batch 400 in 1.7s\n",
      "Idx 0 | Train 0.252 : 0.095\n",
      "Idx 1 | Train 0.168 : 0.112\n",
      "Idx 2 | Train 0.696 : 0.127\n",
      "Idx 3 | Train 0.213 : 0.085\n",
      "Idx 4 | Train 0.275 : 0.115\n",
      "Idx 5 | Train 0.506 : 0.11\n",
      "Idx 6 | Train 0.312 : 0.116\n",
      "Gradients: Encoder Input -2.13 | Encoder Message -1.69 | Encoder  Update -1.61 | Encoder Output -1.5\n",
      "Gradients: Decoder Input -1.87 | Decoder Message -1.52 | Decoder  Update -1.37 | Decoder Output -1.42\n",
      "\n",
      "Batch 600 in 1.7s\n",
      "Idx 0 | Train 0.177 : 0.097\n",
      "Idx 1 | Train 0.125 : 0.109\n",
      "Idx 2 | Train 0.291 : 0.149\n",
      "Idx 3 | Train 0.175 : 0.087\n",
      "Idx 4 | Train 0.2 : 0.105\n",
      "Idx 5 | Train 0.372 : 0.119\n",
      "Idx 6 | Train 0.128 : 0.1\n",
      "Gradients: Encoder Input -2.32 | Encoder Message -1.99 | Encoder  Update -1.89 | Encoder Output -1.29\n",
      "Gradients: Decoder Input -2.24 | Decoder Message -1.87 | Decoder  Update -1.66 | Decoder Output -1.72\n",
      "\n",
      "Batch 800 in 1.7s\n",
      "Idx 0 | Train 0.126 : 0.096\n",
      "Idx 1 | Train 0.101 : 0.107\n",
      "Idx 2 | Train 0.235 : 0.123\n",
      "Idx 3 | Train 0.149 : 0.089\n",
      "Idx 4 | Train 0.189 : 0.107\n",
      "Idx 5 | Train 0.278 : 0.113\n",
      "Idx 6 | Train 0.089 : 0.105\n",
      "Gradients: Encoder Input -2.36 | Encoder Message -1.86 | Encoder  Update -1.76 | Encoder Output -1.34\n",
      "Gradients: Decoder Input -2.1 | Decoder Message -1.83 | Decoder  Update -1.68 | Decoder Output -1.78\n",
      "\n",
      "Idx 0 | Test 0.112 : 0.095\n",
      "Idx 1 | Test 0.107 : 0.102\n",
      "Idx 2 | Test 0.245 : 0.129\n",
      "Idx 3 | Test 0.113 : 0.083\n",
      "Idx 4 | Test 0.19 : 0.105\n",
      "Idx 5 | Test 0.275 : 0.115\n",
      "Idx 6 | Test 0.077 : 0.103\n",
      "Epoch 0 finished in 1627.1\n",
      "Starting Epoch 1\n",
      "Batch 0 in 1.7s\n",
      "Idx 0 | Train 0.11 : 0.089\n",
      "Idx 1 | Train 0.106 : 0.108\n",
      "Idx 2 | Train 0.231 : 0.136\n",
      "Idx 3 | Train 0.121 : 0.085\n",
      "Idx 4 | Train 0.192 : 0.106\n",
      "Idx 5 | Train 0.281 : 0.116\n",
      "Idx 6 | Train 0.072 : 0.1\n",
      "Gradients: Encoder Input -2.23 | Encoder Message -1.86 | Encoder  Update -1.72 | Encoder Output -0.77\n",
      "Gradients: Decoder Input -1.85 | Decoder Message -1.82 | Decoder  Update -1.67 | Decoder Output -1.73\n",
      "\n",
      "Batch 200 in 1.7s\n",
      "Idx 0 | Train 0.124 : 0.098\n",
      "Idx 1 | Train 0.105 : 0.104\n",
      "Idx 2 | Train 0.211 : 0.123\n",
      "Idx 3 | Train 0.113 : 0.077\n",
      "Idx 4 | Train 0.172 : 0.104\n",
      "Idx 5 | Train 0.233 : 0.118\n",
      "Idx 6 | Train 0.077 : 0.097\n",
      "Gradients: Encoder Input -2.44 | Encoder Message -2.09 | Encoder  Update -2.03 | Encoder Output -1.4\n",
      "Gradients: Decoder Input -2.37 | Decoder Message -2.01 | Decoder  Update -1.84 | Decoder Output -1.92\n",
      "\n",
      "Batch 400 in 1.7s\n",
      "Idx 0 | Train 0.132 : 0.085\n",
      "Idx 1 | Train 0.109 : 0.1\n",
      "Idx 2 | Train 0.289 : 0.106\n",
      "Idx 3 | Train 0.112 : 0.073\n",
      "Idx 4 | Train 0.178 : 0.099\n",
      "Idx 5 | Train 0.357 : 0.106\n",
      "Idx 6 | Train 0.159 : 0.09\n",
      "Gradients: Encoder Input -2.06 | Encoder Message -1.3 | Encoder  Update -1.13 | Encoder Output -0.06\n",
      "Gradients: Decoder Input -1.3 | Decoder Message -1.32 | Decoder  Update -1.16 | Decoder Output -1.24\n",
      "\n",
      "Batch 600 in 1.7s\n",
      "Idx 0 | Train 0.121 : 0.091\n",
      "Idx 1 | Train 0.11 : 0.112\n",
      "Idx 2 | Train 0.233 : 0.118\n",
      "Idx 3 | Train 0.096 : 0.077\n",
      "Idx 4 | Train 0.163 : 0.102\n",
      "Idx 5 | Train 0.193 : 0.115\n",
      "Idx 6 | Train 0.062 : 0.098\n",
      "Gradients: Encoder Input -2.04 | Encoder Message -1.7 | Encoder  Update -1.62 | Encoder Output -1.43\n",
      "Gradients: Decoder Input -1.84 | Decoder Message -1.67 | Decoder  Update -1.41 | Decoder Output -1.43\n",
      "\n",
      "Batch 800 in 1.7s\n",
      "Idx 0 | Train 0.103 : 0.097\n",
      "Idx 1 | Train 0.104 : 0.1\n",
      "Idx 2 | Train 0.207 : 0.118\n",
      "Idx 3 | Train 0.126 : 0.088\n",
      "Idx 4 | Train 0.169 : 0.113\n",
      "Idx 5 | Train 0.21 : 0.119\n",
      "Idx 6 | Train 0.085 : 0.107\n",
      "Gradients: Encoder Input -2.27 | Encoder Message -1.9 | Encoder  Update -1.85 | Encoder Output -1.28\n",
      "Gradients: Decoder Input -2.29 | Decoder Message -1.86 | Decoder  Update -1.61 | Decoder Output -1.47\n",
      "\n",
      "Idx 0 | Test 0.113 : 0.089\n",
      "Idx 1 | Test 0.176 : 0.1\n",
      "Idx 2 | Test 0.337 : 0.111\n",
      "Idx 3 | Test 0.103 : 0.078\n",
      "Idx 4 | Test 0.166 : 0.101\n",
      "Idx 5 | Test 0.231 : 0.109\n",
      "Idx 6 | Test 0.076 : 0.096\n",
      "Epoch 1 finished in 1582.1\n",
      "Starting Epoch 2\n",
      "Batch 0 in 1.7s\n",
      "Idx 0 | Train 0.114 : 0.087\n",
      "Idx 1 | Train 0.176 : 0.096\n",
      "Idx 2 | Train 0.323 : 0.108\n",
      "Idx 3 | Train 0.094 : 0.076\n",
      "Idx 4 | Train 0.175 : 0.103\n",
      "Idx 5 | Train 0.254 : 0.111\n",
      "Idx 6 | Train 0.071 : 0.1\n",
      "Gradients: Encoder Input -2.08 | Encoder Message -1.28 | Encoder  Update -1.28 | Encoder Output -0.62\n",
      "Gradients: Decoder Input -1.31 | Decoder Message -1.3 | Decoder  Update -1.14 | Decoder Output -1.1\n",
      "\n",
      "Batch 200 in 1.7s\n",
      "Idx 0 | Train 0.075 : 0.092\n",
      "Idx 1 | Train 0.08 : 0.1\n",
      "Idx 2 | Train 0.204 : 0.11\n",
      "Idx 3 | Train 0.087 : 0.075\n",
      "Idx 4 | Train 0.141 : 0.108\n",
      "Idx 5 | Train 0.162 : 0.117\n",
      "Idx 6 | Train 0.058 : 0.091\n",
      "Gradients: Encoder Input -2.18 | Encoder Message -1.72 | Encoder  Update -1.73 | Encoder Output -1.83\n",
      "Gradients: Decoder Input -1.86 | Decoder Message -1.76 | Decoder  Update -1.63 | Decoder Output -1.79\n",
      "\n",
      "Batch 400 in 1.7s\n",
      "Idx 0 | Train 0.082 : 0.088\n",
      "Idx 1 | Train 0.064 : 0.095\n",
      "Idx 2 | Train 0.184 : 0.104\n",
      "Idx 3 | Train 0.08 : 0.074\n",
      "Idx 4 | Train 0.111 : 0.094\n",
      "Idx 5 | Train 0.154 : 0.105\n",
      "Idx 6 | Train 0.052 : 0.092\n",
      "Gradients: Encoder Input -2.23 | Encoder Message -1.95 | Encoder  Update -1.9 | Encoder Output -1.27\n",
      "Gradients: Decoder Input -2.14 | Decoder Message -1.94 | Decoder  Update -1.79 | Decoder Output -1.77\n",
      "\n",
      "Batch 600 in 1.7s\n",
      "Idx 0 | Train 0.085 : 0.084\n",
      "Idx 1 | Train 0.074 : 0.095\n",
      "Idx 2 | Train 0.166 : 0.104\n",
      "Idx 3 | Train 0.083 : 0.073\n",
      "Idx 4 | Train 0.127 : 0.098\n",
      "Idx 5 | Train 0.155 : 0.105\n",
      "Idx 6 | Train 0.05 : 0.091\n",
      "Gradients: Encoder Input -2.13 | Encoder Message -1.94 | Encoder  Update -1.78 | Encoder Output -1.12\n",
      "Gradients: Decoder Input -1.99 | Decoder Message -1.91 | Decoder  Update -1.82 | Decoder Output -2.16\n",
      "\n",
      "Batch 800 in 1.7s\n",
      "Idx 0 | Train 0.106 : 0.088\n",
      "Idx 1 | Train 0.088 : 0.094\n",
      "Idx 2 | Train 0.265 : 0.102\n",
      "Idx 3 | Train 0.1 : 0.074\n",
      "Idx 4 | Train 0.169 : 0.103\n",
      "Idx 5 | Train 0.188 : 0.105\n",
      "Idx 6 | Train 0.078 : 0.09\n",
      "Gradients: Encoder Input -2.05 | Encoder Message -1.26 | Encoder  Update -1.24 | Encoder Output -0.45\n",
      "Gradients: Decoder Input -1.35 | Decoder Message -1.29 | Decoder  Update -1.18 | Decoder Output -1.23\n",
      "\n",
      "Idx 0 | Test 0.079 : 0.089\n",
      "Idx 1 | Test 0.074 : 0.097\n",
      "Idx 2 | Test 0.172 : 0.108\n",
      "Idx 3 | Test 0.083 : 0.075\n",
      "Idx 4 | Test 0.116 : 0.1\n",
      "Idx 5 | Test 0.148 : 0.108\n",
      "Idx 6 | Test 0.044 : 0.091\n",
      "Epoch 2 finished in 1574.4\n",
      "Starting Epoch 3\n",
      "Batch 0 in 1.7s\n",
      "Idx 0 | Train 0.08 : 0.087\n",
      "Idx 1 | Train 0.072 : 0.106\n",
      "Idx 2 | Train 0.154 : 0.11\n",
      "Idx 3 | Train 0.087 : 0.074\n",
      "Idx 4 | Train 0.115 : 0.097\n",
      "Idx 5 | Train 0.147 : 0.108\n",
      "Idx 6 | Train 0.044 : 0.087\n",
      "Gradients: Encoder Input -2.22 | Encoder Message -1.87 | Encoder  Update -1.77 | Encoder Output -0.96\n",
      "Gradients: Decoder Input -2.02 | Decoder Message -1.88 | Decoder  Update -1.7 | Decoder Output -1.78\n",
      "\n",
      "Batch 200 in 1.7s\n",
      "Idx 0 | Train 0.086 : 0.089\n",
      "Idx 1 | Train 0.07 : 0.097\n",
      "Idx 2 | Train 0.191 : 0.104\n",
      "Idx 3 | Train 0.078 : 0.083\n",
      "Idx 4 | Train 0.11 : 0.104\n",
      "Idx 5 | Train 0.158 : 0.106\n",
      "Idx 6 | Train 0.054 : 0.093\n",
      "Gradients: Encoder Input -2.19 | Encoder Message -1.65 | Encoder  Update -1.46 | Encoder Output -0.7\n",
      "Gradients: Decoder Input -1.71 | Decoder Message -1.65 | Decoder  Update -1.48 | Decoder Output -1.8\n",
      "\n",
      "Batch 400 in 1.7s\n",
      "Idx 0 | Train 0.065 : 0.085\n",
      "Idx 1 | Train 0.068 : 0.097\n",
      "Idx 2 | Train 0.162 : 0.104\n",
      "Idx 3 | Train 0.081 : 0.076\n",
      "Idx 4 | Train 0.091 : 0.101\n",
      "Idx 5 | Train 0.143 : 0.098\n",
      "Idx 6 | Train 0.045 : 0.092\n",
      "Gradients: Encoder Input -2.19 | Encoder Message -1.91 | Encoder  Update -1.81 | Encoder Output -1.36\n",
      "Gradients: Decoder Input -2.14 | Decoder Message -1.92 | Decoder  Update -1.77 | Decoder Output -2.04\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600 in 1.7s\n",
      "Idx 0 | Train 0.093 : 0.083\n",
      "Idx 1 | Train 0.075 : 0.099\n",
      "Idx 2 | Train 0.169 : 0.096\n",
      "Idx 3 | Train 0.087 : 0.069\n",
      "Idx 4 | Train 0.092 : 0.098\n",
      "Idx 5 | Train 0.164 : 0.104\n",
      "Idx 6 | Train 0.061 : 0.085\n",
      "Gradients: Encoder Input -2.07 | Encoder Message -1.58 | Encoder  Update -1.4 | Encoder Output -0.41\n",
      "Gradients: Decoder Input -1.6 | Decoder Message -1.55 | Decoder  Update -1.36 | Decoder Output -1.44\n",
      "\n",
      "Batch 800 in 1.7s\n",
      "Idx 0 | Train 0.089 : 0.09\n",
      "Idx 1 | Train 0.074 : 0.095\n",
      "Idx 2 | Train 0.164 : 0.108\n",
      "Idx 3 | Train 0.109 : 0.069\n",
      "Idx 4 | Train 0.115 : 0.097\n",
      "Idx 5 | Train 0.161 : 0.099\n",
      "Idx 6 | Train 0.042 : 0.092\n",
      "Gradients: Encoder Input -2.09 | Encoder Message -1.47 | Encoder  Update -1.39 | Encoder Output -0.57\n",
      "Gradients: Decoder Input -1.55 | Decoder Message -1.54 | Decoder  Update -1.38 | Decoder Output -1.59\n",
      "\n",
      "Idx 0 | Test 0.073 : 0.086\n",
      "Idx 1 | Test 0.069 : 0.092\n",
      "Idx 2 | Test 0.157 : 0.101\n",
      "Idx 3 | Test 0.075 : 0.075\n",
      "Idx 4 | Test 0.083 : 0.098\n",
      "Idx 5 | Test 0.136 : 0.101\n",
      "Idx 6 | Test 0.039 : 0.09\n",
      "Epoch 3 finished in 1571.1\n",
      "Starting Epoch 4\n",
      "Batch 0 in 1.6s\n",
      "Idx 0 | Train 0.07 : 0.086\n",
      "Idx 1 | Train 0.058 : 0.096\n",
      "Idx 2 | Train 0.153 : 0.102\n",
      "Idx 3 | Train 0.073 : 0.072\n",
      "Idx 4 | Train 0.085 : 0.098\n",
      "Idx 5 | Train 0.138 : 0.102\n",
      "Idx 6 | Train 0.038 : 0.091\n",
      "Gradients: Encoder Input -2.32 | Encoder Message -1.99 | Encoder  Update -1.85 | Encoder Output -1.32\n",
      "Gradients: Decoder Input -2.05 | Decoder Message -1.85 | Decoder  Update -1.72 | Decoder Output -2.01\n",
      "\n",
      "Batch 200 in 1.7s\n",
      "Idx 0 | Train 0.063 : 0.085\n",
      "Idx 1 | Train 0.074 : 0.095\n",
      "Idx 2 | Train 0.149 : 0.103\n",
      "Idx 3 | Train 0.073 : 0.066\n",
      "Idx 4 | Train 0.081 : 0.097\n",
      "Idx 5 | Train 0.132 : 0.095\n",
      "Idx 6 | Train 0.042 : 0.085\n",
      "Gradients: Encoder Input -2.05 | Encoder Message -1.8 | Encoder  Update -1.69 | Encoder Output -0.86\n",
      "Gradients: Decoder Input -1.81 | Decoder Message -1.79 | Decoder  Update -1.64 | Decoder Output -1.94\n",
      "\n",
      "Batch 400 in 1.7s\n",
      "Idx 0 | Train 0.079 : 0.087\n",
      "Idx 1 | Train 0.06 : 0.09\n",
      "Idx 2 | Train 0.156 : 0.102\n",
      "Idx 3 | Train 0.065 : 0.071\n",
      "Idx 4 | Train 0.071 : 0.096\n",
      "Idx 5 | Train 0.129 : 0.095\n",
      "Idx 6 | Train 0.042 : 0.096\n",
      "Gradients: Encoder Input -2.15 | Encoder Message -1.93 | Encoder  Update -1.7 | Encoder Output -0.8\n",
      "Gradients: Decoder Input -1.92 | Decoder Message -1.9 | Decoder  Update -1.81 | Decoder Output -2.09\n",
      "\n",
      "Batch 600 in 1.7s\n",
      "Idx 0 | Train 0.074 : 0.084\n",
      "Idx 1 | Train 0.061 : 0.092\n",
      "Idx 2 | Train 0.15 : 0.102\n",
      "Idx 3 | Train 0.074 : 0.074\n",
      "Idx 4 | Train 0.127 : 0.103\n",
      "Idx 5 | Train 0.126 : 0.096\n",
      "Idx 6 | Train 0.038 : 0.087\n",
      "Gradients: Encoder Input -2.2 | Encoder Message -1.97 | Encoder  Update -1.85 | Encoder Output -1.24\n",
      "Gradients: Decoder Input -1.97 | Decoder Message -1.94 | Decoder  Update -1.79 | Decoder Output -2.05\n",
      "\n",
      "Batch 800 in 1.7s\n",
      "Idx 0 | Train 0.072 : 0.089\n",
      "Idx 1 | Train 0.07 : 0.091\n",
      "Idx 2 | Train 0.161 : 0.098\n",
      "Idx 3 | Train 0.108 : 0.079\n",
      "Idx 4 | Train 0.098 : 0.1\n",
      "Idx 5 | Train 0.133 : 0.103\n",
      "Idx 6 | Train 0.049 : 0.089\n",
      "Gradients: Encoder Input -2.11 | Encoder Message -1.42 | Encoder  Update -1.33 | Encoder Output -0.59\n",
      "Gradients: Decoder Input -1.65 | Decoder Message -1.51 | Decoder  Update -1.33 | Decoder Output -1.78\n",
      "\n",
      "Idx 0 | Test 0.07 : 0.085\n",
      "Idx 1 | Test 0.065 : 0.093\n",
      "Idx 2 | Test 0.153 : 0.101\n",
      "Idx 3 | Test 0.073 : 0.074\n",
      "Idx 4 | Test 0.095 : 0.096\n",
      "Idx 5 | Test 0.134 : 0.099\n",
      "Idx 6 | Test 0.038 : 0.086\n",
      "Epoch 4 finished in 1569.0\n",
      "Starting Epoch 5\n",
      "Batch 0 in 1.6s\n",
      "Idx 0 | Train 0.074 : 0.085\n",
      "Idx 1 | Train 0.065 : 0.094\n",
      "Idx 2 | Train 0.156 : 0.106\n",
      "Idx 3 | Train 0.075 : 0.071\n",
      "Idx 4 | Train 0.093 : 0.1\n",
      "Idx 5 | Train 0.13 : 0.092\n",
      "Idx 6 | Train 0.039 : 0.081\n",
      "Gradients: Encoder Input -2.22 | Encoder Message -1.72 | Encoder  Update -1.69 | Encoder Output -1.02\n",
      "Gradients: Decoder Input -1.72 | Decoder Message -1.69 | Decoder  Update -1.55 | Decoder Output -1.63\n",
      "\n",
      "Batch 200 in 1.7s\n",
      "Idx 0 | Train 0.073 : 0.08\n",
      "Idx 1 | Train 0.064 : 0.089\n",
      "Idx 2 | Train 0.137 : 0.096\n",
      "Idx 3 | Train 0.069 : 0.066\n",
      "Idx 4 | Train 0.092 : 0.098\n",
      "Idx 5 | Train 0.142 : 0.1\n",
      "Idx 6 | Train 0.035 : 0.088\n",
      "Gradients: Encoder Input -2.27 | Encoder Message -2.1 | Encoder  Update -1.86 | Encoder Output -1.59\n",
      "Gradients: Decoder Input -2.31 | Decoder Message -2.05 | Decoder  Update -1.94 | Decoder Output -2.29\n",
      "\n",
      "Batch 400 in 1.7s\n",
      "Idx 0 | Train 0.069 : 0.09\n",
      "Idx 1 | Train 0.047 : 0.091\n",
      "Idx 2 | Train 0.158 : 0.097\n",
      "Idx 3 | Train 0.076 : 0.074\n",
      "Idx 4 | Train 0.082 : 0.093\n",
      "Idx 5 | Train 0.139 : 0.094\n",
      "Idx 6 | Train 0.037 : 0.086\n",
      "Gradients: Encoder Input -2.22 | Encoder Message -1.77 | Encoder  Update -1.59 | Encoder Output -0.66\n",
      "Gradients: Decoder Input -1.66 | Decoder Message -1.85 | Decoder  Update -1.62 | Decoder Output -1.82\n",
      "\n",
      "Batch 600 in 1.7s\n",
      "Idx 0 | Train 0.067 : 0.087\n",
      "Idx 1 | Train 0.062 : 0.088\n",
      "Idx 2 | Train 0.144 : 0.095\n",
      "Idx 3 | Train 0.065 : 0.075\n",
      "Idx 4 | Train 0.081 : 0.096\n",
      "Idx 5 | Train 0.119 : 0.099\n",
      "Idx 6 | Train 0.034 : 0.084\n",
      "Gradients: Encoder Input -2.16 | Encoder Message -1.89 | Encoder  Update -1.82 | Encoder Output -1.22\n",
      "Gradients: Decoder Input -2.12 | Decoder Message -1.84 | Decoder  Update -1.73 | Decoder Output -1.95\n",
      "\n",
      "Batch 800 in 1.7s\n",
      "Idx 0 | Train 0.095 : 0.086\n",
      "Idx 1 | Train 0.072 : 0.091\n",
      "Idx 2 | Train 0.174 : 0.097\n",
      "Idx 3 | Train 0.226 : 0.071\n",
      "Idx 4 | Train 0.148 : 0.091\n",
      "Idx 5 | Train 0.135 : 0.095\n",
      "Idx 6 | Train 0.102 : 0.086\n",
      "Gradients: Encoder Input -1.96 | Encoder Message -1.14 | Encoder  Update -1.06 | Encoder Output -0.4\n",
      "Gradients: Decoder Input -1.45 | Decoder Message -1.13 | Decoder  Update -1.0 | Decoder Output -1.48\n",
      "\n",
      "Idx 0 | Test 0.066 : 0.085\n",
      "Idx 1 | Test 0.06 : 0.09\n",
      "Idx 2 | Test 0.146 : 0.098\n",
      "Idx 3 | Test 0.068 : 0.074\n",
      "Idx 4 | Test 0.075 : 0.094\n",
      "Idx 5 | Test 0.127 : 0.098\n",
      "Idx 6 | Test 0.035 : 0.086\n",
      "Epoch 5 finished in 1571.7\n",
      "Starting Epoch 6\n",
      "Batch 0 in 1.6s\n",
      "Idx 0 | Train 0.066 : 0.085\n",
      "Idx 1 | Train 0.069 : 0.09\n",
      "Idx 2 | Train 0.152 : 0.096\n",
      "Idx 3 | Train 0.074 : 0.073\n",
      "Idx 4 | Train 0.068 : 0.094\n",
      "Idx 5 | Train 0.132 : 0.095\n",
      "Idx 6 | Train 0.034 : 0.084\n",
      "Gradients: Encoder Input -2.23 | Encoder Message -1.71 | Encoder  Update -1.48 | Encoder Output -0.58\n",
      "Gradients: Decoder Input -1.81 | Decoder Message -1.7 | Decoder  Update -1.61 | Decoder Output -2.07\n",
      "\n",
      "Batch 200 in 1.7s\n",
      "Idx 0 | Train 0.061 : 0.08\n",
      "Idx 1 | Train 0.053 : 0.084\n",
      "Idx 2 | Train 0.134 : 0.087\n",
      "Idx 3 | Train 0.068 : 0.086\n",
      "Idx 4 | Train 0.075 : 0.095\n",
      "Idx 5 | Train 0.135 : 0.098\n",
      "Idx 6 | Train 0.035 : 0.084\n",
      "Gradients: Encoder Input -2.3 | Encoder Message -2.09 | Encoder  Update -1.94 | Encoder Output -0.96\n",
      "Gradients: Decoder Input -2.29 | Decoder Message -2.01 | Decoder  Update -1.91 | Decoder Output -2.4\n",
      "\n",
      "Batch 400 in 1.7s\n",
      "Idx 0 | Train 0.064 : 0.085\n",
      "Idx 1 | Train 0.064 : 0.094\n",
      "Idx 2 | Train 0.128 : 0.098\n",
      "Idx 3 | Train 0.057 : 0.078\n",
      "Idx 4 | Train 0.076 : 0.095\n",
      "Idx 5 | Train 0.118 : 0.094\n",
      "Idx 6 | Train 0.03 : 0.091\n",
      "Gradients: Encoder Input -2.18 | Encoder Message -1.89 | Encoder  Update -1.75 | Encoder Output -1.12\n",
      "Gradients: Decoder Input -1.85 | Decoder Message -1.89 | Decoder  Update -1.75 | Decoder Output -2.23\n",
      "\n",
      "Batch 600 in 1.7s\n",
      "Idx 0 | Train 0.068 : 0.085\n",
      "Idx 1 | Train 0.065 : 0.091\n",
      "Idx 2 | Train 0.153 : 0.097\n",
      "Idx 3 | Train 0.063 : 0.074\n",
      "Idx 4 | Train 0.08 : 0.089\n",
      "Idx 5 | Train 0.127 : 0.102\n",
      "Idx 6 | Train 0.037 : 0.083\n",
      "Gradients: Encoder Input -2.02 | Encoder Message -1.56 | Encoder  Update -1.3 | Encoder Output -0.38\n",
      "Gradients: Decoder Input -1.55 | Decoder Message -1.52 | Decoder  Update -1.39 | Decoder Output -1.79\n",
      "\n",
      "Batch 800 in 1.7s\n",
      "Idx 0 | Train 0.058 : 0.086\n",
      "Idx 1 | Train 0.063 : 0.093\n",
      "Idx 2 | Train 0.154 : 0.098\n",
      "Idx 3 | Train 0.063 : 0.068\n",
      "Idx 4 | Train 0.07 : 0.093\n",
      "Idx 5 | Train 0.12 : 0.092\n",
      "Idx 6 | Train 0.039 : 0.08\n",
      "Gradients: Encoder Input -2.04 | Encoder Message -1.58 | Encoder  Update -1.5 | Encoder Output -1.36\n",
      "Gradients: Decoder Input -1.77 | Decoder Message -1.51 | Decoder  Update -1.47 | Decoder Output -1.9\n",
      "\n",
      "Idx 0 | Test 0.064 : 0.084\n",
      "Idx 1 | Test 0.058 : 0.09\n",
      "Idx 2 | Test 0.141 : 0.095\n",
      "Idx 3 | Test 0.066 : 0.072\n",
      "Idx 4 | Test 0.069 : 0.094\n",
      "Idx 5 | Test 0.122 : 0.096\n",
      "Idx 6 | Test 0.032 : 0.084\n",
      "Epoch 6 finished in 1572.6\n",
      "Starting Epoch 7\n",
      "Batch 0 in 1.7s\n",
      "Idx 0 | Train 0.062 : 0.085\n",
      "Idx 1 | Train 0.056 : 0.082\n",
      "Idx 2 | Train 0.138 : 0.098\n",
      "Idx 3 | Train 0.074 : 0.07\n",
      "Idx 4 | Train 0.069 : 0.098\n",
      "Idx 5 | Train 0.122 : 0.096\n",
      "Idx 6 | Train 0.033 : 0.077\n",
      "Gradients: Encoder Input -2.18 | Encoder Message -1.84 | Encoder  Update -1.62 | Encoder Output -1.06\n",
      "Gradients: Decoder Input -2.06 | Decoder Message -1.82 | Decoder  Update -1.68 | Decoder Output -2.11\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200 in 1.7s\n",
      "Idx 0 | Train 0.061 : 0.081\n",
      "Idx 1 | Train 0.055 : 0.088\n",
      "Idx 2 | Train 0.126 : 0.092\n",
      "Idx 3 | Train 0.066 : 0.075\n",
      "Idx 4 | Train 0.072 : 0.085\n",
      "Idx 5 | Train 0.119 : 0.095\n",
      "Idx 6 | Train 0.03 : 0.082\n",
      "Gradients: Encoder Input -2.05 | Encoder Message -1.92 | Encoder  Update -1.85 | Encoder Output -0.8\n",
      "Gradients: Decoder Input -1.94 | Decoder Message -1.95 | Decoder  Update -1.79 | Decoder Output -2.06\n",
      "\n",
      "Batch 400 in 1.7s\n",
      "Idx 0 | Train 0.063 : 0.083\n",
      "Idx 1 | Train 0.049 : 0.1\n",
      "Idx 2 | Train 0.138 : 0.091\n",
      "Idx 3 | Train 0.074 : 0.077\n",
      "Idx 4 | Train 0.063 : 0.091\n",
      "Idx 5 | Train 0.117 : 0.099\n",
      "Idx 6 | Train 0.03 : 0.083\n",
      "Gradients: Encoder Input -2.19 | Encoder Message -1.79 | Encoder  Update -1.67 | Encoder Output -0.9\n",
      "Gradients: Decoder Input -1.91 | Decoder Message -1.81 | Decoder  Update -1.64 | Decoder Output -1.93\n",
      "\n",
      "Batch 600 in 1.7s\n",
      "Idx 0 | Train 0.06 : 0.083\n",
      "Idx 1 | Train 0.053 : 0.086\n",
      "Idx 2 | Train 0.134 : 0.097\n",
      "Idx 3 | Train 0.063 : 0.071\n",
      "Idx 4 | Train 0.07 : 0.092\n",
      "Idx 5 | Train 0.115 : 0.099\n",
      "Idx 6 | Train 0.036 : 0.081\n",
      "Gradients: Encoder Input -2.23 | Encoder Message -1.83 | Encoder  Update -1.74 | Encoder Output -1.11\n",
      "Gradients: Decoder Input -1.91 | Decoder Message -1.82 | Decoder  Update -1.74 | Decoder Output -2.15\n",
      "\n",
      "Batch 800 in 1.7s\n",
      "Idx 0 | Train 0.057 : 0.085\n",
      "Idx 1 | Train 0.053 : 0.093\n",
      "Idx 2 | Train 0.122 : 0.093\n",
      "Idx 3 | Train 0.078 : 0.068\n",
      "Idx 4 | Train 0.059 : 0.086\n",
      "Idx 5 | Train 0.114 : 0.088\n",
      "Idx 6 | Train 0.032 : 0.083\n",
      "Gradients: Encoder Input -2.18 | Encoder Message -1.94 | Encoder  Update -1.81 | Encoder Output -1.02\n",
      "Gradients: Decoder Input -1.94 | Decoder Message -1.96 | Decoder  Update -1.84 | Decoder Output -2.32\n",
      "\n",
      "Idx 0 | Test 0.063 : 0.084\n",
      "Idx 1 | Test 0.058 : 0.089\n",
      "Idx 2 | Test 0.141 : 0.096\n",
      "Idx 3 | Test 0.065 : 0.072\n",
      "Idx 4 | Test 0.067 : 0.093\n",
      "Idx 5 | Test 0.12 : 0.095\n",
      "Idx 6 | Test 0.032 : 0.084\n",
      "Epoch 7 finished in 1577.5\n",
      "Starting Epoch 8\n",
      "Batch 0 in 1.7s\n",
      "Idx 0 | Train 0.065 : 0.081\n",
      "Idx 1 | Train 0.06 : 0.099\n",
      "Idx 2 | Train 0.131 : 0.089\n",
      "Idx 3 | Train 0.062 : 0.076\n",
      "Idx 4 | Train 0.065 : 0.088\n",
      "Idx 5 | Train 0.112 : 0.089\n",
      "Idx 6 | Train 0.031 : 0.081\n",
      "Gradients: Encoder Input -1.95 | Encoder Message -1.7 | Encoder  Update -1.52 | Encoder Output -0.53\n",
      "Gradients: Decoder Input -1.56 | Decoder Message -1.7 | Decoder  Update -1.55 | Decoder Output -2.04\n",
      "\n",
      "Batch 200 in 1.7s\n",
      "Idx 0 | Train 0.067 : 0.084\n",
      "Idx 1 | Train 0.047 : 0.088\n",
      "Idx 2 | Train 0.153 : 0.102\n",
      "Idx 3 | Train 0.061 : 0.071\n",
      "Idx 4 | Train 0.064 : 0.09\n",
      "Idx 5 | Train 0.113 : 0.093\n",
      "Idx 6 | Train 0.029 : 0.081\n",
      "Gradients: Encoder Input -2.22 | Encoder Message -1.98 | Encoder  Update -1.9 | Encoder Output -1.23\n",
      "Gradients: Decoder Input -2.14 | Decoder Message -1.92 | Decoder  Update -1.86 | Decoder Output -2.14\n",
      "\n",
      "Batch 400 in 1.7s\n",
      "Idx 0 | Train 0.062 : 0.082\n",
      "Idx 1 | Train 0.063 : 0.091\n",
      "Idx 2 | Train 0.127 : 0.099\n",
      "Idx 3 | Train 0.067 : 0.08\n",
      "Idx 4 | Train 0.071 : 0.096\n",
      "Idx 5 | Train 0.126 : 0.091\n",
      "Idx 6 | Train 0.029 : 0.082\n",
      "Gradients: Encoder Input -2.04 | Encoder Message -2.05 | Encoder  Update -1.91 | Encoder Output -1.19\n",
      "Gradients: Decoder Input -2.04 | Decoder Message -2.0 | Decoder  Update -1.85 | Decoder Output -2.32\n",
      "\n",
      "Batch 600 in 1.7s\n",
      "Idx 0 | Train 0.063 : 0.084\n",
      "Idx 1 | Train 0.053 : 0.092\n",
      "Idx 2 | Train 0.137 : 0.098\n",
      "Idx 3 | Train 0.072 : 0.1\n",
      "Idx 4 | Train 0.066 : 0.093\n",
      "Idx 5 | Train 0.122 : 0.1\n",
      "Idx 6 | Train 0.028 : 0.084\n",
      "Gradients: Encoder Input -2.15 | Encoder Message -1.77 | Encoder  Update -1.68 | Encoder Output -1.24\n",
      "Gradients: Decoder Input -2.12 | Decoder Message -1.84 | Decoder  Update -1.73 | Decoder Output -2.3\n",
      "\n",
      "Batch 800 in 1.7s\n",
      "Idx 0 | Train 0.059 : 0.083\n",
      "Idx 1 | Train 0.063 : 0.094\n",
      "Idx 2 | Train 0.148 : 0.096\n",
      "Idx 3 | Train 0.064 : 0.069\n",
      "Idx 4 | Train 0.069 : 0.091\n",
      "Idx 5 | Train 0.111 : 0.094\n",
      "Idx 6 | Train 0.03 : 0.085\n",
      "Gradients: Encoder Input -2.21 | Encoder Message -1.73 | Encoder  Update -1.49 | Encoder Output -0.67\n",
      "Gradients: Decoder Input -1.71 | Decoder Message -1.74 | Decoder  Update -1.59 | Decoder Output -1.92\n",
      "\n",
      "Idx 0 | Test 0.061 : 0.084\n",
      "Idx 1 | Test 0.057 : 0.09\n",
      "Idx 2 | Test 0.137 : 0.095\n",
      "Idx 3 | Test 0.064 : 0.072\n",
      "Idx 4 | Test 0.064 : 0.093\n",
      "Idx 5 | Test 0.117 : 0.095\n",
      "Idx 6 | Test 0.03 : 0.084\n",
      "Epoch 8 finished in 1579.7\n",
      "Starting Epoch 9\n",
      "Batch 0 in 1.7s\n",
      "Idx 0 | Train 0.057 : 0.079\n",
      "Idx 1 | Train 0.05 : 0.086\n",
      "Idx 2 | Train 0.135 : 0.095\n",
      "Idx 3 | Train 0.058 : 0.065\n",
      "Idx 4 | Train 0.069 : 0.094\n",
      "Idx 5 | Train 0.119 : 0.089\n",
      "Idx 6 | Train 0.032 : 0.081\n",
      "Gradients: Encoder Input -2.19 | Encoder Message -1.92 | Encoder  Update -1.81 | Encoder Output -0.83\n",
      "Gradients: Decoder Input -1.92 | Decoder Message -1.9 | Decoder  Update -1.78 | Decoder Output -2.39\n",
      "\n",
      "Batch 200 in 1.7s\n",
      "Idx 0 | Train 0.058 : 0.079\n",
      "Idx 1 | Train 0.046 : 0.09\n",
      "Idx 2 | Train 0.137 : 0.09\n",
      "Idx 3 | Train 0.061 : 0.07\n",
      "Idx 4 | Train 0.055 : 0.097\n",
      "Idx 5 | Train 0.112 : 0.094\n",
      "Idx 6 | Train 0.029 : 0.081\n",
      "Gradients: Encoder Input -2.11 | Encoder Message -2.12 | Encoder  Update -2.04 | Encoder Output -1.33\n",
      "Gradients: Decoder Input -2.37 | Decoder Message -2.18 | Decoder  Update -2.05 | Decoder Output -2.32\n",
      "\n",
      "Batch 400 in 1.7s\n",
      "Idx 0 | Train 0.061 : 0.084\n",
      "Idx 1 | Train 0.052 : 0.084\n",
      "Idx 2 | Train 0.131 : 0.097\n",
      "Idx 3 | Train 0.055 : 0.069\n",
      "Idx 4 | Train 0.064 : 0.09\n",
      "Idx 5 | Train 0.11 : 0.089\n",
      "Idx 6 | Train 0.029 : 0.084\n",
      "Gradients: Encoder Input -2.16 | Encoder Message -2.01 | Encoder  Update -1.87 | Encoder Output -1.21\n",
      "Gradients: Decoder Input -2.23 | Decoder Message -2.03 | Decoder  Update -1.97 | Decoder Output -2.55\n",
      "\n",
      "Batch 600 in 1.7s\n",
      "Idx 0 | Train 0.055 : 0.083\n",
      "Idx 1 | Train 0.067 : 0.098\n",
      "Idx 2 | Train 0.15 : 0.097\n",
      "Idx 3 | Train 0.077 : 0.072\n",
      "Idx 4 | Train 0.06 : 0.094\n",
      "Idx 5 | Train 0.111 : 0.099\n",
      "Idx 6 | Train 0.024 : 0.08\n",
      "Gradients: Encoder Input -2.2 | Encoder Message -1.89 | Encoder  Update -1.72 | Encoder Output -0.99\n",
      "Gradients: Decoder Input -1.95 | Decoder Message -1.83 | Decoder  Update -1.75 | Decoder Output -2.3\n",
      "\n",
      "Batch 800 in 1.7s\n",
      "Idx 0 | Train 0.072 : 0.081\n",
      "Idx 1 | Train 0.061 : 0.088\n",
      "Idx 2 | Train 0.124 : 0.091\n",
      "Idx 3 | Train 0.075 : 0.076\n",
      "Idx 4 | Train 0.061 : 0.093\n",
      "Idx 5 | Train 0.11 : 0.095\n",
      "Idx 6 | Train 0.03 : 0.08\n",
      "Gradients: Encoder Input -2.15 | Encoder Message -1.94 | Encoder  Update -1.74 | Encoder Output -0.8\n",
      "Gradients: Decoder Input -1.82 | Decoder Message -1.85 | Decoder  Update -1.72 | Decoder Output -2.34\n",
      "\n",
      "Idx 0 | Test 0.06 : 0.084\n",
      "Idx 1 | Test 0.056 : 0.091\n",
      "Idx 2 | Test 0.135 : 0.095\n",
      "Idx 3 | Test 0.063 : 0.071\n",
      "Idx 4 | Test 0.063 : 0.093\n",
      "Idx 5 | Test 0.114 : 0.095\n",
      "Idx 6 | Test 0.03 : 0.083\n",
      "Epoch 9 finished in 1580.3\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    \n",
    "    print('Starting Epoch {}'.format(epoch))\n",
    "    epoch_t0 = time.time()\n",
    "    \n",
    "    for morphIdx in trainingIdxs:\n",
    "        permutation = np.random.permutation(X_train[morphIdx].shape[0])\n",
    "        X_train[morphIdx] = X_train[morphIdx][permutation]\n",
    "        Y_train[morphIdx] = Y_train[morphIdx][permutation]\n",
    "        \n",
    "    stepLoss = None\n",
    "\n",
    "    for batch in range(0, numTrainingBatches-1, numBatchesPerTrainingStep):\n",
    "                \n",
    "        t0 = time.time()\n",
    "        \n",
    "        for morphIdx in trainingIdxs:\n",
    "            numNodes = (X_train[morphIdx].shape[1] - 5) // 2\n",
    "            trainLosses[morphIdx].append(np.zeros(2))\n",
    "        \n",
    "        \n",
    "        for batchOffset in range(numBatchesPerTrainingStep):\n",
    "                        \n",
    "            if batch + batchOffset >= numTrainingBatches - 1:\n",
    "                break\n",
    "                \n",
    "            for morphIdx in trainingIdxs:\n",
    "                encoder_graph = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                decoder_graph = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                                \n",
    "                current_states = X_train[morphIdx][(batch+batchOffset) * batch_size:(batch+batchOffset+1)*batch_size]\n",
    "                next_states = Y_train[morphIdx][(batch+batchOffset) * batch_size:(batch+batchOffset+1)*batch_size]\n",
    "                random_indexes = np.random.choice(X_train[0].shape[0],size=current_states.shape[0], replace=False)\n",
    "                random_states = X_train[morphIdx][random_indexes]\n",
    "                \n",
    "                encoderInput = torch.cat((current_states, next_states, random_states), dim=0).to(device)\n",
    "                latent_states = encoderGNN(encoder_graph, encoderInput)\n",
    "                normalized_latent_states = latent_states / torch.sqrt(1e-8 + (latent_states ** 2).sum(dim=-1)).unsqueeze(2)\n",
    "                \n",
    "                current_state_reconstruction = decoderGNN(decoder_graph, normalized_latent_states[:, 0:current_states.shape[0], :])\n",
    "                current_state_reconstruction[:, :, 0:5] = current_state_reconstruction[:, :, 0:5].mean(dim=0)\n",
    "                autoencoder_loss = criterion(encoder_graph.ndata['input'][:, 0:batch_size, :7], current_state_reconstruction).mean()\n",
    "                \n",
    "                # Calculate 2-norm for positive/sequential samples over the data dimension - result is of dimension (nodes, batch_size)\n",
    "                sequential_distances = torch.norm(normalized_latent_states[:, 0:batch_size, :] - normalized_latent_states[:, batch_size:batch_size * 2, :], p=None, dim=2)\n",
    "                # Calculate 2-norm for negative/random samples over data dimension - result is of dimension (nodes, batch_size)\n",
    "                random_distances = torch.norm(normalized_latent_states[:, 0:batch_size, :] - normalized_latent_states[:, 2 * batch_size: 3 * batch_size, :], p=None, dim=2)\n",
    "                # Calculate contrastive loss for each entry - result is of dimension (nodes, batch_size)\n",
    "                contrastive_loss = torch.max(zeroTensor, sequential_distances - random_distances + minDistanceSeqAndRand)\n",
    "                # get 0-1 matrix which is True if entry is not 0\n",
    "                mask = contrastive_loss != 0\n",
    "                # Compute average over nonzero entries in batch, result will be scalar\n",
    "                final_contrastive_loss = contrastive_loss.sum() / mask.sum()\n",
    "                \n",
    "                # contrastive_loss_1 = torch.max(torch.zeros(1).to(device), criterion(latent_states[:, 0:batch_size, :], latent_states[:, batch_size:batch_size * 2, :]).mean() - maxSequentialDistance).mean()\n",
    "                # contrastive_loss_2 = torch.max(torch.zeros(1).to(device), minRandomDistance - criterion(latent_states[:, 0:batch_size, :], latent_states[:, 2 * batch_size: 3 * batch_size, :]).mean()).mean()\n",
    "                \n",
    "                trainLosses[morphIdx][-1][0] += autoencoder_loss.item() / numBatchesPerTrainingStep\n",
    "                trainLosses[morphIdx][-1][1] += final_contrastive_loss.item() / numBatchesPerTrainingStep\n",
    "            \n",
    "                stepLoss = autoencoder_loss + final_contrastive_loss\n",
    "                stepLoss /= (len(trainingIdxs) * numBatchesPerTrainingStep)\n",
    "                stepLoss.backward()\n",
    "                        \n",
    "        \n",
    "        if batch % 200 == 0:\n",
    "            print('Batch {} in {}s'.format(batch, np.round(time.time() - t0, decimals=1)))\n",
    "            for morphIdx in trainingIdxs:\n",
    "                print('Idx {} | Train {} : {}'.format(\n",
    "                    morphIdx, np.round(trainLosses[morphIdx][-1][0], decimals=3), \n",
    "                    np.round(trainLosses[morphIdx][-1][1], decimals=3)))\n",
    "\n",
    "            \n",
    "            s = 0\n",
    "            for parameter in encoderInputNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            encoderInputNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0\n",
    "            for parameter in encoderMessageNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            encoderMessageNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0\n",
    "            for parameter in encoderUpdateNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            encoderUpdateNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0\n",
    "            for parameter in encoderOutputNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            encoderOutputNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0\n",
    "            for parameter in decoderInputNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            decoderInputNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0\n",
    "            for parameter in decoderMessageNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            decoderMessageNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0\n",
    "            for parameter in decoderUpdateNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            decoderUpdateNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0\n",
    "            for parameter in decoderOutputNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            decoderOutputNetworkGradients.append(s.item())\n",
    "\n",
    "            print('Gradients: Encoder Input {} | Encoder Message {} | Encoder  Update {} | Encoder Output {}'.format(\n",
    "                np.round(np.log10(encoderInputNetworkGradients[-1]), decimals=2), np.round(np.log10(encoderMessageNetworkGradients[-1]), decimals=2), np.round(np.log10(encoderUpdateNetworkGradients[-1]), decimals=2), np.round(np.log10(encoderOutputNetworkGradients[-1]), decimals=2)))    \n",
    "\n",
    "            print('Gradients: Decoder Input {} | Decoder Message {} | Decoder  Update {} | Decoder Output {}'.format(\n",
    "                np.round(np.log10(decoderInputNetworkGradients[-1]), decimals=2), np.round(np.log10(decoderMessageNetworkGradients[-1]), decimals=2), np.round(np.log10(decoderUpdateNetworkGradients[-1]), decimals=2), np.round(np.log10(decoderOutputNetworkGradients[-1]), decimals=2)))\n",
    "            \n",
    "            print()\n",
    "            \n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # Dereference variables to release memory\n",
    "    stepLoss = None\n",
    "    encoder_graph = None\n",
    "    decoder_graph = None\n",
    "    encoderInput = None\n",
    "    latent_states = None\n",
    "    normalized_latent_states = None\n",
    "    current_state_reconstruction = None\n",
    "    autoencoder_loss = None\n",
    "    contrastive_loss_1 = None\n",
    "    contrastive_loss_2 = None\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for morphIdx in trainingIdxs:\n",
    "            testLosses[morphIdx].append(np.zeros(2))\n",
    "            for batch_ in range(0, numTestingBatches-1):\n",
    "\n",
    "                encoder_graph = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                decoder_graph = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "\n",
    "                current_states = X_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "                next_states = Y_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "                random_indexes = np.random.choice(X_train[0].shape[0],size=batch_size, replace=False)\n",
    "                random_states = X_train[morphIdx][random_indexes]\n",
    "\n",
    "                encoderInput = torch.cat((current_states, next_states, random_states), dim=0).to(device)\n",
    "\n",
    "                latent_states = encoderGNN(encoder_graph, encoderInput)\n",
    "                normalized_latent_states = latent_states / torch.sqrt(1e-8 + (latent_states ** 2).sum(dim=-1)).unsqueeze(2)\n",
    "                current_state_reconstruction = decoderGNN(decoder_graph, normalized_latent_states[:, 0:current_states.shape[0], :])\n",
    "                current_state_reconstruction[:, :, 0:5] = current_state_reconstruction[:, :, 0:5].mean(dim=0)\n",
    "\n",
    "                autoencoder_loss = criterion(encoder_graph.ndata['input'][:, 0:batch_size, :7], current_state_reconstruction).mean()\n",
    "                # Calculate 2-norm for positive/sequential samples over the data dimension - result is of dimension (nodes, batch_size)\n",
    "                sequential_distances = torch.norm(normalized_latent_states[:, 0:batch_size, :] - normalized_latent_states[:, batch_size:batch_size * 2, :], p=None, dim=2)\n",
    "                # Calculate 2-norm for negative/random samples over data dimension - result is of dimension (nodes, batch_size)\n",
    "                random_distances = torch.norm(normalized_latent_states[:, 0:batch_size, :] - normalized_latent_states[:, 2 * batch_size: 3 * batch_size, :], p=None, dim=2)\n",
    "                # Calculate contrastive loss for each entry - result is of dimension (nodes, batch_size)\n",
    "                contrastive_loss = torch.max(zeroTensor, sequential_distances - random_distances + minDistanceSeqAndRand)\n",
    "                # get 0-1 matrix which is True if entry is not 0\n",
    "                mask = contrastive_loss != 0\n",
    "                # Compute average over nonzero entries in batch, result will be scalar\n",
    "                final_contrastive_loss = contrastive_loss.sum() / mask.sum()\n",
    "                \n",
    "#                 contrastive_loss_1 = torch.max(torch.zeros(1).to(device), criterion(latent_states[:, 0:batch_size, :], latent_states[:, batch_size:batch_size * 2, :]).mean() - maxSequentialDistance).mean()\n",
    "#                 contrastive_loss_2 = torch.max(torch.zeros(1).to(device), minRandomDistance - criterion(latent_states[:, 0:batch_size, :], latent_states[:, 2 * batch_size: 3 * batch_size, :]).mean()).mean()\n",
    "                \n",
    "                testLosses[morphIdx][-1][0] += autoencoder_loss.item() / numBatchesPerTrainingStep\n",
    "                testLosses[morphIdx][-1][1] += final_contrastive_loss.item() / numBatchesPerTrainingStep\n",
    "            testLosses[morphIdx][-1] /= numTestingBatches-1\n",
    "        \n",
    "        \n",
    "        t_final = time.time() - t0\n",
    "\n",
    "        for morphIdx in trainingIdxs:\n",
    "            print('Idx {} | Test {} : {}'.format(\n",
    "                morphIdx, np.round(testLosses[morphIdx][-1][0], decimals=3), np.round(testLosses[morphIdx][-1][1], decimals=3)))\n",
    "\n",
    "    print('Epoch {} finished in {}'.format(epoch, np.round(time.time() - epoch_t0, decimals=1)))\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, sharex=True)\n",
    "for morphIdx in trainingIdxs:\n",
    "    lossArr = np.sum(np.array(testLosses[morphIdx]), 1)\n",
    "    ax.plot(range(lossArr.shape[0]-1), lossArr[1:])\n",
    "for morphIdx in validationIdxs:\n",
    "    lossArr = np.sum(np.array(validLosses[morphIdx]), 1)\n",
    "    ax.plot(range(lossArr.shape[0]), lossArr)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('L2 Loss')\n",
    "plt.title('Testing Set Reconstruction Loss Per Morphology')\n",
    "plt.legend(trainingIdxs + validationIdxs)\n",
    "plt.savefig('time-contrastive-losses.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoderGNN.state_dict(), 'encoderGNN-normalized-time-contrastive.pt')\n",
    "torch.save(decoderGNN.state_dict(), 'decoderGNN-normalized-time-contrastive.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainingParameters.txt', 'w') as file:\n",
    "    file.write('hidden_sizes ' + str(hidden_sizes) + '\\n')\n",
    "    file.write('inputSize ' + str(inputSize) + '\\n')\n",
    "    file.write('stateSize ' + str(stateSize) + '\\n')\n",
    "    file.write('latentSize ' + str(latentSize) + '\\n')\n",
    "    file.write('numMessagePassingIterations ' + str(numMessagePassingIterations) + '\\n')\n",
    "    file.write('batch_size ' + str(batch_size) + '\\n')\n",
    "    file.write('numBatchesPerTrainingStep ' + str(numBatchesPerTrainingStep) + '\\n')\n",
    "    file.write('minRandomDistance ' + str(minRandomDistance) + '\\n')\n",
    "    file.write('maxSequentialDistance ' + str(maxSequentialDistance) + '\\n')\n",
    "    file.write('with_batch_norm ' + str(with_batch_norm) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
