{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.autograd\n",
    "import os\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet as p \n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import pybullet \n",
    "import pybullet_envs.gym_pendulum_envs \n",
    "import pybullet_envs.gym_locomotion_envs\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import dgl\n",
    "import morphsim as m\n",
    "from graphenvs import HalfCheetahGraphEnv\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        hidden_sizes,\n",
    "        batch_size,\n",
    "        with_batch_norm=False,\n",
    "        activation=None\n",
    "    ):\n",
    "        super(Network, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        if with_batch_norm:\n",
    "            self.layers.append(nn.BatchNorm1d(batch_size))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            if with_batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(batch_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], self.output_size))\n",
    "        \n",
    "        if activation is not None:\n",
    "            self.layers.append(activation())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputNetwork,\n",
    "        messageNetwork,\n",
    "        updateNetwork,\n",
    "        outputNetwork,\n",
    "        numMessagePassingIterations,\n",
    "        encoder = True\n",
    "    ):\n",
    "        \n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "                \n",
    "        self.inputNetwork = inputNetwork\n",
    "        self.messageNetwork = messageNetwork\n",
    "        self.updateNetwork = updateNetwork\n",
    "        self.outputNetwork = outputNetwork\n",
    "        \n",
    "        self.numMessagePassingIterations = numMessagePassingIterations\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    def inputFunction(self, nodes):\n",
    "        return {'state' : self.inputNetwork(nodes.data['input'])}\n",
    "    \n",
    "    def messageFunction(self, edges):\n",
    "        \n",
    "        edgeData = edges.data['feature'].repeat(edges.src['state'].shape[1], 1).T.unsqueeze(-1)\n",
    "        return {'m' : self.messageNetwork(torch.cat((edges.src['state'], edgeData), -1))}\n",
    "        \n",
    " \n",
    "    def updateFunction(self, nodes):\n",
    "        return {'state': self.updateNetwork(torch.cat((nodes.data['m_hat'], nodes.data['state']), -1))}\n",
    "    \n",
    "    def outputFunction(self, nodes):\n",
    "        \n",
    "#         numNodes, batchSize, stateSize = graph.ndata['state'].shape\n",
    "#         return self.outputNetwork.forward(graph.ndata['state'])\n",
    "        return {'output': self.outputNetwork(nodes.data['state'])}\n",
    "\n",
    "\n",
    "    def forward(self, graph, state):\n",
    "        \n",
    "        self.update_states_in_graph(graph, state)\n",
    "        \n",
    "        graph.apply_nodes(self.inputFunction)\n",
    "        \n",
    "        for messagePassingIteration in range(self.numMessagePassingIterations):\n",
    "            graph.update_all(self.messageFunction, dgl.function.max('m', 'm_hat'), self.updateFunction)\n",
    "        \n",
    "        graph.apply_nodes(self.outputFunction)\n",
    "        \n",
    "        output = graph.ndata['output']\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def update_states_in_graph(self, graph, state):\n",
    "        \n",
    "        if self.encoder:\n",
    "            if len(state.shape) == 1:\n",
    "                state = state.unsqueeze(0)\n",
    "\n",
    "            numGraphFeature = 6\n",
    "            numGlobalStateInformation = 5\n",
    "            numLocalStateInformation = 2\n",
    "            numStateVar = state.shape[1] \n",
    "            globalInformation = state[:, 0:5]\n",
    "\n",
    "            numNodes = (numStateVar - 5) // 2\n",
    "\n",
    "            nodeData = torch.empty((numNodes, state.shape[0], numGraphFeature + numGlobalStateInformation + numLocalStateInformation)).to(device)\n",
    "\n",
    "            nodeData[:, :, 0:numGlobalStateInformation] = globalInformation\n",
    "\n",
    "            for nodeIdx in range(numNodes):\n",
    "                # Assign global features from graph\n",
    "                nodeData[nodeIdx, :, numGlobalStateInformation:numGlobalStateInformation + numGraphFeature] = graph.ndata['feature'][nodeIdx]\n",
    "                # Assign local state information\n",
    "                nodeData[nodeIdx, :, numGlobalStateInformation + numGraphFeature] = state[:, 5 + nodeIdx]\n",
    "                nodeData[nodeIdx, :, numGlobalStateInformation + numGraphFeature + 1] = state[:, 5 + numNodes + nodeIdx]\n",
    "\n",
    "            graph.ndata['input'] = nodeData\n",
    "        \n",
    "        else:\n",
    "            numNodes, batchSize, inputSize = state.shape\n",
    "            nodeData = torch.empty((numNodes, batchSize, inputSize + 6))\n",
    "            nodeData[:, :, :inputSize] = state\n",
    "            for nodeIdx in range(numNodes):\n",
    "                nodeData[nodeIdx, :, inputSize : inputSize + 6] = graph.ndata['feature'][nodeIdx]\n",
    "                \n",
    "            graph.ndata['input'] = nodeData.to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ovi/anaconda3/envs/honors-project/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "NoneType: None\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    }
   ],
   "source": [
    "states = {}\n",
    "actions = {}\n",
    "rewards = {}\n",
    "next_states = {}\n",
    "dones = {}\n",
    "env = {}\n",
    "\n",
    "for morphIdx in range(7):\n",
    "\n",
    "    prefix = '../datasets/{}/'.format(morphIdx)\n",
    "    \n",
    "    states[morphIdx] = np.load(prefix + 'states_array.npy')\n",
    "    actions[morphIdx] = np.load(prefix + 'actions_array.npy')\n",
    "    rewards[morphIdx] = np.load(prefix + 'rewards_array.npy')\n",
    "    next_states[morphIdx] = np.load(prefix + 'next_states_array.npy')\n",
    "    dones[morphIdx] = np.load(prefix + 'dones_array.npy')\n",
    "    \n",
    "    env[morphIdx] = HalfCheetahGraphEnv(None)\n",
    "    env[morphIdx].set_morphology(morphIdx)\n",
    "    env[morphIdx].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = {}\n",
    "X_train = {}\n",
    "Y_test = {}\n",
    "Y_train = {}\n",
    "\n",
    "for morphIdx in range(7):\n",
    "    X = states[morphIdx]\n",
    "    Y = next_states[morphIdx]\n",
    "    permutation = np.random.permutation(X.shape[0])\n",
    "    X = X[permutation]\n",
    "    X_test[morphIdx] = torch.from_numpy(X[:100000]).float()\n",
    "    X_train[morphIdx] = torch.from_numpy(X[100000:]).float()\n",
    "    Y = Y[permutation]\n",
    "    Y_test[morphIdx] = torch.from_numpy(Y[:100000]).float()\n",
    "    Y_train[morphIdx] = torch.from_numpy(Y[100000:]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [64, 64]\n",
    "\n",
    "inputSize = 13\n",
    "stateSize = 32\n",
    "messageSize = 32\n",
    "latentSize = 16\n",
    "numMessagePassingIterations = 4\n",
    "batch_size = 1024\n",
    "numBatchesPerTrainingStep = 1\n",
    "minRandomDistance = 1\n",
    "\n",
    "# Encoder Networks \n",
    "encoderInputNetwork = Network(inputSize, stateSize, hidden_sizes, batch_size)\n",
    "encoderMessageNetwork = Network(stateSize + 1, messageSize, hidden_sizes, batch_size, activation=nn.Tanh)\n",
    "encoderUpdateNetwork = Network(stateSize + messageSize, stateSize, hidden_sizes, batch_size)\n",
    "encoderOutputNetwork = Network(stateSize, latentSize, hidden_sizes, batch_size, activation=nn.Tanh)\n",
    "encoderGNN = GraphNeuralNetwork(encoderInputNetwork, encoderMessageNetwork, encoderUpdateNetwork, encoderOutputNetwork, numMessagePassingIterations, encoder=True).to(device)\n",
    "\n",
    "# Decoder Networks\n",
    "decoderInputNetwork = Network(latentSize + 6, stateSize, hidden_sizes, batch_size)\n",
    "decoderMessageNetwork = Network(stateSize + 1, messageSize, hidden_sizes, batch_size, activation=nn.Tanh)\n",
    "decoderUpdateNetwork = Network(stateSize + messageSize, stateSize, hidden_sizes, batch_size)\n",
    "decoderOutputNetwork = Network(stateSize, inputSize, hidden_sizes, batch_size, activation=nn.Tanh)\n",
    "decoderGNN = GraphNeuralNetwork(decoderInputNetwork, decoderMessageNetwork, decoderUpdateNetwork, decoderOutputNetwork, numMessagePassingIterations, encoder=False).to(device)\n",
    "\n",
    "# Optimizer\n",
    "lr = 1e-4\n",
    "optimizer = optim.Adam(itertools.chain(\n",
    "                    encoderInputNetwork.parameters(), encoderMessageNetwork.parameters(), \n",
    "                    encoderUpdateNetwork.parameters(), encoderOutputNetwork.parameters(),\n",
    "                    decoderInputNetwork.parameters(), decoderMessageNetwork.parameters(), \n",
    "                    decoderUpdateNetwork.parameters(), decoderOutputNetwork.parameters()),\n",
    "                    lr, weight_decay=0)\n",
    "\n",
    "lr_lambda = lambda epoch: 0.8\n",
    "lr_scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda)\n",
    "criterion  = nn.L1Loss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 49 in 0.0s - Train AE 0.917 CO 1.0 | Test AE 0.918 CO 1.0\n",
      "\n",
      "Batch 99 in 0.1s - Train AE 0.899 CO 0.977 | Test AE 0.905 CO 0.977\n",
      "\n",
      "Batch 149 in 0.1s - Train AE 0.816 CO 0.601 | Test AE 0.818 CO 0.594\n",
      "\n",
      "Batch 199 in 0.1s - Train AE 0.727 CO 0.244 | Test AE 0.73 CO 0.202\n",
      "\n",
      "Batch 249 in 0.1s - Train AE 0.721 CO 0.202 | Test AE 0.724 CO 0.208\n",
      "\n",
      "Batch 299 in 0.1s - Train AE 0.721 CO 0.188 | Test AE 0.719 CO 0.2\n",
      "\n",
      "Batch 349 in 0.1s - Train AE 0.724 CO 0.268 | Test AE 0.717 CO 0.199\n",
      "\n",
      "Batch 399 in 0.1s - Train AE 0.713 CO 0.196 | Test AE 0.717 CO 0.205\n",
      "\n",
      "Batch 449 in 0.1s - Train AE 0.719 CO 0.194 | Test AE 0.723 CO 0.203\n",
      "\n",
      "Batch 499 in 0.1s - Train AE 0.721 CO 0.224 | Test AE 0.718 CO 0.205\n",
      "\n",
      "Batch 549 in 0.1s - Train AE 0.714 CO 0.186 | Test AE 0.713 CO 0.196\n",
      "\n",
      "Batch 599 in 0.1s - Train AE 0.705 CO 0.234 | Test AE 0.71 CO 0.193\n",
      "\n",
      "Batch 649 in 0.1s - Train AE 0.691 CO 0.173 | Test AE 0.687 CO 0.199\n",
      "\n",
      "Batch 699 in 0.1s - Train AE 0.684 CO 0.191 | Test AE 0.697 CO 0.191\n",
      "\n",
      "Batch 749 in 0.1s - Train AE 0.679 CO 0.192 | Test AE 0.676 CO 0.202\n",
      "\n",
      "Batch 799 in 0.1s - Train AE 0.666 CO 0.209 | Test AE 0.667 CO 0.204\n",
      "\n",
      "Batch 849 in 0.1s - Train AE 0.65 CO 0.211 | Test AE 0.653 CO 0.205\n",
      "\n",
      "Batch 49 in 0.1s - Train AE 0.634 CO 0.21 | Test AE 0.638 CO 0.201\n",
      "\n",
      "Batch 99 in 0.1s - Train AE 0.628 CO 0.229 | Test AE 0.628 CO 0.194\n",
      "\n",
      "Batch 149 in 0.1s - Train AE 0.633 CO 0.185 | Test AE 0.634 CO 0.194\n",
      "\n",
      "Batch 199 in 0.1s - Train AE 0.626 CO 0.18 | Test AE 0.623 CO 0.19\n",
      "\n",
      "Batch 249 in 0.1s - Train AE 0.636 CO 0.163 | Test AE 0.634 CO 0.199\n",
      "\n",
      "Batch 299 in 0.1s - Train AE 0.623 CO 0.18 | Test AE 0.626 CO 0.195\n",
      "\n",
      "Batch 349 in 0.1s - Train AE 0.622 CO 0.192 | Test AE 0.623 CO 0.199\n",
      "\n",
      "Batch 399 in 0.1s - Train AE 0.629 CO 0.278 | Test AE 0.623 CO 0.198\n",
      "\n",
      "Batch 449 in 0.1s - Train AE 0.616 CO 0.147 | Test AE 0.618 CO 0.193\n",
      "\n",
      "Batch 499 in 0.1s - Train AE 0.621 CO 0.233 | Test AE 0.614 CO 0.198\n",
      "\n",
      "Batch 549 in 0.1s - Train AE 0.619 CO 0.139 | Test AE 0.618 CO 0.196\n",
      "\n",
      "Batch 599 in 0.1s - Train AE 0.627 CO 0.23 | Test AE 0.621 CO 0.198\n",
      "\n",
      "Batch 649 in 0.1s - Train AE 0.618 CO 0.158 | Test AE 0.627 CO 0.196\n",
      "\n",
      "Batch 699 in 0.1s - Train AE 0.613 CO 0.201 | Test AE 0.628 CO 0.187\n",
      "\n",
      "Batch 749 in 0.1s - Train AE 0.618 CO 0.2 | Test AE 0.618 CO 0.196\n",
      "\n",
      "Batch 799 in 0.1s - Train AE 0.612 CO 0.231 | Test AE 0.615 CO 0.19\n",
      "\n",
      "Batch 849 in 0.1s - Train AE 0.62 CO 0.204 | Test AE 0.62 CO 0.194\n",
      "\n",
      "Batch 49 in 0.1s - Train AE 0.622 CO 0.156 | Test AE 0.62 CO 0.192\n",
      "\n",
      "Batch 99 in 0.1s - Train AE 0.615 CO 0.175 | Test AE 0.615 CO 0.185\n",
      "\n",
      "Batch 149 in 0.1s - Train AE 0.615 CO 0.195 | Test AE 0.627 CO 0.196\n",
      "\n",
      "Batch 199 in 0.1s - Train AE 0.614 CO 0.188 | Test AE 0.615 CO 0.198\n",
      "\n",
      "Batch 249 in 0.1s - Train AE 0.615 CO 0.171 | Test AE 0.622 CO 0.195\n",
      "\n",
      "Batch 299 in 0.1s - Train AE 0.612 CO 0.153 | Test AE 0.625 CO 0.189\n",
      "\n",
      "Batch 349 in 0.1s - Train AE 0.617 CO 0.154 | Test AE 0.622 CO 0.197\n",
      "\n",
      "Batch 399 in 0.1s - Train AE 0.619 CO 0.252 | Test AE 0.616 CO 0.188\n",
      "\n",
      "Batch 449 in 0.1s - Train AE 0.618 CO 0.201 | Test AE 0.612 CO 0.188\n",
      "\n",
      "Batch 499 in 0.1s - Train AE 0.62 CO 0.186 | Test AE 0.618 CO 0.19\n",
      "\n",
      "Batch 549 in 0.1s - Train AE 0.623 CO 0.217 | Test AE 0.623 CO 0.196\n",
      "\n",
      "Batch 599 in 0.1s - Train AE 0.611 CO 0.181 | Test AE 0.613 CO 0.201\n",
      "\n",
      "Batch 649 in 0.1s - Train AE 0.606 CO 0.182 | Test AE 0.615 CO 0.197\n",
      "\n",
      "Batch 699 in 0.1s - Train AE 0.616 CO 0.172 | Test AE 0.614 CO 0.19\n",
      "\n",
      "Batch 749 in 0.1s - Train AE 0.621 CO 0.242 | Test AE 0.621 CO 0.188\n",
      "\n",
      "Batch 799 in 0.1s - Train AE 0.625 CO 0.188 | Test AE 0.619 CO 0.197\n",
      "\n",
      "Batch 849 in 0.1s - Train AE 0.622 CO 0.165 | Test AE 0.618 CO 0.19\n",
      "\n",
      "Batch 49 in 0.1s - Train AE 0.62 CO 0.257 | Test AE 0.62 CO 0.192\n",
      "\n",
      "Batch 99 in 0.1s - Train AE 0.62 CO 0.255 | Test AE 0.62 CO 0.191\n",
      "\n",
      "Batch 149 in 0.1s - Train AE 0.62 CO 0.164 | Test AE 0.609 CO 0.198\n",
      "\n",
      "Batch 199 in 0.1s - Train AE 0.617 CO 0.202 | Test AE 0.617 CO 0.185\n",
      "\n",
      "Batch 249 in 0.1s - Train AE 0.616 CO 0.162 | Test AE 0.615 CO 0.195\n",
      "\n",
      "Batch 299 in 0.1s - Train AE 0.611 CO 0.17 | Test AE 0.612 CO 0.197\n",
      "\n",
      "Batch 349 in 0.1s - Train AE 0.611 CO 0.186 | Test AE 0.61 CO 0.208\n",
      "\n",
      "Batch 399 in 0.1s - Train AE 0.616 CO 0.173 | Test AE 0.613 CO 0.188\n",
      "\n",
      "Batch 449 in 0.1s - Train AE 0.618 CO 0.211 | Test AE 0.619 CO 0.2\n",
      "\n",
      "Batch 499 in 0.1s - Train AE 0.611 CO 0.159 | Test AE 0.613 CO 0.194\n",
      "\n",
      "Batch 549 in 0.1s - Train AE 0.619 CO 0.197 | Test AE 0.611 CO 0.184\n",
      "\n",
      "Batch 599 in 0.1s - Train AE 0.614 CO 0.153 | Test AE 0.614 CO 0.181\n",
      "\n",
      "Batch 649 in 0.1s - Train AE 0.615 CO 0.174 | Test AE 0.614 CO 0.184\n",
      "\n",
      "Batch 699 in 0.1s - Train AE 0.614 CO 0.191 | Test AE 0.62 CO 0.2\n",
      "\n",
      "Batch 749 in 0.1s - Train AE 0.611 CO 0.21 | Test AE 0.612 CO 0.21\n",
      "\n",
      "Batch 799 in 0.1s - Train AE 0.618 CO 0.226 | Test AE 0.61 CO 0.202\n",
      "\n",
      "Batch 849 in 0.1s - Train AE 0.617 CO 0.189 | Test AE 0.622 CO 0.188\n",
      "\n",
      "Batch 49 in 0.1s - Train AE 0.62 CO 0.179 | Test AE 0.615 CO 0.202\n",
      "\n",
      "Batch 99 in 0.1s - Train AE 0.622 CO 0.19 | Test AE 0.615 CO 0.188\n",
      "\n",
      "Batch 149 in 0.1s - Train AE 0.616 CO 0.188 | Test AE 0.613 CO 0.187\n",
      "\n",
      "Batch 199 in 0.1s - Train AE 0.614 CO 0.185 | Test AE 0.613 CO 0.185\n",
      "\n",
      "Batch 249 in 0.1s - Train AE 0.62 CO 0.223 | Test AE 0.612 CO 0.193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numTrainingBatches = int(np.ceil(X_train[0].shape[0] / batch_size))\n",
    "numTestingBatches = int(np.ceil(X_test[0].shape[0] / batch_size))\n",
    "\n",
    "trainLosses = {}\n",
    "testLosses = {}\n",
    "validLosses = {}\n",
    "trainingIdxs = [1]\n",
    "validationIdxs = []\n",
    "\n",
    "encoderInputNetworkGradients = []\n",
    "encoderMessageNetworkGradients = []\n",
    "encoderUpdateNetworkGradients = []\n",
    "encoderOutputNetworkGradients = []\n",
    "decoderInputNetworkGradients = []\n",
    "decoderMessageNetworkGradients = []\n",
    "decoderUpdateNetworkGradients = []\n",
    "decoderOutputNetworkGradients = []\n",
    "\n",
    "for morphIdx in range(7):\n",
    "    trainLosses[morphIdx] = []\n",
    "    testLosses[morphIdx] = []\n",
    "    validLosses[morphIdx] = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for morphIdx in trainingIdxs:\n",
    "        permutation = np.random.permutation(X_train[morphIdx].shape[0])\n",
    "        X_train[morphIdx] = X_train[morphIdx][permutation]\n",
    "        Y_train[morphIdx] = Y_train[morphIdx][permutation]\n",
    "        \n",
    "    stepLoss = None\n",
    "    encoderGraph = []\n",
    "    decoderGraph = []\n",
    "\n",
    "    for batch in range(0, numTrainingBatches, numBatchesPerTrainingStep):\n",
    "                \n",
    "        t0 = time.time()\n",
    "        \n",
    "        for morphIdx in trainingIdxs:\n",
    "            numNodes = (X_train[morphIdx].shape[1] - 5) // 2\n",
    "            trainLosses[morphIdx].append(np.zeros(2))\n",
    "\n",
    "        for batchOffset in range(numBatchesPerTrainingStep):\n",
    "            \n",
    "            if batch + batchOffset >= numTrainingBatches:\n",
    "                break\n",
    "                \n",
    "            for morphIdx in trainingIdxs:\n",
    "                encoderGraph.append(env[morphIdx].get_graph()._get_dgl_graph())\n",
    "                decoderGraph.append(env[morphIdx].get_graph()._get_dgl_graph())\n",
    "\n",
    "                current_states = X_train[morphIdx][(batch+batchOffset) * batch_size:(batch+batchOffset+1)*batch_size]\n",
    "                next_states = Y_train[morphIdx][(batch+batchOffset) * batch_size:(batch+batchOffset+1)*batch_size]\n",
    "                random_indexes = np.random.choice(X_train[0].shape[0],size=current_states.shape[0], replace=False)\n",
    "                random_states = X_train[morphIdx][random_indexes]\n",
    "                \n",
    "                encoderInput = torch.cat((current_states, next_states, random_states), dim=0).to(device)\n",
    "                latent_states = encoderGNN.forward(encoderGraph[-1], encoderInput)\n",
    "                curren_state_reconstruction = decoderGNN.forward(decoderGraph[-1], latent_states[:, 0:current_states.shape[0], :])\n",
    "                \n",
    "                temp_batch_size = current_states.shape[0]\n",
    "                autoencoder_loss = criterion(encoderGraph[-1].ndata['input'][:, 0:temp_batch_size, :], curren_state_reconstruction).mean()\n",
    "                contrastive_loss = criterion(latent_states[:, 0:temp_batch_size, :], latent_states[:, temp_batch_size:temp_batch_size * 2, :]).mean()\n",
    "                contrastive_loss += torch.max(torch.zeros(1).to(device), minRandomDistance - criterion(latent_states[:, 0:temp_batch_size, :], latent_states[:, 2 * temp_batch_size: 3 * temp_batch_size, :]).mean()).mean()\n",
    "                \n",
    "                trainLosses[morphIdx][-1][0] += autoencoder_loss.item()\n",
    "                trainLosses[morphIdx][-1][1] += contrastive_loss.item()\n",
    "\n",
    "                if stepLoss is None:\n",
    "                    stepLoss = autoencoder_loss + contrastive_loss\n",
    "\n",
    "                else:\n",
    "                    stepLoss += autoencoder_loss + contrastive_loss\n",
    "                    \n",
    "        trainLosses[morphIdx][-1] /= numBatchesPerTrainingStep\n",
    "        stepLoss /= numBatchesPerTrainingStep\n",
    "        optimizer.zero_grad()\n",
    "        stepLoss.backward()\n",
    "        \n",
    "        if batch % 50 == 49:\n",
    "            print('Batch {} in {}s - Train AE {} CO {} | Test AE {} CO {}'.format(\n",
    "                batch, np.round(time.time() - t0, decimals=1), np.round(trainLosses[morphIdx][-1][0], decimals=3), \n",
    "                np.round(trainLosses[morphIdx][-1][1], decimals=3), np.round(testLosses[morphIdx][-1][0], decimals=3), np.round(testLosses[morphIdx][-1][1], decimals=3)))\n",
    "            \n",
    "#             s = 0\n",
    "#             for parameter in encoderInputNetwork.parameters():\n",
    "#                 s += torch.abs(parameter.grad).mean()\n",
    "#             encoderInputNetworkGradients.append(s.item())\n",
    "\n",
    "#             s = 0\n",
    "#             for parameter in encoderMessageNetwork.parameters():\n",
    "#                 s += torch.abs(parameter.grad).mean()\n",
    "#             encoderMessageNetworkGradients.append(s.item())\n",
    "\n",
    "#             s = 0\n",
    "#             for parameter in encoderUpdateNetwork.parameters():\n",
    "#                 s += torch.abs(parameter.grad).mean()\n",
    "#             encoderUpdateNetworkGradients.append(s.item())\n",
    "\n",
    "#             s = 0\n",
    "#             for parameter in encoderOutputNetwork.parameters():\n",
    "#                 s += torch.abs(parameter.grad).mean()\n",
    "#             encoderOutputNetworkGradients.append(s.item())\n",
    "\n",
    "#             s = 0\n",
    "#             for parameter in decoderInputNetwork.parameters():\n",
    "#                 s += torch.abs(parameter.grad).mean()\n",
    "#             decoderInputNetworkGradients.append(s.item())\n",
    "\n",
    "#             s = 0\n",
    "#             for parameter in decoderMessageNetwork.parameters():\n",
    "#                 s += torch.abs(parameter.grad).mean()\n",
    "#             decoderMessageNetworkGradients.append(s.item())\n",
    "\n",
    "#             s = 0\n",
    "#             for parameter in decoderUpdateNetwork.parameters():\n",
    "#                 s += torch.abs(parameter.grad).mean()\n",
    "#             decoderUpdateNetworkGradients.append(s.item())\n",
    "\n",
    "#             s = 0\n",
    "#             for parameter in decoderOutputNetwork.parameters():\n",
    "#                 s += torch.abs(parameter.grad).mean()\n",
    "#             decoderOutputNetworkGradients.append(s.item())\n",
    "\n",
    "#             print('Gradients: Encoder Input {} | Encoder Message {} | Encoder  Update {} | Encoder Output {}'.format(\n",
    "#                 np.log10(encoderInputNetworkGradients[-1]), np.log10(encoderMessageNetworkGradients[-1]), np.log10(encoderUpdateNetworkGradients[-1]), np.log10(encoderOutputNetworkGradients[-1])))    \n",
    "\n",
    "#             print('Gradients: Decoder Input {} | Decoder Message {} | Decoder  Update {} | Decoder Output {}'.format(\n",
    "#                 np.log10(decoderInputNetworkGradients[-1]), np.log10(decoderMessageNetworkGradients[-1]), np.log10(decoderUpdateNetworkGradients[-1]), np.log10(decoderOutputNetworkGradients[-1])))    \n",
    "            \n",
    "            print()\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        stepLoss = None\n",
    "        currentStateEncoderGraph = []\n",
    "        currentStateDecoderGraph = []\n",
    "        nextStateEncoderGraph = []\n",
    "        randomStateEncoderGraph = []\n",
    "        randomStateGraph = []\n",
    "        \n",
    "        \n",
    "        numBatchesForExectution = 50                \n",
    "        for morphIdx in trainingIdxs:\n",
    "            testLosses[morphIdx].append(np.zeros(2))\n",
    "            for batch_ in np.random.choice(np.arange(numTestingBatches-1), numBatchesForExectution):\n",
    "                \n",
    "                encoder_graph = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                decoder_graph = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "\n",
    "                current_states = X_test[morphIdx][(batch_+batchOffset) * batch_size:(batch_+batchOffset+1)*batch_size]\n",
    "                next_states = Y_test[morphIdx][(batch_+batchOffset) * batch_size:(batch_+batchOffset+1)*batch_size]\n",
    "                random_indexes = np.random.choice(X_test[0].shape[0],size=batch_size, replace=False)\n",
    "                random_states = X_test[morphIdx][random_indexes]\n",
    "                \n",
    "                encoderInput = torch.cat((current_states, next_states, random_states), dim=0).to(device)\n",
    "\n",
    "                latent_states = encoderGNN.forward(encoder_graph, encoderInput)\n",
    "                curren_state_reconstruction = decoderGNN.forward(decoder_graph, latent_states[:, 0:batch_size, :])\n",
    "                \n",
    "                autoencoder_loss = criterion(encoderGraph[-1].ndata['input'][:, 0:batch_size, :], curren_state_reconstruction).mean()\n",
    "                contrastive_loss = criterion(latent_states[:, 0:batch_size, :], latent_states[:, batch_size:batch_size * 2, :]).mean()\n",
    "                contrastive_loss += torch.max(torch.zeros(1).to(device), minRandomDistance - criterion(latent_states[:, 0:batch_size, :], latent_states[:, 2 * batch_size: 3 * batch_size, :]).mean()).mean()\n",
    "                \n",
    "                testLosses[morphIdx][-1][0] += autoencoder_loss.item()\n",
    "                testLosses[morphIdx][-1][1] += contrastive_loss.item()\n",
    "            testLosses[morphIdx][-1] /= numBatchesForExectution\n",
    "            \n",
    "#         for morphIdx in validationIdxs:\n",
    "#             numNodes = (X_train[morphIdx].shape[1] - 6) // 2\n",
    "#             validLosses[morphIdx].append(torch.zeros(numNodes))\n",
    "#             for batch_ in np.random.choice(np.arange(numTestingBatches-1), numBatchesForExectution):\n",
    "\n",
    "#                 g = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "#                 x = X_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "#                 y = Y_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "\n",
    "#                 y_hat = gnn.forward(g, x)\n",
    "#                 loss = criterion(y, y_hat).mean(dim=0)\n",
    "\n",
    "#                 validLosses[morphIdx][-1] += loss.cpu().detach()\n",
    "#             validLosses[morphIdx][-1] /= numBatchesForExectution\n",
    "\n",
    "#         print('\\n************** Batch {} in {} **************\\n'.format(batch, time.time() - t0))\n",
    "#         for morphIdx in trainingIdxs:\n",
    "#             print('Training Idx {} \\nTrain Loss {} \\nTest Loss {}\\n'.format(morphIdx, trainLosses[morphIdx][-1], testLosses[morphIdx][-1]))\n",
    "#         for morphIdx in validationIdxs:\n",
    "#             print('Valid Idx {} | Loss {}'.format(morphIdx, validLosses[morphIdx][-1]))\n",
    "            \n",
    "#         if batch % 20 ==0:\n",
    "#             print('Gradients: Input {} | Message {} | Update {} | Output {}'.format(\n",
    "#                 inputNetworkGradients[-1], messageNetworkGradients[-1], updateNetworkGradients[-1], outputNetworkGradients[-1]))    \n",
    "#         if batch % 100 == 99:\n",
    "#             lr_scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphIdx = 0\n",
    "lossArr = torch.stack(testLosses[morphIdx]).T\n",
    "fig, ax = plt.subplots(1, sharex=True)\n",
    "for i in range(lossArr.shape[0]):\n",
    "    ax.plot(range(lossArr.shape[1]), torch.log10(lossArr[i]))\n",
    "plt.legend(range(lossArr.shape[0]))\n",
    "plt.xlabel('Training Step')\n",
    "plt.grid()\n",
    "plt.ylabel('Smooth L1 Loss')\n",
    "plt.title('Per Node Loss Morphology {}, Train = {}'.format(morphIdx, morphIdx in trainingIdxs))\n",
    "plt.savefig('per-node-loss-{}.jpg'.format(morphIdx))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell for producing Per Node Loss for each Morphology\n",
    "\n",
    "for morphIdx in range(7):\n",
    "    if morphIdx in trainingIdxs:\n",
    "        lossArr = torch.stack(testLosses[morphIdx]).T\n",
    "    else:\n",
    "        lossArr = torch.stack(validLosses[morphIdx]).T\n",
    "    \n",
    "    fig, ax = plt.subplots(1, sharex=True)\n",
    "    for i in range(lossArr.shape[0]):\n",
    "        ax.plot(range(lossArr.shape[1]), lossArr[i])\n",
    "    plt.legend(range(lossArr.shape[0]))\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.grid()\n",
    "    plt.ylabel('Smooth L1 Loss')\n",
    "    plt.title('Per Node Loss Morphology {}, Train = {}'.format(morphIdx, morphIdx in trainingIdxs))\n",
    "    plt.savefig('per-node-loss-{}.jpg'.format(morphIdx))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, sharex=True)\n",
    "for morphIdx in trainingIdxs:\n",
    "    lossArr = torch.stack(testLosses[morphIdx]).mean(dim=1)\n",
    "    ax.plot(range(lossArr.shape[0]), lossArr)\n",
    "for morphIdx in validationIdxs:\n",
    "    lossArr = torch.stack(validLosses[morphIdx]).mean(dim=1)\n",
    "    ax.plot(range(lossArr.shape[0]), lossArr)\n",
    "\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Smooth L1 Loss')\n",
    "plt.title('Mean Node Loss per Morphology')\n",
    "plt.legend(trainingIdxs + validationIdxs)\n",
    "plt.savefig('mean-node-losses.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, sharex=True)\n",
    "ax.plot(range(len(inputNetworkGradients)), np.log10(inputNetworkGradients))\n",
    "ax.plot(range(len(messageNetworkGradients)), np.log10(messageNetworkGradients))\n",
    "ax.plot(range(len(updateNetworkGradients)), np.log10(updateNetworkGradients))\n",
    "ax.plot(range(len(outputNetworkGradients)), np.log10(outputNetworkGradients))\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Mean Absolute Gradient (Log 10)')\n",
    "plt.title('Gradient Magnitudes Over Time')\n",
    "plt.legend(['Input Network', 'Message Network', 'Update Network', ' Output Network'])\n",
    "plt.savefig('gradients.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainLosses = {}\n",
    "testLosses = {}\n",
    "validLosses = {}\n",
    "trainingIdxs = []\n",
    "validationIdxs = []\n",
    "\n",
    "inputNetworkGradients = []\n",
    "messageNetworkGradients = []\n",
    "updateNetworkGradients = []\n",
    "outputNetworkGradients = []\n",
    "\n",
    "\n",
    "for morphIdx in range(7):\n",
    "    trainLosses[morphIdx] = []\n",
    "    testLosses[morphIdx] = []\n",
    "    validLosses[morphIdx] = []\n",
    "\n",
    "for index in [0]:\n",
    "    \n",
    "    inputNetwork = Network(inputSize, stateSize, hidden_sizes, batch_size, with_batch_norm)\n",
    "    messageNetwork = Network(stateSize + 1, messageSize, hidden_sizes, batch_size, with_batch_norm, nn.Tanh)\n",
    "    updateNetwork = Network(stateSize + messageSize, stateSize, hidden_sizes, batch_size, with_batch_norm)\n",
    "    outputNetwork = Network(stateSize, outputSize, hidden_sizes, batch_size, with_batch_norm, nn.Tanh)\n",
    "\n",
    "    gnn = GraphNeuralNetwork(inputNetwork, messageNetwork, updateNetwork, outputNetwork, numMessagePassingIterations).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(itertools.chain(inputNetwork.parameters(), messageNetwork.parameters(), updateNetwork.parameters(), outputNetwork.parameters())\n",
    "                           , lr, weight_decay=1e-5)\n",
    "    \n",
    "    trainingIdxs = [index]\n",
    "    \n",
    "    for epoch in range(10):\n",
    "\n",
    "        for morphIdx in trainingIdxs:\n",
    "            permutation = np.random.permutation(X_train[morphIdx].shape[0])\n",
    "            X_train[morphIdx] = X_train[morphIdx][permutation]\n",
    "            Y_train[morphIdx] = Y_train[morphIdx][permutation]\n",
    "\n",
    "        stepLoss = None\n",
    "        graphs = []\n",
    "        numAggregatedBatches = 0\n",
    "\n",
    "        for batch in range(0, numTrainingBatches, numBatchesPerTrainingStep):\n",
    "\n",
    "            inputNetwork.train()\n",
    "            messageNetwork.train()\n",
    "            updateNetwork.train()\n",
    "            outputNetwork.train()\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            for morphIdx in trainingIdxs:\n",
    "                numNodes = (X_train[morphIdx].shape[1] - 6) // 2\n",
    "                trainLosses[morphIdx].append(torch.zeros(numNodes))\n",
    "\n",
    "            for batchOffset in range(numBatchesPerTrainingStep):\n",
    "\n",
    "                if batch + batchOffset >= numTrainingBatches:\n",
    "                    break\n",
    "\n",
    "                for morphIdx in trainingIdxs:\n",
    "                    graphs.append(env[morphIdx].get_graph()._get_dgl_graph())\n",
    "                    x = X_train[morphIdx][(batch+batchOffset) * batch_size:(batch+batchOffset+1)*batch_size].to(device)\n",
    "                    y = Y_train[morphIdx][(batch+batchOffset) * batch_size:(batch+batchOffset+1)*batch_size].to(device)\n",
    "\n",
    "                    y_hat = gnn.forward(graphs[-1], x)\n",
    "\n",
    "                    loss_tmp = criterion(y, y_hat).mean(dim=0)\n",
    "\n",
    "                    trainLosses[morphIdx][-1] += loss_tmp.cpu().detach() / numBatchesPerTrainingStep\n",
    "\n",
    "                    if stepLoss is None:\n",
    "                        stepLoss = loss_tmp.mean()\n",
    "\n",
    "                    else:\n",
    "                        stepLoss += loss_tmp.mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            stepLoss.backward()\n",
    "\n",
    "\n",
    "            s = 0\n",
    "            for parameter in inputNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            inputNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0\n",
    "            for parameter in messageNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            messageNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0        \n",
    "            for parameter in updateNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            updateNetworkGradients.append(s.item())\n",
    "\n",
    "            s = 0        \n",
    "            for parameter in outputNetwork.parameters():\n",
    "                s += torch.abs(parameter.grad).mean()\n",
    "            outputNetworkGradients.append(s.item())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            stepLoss = None\n",
    "            graphs = []\n",
    "\n",
    "            inputNetwork.eval()\n",
    "            messageNetwork.eval()\n",
    "            updateNetwork.eval()\n",
    "            outputNetwork.eval()\n",
    "\n",
    "            numBatchesForExectution = 50\n",
    "            for morphIdx in trainingIdxs:\n",
    "                numNodes = (X_train[morphIdx].shape[1] - 6) // 2\n",
    "                testLosses[morphIdx].append(torch.zeros(numNodes))\n",
    "                for batch_ in np.random.choice(np.arange(numTestingBatches-1), numBatchesForExectution):\n",
    "                    g = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                    x = X_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "                    y = Y_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "                    y_hat = gnn.forward(g, x)\n",
    "                    loss = criterion(y, y_hat).mean(dim=0)\n",
    "                    testLosses[morphIdx][-1] += loss.cpu().detach()\n",
    "                testLosses[morphIdx][-1] /= numBatchesForExectution\n",
    "\n",
    "            for morphIdx in validationIdxs:\n",
    "                numNodes = (X_train[morphIdx].shape[1] // - 6) // 2\n",
    "                validLosses[morphIdx].append(torch.zeros(numNodes))\n",
    "                for batch_ in np.random.choice(np.arange(numTestingBatches-1), numBatchesForExectution):\n",
    "\n",
    "                    g = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                    x = X_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "                    y = Y_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size].to(device)\n",
    "\n",
    "                    y_hat = gnn.forward(g, x)\n",
    "                    loss = criterion(y, y_hat).mean(dim=0)\n",
    "\n",
    "                    validLosses[morphIdx][-1] += loss.cpu().detach()\n",
    "                validLosses[morphIdx][-1] /= numBatchesForExectution\n",
    "\n",
    "            print('\\n************** Batch {} in {} **************\\n'.format(batch, time.time() - t0))\n",
    "            for morphIdx in trainingIdxs:\n",
    "                print('Training Idx {} \\nTrain Loss {} \\nTest Loss {}\\n'.format(morphIdx, trainLosses[morphIdx][-1], testLosses[morphIdx][-1]))\n",
    "            for morphIdx in validationIdxs:\n",
    "                print('Valid Idx {} | Loss {}'.format(morphIdx, validLosses[morphIdx][-1]))\n",
    "                \n",
    "    lossArr = torch.stack(testLosses[index]).T\n",
    "    fig, ax = plt.subplots(1, sharex=True)\n",
    "    for i in range(lossArr.shape[0]):\n",
    "        ax.plot(range(lossArr.shape[1]), torch.log10(lossArr[i]))\n",
    "    plt.legend(range(lossArr.shape[0]))\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.grid()\n",
    "    plt.ylabel('Smooth L1 Loss')\n",
    "    plt.title('Per Node Loss Morphology {}'.format(index))\n",
    "    plt.savefig('xv-per-node-loss-{}.jpg'.format(morphIdx))\n",
    "    plt.show()\n",
    "\n",
    "#             if batch % 20 ==0:\n",
    "#                 print('Gradients: Input {} | Message {} | Update {} | Output {}'.format(\n",
    "#                     inputNetworkGradients[-1], messageNetworkGradients[-1], updateNetworkGradients[-1], outputNetworkGradients[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '../models/mix/'\n",
    "\n",
    "testLossesArr = np.array(testLosses)\n",
    "trainLossesArr = np.array(trainLosses)\n",
    "validLossesArr = np.array(validLosses)\n",
    "learningRatesArr = np.array(learningRates)\n",
    "\n",
    "np.save(prefix + 'testLosses', testLossesArr)\n",
    "np.save(prefix + 'trainLosses', trainLossesArr)\n",
    "np.save(prefix + 'validLosses', validLossesArr)\n",
    "np.save(prefix + 'learningRates', learningRatesArr)\n",
    "\n",
    "# torch.save(inputNetwork, prefix + 'inputNetwork.pt')\n",
    "# torch.save(outputNetwork, prefix + 'outputNetwork.pt')\n",
    "# torch.save(messageNetwork, prefix + 'messageNetwork.pt')\n",
    "# torch.save(updateNetwork, prefix + 'updateNetwork.pt')\n",
    "\n",
    "fig, ax = plt.subplots(1, sharex=True)\n",
    "ax.plot(range(len(testLossesArr)), testLossesArr)\n",
    "ax.plot(range(len(trainLossesArr)), trainLossesArr, 'b')\n",
    "ax.plot(range(len(validLossesArr)), validLossesArr)\n",
    "ax.set(ylabel='Smooth L1 Loss')\n",
    "# ax[1].plot(range(len(learningRates)), learningRatesArr)\n",
    "# ax[1].set(ylabel='Learning Rate')\n",
    "ax.legend([\"Testing\", \"Training\", \"Valid\"])\n",
    "plt.xlabel('Batch')\n",
    "plt.savefig(prefix + 'valid-0.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '../models/mix/'\n",
    "\n",
    "inputNetwork = torch.load(prefix + 'inputNetwork.pt')\n",
    "messageNetwork = torch.load(prefix + 'messageNetwork.pt')\n",
    "updateNetwork = torch.load(prefix + 'updateNetwork.pt')\n",
    "outputNetwork = torch.load(prefix + 'outputNetwork.pt')\n",
    "\n",
    "gnn = GraphNeuralNetwork(inputNetwork, messageNetwork, updateNetwork, outputNetwork, numMessagePassingIterations=3).to(device)\n",
    "\n",
    "testLoss = 0\n",
    "for morphIdx in [0]:\n",
    "    \n",
    "    for batch in range(numTestingBatches):\n",
    "\n",
    "        g = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "        x = X_test[morphIdx][batch * batch_size:(batch+1)*batch_size].to(device)\n",
    "        y = Y_test[morphIdx][batch * batch_size:(batch+1)*batch_size].to(device)\n",
    "\n",
    "        y_hat = gnn.forward(g, x)\n",
    "        loss = criterion(y, y_hat)\n",
    "        testLoss += loss.item()\n",
    "        \n",
    "print(testLoss / X_test[morphIdx].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
