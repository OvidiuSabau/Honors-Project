{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.autograd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "import dgl\n",
    "from graphenvs import HalfCheetahGraphEnv\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        hidden_sizes,\n",
    "        with_batch_norm=False,\n",
    "        activation=None\n",
    "    ):\n",
    "        super(Network, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        if with_batch_norm:\n",
    "            self.layers.append(nn.LayerNorm(normalized_shape=(hidden_sizes[0])))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            if with_batch_norm:\n",
    "                self.layers.append(nn.LayerNorm(normalized_shape=(hidden_sizes[i+1])))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], self.output_size))\n",
    "        \n",
    "        if activation is not None:\n",
    "            self.layers.append(activation())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputNetwork,\n",
    "        messageNetwork,\n",
    "        updateNetwork,\n",
    "        outputNetwork,\n",
    "        numMessagePassingIterations,\n",
    "        withInputNetwork = True\n",
    "    ):\n",
    "        \n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "                \n",
    "        self.inputNetwork = inputNetwork\n",
    "        self.messageNetwork = messageNetwork\n",
    "        self.updateNetwork = updateNetwork\n",
    "        self.outputNetwork = outputNetwork\n",
    "        \n",
    "        self.numMessagePassingIterations = numMessagePassingIterations\n",
    "        self.withInputNetwork = withInputNetwork\n",
    "        \n",
    "    def inputFunction(self, nodes):\n",
    "        return {'state' : self.inputNetwork(nodes.data['input'])}\n",
    "    \n",
    "    def messageFunction(self, edges):\n",
    "        \n",
    "        batchSize = edges.src['state'].shape[1]\n",
    "        edgeData = edges.data['feature'].repeat(batchSize, 1).T.unsqueeze(-1)\n",
    "        nodeInput = edges.src['input']\n",
    "        \n",
    "        return {'m' : self.messageNetwork(torch.cat((edges.src['state'], edgeData, nodeInput), -1))}\n",
    "    \n",
    "    def updateFunction(self, nodes):\n",
    "        return {'state': self.updateNetwork(torch.cat((nodes.data['m_hat'], nodes.data['state']), -1))}\n",
    "    \n",
    "    def outputFunction(self, nodes):\n",
    "        \n",
    "        return {'output': self.outputNetwork(nodes.data['state'])}\n",
    "\n",
    "\n",
    "    def forward(self, graph, state):\n",
    "        \n",
    "        self.update_states_in_graph(graph, state)\n",
    "        \n",
    "        if self.withInputNetwork:\n",
    "            graph.apply_nodes(self.inputFunction)\n",
    "        \n",
    "        for messagePassingIteration in range(self.numMessagePassingIterations):\n",
    "            graph.update_all(self.messageFunction, dgl.function.mean('m', 'm_hat'), self.updateFunction)\n",
    "        \n",
    "        graph.apply_nodes(self.outputFunction)\n",
    "        \n",
    "        output = graph.ndata['output']\n",
    "        output = output.squeeze(-1).mean(0)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    def update_states_in_graph(self, graph, state):\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        numGraphFeature = 6\n",
    "        numGlobalStateInformation = 5\n",
    "        numLocalStateInformation = 2\n",
    "        numStateVar = state.shape[1] // 2\n",
    "        globalInformation = torch.cat((state[:, 0:5], state[:, numStateVar:numStateVar+5]), -1)\n",
    "        \n",
    "        numNodes = (numStateVar - 5) // 2\n",
    "\n",
    "        nodeData = torch.empty((numNodes, state.shape[0], numGraphFeature + 2 * numGlobalStateInformation + 2 * numLocalStateInformation)).to(device)\n",
    "        for nodeIdx in range(numNodes):\n",
    "\n",
    "            # Assign global features from graph\n",
    "            nodeData[nodeIdx, :, :6] = graph.ndata['feature'][nodeIdx]\n",
    "            # Assign local state information\n",
    "            nodeData[nodeIdx, :, 16] = state[:, 5 + nodeIdx]\n",
    "            nodeData[nodeIdx, :, 17] = state[:, 5 + numNodes + nodeIdx]\n",
    "            nodeData[nodeIdx, :, 18] = state[:, numStateVar + 5 + nodeIdx]\n",
    "            nodeData[nodeIdx, :, 19] = state[:, numStateVar + 5 + numNodes + nodeIdx]\n",
    "\n",
    "        # Assdign global state information\n",
    "        nodeData[:, :, 6:16] = globalInformation\n",
    "        \n",
    "        if self.withInputNetwork:\n",
    "            graph.ndata['input'] = nodeData        \n",
    "        \n",
    "        else:\n",
    "            graph.ndata['state'] = nodeData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingIdxs = [0, 1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ovi/anaconda3/envs/honors-project/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    }
   ],
   "source": [
    "states = {}\n",
    "actions = {}\n",
    "rewards = {}\n",
    "next_states = {}\n",
    "dones = {}\n",
    "env = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "\n",
    "    prefix = '../datasets/{}/'.format(morphIdx)\n",
    "    \n",
    "    states[morphIdx] = np.load(prefix + 'states_array.npy')\n",
    "    actions[morphIdx] = np.load(prefix + 'actions_array.npy')\n",
    "    rewards[morphIdx] = np.load(prefix + 'rewards_array.npy')\n",
    "    next_states[morphIdx] = np.load(prefix + 'next_states_array.npy')\n",
    "    dones[morphIdx] = np.load(prefix + 'dones_array.npy')\n",
    "    \n",
    "    env[morphIdx] = HalfCheetahGraphEnv(None)\n",
    "    env[morphIdx].set_morphology(morphIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_train = {}\n",
    "states_test = {}\n",
    "next_states_train = {}\n",
    "next_states_test = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    permutation = np.random.permutation(states[morphIdx].shape[0])\n",
    "    \n",
    "    states[morphIdx] = states[morphIdx][permutation]\n",
    "    next_states[morphIdx] = next_states[morphIdx][permutation]\n",
    "    \n",
    "    states_train[morphIdx] = torch.from_numpy(states[morphIdx][100000:]).float()\n",
    "    states_test[morphIdx] = torch.from_numpy(states[morphIdx][:100000]).float()\n",
    "    \n",
    "    next_states_train[morphIdx] = torch.from_numpy(next_states[morphIdx][100000:]).float()\n",
    "    next_states_test[morphIdx] = torch.from_numpy(next_states[morphIdx][:100000]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [256, 256]\n",
    "\n",
    "inputSize = 20\n",
    "stateSize = 64\n",
    "messageSize = 64\n",
    "outputSize = 1\n",
    "numMessagePassingIterations = 6\n",
    "batch_size = 1024\n",
    "with_batch_norm=True\n",
    "numBatchesPerTrainingStep = 1\n",
    "\n",
    "inputNetwork = Network(inputSize, stateSize, hidden_sizes, with_batch_norm)\n",
    "messageNetwork = Network(stateSize + inputSize + 1, messageSize, hidden_sizes, with_batch_norm, nn.Tanh)\n",
    "updateNetwork = Network(stateSize + messageSize, stateSize, hidden_sizes, with_batch_norm)\n",
    "outputNetwork = Network(stateSize, outputSize, hidden_sizes, with_batch_norm, nn.Sigmoid)\n",
    "\n",
    "gnn = GraphNeuralNetwork(inputNetwork, messageNetwork, updateNetwork, outputNetwork, numMessagePassingIterations).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "optimizer = optim.Adam(itertools.chain(inputNetwork.parameters(), messageNetwork.parameters(), updateNetwork.parameters(), outputNetwork.parameters())\n",
    "                       , lr=lr, weight_decay=1e-5)\n",
    "\n",
    "# lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0, verbose=True, min_lr=1e-5)\n",
    "binaryLoss = nn.BCELoss()\n",
    "\n",
    "zeroTensor = torch.zeros([batch_size]).to(device)\n",
    "oneTensor = torch.ones([batch_size]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrainingBatches = int(np.ceil(states_train[trainingIdxs[0]].shape[0] / batch_size))\n",
    "numTestingBatches = int(np.ceil(states_test[trainingIdxs[0]].shape[0] / batch_size))\n",
    "\n",
    "trainLosses = {}\n",
    "testLosses = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    trainLosses[morphIdx] = []\n",
    "    testLosses[morphIdx] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0\n",
      " ** Testing ** \n",
      "0: F-Loss 0.705 | F-Acc 0.163| R-Loss 0.683 | R-Acc 0.566 | V-Loss 0.660 | V-Acc 0.878\n",
      "1: F-Loss 0.682 | F-Acc 0.610| R-Loss 0.695 | R-Acc 0.429 | V-Loss 0.687 | V-Acc 0.475\n",
      "2: F-Loss 0.680 | F-Acc 0.666| R-Loss 0.701 | R-Acc 0.275 | V-Loss 0.685 | V-Acc 0.580\n",
      "3: F-Loss 0.688 | F-Acc 0.412| R-Loss 0.694 | R-Acc 0.451 | V-Loss 0.677 | V-Acc 0.727\n",
      "4: F-Loss 0.681 | F-Acc 0.565| R-Loss 0.701 | R-Acc 0.314 | V-Loss 0.680 | V-Acc 0.696\n",
      "5: F-Loss 0.691 | F-Acc 0.381| R-Loss 0.691 | R-Acc 0.477 | V-Loss 0.680 | V-Acc 0.640\n",
      "Batch 0 in 1.55s\n",
      "0: F-Loss 0.713 | F-Acc 0.155| R-Loss 0.691 | R-Acc 0.553 | V-Loss 0.668 | V-Acc 0.879\n",
      "1: F-Loss 0.689 | F-Acc 0.615| R-Loss 0.702 | R-Acc 0.440 | V-Loss 0.694 | V-Acc 0.472\n",
      "2: F-Loss 0.686 | F-Acc 0.688| R-Loss 0.708 | R-Acc 0.265 | V-Loss 0.692 | V-Acc 0.590\n",
      "3: F-Loss 0.695 | F-Acc 0.437| R-Loss 0.703 | R-Acc 0.442 | V-Loss 0.685 | V-Acc 0.705\n",
      "4: F-Loss 0.687 | F-Acc 0.582| R-Loss 0.708 | R-Acc 0.325 | V-Loss 0.687 | V-Acc 0.702\n",
      "5: F-Loss 0.697 | F-Acc 0.394| R-Loss 0.699 | R-Acc 0.455 | V-Loss 0.687 | V-Acc 0.653\n",
      "Batch 200 in 1.80s\n",
      "0: F-Loss 0.208 | F-Acc 0.947| R-Loss 0.263 | R-Acc 0.889 | V-Loss 0.400 | V-Acc 0.813\n",
      "1: F-Loss 0.160 | F-Acc 0.965| R-Loss 0.144 | R-Acc 0.943 | V-Loss 0.211 | V-Acc 0.914\n",
      "2: F-Loss 0.145 | F-Acc 0.975| R-Loss 0.181 | R-Acc 0.929 | V-Loss 0.213 | V-Acc 0.914\n",
      "3: F-Loss 0.128 | F-Acc 0.982| R-Loss 0.225 | R-Acc 0.912 | V-Loss 0.129 | V-Acc 0.950\n",
      "4: F-Loss 0.176 | F-Acc 0.958| R-Loss 0.207 | R-Acc 0.924 | V-Loss 0.227 | V-Acc 0.931\n",
      "5: F-Loss 0.167 | F-Acc 0.973| R-Loss 0.191 | R-Acc 0.925 | V-Loss 0.236 | V-Acc 0.903\n",
      "Batch 400 in 1.99s\n",
      "0: F-Loss 0.125 | F-Acc 0.968| R-Loss 0.121 | R-Acc 0.957 | V-Loss 0.165 | V-Acc 0.945\n",
      "1: F-Loss 0.083 | F-Acc 0.975| R-Loss 0.106 | R-Acc 0.957 | V-Loss 0.086 | V-Acc 0.968\n",
      "2: F-Loss 0.073 | F-Acc 0.987| R-Loss 0.095 | R-Acc 0.964 | V-Loss 0.074 | V-Acc 0.981\n",
      "3: F-Loss 0.071 | F-Acc 0.994| R-Loss 0.122 | R-Acc 0.960 | V-Loss 0.036 | V-Acc 0.992\n",
      "4: F-Loss 0.123 | F-Acc 0.969| R-Loss 0.135 | R-Acc 0.942 | V-Loss 0.106 | V-Acc 0.975\n",
      "5: F-Loss 0.087 | F-Acc 0.988| R-Loss 0.094 | R-Acc 0.966 | V-Loss 0.104 | V-Acc 0.963\n",
      "Batch 600 in 2.03s\n",
      "0: F-Loss 0.100 | F-Acc 0.973| R-Loss 0.120 | R-Acc 0.955 | V-Loss 0.135 | V-Acc 0.952\n",
      "1: F-Loss 0.066 | F-Acc 0.985| R-Loss 0.073 | R-Acc 0.976 | V-Loss 0.036 | V-Acc 0.988\n",
      "2: F-Loss 0.057 | F-Acc 0.990| R-Loss 0.057 | R-Acc 0.977 | V-Loss 0.042 | V-Acc 0.987\n",
      "3: F-Loss 0.055 | F-Acc 0.992| R-Loss 0.118 | R-Acc 0.956 | V-Loss 0.025 | V-Acc 0.994\n",
      "4: F-Loss 0.082 | F-Acc 0.982| R-Loss 0.109 | R-Acc 0.959 | V-Loss 0.041 | V-Acc 0.989\n",
      "5: F-Loss 0.084 | F-Acc 0.984| R-Loss 0.092 | R-Acc 0.969 | V-Loss 0.072 | V-Acc 0.976\n",
      "Batch 800 in 2.08s\n",
      "0: F-Loss 0.082 | F-Acc 0.977| R-Loss 0.117 | R-Acc 0.961 | V-Loss 0.137 | V-Acc 0.955\n",
      "1: F-Loss 0.051 | F-Acc 0.985| R-Loss 0.056 | R-Acc 0.980 | V-Loss 0.037 | V-Acc 0.987\n",
      "2: F-Loss 0.039 | F-Acc 0.995| R-Loss 0.073 | R-Acc 0.971 | V-Loss 0.031 | V-Acc 0.993\n",
      "3: F-Loss 0.033 | F-Acc 0.996| R-Loss 0.101 | R-Acc 0.963 | V-Loss 0.021 | V-Acc 0.996\n",
      "4: F-Loss 0.055 | F-Acc 0.991| R-Loss 0.097 | R-Acc 0.963 | V-Loss 0.079 | V-Acc 0.979\n",
      "5: F-Loss 0.049 | F-Acc 0.995| R-Loss 0.074 | R-Acc 0.974 | V-Loss 0.061 | V-Acc 0.981\n",
      "Epoch 0 finished in 1730.7s\n",
      "Starting Epoch 1\n",
      " ** Testing ** \n",
      "0: F-Loss 0.067 | F-Acc 0.972| R-Loss 0.101 | R-Acc 0.953 | V-Loss 0.110 | V-Acc 0.953\n",
      "1: F-Loss 0.041 | F-Acc 0.979| R-Loss 0.065 | R-Acc 0.969 | V-Loss 0.039 | V-Acc 0.979\n",
      "2: F-Loss 0.037 | F-Acc 0.984| R-Loss 0.073 | R-Acc 0.964 | V-Loss 0.031 | V-Acc 0.981\n",
      "3: F-Loss 0.034 | F-Acc 0.984| R-Loss 0.081 | R-Acc 0.962 | V-Loss 0.024 | V-Acc 0.985\n",
      "4: F-Loss 0.054 | F-Acc 0.981| R-Loss 0.095 | R-Acc 0.955 | V-Loss 0.051 | V-Acc 0.977\n",
      "5: F-Loss 0.047 | F-Acc 0.983| R-Loss 0.072 | R-Acc 0.965 | V-Loss 0.064 | V-Acc 0.970\n",
      "Batch 0 in 2.03s\n",
      "0: F-Loss 0.056 | F-Acc 0.987| R-Loss 0.090 | R-Acc 0.966 | V-Loss 0.126 | V-Acc 0.963\n",
      "1: F-Loss 0.038 | F-Acc 0.993| R-Loss 0.048 | R-Acc 0.987 | V-Loss 0.041 | V-Acc 0.985\n",
      "2: F-Loss 0.042 | F-Acc 0.991| R-Loss 0.075 | R-Acc 0.969 | V-Loss 0.039 | V-Acc 0.989\n",
      "3: F-Loss 0.035 | F-Acc 0.993| R-Loss 0.099 | R-Acc 0.969 | V-Loss 0.023 | V-Acc 0.994\n",
      "4: F-Loss 0.060 | F-Acc 0.987| R-Loss 0.098 | R-Acc 0.959 | V-Loss 0.056 | V-Acc 0.986\n",
      "5: F-Loss 0.038 | F-Acc 0.998| R-Loss 0.071 | R-Acc 0.976 | V-Loss 0.065 | V-Acc 0.980\n",
      "Batch 200 in 1.84s\n",
      "0: F-Loss 0.062 | F-Acc 0.979| R-Loss 0.112 | R-Acc 0.954 | V-Loss 0.082 | V-Acc 0.973\n",
      "1: F-Loss 0.035 | F-Acc 0.993| R-Loss 0.074 | R-Acc 0.975 | V-Loss 0.029 | V-Acc 0.993\n",
      "2: F-Loss 0.030 | F-Acc 0.993| R-Loss 0.075 | R-Acc 0.978 | V-Loss 0.029 | V-Acc 0.991\n",
      "3: F-Loss 0.025 | F-Acc 0.998| R-Loss 0.085 | R-Acc 0.974 | V-Loss 0.011 | V-Acc 0.997\n",
      "4: F-Loss 0.042 | F-Acc 0.995| R-Loss 0.094 | R-Acc 0.964 | V-Loss 0.052 | V-Acc 0.985\n",
      "5: F-Loss 0.038 | F-Acc 0.994| R-Loss 0.056 | R-Acc 0.980 | V-Loss 0.043 | V-Acc 0.989\n",
      "Batch 400 in 1.64s\n",
      "0: F-Loss 0.057 | F-Acc 0.986| R-Loss 0.081 | R-Acc 0.971 | V-Loss 0.083 | V-Acc 0.969\n",
      "1: F-Loss 0.030 | F-Acc 0.992| R-Loss 0.048 | R-Acc 0.982 | V-Loss 0.033 | V-Acc 0.992\n",
      "2: F-Loss 0.028 | F-Acc 0.994| R-Loss 0.057 | R-Acc 0.978 | V-Loss 0.025 | V-Acc 0.994\n",
      "3: F-Loss 0.041 | F-Acc 0.992| R-Loss 0.058 | R-Acc 0.985 | V-Loss 0.026 | V-Acc 0.995\n",
      "4: F-Loss 0.042 | F-Acc 0.994| R-Loss 0.087 | R-Acc 0.962 | V-Loss 0.048 | V-Acc 0.984\n",
      "5: F-Loss 0.045 | F-Acc 0.993| R-Loss 0.065 | R-Acc 0.981 | V-Loss 0.037 | V-Acc 0.989\n",
      "Batch 600 in 1.86s\n",
      "0: F-Loss 0.034 | F-Acc 0.990| R-Loss 0.067 | R-Acc 0.976 | V-Loss 0.084 | V-Acc 0.964\n",
      "1: F-Loss 0.027 | F-Acc 0.996| R-Loss 0.063 | R-Acc 0.978 | V-Loss 0.033 | V-Acc 0.990\n",
      "2: F-Loss 0.020 | F-Acc 0.997| R-Loss 0.073 | R-Acc 0.979 | V-Loss 0.028 | V-Acc 0.994\n",
      "3: F-Loss 0.018 | F-Acc 0.998| R-Loss 0.067 | R-Acc 0.980 | V-Loss 0.007 | V-Acc 0.999\n",
      "4: F-Loss 0.032 | F-Acc 0.994| R-Loss 0.066 | R-Acc 0.978 | V-Loss 0.048 | V-Acc 0.986\n",
      "5: F-Loss 0.032 | F-Acc 0.994| R-Loss 0.057 | R-Acc 0.981 | V-Loss 0.048 | V-Acc 0.987\n",
      "Batch 800 in 1.67s\n",
      "0: F-Loss 0.054 | F-Acc 0.982| R-Loss 0.060 | R-Acc 0.979 | V-Loss 0.052 | V-Acc 0.978\n",
      "1: F-Loss 0.036 | F-Acc 0.987| R-Loss 0.042 | R-Acc 0.987 | V-Loss 0.014 | V-Acc 0.997\n",
      "2: F-Loss 0.035 | F-Acc 0.994| R-Loss 0.041 | R-Acc 0.987 | V-Loss 0.022 | V-Acc 0.995\n",
      "3: F-Loss 0.049 | F-Acc 0.991| R-Loss 0.049 | R-Acc 0.982 | V-Loss 0.011 | V-Acc 0.996\n",
      "4: F-Loss 0.054 | F-Acc 0.991| R-Loss 0.073 | R-Acc 0.973 | V-Loss 0.030 | V-Acc 0.993\n",
      "5: F-Loss 0.048 | F-Acc 0.991| R-Loss 0.043 | R-Acc 0.984 | V-Loss 0.029 | V-Acc 0.993\n",
      "Epoch 1 finished in 1697.7s\n",
      "Starting Epoch 2\n",
      " ** Testing ** \n",
      "0: F-Loss 0.042 | F-Acc 0.980| R-Loss 0.067 | R-Acc 0.967 | V-Loss 0.061 | V-Acc 0.970\n",
      "1: F-Loss 0.027 | F-Acc 0.983| R-Loss 0.046 | R-Acc 0.976 | V-Loss 0.021 | V-Acc 0.985\n",
      "2: F-Loss 0.026 | F-Acc 0.985| R-Loss 0.047 | R-Acc 0.974 | V-Loss 0.017 | V-Acc 0.986\n",
      "3: F-Loss 0.025 | F-Acc 0.985| R-Loss 0.055 | R-Acc 0.971 | V-Loss 0.015 | V-Acc 0.986\n",
      "4: F-Loss 0.034 | F-Acc 0.985| R-Loss 0.073 | R-Acc 0.964 | V-Loss 0.031 | V-Acc 0.982\n",
      "5: F-Loss 0.031 | F-Acc 0.985| R-Loss 0.051 | R-Acc 0.972 | V-Loss 0.032 | V-Acc 0.982\n",
      "Batch 0 in 1.67s\n",
      "0: F-Loss 0.040 | F-Acc 0.989| R-Loss 0.100 | R-Acc 0.968 | V-Loss 0.041 | V-Acc 0.986\n",
      "1: F-Loss 0.027 | F-Acc 0.989| R-Loss 0.056 | R-Acc 0.986 | V-Loss 0.020 | V-Acc 0.994\n",
      "2: F-Loss 0.024 | F-Acc 0.997| R-Loss 0.034 | R-Acc 0.984 | V-Loss 0.023 | V-Acc 0.995\n",
      "3: F-Loss 0.027 | F-Acc 0.996| R-Loss 0.048 | R-Acc 0.985 | V-Loss 0.009 | V-Acc 0.999\n",
      "4: F-Loss 0.039 | F-Acc 0.994| R-Loss 0.094 | R-Acc 0.967 | V-Loss 0.037 | V-Acc 0.988\n",
      "5: F-Loss 0.036 | F-Acc 0.996| R-Loss 0.046 | R-Acc 0.985 | V-Loss 0.043 | V-Acc 0.988\n",
      "Batch 200 in 1.65s\n",
      "0: F-Loss 0.028 | F-Acc 0.992| R-Loss 0.072 | R-Acc 0.970 | V-Loss 0.047 | V-Acc 0.983\n",
      "1: F-Loss 0.019 | F-Acc 0.994| R-Loss 0.043 | R-Acc 0.987 | V-Loss 0.021 | V-Acc 0.995\n",
      "2: F-Loss 0.014 | F-Acc 1.000| R-Loss 0.057 | R-Acc 0.979 | V-Loss 0.014 | V-Acc 0.994\n",
      "3: F-Loss 0.021 | F-Acc 0.996| R-Loss 0.051 | R-Acc 0.982 | V-Loss 0.010 | V-Acc 0.998\n",
      "4: F-Loss 0.028 | F-Acc 0.995| R-Loss 0.071 | R-Acc 0.971 | V-Loss 0.019 | V-Acc 0.994\n",
      "5: F-Loss 0.027 | F-Acc 0.998| R-Loss 0.049 | R-Acc 0.986 | V-Loss 0.031 | V-Acc 0.993\n",
      "Batch 400 in 1.93s\n",
      "0: F-Loss 0.046 | F-Acc 0.985| R-Loss 0.048 | R-Acc 0.988 | V-Loss 0.054 | V-Acc 0.980\n",
      "1: F-Loss 0.023 | F-Acc 0.992| R-Loss 0.036 | R-Acc 0.988 | V-Loss 0.015 | V-Acc 0.994\n",
      "2: F-Loss 0.025 | F-Acc 0.996| R-Loss 0.039 | R-Acc 0.986 | V-Loss 0.007 | V-Acc 0.999\n",
      "3: F-Loss 0.027 | F-Acc 0.991| R-Loss 0.063 | R-Acc 0.979 | V-Loss 0.013 | V-Acc 0.996\n",
      "4: F-Loss 0.027 | F-Acc 0.995| R-Loss 0.084 | R-Acc 0.976 | V-Loss 0.025 | V-Acc 0.993\n",
      "5: F-Loss 0.032 | F-Acc 0.995| R-Loss 0.043 | R-Acc 0.983 | V-Loss 0.022 | V-Acc 0.993\n",
      "Batch 600 in 2.05s\n",
      "0: F-Loss 0.039 | F-Acc 0.988| R-Loss 0.068 | R-Acc 0.978 | V-Loss 0.038 | V-Acc 0.987\n",
      "1: F-Loss 0.021 | F-Acc 0.995| R-Loss 0.042 | R-Acc 0.989 | V-Loss 0.016 | V-Acc 0.997\n",
      "2: F-Loss 0.016 | F-Acc 0.999| R-Loss 0.059 | R-Acc 0.979 | V-Loss 0.013 | V-Acc 0.997\n",
      "3: F-Loss 0.015 | F-Acc 0.999| R-Loss 0.056 | R-Acc 0.981 | V-Loss 0.012 | V-Acc 0.996\n",
      "4: F-Loss 0.028 | F-Acc 0.999| R-Loss 0.064 | R-Acc 0.978 | V-Loss 0.022 | V-Acc 0.995\n",
      "5: F-Loss 0.028 | F-Acc 0.997| R-Loss 0.030 | R-Acc 0.990 | V-Loss 0.017 | V-Acc 0.997\n",
      "Batch 800 in 2.48s\n",
      "0: F-Loss 0.037 | F-Acc 0.990| R-Loss 0.060 | R-Acc 0.979 | V-Loss 0.039 | V-Acc 0.984\n",
      "1: F-Loss 0.013 | F-Acc 0.996| R-Loss 0.035 | R-Acc 0.992 | V-Loss 0.018 | V-Acc 0.997\n",
      "2: F-Loss 0.028 | F-Acc 0.993| R-Loss 0.046 | R-Acc 0.988 | V-Loss 0.015 | V-Acc 0.996\n",
      "3: F-Loss 0.022 | F-Acc 0.994| R-Loss 0.050 | R-Acc 0.983 | V-Loss 0.028 | V-Acc 0.994\n",
      "4: F-Loss 0.028 | F-Acc 0.995| R-Loss 0.071 | R-Acc 0.976 | V-Loss 0.021 | V-Acc 0.995\n",
      "5: F-Loss 0.025 | F-Acc 0.994| R-Loss 0.058 | R-Acc 0.979 | V-Loss 0.023 | V-Acc 0.994\n",
      "Epoch 2 finished in 1775.6s\n",
      "Starting Epoch 3\n",
      " ** Testing ** \n",
      "0: F-Loss 0.035 | F-Acc 0.980| R-Loss 0.053 | R-Acc 0.973 | V-Loss 0.047 | V-Acc 0.974\n",
      "1: F-Loss 0.021 | F-Acc 0.984| R-Loss 0.037 | R-Acc 0.979 | V-Loss 0.012 | V-Acc 0.987\n",
      "2: F-Loss 0.021 | F-Acc 0.985| R-Loss 0.039 | R-Acc 0.977 | V-Loss 0.013 | V-Acc 0.987\n",
      "3: F-Loss 0.021 | F-Acc 0.986| R-Loss 0.041 | R-Acc 0.976 | V-Loss 0.011 | V-Acc 0.987\n",
      "4: F-Loss 0.028 | F-Acc 0.985| R-Loss 0.051 | R-Acc 0.972 | V-Loss 0.022 | V-Acc 0.984\n",
      "5: F-Loss 0.026 | F-Acc 0.985| R-Loss 0.038 | R-Acc 0.977 | V-Loss 0.024 | V-Acc 0.984\n",
      "Batch 0 in 1.72s\n",
      "0: F-Loss 0.029 | F-Acc 0.993| R-Loss 0.058 | R-Acc 0.985 | V-Loss 0.048 | V-Acc 0.979\n",
      "1: F-Loss 0.035 | F-Acc 0.988| R-Loss 0.025 | R-Acc 0.994 | V-Loss 0.018 | V-Acc 0.995\n",
      "2: F-Loss 0.018 | F-Acc 0.996| R-Loss 0.028 | R-Acc 0.993 | V-Loss 0.020 | V-Acc 0.995\n",
      "3: F-Loss 0.027 | F-Acc 0.993| R-Loss 0.047 | R-Acc 0.981 | V-Loss 0.002 | V-Acc 1.000\n",
      "4: F-Loss 0.032 | F-Acc 0.993| R-Loss 0.035 | R-Acc 0.988 | V-Loss 0.027 | V-Acc 0.993\n",
      "5: F-Loss 0.023 | F-Acc 0.998| R-Loss 0.031 | R-Acc 0.991 | V-Loss 0.027 | V-Acc 0.994\n",
      "Batch 200 in 2.40s\n",
      "0: F-Loss 0.039 | F-Acc 0.989| R-Loss 0.024 | R-Acc 0.989 | V-Loss 0.064 | V-Acc 0.977\n",
      "1: F-Loss 0.014 | F-Acc 0.998| R-Loss 0.035 | R-Acc 0.987 | V-Loss 0.010 | V-Acc 0.999\n",
      "2: F-Loss 0.016 | F-Acc 0.997| R-Loss 0.032 | R-Acc 0.987 | V-Loss 0.007 | V-Acc 0.998\n",
      "3: F-Loss 0.021 | F-Acc 0.995| R-Loss 0.033 | R-Acc 0.990 | V-Loss 0.006 | V-Acc 0.997\n",
      "4: F-Loss 0.026 | F-Acc 0.996| R-Loss 0.047 | R-Acc 0.982 | V-Loss 0.028 | V-Acc 0.990\n",
      "5: F-Loss 0.026 | F-Acc 0.996| R-Loss 0.051 | R-Acc 0.983 | V-Loss 0.027 | V-Acc 0.991\n",
      "Batch 400 in 2.72s\n",
      "0: F-Loss 0.024 | F-Acc 0.992| R-Loss 0.052 | R-Acc 0.981 | V-Loss 0.049 | V-Acc 0.980\n",
      "1: F-Loss 0.016 | F-Acc 0.997| R-Loss 0.031 | R-Acc 0.993 | V-Loss 0.014 | V-Acc 0.996\n",
      "2: F-Loss 0.014 | F-Acc 0.998| R-Loss 0.019 | R-Acc 0.994 | V-Loss 0.009 | V-Acc 0.998\n",
      "3: F-Loss 0.017 | F-Acc 0.995| R-Loss 0.062 | R-Acc 0.981 | V-Loss 0.005 | V-Acc 0.998\n",
      "4: F-Loss 0.022 | F-Acc 0.998| R-Loss 0.040 | R-Acc 0.986 | V-Loss 0.018 | V-Acc 0.997\n",
      "5: F-Loss 0.016 | F-Acc 0.998| R-Loss 0.047 | R-Acc 0.990 | V-Loss 0.022 | V-Acc 0.995\n",
      "Batch 600 in 1.86s\n",
      "0: F-Loss 0.026 | F-Acc 0.993| R-Loss 0.071 | R-Acc 0.981 | V-Loss 0.043 | V-Acc 0.987\n",
      "1: F-Loss 0.017 | F-Acc 0.995| R-Loss 0.035 | R-Acc 0.988 | V-Loss 0.007 | V-Acc 0.998\n",
      "2: F-Loss 0.015 | F-Acc 0.996| R-Loss 0.036 | R-Acc 0.988 | V-Loss 0.015 | V-Acc 0.997\n",
      "3: F-Loss 0.013 | F-Acc 0.998| R-Loss 0.037 | R-Acc 0.990 | V-Loss 0.015 | V-Acc 0.997\n",
      "4: F-Loss 0.026 | F-Acc 0.995| R-Loss 0.040 | R-Acc 0.979 | V-Loss 0.032 | V-Acc 0.992\n",
      "5: F-Loss 0.015 | F-Acc 0.998| R-Loss 0.052 | R-Acc 0.982 | V-Loss 0.024 | V-Acc 0.994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a5b6cbaaadfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mvelcocity_changed_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mrandom_velocities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mvelocity_changed_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelcocity_changed_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0mpredicted_sigmoids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocity_changed_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0mvelocity_changed_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinaryLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_sigmoids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/honors-project/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-22e33770dd70>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, graph, state)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmessagePassingIteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumMessagePassingIterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessageFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'm_hat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/honors-project/lib/python3.8/site-packages/dgl/heterograph.py\u001b[0m in \u001b[0;36mupdate_all\u001b[0;34m(self, message_func, reduce_func, apply_node_func, etype)\u001b[0m\n\u001b[1;32m   4684\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetagraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4685\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0metype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4686\u001b[0;31m         \u001b[0mndata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_passing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_node_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4687\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_n_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/honors-project/lib/python3.8/site-packages/dgl/core.py\u001b[0m in \u001b[0;36mmessage_passing\u001b[0;34m(g, mfunc, rfunc, afunc)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0morig_eid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mmsgdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvoke_edge_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonical_etypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_eid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morig_eid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;31m# reduce phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_builtin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/honors-project/lib/python3.8/site-packages/dgl/core.py\u001b[0m in \u001b[0;36minvoke_edge_udf\u001b[0;34m(graph, eid, etype, func, orig_eid)\u001b[0m\n\u001b[1;32m     83\u001b[0m     ebatch = EdgeBatch(graph, eid if orig_eid is None else orig_eid,\n\u001b[1;32m     84\u001b[0m                        etype, srcdata, edata, dstdata)\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mebatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minvoke_udf_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsgdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_nid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-22e33770dd70>\u001b[0m in \u001b[0;36mmessageFunction\u001b[0;34m(self, edges)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnodeInput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'm'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessageNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgeData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodeInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdateFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/honors-project/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0fcd6811522f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/honors-project/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/honors-project/lib/python3.8/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    174\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/honors-project/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2344\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2345\u001b[0m         )\n\u001b[0;32m-> 2346\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    \n",
    "    print('Starting Epoch {}'.format(epoch))\n",
    "    # Record epoch start time to calculate per epoch time\n",
    "    epoch_t0 = time.time()\n",
    "    \n",
    "    # Randomize the order of traininig examples\n",
    "    for morphIdx in trainingIdxs:\n",
    "        permutation = np.random.permutation(states_train[morphIdx].shape[0])\n",
    "\n",
    "        states_train[morphIdx] = states_train[morphIdx][permutation]\n",
    "        next_states_train[morphIdx] = next_states_train[morphIdx][permutation]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for morphIdx in trainingIdxs:\n",
    "        \n",
    "            testLosses[morphIdx].append(torch.zeros(6))\n",
    "            \n",
    "            for batch_ in range(0, numTestingBatches-1):\n",
    "                \n",
    "                # Get new graphs for each iteration\n",
    "                g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                g2 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                g3 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "\n",
    "                current_states = states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "#                 forward_states = next_states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "                forward_states = current_states - next_states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "\n",
    "                forward_x = torch.cat((current_states, forward_states), -1).to(device)\n",
    "                predicted_sigmoids = gnn(g1, forward_x)\n",
    "                forwardLoss = binaryLoss(predicted_sigmoids, oneTensor)\n",
    "                \n",
    "                # Save Forward Loss and Accuracy\n",
    "                testLosses[morphIdx][-1][0] += forwardLoss.item()\n",
    "                testLosses[morphIdx][-1][1] += torch.eq(oneTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "                choices_range = np.arange(states_test[morphIdx].shape[0])\n",
    "                random_indeces = np.random.choice(choices_range, size=current_states.shape[0])\n",
    "                \n",
    "#                 random_states = states_test[morphIdx][random_indeces]\n",
    "                random_states = current_states - states_test[morphIdx][random_indeces]\n",
    "                random_x = torch.cat((current_states, random_states), -1).to(device)\n",
    "                predicted_sigmoids = gnn(g2, random_x)\n",
    "                randomLoss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "\n",
    "                testLosses[morphIdx][-1][2] += randomLoss.item()\n",
    "                testLosses[morphIdx][-1][3] += torch.eq(zeroTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "                \n",
    "                random_velocities = torch.from_numpy(np.random.normal(loc=0, scale=1.0, size=batch_size))\n",
    "                velcocity_changed_states = torch.zeros(batch_size, current_states.shape[-1])\n",
    "                velcocity_changed_states[:, 0] = -random_velocities\n",
    "                velocity_changed_x = torch.cat((current_states, velcocity_changed_states), -1).to(device)\n",
    "                predicted_sigmoids = gnn(g3, velocity_changed_x)\n",
    "                velocity_changed_loss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "\n",
    "                testLosses[morphIdx][-1][4] += velocity_changed_loss.item()\n",
    "                testLosses[morphIdx][-1][5] += torch.eq(zeroTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "            testLosses[morphIdx][-1] /= numTestingBatches\n",
    "    \n",
    "    print(' ** Testing ** ')\n",
    "    for morphIdx in trainingIdxs:\n",
    "        print('{}: F-Loss {:.3f} | F-Acc {:.3f}| R-Loss {:.3f} | R-Acc {:.3f} | V-Loss {:.3f} | V-Acc {:.3f}'.\n",
    "            format(morphIdx, testLosses[morphIdx][-1][0], testLosses[morphIdx][-1][1], testLosses[morphIdx][-1][2], testLosses[morphIdx][-1][3], testLosses[morphIdx][-1][4], testLosses[morphIdx][-1][5]))\n",
    "\n",
    "    \n",
    "    for batch in range(0, numTrainingBatches-1, numBatchesPerTrainingStep):\n",
    "                \n",
    "        batch_t0 = time.time()\n",
    "        \n",
    "        for morphIdx in trainingIdxs:\n",
    "            trainLosses[morphIdx].append(torch.zeros(6))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batchOffset in range(numBatchesPerTrainingStep):\n",
    "\n",
    "            if batch + batchOffset >= numTrainingBatches - 1:\n",
    "                break\n",
    "                \n",
    "            for morphIdx in trainingIdxs:\n",
    "                \n",
    "                # Get new graphs for each iteration\n",
    "                g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                g2 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                g3 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                \n",
    "                current_states = states_train[morphIdx][(batch + batchOffset) * batch_size:(batch + batchOffset + 1)*batch_size]\n",
    "#                 forward_states = next_states_train[morphIdx][(batch + batchOffset) * batch_size:(batch + batchOffset + 1)*batch_size]\n",
    "                forward_states = current_states - next_states_train[morphIdx][(batch + batchOffset) * batch_size:(batch + batchOffset + 1)*batch_size]\n",
    "\n",
    "                forward_x = torch.cat((current_states, forward_states), -1).to(device)\n",
    "                \n",
    "                predicted_sigmoids = gnn(g1, forward_x)\n",
    "                forwardLoss = binaryLoss(predicted_sigmoids, oneTensor)\n",
    "                \n",
    "                # Save Forward Loss and Accuracy\n",
    "                trainLosses[morphIdx][-1][0] += forwardLoss.item()\n",
    "                trainLosses[morphIdx][-1][1] += torch.eq(oneTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "                (forwardLoss / numBatchesPerTrainingStep).backward()\n",
    "\n",
    "                choices_range = np.arange(states_train[morphIdx].shape[0])\n",
    "                random_indeces = np.random.choice(choices_range, size=batch_size)\n",
    "\n",
    "#                 random_states = states_train[morphIdx][random_indeces]\n",
    "                random_states = current_states - states_train[morphIdx][random_indeces]\n",
    "                \n",
    "                random_x = torch.cat((current_states, random_states), -1).to(device)\n",
    "                \n",
    "                predicted_sigmoids = gnn(g2, random_x)\n",
    "\n",
    "                randomLoss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "                (9 * (randomLoss / numBatchesPerTrainingStep) / 10).backward()\n",
    "\n",
    "                trainLosses[morphIdx][-1][2] += randomLoss.item()\n",
    "                trainLosses[morphIdx][-1][3] += torch.eq(zeroTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "                \n",
    "                random_velocities = torch.from_numpy(np.random.normal(loc=0, scale=1.0, size=batch_size))\n",
    "                velcocity_changed_states = torch.zeros(batch_size, current_states.shape[-1])\n",
    "                velcocity_changed_states[:, 0] = -random_velocities\n",
    "                velocity_changed_x = torch.cat((current_states, velcocity_changed_states), -1).to(device)\n",
    "                predicted_sigmoids = gnn(g3, velocity_changed_x)\n",
    "                velocity_changed_loss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "                \n",
    "                ((velocity_changed_loss / numBatchesPerTrainingStep) / 10).backward()\n",
    "                \n",
    "                trainLosses[morphIdx][-1][4] += velocity_changed_loss.item()\n",
    "                trainLosses[morphIdx][-1][5] += torch.eq(zeroTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "                        \n",
    "        for morphIdx in trainingIdxs:\n",
    "            trainLosses[morphIdx][-1] /= numBatchesPerTrainingStep\n",
    "\n",
    "        optimizer.step()\n",
    "        batch_time = time.time() - batch_t0\n",
    "\n",
    "        if batch % 200 == 0:\n",
    "            print('Batch {} in {:.2f}s'.format(batch, batch_time))\n",
    "            \n",
    "            for morphIdx in trainingIdxs:\n",
    "                print('{}: F-Loss {:.3f} | F-Acc {:.3f}| R-Loss {:.3f} | R-Acc {:.3f} | V-Loss {:.3f} | V-Acc {:.3f}'.\n",
    "                    format(morphIdx, trainLosses[morphIdx][-1][0], trainLosses[morphIdx][-1][1], trainLosses[morphIdx][-1][2], trainLosses[morphIdx][-1][3], trainLosses[morphIdx][-1][4], trainLosses[morphIdx][-1][5]))\n",
    "\n",
    "\n",
    "    print('Epoch {} finished in {:.1f}s'.format(epoch, time.time() - epoch_t0))\n",
    "    torch.save(gnn.state_dict(), 'mixed-delta-validTransition-withVelocity-rightWayAround.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gnn.state_dict(), 'mixed-delta-validTransition-withVelocity-rightWayAround.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn.load_state_dict(torch.load('mixed-delta-validTransition.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0.62854996, 0.82670998]), 1: array([0.03162608, 0.98389668]), 2: array([0.02427717, 0.98458426]), 3: array([0.04707087, 0.98125598]), 4: array([0.07193574, 0.97553611]), 5: array([0.06604599, 0.97768854])}\n"
     ]
    }
   ],
   "source": [
    "backwardLosses = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    \n",
    "    backwardLosses[morphIdx] = np.zeros(2)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch_ in range(0, numTestingBatches-1):\n",
    "\n",
    "            # Get new graphs for each iteration\n",
    "            g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "\n",
    "            current_states = states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "            forward_states = next_states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "            backward_x = torch.cat((forward_states, forward_states - current_states), -1).to(device)\n",
    "            predicted_sigmoids = gnn(g1, backward_x)\n",
    "            backward_loss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "\n",
    "            # Save Forward Loss and Accuracy\n",
    "            backwardLosses[morphIdx][0] += backward_loss.item()\n",
    "            backwardLosses[morphIdx][1] += torch.eq(zeroTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "        backwardLosses[morphIdx] /= numTestingBatches\n",
    "            \n",
    "print(backwardLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0.19854589, 0.9530957 ]), 1: array([0.03052252, 0.99081055]), 2: array([0.00587696, 0.99836914]), 3: array([0.00777521, 0.99623047]), 4: array([0.15561728, 0.96998047]), 5: array([0.08751803, 0.98418945])}\n"
     ]
    }
   ],
   "source": [
    "velocityChangeLosses = {}\n",
    "num_samples = 1024\n",
    "num_states = 100\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    velocityChangeLosses[morphIdx] = np.zeros(2)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "                    \n",
    "        state_indeces = np.random.choice(np.arange(states_test[morphIdx].shape[0]), size=num_states)\n",
    "\n",
    "        for state_idx in state_indeces:\n",
    "\n",
    "            # Get new graphs for each iteration\n",
    "            g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "\n",
    "            current_states = states_test[morphIdx][state_idx].repeat(num_samples, 1)\n",
    "#             velocities = torch.from_numpy(np.linspace(start=0, stop=2, num=num_samples))\n",
    "            random_velocities = torch.from_numpy(np.random.normal(loc=0, scale=1.0, size=batch_size))\n",
    "            velcocity_changed_states = torch.zeros(batch_size, current_states.shape[-1])\n",
    "            velcocity_changed_states[:, 0] = random_velocities\n",
    "            velocities_changed_x = torch.cat((current_states, velcocity_changed_states), -1).to(device)\n",
    "            predicted_sigmoids = gnn(g1, velocities_changed_x)\n",
    "            velocity_changed_loss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "            \n",
    "#             print(predicted_sigmoids)\n",
    "            \n",
    "            # Save Forward Loss and Accuracy\n",
    "            velocityChangeLosses[morphIdx][0] += velocity_changed_loss.item()\n",
    "            velocityChangeLosses[morphIdx][1] += torch.eq(zeroTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "        velocityChangeLosses[morphIdx] /= num_states\n",
    "            \n",
    "print(velocityChangeLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0.00762939, 0.98811185]), 1: array([0.00558225, 0.98852041]), 2: array([0.00624063, 0.98839086]), 3: array([0.00531584, 0.98840083]), 4: array([0.0068727 , 0.98844069]), 5: array([0.00683172, 0.98847058])}\n"
     ]
    }
   ],
   "source": [
    "forward_losses = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    \n",
    "    forward_losses[morphIdx] = np.zeros(2)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch_ in range(0, numTestingBatches-1):\n",
    "\n",
    "            # Get new graphs for each iteration\n",
    "            g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "\n",
    "            current_states = states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "            forward_states = next_states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "            forward_x = torch.cat((current_states, current_states - forward_states), -1).to(device)\n",
    "            predicted_sigmoids = gnn(g1, forward_x)\n",
    "            forward_loss = binaryLoss(predicted_sigmoids, oneTensor)\n",
    "\n",
    "            # Save Forward Loss and Accuracy\n",
    "            forward_losses[morphIdx][0] += forward_loss.item()\n",
    "            forward_losses[morphIdx][1] += torch.eq(oneTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "        forward_losses[morphIdx] /= numTestingBatches\n",
    "            \n",
    "print(forward_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
