{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.autograd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "import dgl\n",
    "from graphenvs import HalfCheetahGraphEnv\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        hidden_sizes,\n",
    "        with_batch_norm=False,\n",
    "        activation=None\n",
    "    ):\n",
    "        super(Network, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        if with_batch_norm:\n",
    "            self.layers.append(nn.LayerNorm(normalized_shape=(hidden_sizes[0])))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            if with_batch_norm:\n",
    "                self.layers.append(nn.LayerNorm(normalized_shape=(hidden_sizes[i+1])))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], self.output_size))\n",
    "        \n",
    "        if activation is not None:\n",
    "            self.layers.append(activation())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputNetwork,\n",
    "        messageNetwork,\n",
    "        updateNetwork,\n",
    "        outputNetwork,\n",
    "        numMessagePassingIterations,\n",
    "        withInputNetwork = True\n",
    "    ):\n",
    "        \n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "                \n",
    "        self.inputNetwork = inputNetwork\n",
    "        self.messageNetwork = messageNetwork\n",
    "        self.updateNetwork = updateNetwork\n",
    "        self.outputNetwork = outputNetwork\n",
    "        \n",
    "        self.numMessagePassingIterations = numMessagePassingIterations\n",
    "        self.withInputNetwork = withInputNetwork\n",
    "        \n",
    "    def inputFunction(self, nodes):\n",
    "        return {'state' : self.inputNetwork(nodes.data['input'])}\n",
    "    \n",
    "    def messageFunction(self, edges):\n",
    "        \n",
    "        batchSize = edges.src['state'].shape[1]\n",
    "        edgeData = edges.data['feature'].repeat(batchSize, 1).T.unsqueeze(-1)\n",
    "        nodeInput = edges.src['input']\n",
    "        \n",
    "        return {'m' : self.messageNetwork(torch.cat((edges.src['state'], edgeData, nodeInput), -1))}\n",
    "    \n",
    "    def updateFunction(self, nodes):\n",
    "        return {'state': self.updateNetwork(torch.cat((nodes.data['m_hat'], nodes.data['state']), -1))}\n",
    "    \n",
    "    def outputFunction(self, nodes):\n",
    "        \n",
    "        return {'output': self.outputNetwork(nodes.data['state'])}\n",
    "\n",
    "\n",
    "    def forward(self, graph, state):\n",
    "        \n",
    "        self.update_states_in_graph(graph, state)\n",
    "        \n",
    "        if self.withInputNetwork:\n",
    "            graph.apply_nodes(self.inputFunction)\n",
    "        \n",
    "        for messagePassingIteration in range(self.numMessagePassingIterations):\n",
    "            graph.update_all(self.messageFunction, dgl.function.mean('m', 'm_hat'), self.updateFunction)\n",
    "        \n",
    "        graph.apply_nodes(self.outputFunction)\n",
    "        \n",
    "        output = graph.ndata['output']\n",
    "        output = output.squeeze(-1).mean(0)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    def update_states_in_graph(self, graph, state):\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        numGraphFeature = 6\n",
    "        numGlobalStateInformation = 5\n",
    "        numLocalStateInformation = 2\n",
    "        numStateVar = state.shape[1] // 2\n",
    "        globalInformation = torch.cat((state[:, 0:5], state[:, numStateVar:numStateVar+5]), -1)\n",
    "        \n",
    "        numNodes = (numStateVar - 5) // 2\n",
    "\n",
    "        nodeData = torch.empty((numNodes, state.shape[0], numGraphFeature + 2 * numGlobalStateInformation + 2 * numLocalStateInformation)).to(device)\n",
    "        for nodeIdx in range(numNodes):\n",
    "\n",
    "            # Assign global features from graph\n",
    "            nodeData[nodeIdx, :, :6] = graph.ndata['feature'][nodeIdx]\n",
    "            # Assign local state information\n",
    "            nodeData[nodeIdx, :, 16] = state[:, 5 + nodeIdx]\n",
    "            nodeData[nodeIdx, :, 17] = state[:, 5 + numNodes + nodeIdx]\n",
    "            nodeData[nodeIdx, :, 18] = state[:, numStateVar + 5 + nodeIdx]\n",
    "            nodeData[nodeIdx, :, 19] = state[:, numStateVar + 5 + numNodes + nodeIdx]\n",
    "\n",
    "        # Assdign global state information\n",
    "        nodeData[:, :, 6:16] = globalInformation\n",
    "        \n",
    "        if self.withInputNetwork:\n",
    "            graph.ndata['input'] = nodeData        \n",
    "        \n",
    "        else:\n",
    "            graph.ndata['state'] = nodeData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingIdxs = [0, 1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ovi/anaconda3/envs/honors-project/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    }
   ],
   "source": [
    "states = {}\n",
    "actions = {}\n",
    "rewards = {}\n",
    "next_states = {}\n",
    "dones = {}\n",
    "env = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "\n",
    "    prefix = '../datasets/{}/'.format(morphIdx)\n",
    "    \n",
    "    states[morphIdx] = np.load(prefix + 'states_array.npy')\n",
    "    actions[morphIdx] = np.load(prefix + 'actions_array.npy')\n",
    "    rewards[morphIdx] = np.load(prefix + 'rewards_array.npy')\n",
    "    next_states[morphIdx] = np.load(prefix + 'next_states_array.npy')\n",
    "    dones[morphIdx] = np.load(prefix + 'dones_array.npy')\n",
    "    \n",
    "    env[morphIdx] = HalfCheetahGraphEnv(None)\n",
    "    env[morphIdx].set_morphology(morphIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_train = {}\n",
    "states_test = {}\n",
    "next_states_train = {}\n",
    "next_states_test = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    permutation = np.random.permutation(states[morphIdx].shape[0])\n",
    "    \n",
    "    states[morphIdx] = states[morphIdx][permutation]\n",
    "    next_states[morphIdx] = next_states[morphIdx][permutation]\n",
    "    \n",
    "    states_train[morphIdx] = torch.from_numpy(states[morphIdx][100000:]).float()\n",
    "    states_test[morphIdx] = torch.from_numpy(states[morphIdx][:100000]).float()\n",
    "    \n",
    "    next_states_train[morphIdx] = torch.from_numpy(next_states[morphIdx][100000:]).float()\n",
    "    next_states_test[morphIdx] = torch.from_numpy(next_states[morphIdx][:100000]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [256, 256]\n",
    "\n",
    "inputSize = 20\n",
    "stateSize = 64\n",
    "messageSize = 64\n",
    "outputSize = 1\n",
    "numMessagePassingIterations = 6\n",
    "batch_size = 2048\n",
    "with_batch_norm=True\n",
    "numBatchesPerTrainingStep = 1\n",
    "\n",
    "inputNetwork = Network(inputSize, stateSize, hidden_sizes, with_batch_norm)\n",
    "messageNetwork = Network(stateSize + inputSize + 1, messageSize, hidden_sizes, with_batch_norm, nn.Tanh)\n",
    "updateNetwork = Network(stateSize + messageSize, stateSize, hidden_sizes, with_batch_norm)\n",
    "outputNetwork = Network(stateSize, outputSize, hidden_sizes, with_batch_norm, nn.Sigmoid)\n",
    "\n",
    "gnn = GraphNeuralNetwork(inputNetwork, messageNetwork, updateNetwork, outputNetwork, numMessagePassingIterations).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "optimizer = optim.Adam(itertools.chain(inputNetwork.parameters(), messageNetwork.parameters(), updateNetwork.parameters(), outputNetwork.parameters())\n",
    "                       , lr=lr, weight_decay=0)\n",
    "\n",
    "# lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0, verbose=True, min_lr=1e-5)\n",
    "binaryLoss = nn.BCELoss()\n",
    "\n",
    "zeroTensor = torch.zeros([batch_size]).to(device)\n",
    "oneTensor = torch.ones([batch_size]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrainingBatches = int(np.ceil(states_train[trainingIdxs[0]].shape[0] / batch_size))\n",
    "numTestingBatches = int(np.ceil(states_test[trainingIdxs[0]].shape[0] / batch_size))\n",
    "\n",
    "trainLosses = {}\n",
    "testLosses = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    trainLosses[morphIdx] = []\n",
    "    testLosses[morphIdx] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0\n",
      "Test Idx 0 | F-Loss 0.870 | F-Acc 0.000 | R-Loss 0.516 | R-Acc 0.980\n",
      "Test Idx 1 | F-Loss 0.861 | F-Acc 0.000 | R-Loss 0.519 | R-Acc 0.980\n",
      "Test Idx 2 | F-Loss 0.870 | F-Acc 0.000 | R-Loss 0.516 | R-Acc 0.980\n",
      "Test Idx 3 | F-Loss 0.863 | F-Acc 0.000 | R-Loss 0.519 | R-Acc 0.980\n",
      "Test Idx 4 | F-Loss 0.864 | F-Acc 0.000 | R-Loss 0.526 | R-Acc 0.980\n",
      "Test Idx 5 | F-Loss 0.858 | F-Acc 0.000 | R-Loss 0.526 | R-Acc 0.980\n",
      "Batch 0 in 1.95s\n",
      "Train Idx 0 | F-Loss 0.889 | F-Acc 0.000| R-Loss 0.527 | R-Acc 1.000\n",
      "Train Idx 1 | F-Loss 0.879 | F-Acc 0.000| R-Loss 0.531 | R-Acc 1.000\n",
      "Train Idx 2 | F-Loss 0.887 | F-Acc 0.000| R-Loss 0.526 | R-Acc 1.000\n",
      "Train Idx 3 | F-Loss 0.882 | F-Acc 0.000| R-Loss 0.531 | R-Acc 1.000\n",
      "Train Idx 4 | F-Loss 0.882 | F-Acc 0.000| R-Loss 0.537 | R-Acc 1.000\n",
      "Train Idx 5 | F-Loss 0.874 | F-Acc 0.000| R-Loss 0.538 | R-Acc 1.000\n",
      "Batch 200 in 2.36s\n",
      "Train Idx 0 | F-Loss 0.199 | F-Acc 0.953| R-Loss 0.255 | R-Acc 0.897\n",
      "Train Idx 1 | F-Loss 0.157 | F-Acc 0.963| R-Loss 0.157 | R-Acc 0.941\n",
      "Train Idx 2 | F-Loss 0.161 | F-Acc 0.965| R-Loss 0.204 | R-Acc 0.915\n",
      "Train Idx 3 | F-Loss 0.148 | F-Acc 0.974| R-Loss 0.222 | R-Acc 0.910\n",
      "Train Idx 4 | F-Loss 0.189 | F-Acc 0.956| R-Loss 0.215 | R-Acc 0.916\n",
      "Train Idx 5 | F-Loss 0.171 | F-Acc 0.963| R-Loss 0.171 | R-Acc 0.940\n",
      "Batch 400 in 2.01s\n",
      "Train Idx 0 | F-Loss 0.111 | F-Acc 0.970| R-Loss 0.151 | R-Acc 0.944\n",
      "Train Idx 1 | F-Loss 0.066 | F-Acc 0.985| R-Loss 0.084 | R-Acc 0.974\n",
      "Train Idx 2 | F-Loss 0.071 | F-Acc 0.985| R-Loss 0.095 | R-Acc 0.968\n",
      "Train Idx 3 | F-Loss 0.064 | F-Acc 0.988| R-Loss 0.102 | R-Acc 0.963\n",
      "Train Idx 4 | F-Loss 0.095 | F-Acc 0.981| R-Loss 0.144 | R-Acc 0.947\n",
      "Train Idx 5 | F-Loss 0.078 | F-Acc 0.985| R-Loss 0.097 | R-Acc 0.967\n",
      "Epoch 0 finished in 979.7s\n",
      "Starting Epoch 1\n",
      "Test Idx 0 | F-Loss 0.109 | F-Acc 0.949 | R-Loss 0.132 | R-Acc 0.932\n",
      "Test Idx 1 | F-Loss 0.060 | F-Acc 0.966 | R-Loss 0.090 | R-Acc 0.950\n",
      "Test Idx 2 | F-Loss 0.067 | F-Acc 0.966 | R-Loss 0.099 | R-Acc 0.946\n",
      "Test Idx 3 | F-Loss 0.062 | F-Acc 0.968 | R-Loss 0.106 | R-Acc 0.944\n",
      "Test Idx 4 | F-Loss 0.088 | F-Acc 0.960 | R-Loss 0.126 | R-Acc 0.935\n",
      "Test Idx 5 | F-Loss 0.073 | F-Acc 0.967 | R-Loss 0.093 | R-Acc 0.947\n",
      "Batch 0 in 2.48s\n",
      "Train Idx 0 | F-Loss 0.116 | F-Acc 0.969| R-Loss 0.120 | R-Acc 0.953\n",
      "Train Idx 1 | F-Loss 0.060 | F-Acc 0.986| R-Loss 0.090 | R-Acc 0.972\n",
      "Train Idx 2 | F-Loss 0.062 | F-Acc 0.992| R-Loss 0.101 | R-Acc 0.965\n",
      "Train Idx 3 | F-Loss 0.061 | F-Acc 0.989| R-Loss 0.117 | R-Acc 0.959\n",
      "Train Idx 4 | F-Loss 0.098 | F-Acc 0.977| R-Loss 0.133 | R-Acc 0.950\n",
      "Train Idx 5 | F-Loss 0.077 | F-Acc 0.986| R-Loss 0.084 | R-Acc 0.969\n",
      "Batch 200 in 2.06s\n",
      "Train Idx 0 | F-Loss 0.088 | F-Acc 0.974| R-Loss 0.133 | R-Acc 0.958\n",
      "Train Idx 1 | F-Loss 0.057 | F-Acc 0.985| R-Loss 0.079 | R-Acc 0.974\n",
      "Train Idx 2 | F-Loss 0.061 | F-Acc 0.981| R-Loss 0.076 | R-Acc 0.973\n",
      "Train Idx 3 | F-Loss 0.060 | F-Acc 0.985| R-Loss 0.086 | R-Acc 0.972\n",
      "Train Idx 4 | F-Loss 0.063 | F-Acc 0.988| R-Loss 0.098 | R-Acc 0.968\n",
      "Train Idx 5 | F-Loss 0.054 | F-Acc 0.992| R-Loss 0.063 | R-Acc 0.977\n",
      "Batch 400 in 2.15s\n",
      "Train Idx 0 | F-Loss 0.057 | F-Acc 0.985| R-Loss 0.127 | R-Acc 0.958\n",
      "Train Idx 1 | F-Loss 0.028 | F-Acc 0.994| R-Loss 0.068 | R-Acc 0.979\n",
      "Train Idx 2 | F-Loss 0.031 | F-Acc 0.996| R-Loss 0.092 | R-Acc 0.972\n",
      "Train Idx 3 | F-Loss 0.030 | F-Acc 0.994| R-Loss 0.089 | R-Acc 0.972\n",
      "Train Idx 4 | F-Loss 0.042 | F-Acc 0.992| R-Loss 0.103 | R-Acc 0.966\n",
      "Train Idx 5 | F-Loss 0.035 | F-Acc 0.997| R-Loss 0.079 | R-Acc 0.977\n",
      "Epoch 1 finished in 1021.6s\n",
      "Starting Epoch 2\n",
      "Test Idx 0 | F-Loss 0.060 | F-Acc 0.963 | R-Loss 0.096 | R-Acc 0.948\n",
      "Test Idx 1 | F-Loss 0.031 | F-Acc 0.973 | R-Loss 0.065 | R-Acc 0.959\n",
      "Test Idx 2 | F-Loss 0.033 | F-Acc 0.973 | R-Loss 0.069 | R-Acc 0.958\n",
      "Test Idx 3 | F-Loss 0.032 | F-Acc 0.973 | R-Loss 0.071 | R-Acc 0.957\n",
      "Test Idx 4 | F-Loss 0.047 | F-Acc 0.969 | R-Loss 0.087 | R-Acc 0.951\n",
      "Test Idx 5 | F-Loss 0.038 | F-Acc 0.974 | R-Loss 0.066 | R-Acc 0.957\n",
      "Batch 0 in 2.26s\n",
      "Train Idx 0 | F-Loss 0.062 | F-Acc 0.983| R-Loss 0.092 | R-Acc 0.966\n",
      "Train Idx 1 | F-Loss 0.028 | F-Acc 0.997| R-Loss 0.058 | R-Acc 0.983\n",
      "Train Idx 2 | F-Loss 0.037 | F-Acc 0.989| R-Loss 0.075 | R-Acc 0.974\n",
      "Train Idx 3 | F-Loss 0.031 | F-Acc 0.993| R-Loss 0.081 | R-Acc 0.977\n",
      "Train Idx 4 | F-Loss 0.043 | F-Acc 0.992| R-Loss 0.080 | R-Acc 0.973\n",
      "Train Idx 5 | F-Loss 0.040 | F-Acc 0.993| R-Loss 0.072 | R-Acc 0.974\n",
      "Batch 200 in 2.04s\n",
      "Train Idx 0 | F-Loss 0.056 | F-Acc 0.984| R-Loss 0.090 | R-Acc 0.967\n",
      "Train Idx 1 | F-Loss 0.038 | F-Acc 0.991| R-Loss 0.048 | R-Acc 0.983\n",
      "Train Idx 2 | F-Loss 0.036 | F-Acc 0.993| R-Loss 0.046 | R-Acc 0.988\n",
      "Train Idx 3 | F-Loss 0.039 | F-Acc 0.991| R-Loss 0.056 | R-Acc 0.981\n",
      "Train Idx 4 | F-Loss 0.041 | F-Acc 0.992| R-Loss 0.074 | R-Acc 0.979\n",
      "Train Idx 5 | F-Loss 0.047 | F-Acc 0.988| R-Loss 0.052 | R-Acc 0.982\n",
      "Batch 400 in 2.08s\n",
      "Train Idx 0 | F-Loss 0.050 | F-Acc 0.982| R-Loss 0.071 | R-Acc 0.979\n",
      "Train Idx 1 | F-Loss 0.031 | F-Acc 0.992| R-Loss 0.035 | R-Acc 0.989\n",
      "Train Idx 2 | F-Loss 0.031 | F-Acc 0.992| R-Loss 0.044 | R-Acc 0.986\n",
      "Train Idx 3 | F-Loss 0.034 | F-Acc 0.990| R-Loss 0.042 | R-Acc 0.986\n",
      "Train Idx 4 | F-Loss 0.043 | F-Acc 0.991| R-Loss 0.067 | R-Acc 0.978\n",
      "Train Idx 5 | F-Loss 0.047 | F-Acc 0.990| R-Loss 0.034 | R-Acc 0.989\n",
      "Epoch 2 finished in 957.0s\n",
      "Starting Epoch 3\n",
      "Test Idx 0 | F-Loss 0.043 | F-Acc 0.967 | R-Loss 0.077 | R-Acc 0.955\n",
      "Test Idx 1 | F-Loss 0.026 | F-Acc 0.973 | R-Loss 0.045 | R-Acc 0.966\n",
      "Test Idx 2 | F-Loss 0.028 | F-Acc 0.973 | R-Loss 0.049 | R-Acc 0.964\n",
      "Test Idx 3 | F-Loss 0.027 | F-Acc 0.974 | R-Loss 0.050 | R-Acc 0.964\n",
      "Test Idx 4 | F-Loss 0.036 | F-Acc 0.972 | R-Loss 0.069 | R-Acc 0.957\n",
      "Test Idx 5 | F-Loss 0.032 | F-Acc 0.974 | R-Loss 0.048 | R-Acc 0.964\n",
      "Batch 0 in 2.02s\n",
      "Train Idx 0 | F-Loss 0.040 | F-Acc 0.989| R-Loss 0.077 | R-Acc 0.975\n",
      "Train Idx 1 | F-Loss 0.028 | F-Acc 0.994| R-Loss 0.034 | R-Acc 0.989\n",
      "Train Idx 2 | F-Loss 0.034 | F-Acc 0.990| R-Loss 0.048 | R-Acc 0.988\n",
      "Train Idx 3 | F-Loss 0.029 | F-Acc 0.994| R-Loss 0.055 | R-Acc 0.982\n",
      "Train Idx 4 | F-Loss 0.041 | F-Acc 0.993| R-Loss 0.071 | R-Acc 0.981\n",
      "Train Idx 5 | F-Loss 0.036 | F-Acc 0.993| R-Loss 0.052 | R-Acc 0.981\n",
      "Batch 200 in 1.99s\n",
      "Train Idx 0 | F-Loss 0.044 | F-Acc 0.988| R-Loss 0.089 | R-Acc 0.974\n",
      "Train Idx 1 | F-Loss 0.025 | F-Acc 0.993| R-Loss 0.040 | R-Acc 0.989\n",
      "Train Idx 2 | F-Loss 0.033 | F-Acc 0.989| R-Loss 0.034 | R-Acc 0.991\n",
      "Train Idx 3 | F-Loss 0.031 | F-Acc 0.994| R-Loss 0.038 | R-Acc 0.987\n",
      "Train Idx 4 | F-Loss 0.041 | F-Acc 0.990| R-Loss 0.057 | R-Acc 0.980\n",
      "Train Idx 5 | F-Loss 0.035 | F-Acc 0.991| R-Loss 0.039 | R-Acc 0.987\n",
      "Batch 400 in 2.00s\n",
      "Train Idx 0 | F-Loss 0.031 | F-Acc 0.992| R-Loss 0.067 | R-Acc 0.979\n",
      "Train Idx 1 | F-Loss 0.017 | F-Acc 0.997| R-Loss 0.036 | R-Acc 0.988\n",
      "Train Idx 2 | F-Loss 0.015 | F-Acc 0.998| R-Loss 0.055 | R-Acc 0.983\n",
      "Train Idx 3 | F-Loss 0.017 | F-Acc 0.997| R-Loss 0.051 | R-Acc 0.982\n",
      "Train Idx 4 | F-Loss 0.025 | F-Acc 0.996| R-Loss 0.060 | R-Acc 0.981\n",
      "Train Idx 5 | F-Loss 0.020 | F-Acc 0.999| R-Loss 0.043 | R-Acc 0.986\n",
      "Epoch 3 finished in 922.3s\n",
      "Starting Epoch 4\n",
      "Test Idx 0 | F-Loss 0.028 | F-Acc 0.972 | R-Loss 0.070 | R-Acc 0.957\n",
      "Test Idx 1 | F-Loss 0.020 | F-Acc 0.975 | R-Loss 0.041 | R-Acc 0.967\n",
      "Test Idx 2 | F-Loss 0.019 | F-Acc 0.975 | R-Loss 0.047 | R-Acc 0.965\n",
      "Test Idx 3 | F-Loss 0.018 | F-Acc 0.976 | R-Loss 0.047 | R-Acc 0.965\n",
      "Test Idx 4 | F-Loss 0.026 | F-Acc 0.974 | R-Loss 0.062 | R-Acc 0.960\n",
      "Test Idx 5 | F-Loss 0.024 | F-Acc 0.975 | R-Loss 0.045 | R-Acc 0.966\n",
      "Batch 0 in 1.99s\n",
      "Train Idx 0 | F-Loss 0.024 | F-Acc 0.996| R-Loss 0.070 | R-Acc 0.977\n",
      "Train Idx 1 | F-Loss 0.021 | F-Acc 0.995| R-Loss 0.039 | R-Acc 0.989\n",
      "Train Idx 2 | F-Loss 0.020 | F-Acc 0.998| R-Loss 0.041 | R-Acc 0.986\n",
      "Train Idx 3 | F-Loss 0.019 | F-Acc 0.996| R-Loss 0.046 | R-Acc 0.984\n",
      "Train Idx 4 | F-Loss 0.030 | F-Acc 0.993| R-Loss 0.058 | R-Acc 0.981\n",
      "Train Idx 5 | F-Loss 0.026 | F-Acc 0.993| R-Loss 0.053 | R-Acc 0.982\n",
      "Batch 200 in 1.97s\n",
      "Train Idx 0 | F-Loss 0.032 | F-Acc 0.989| R-Loss 0.063 | R-Acc 0.978\n",
      "Train Idx 1 | F-Loss 0.025 | F-Acc 0.993| R-Loss 0.031 | R-Acc 0.989\n",
      "Train Idx 2 | F-Loss 0.022 | F-Acc 0.992| R-Loss 0.052 | R-Acc 0.983\n",
      "Train Idx 3 | F-Loss 0.023 | F-Acc 0.994| R-Loss 0.037 | R-Acc 0.986\n",
      "Train Idx 4 | F-Loss 0.033 | F-Acc 0.994| R-Loss 0.046 | R-Acc 0.984\n",
      "Train Idx 5 | F-Loss 0.029 | F-Acc 0.994| R-Loss 0.040 | R-Acc 0.989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400 in 2.00s\n",
      "Train Idx 0 | F-Loss 0.038 | F-Acc 0.990| R-Loss 0.068 | R-Acc 0.981\n",
      "Train Idx 1 | F-Loss 0.030 | F-Acc 0.992| R-Loss 0.025 | R-Acc 0.993\n",
      "Train Idx 2 | F-Loss 0.029 | F-Acc 0.992| R-Loss 0.026 | R-Acc 0.993\n",
      "Train Idx 3 | F-Loss 0.022 | F-Acc 0.996| R-Loss 0.025 | R-Acc 0.993\n",
      "Train Idx 4 | F-Loss 0.037 | F-Acc 0.988| R-Loss 0.054 | R-Acc 0.983\n",
      "Train Idx 5 | F-Loss 0.032 | F-Acc 0.992| R-Loss 0.038 | R-Acc 0.989\n",
      "Epoch 4 finished in 921.9s\n",
      "Starting Epoch 5\n",
      "Test Idx 0 | F-Loss 0.025 | F-Acc 0.973 | R-Loss 0.061 | R-Acc 0.960\n",
      "Test Idx 1 | F-Loss 0.017 | F-Acc 0.975 | R-Loss 0.033 | R-Acc 0.970\n",
      "Test Idx 2 | F-Loss 0.018 | F-Acc 0.975 | R-Loss 0.038 | R-Acc 0.968\n",
      "Test Idx 3 | F-Loss 0.017 | F-Acc 0.976 | R-Loss 0.037 | R-Acc 0.968\n",
      "Test Idx 4 | F-Loss 0.023 | F-Acc 0.975 | R-Loss 0.053 | R-Acc 0.963\n",
      "Test Idx 5 | F-Loss 0.021 | F-Acc 0.976 | R-Loss 0.038 | R-Acc 0.967\n",
      "Batch 0 in 2.01s\n",
      "Train Idx 0 | F-Loss 0.030 | F-Acc 0.993| R-Loss 0.051 | R-Acc 0.982\n",
      "Train Idx 1 | F-Loss 0.015 | F-Acc 0.997| R-Loss 0.025 | R-Acc 0.992\n",
      "Train Idx 2 | F-Loss 0.019 | F-Acc 0.995| R-Loss 0.031 | R-Acc 0.989\n",
      "Train Idx 3 | F-Loss 0.020 | F-Acc 0.995| R-Loss 0.030 | R-Acc 0.991\n",
      "Train Idx 4 | F-Loss 0.024 | F-Acc 0.995| R-Loss 0.050 | R-Acc 0.982\n",
      "Train Idx 5 | F-Loss 0.020 | F-Acc 0.996| R-Loss 0.037 | R-Acc 0.988\n",
      "Batch 200 in 2.01s\n",
      "Train Idx 0 | F-Loss 0.019 | F-Acc 0.998| R-Loss 0.051 | R-Acc 0.985\n",
      "Train Idx 1 | F-Loss 0.016 | F-Acc 0.996| R-Loss 0.032 | R-Acc 0.990\n",
      "Train Idx 2 | F-Loss 0.011 | F-Acc 0.999| R-Loss 0.038 | R-Acc 0.989\n",
      "Train Idx 3 | F-Loss 0.010 | F-Acc 0.999| R-Loss 0.024 | R-Acc 0.994\n",
      "Train Idx 4 | F-Loss 0.022 | F-Acc 0.994| R-Loss 0.054 | R-Acc 0.981\n",
      "Train Idx 5 | F-Loss 0.020 | F-Acc 0.997| R-Loss 0.039 | R-Acc 0.988\n",
      "Batch 400 in 2.06s\n",
      "Train Idx 0 | F-Loss 0.028 | F-Acc 0.992| R-Loss 0.037 | R-Acc 0.988\n",
      "Train Idx 1 | F-Loss 0.015 | F-Acc 0.997| R-Loss 0.016 | R-Acc 0.993\n",
      "Train Idx 2 | F-Loss 0.019 | F-Acc 0.995| R-Loss 0.024 | R-Acc 0.993\n",
      "Train Idx 3 | F-Loss 0.019 | F-Acc 0.994| R-Loss 0.033 | R-Acc 0.989\n",
      "Train Idx 4 | F-Loss 0.027 | F-Acc 0.994| R-Loss 0.040 | R-Acc 0.988\n",
      "Train Idx 5 | F-Loss 0.020 | F-Acc 0.997| R-Loss 0.031 | R-Acc 0.989\n",
      "Epoch 5 finished in 927.6s\n",
      "Starting Epoch 6\n",
      "Test Idx 0 | F-Loss 0.029 | F-Acc 0.972 | R-Loss 0.046 | R-Acc 0.965\n",
      "Test Idx 1 | F-Loss 0.016 | F-Acc 0.975 | R-Loss 0.029 | R-Acc 0.971\n",
      "Test Idx 2 | F-Loss 0.019 | F-Acc 0.975 | R-Loss 0.030 | R-Acc 0.971\n",
      "Test Idx 3 | F-Loss 0.016 | F-Acc 0.976 | R-Loss 0.030 | R-Acc 0.970\n",
      "Test Idx 4 | F-Loss 0.022 | F-Acc 0.975 | R-Loss 0.043 | R-Acc 0.967\n",
      "Test Idx 5 | F-Loss 0.020 | F-Acc 0.975 | R-Loss 0.034 | R-Acc 0.969\n",
      "Batch 0 in 2.07s\n",
      "Train Idx 0 | F-Loss 0.024 | F-Acc 0.993| R-Loss 0.041 | R-Acc 0.983\n",
      "Train Idx 1 | F-Loss 0.012 | F-Acc 0.998| R-Loss 0.047 | R-Acc 0.986\n",
      "Train Idx 2 | F-Loss 0.026 | F-Acc 0.994| R-Loss 0.035 | R-Acc 0.989\n",
      "Train Idx 3 | F-Loss 0.018 | F-Acc 0.994| R-Loss 0.037 | R-Acc 0.990\n",
      "Train Idx 4 | F-Loss 0.025 | F-Acc 0.995| R-Loss 0.043 | R-Acc 0.985\n",
      "Train Idx 5 | F-Loss 0.020 | F-Acc 0.997| R-Loss 0.023 | R-Acc 0.993\n",
      "Batch 200 in 1.99s\n",
      "Train Idx 0 | F-Loss 0.037 | F-Acc 0.990| R-Loss 0.039 | R-Acc 0.987\n",
      "Train Idx 1 | F-Loss 0.026 | F-Acc 0.995| R-Loss 0.020 | R-Acc 0.993\n",
      "Train Idx 2 | F-Loss 0.019 | F-Acc 0.996| R-Loss 0.031 | R-Acc 0.993\n",
      "Train Idx 3 | F-Loss 0.017 | F-Acc 0.998| R-Loss 0.035 | R-Acc 0.992\n",
      "Train Idx 4 | F-Loss 0.027 | F-Acc 0.994| R-Loss 0.031 | R-Acc 0.990\n",
      "Train Idx 5 | F-Loss 0.019 | F-Acc 0.998| R-Loss 0.023 | R-Acc 0.994\n",
      "Batch 400 in 1.98s\n",
      "Train Idx 0 | F-Loss 0.018 | F-Acc 0.995| R-Loss 0.058 | R-Acc 0.985\n",
      "Train Idx 1 | F-Loss 0.013 | F-Acc 0.997| R-Loss 0.021 | R-Acc 0.991\n",
      "Train Idx 2 | F-Loss 0.013 | F-Acc 0.996| R-Loss 0.039 | R-Acc 0.989\n",
      "Train Idx 3 | F-Loss 0.014 | F-Acc 0.996| R-Loss 0.042 | R-Acc 0.985\n",
      "Train Idx 4 | F-Loss 0.016 | F-Acc 0.998| R-Loss 0.037 | R-Acc 0.989\n",
      "Train Idx 5 | F-Loss 0.016 | F-Acc 0.997| R-Loss 0.047 | R-Acc 0.986\n",
      "Epoch 6 finished in 928.5s\n",
      "Starting Epoch 7\n",
      "Test Idx 0 | F-Loss 0.025 | F-Acc 0.973 | R-Loss 0.039 | R-Acc 0.967\n",
      "Test Idx 1 | F-Loss 0.016 | F-Acc 0.975 | R-Loss 0.025 | R-Acc 0.972\n",
      "Test Idx 2 | F-Loss 0.019 | F-Acc 0.975 | R-Loss 0.024 | R-Acc 0.972\n",
      "Test Idx 3 | F-Loss 0.018 | F-Acc 0.975 | R-Loss 0.024 | R-Acc 0.972\n",
      "Test Idx 4 | F-Loss 0.023 | F-Acc 0.974 | R-Loss 0.037 | R-Acc 0.968\n",
      "Test Idx 5 | F-Loss 0.021 | F-Acc 0.975 | R-Loss 0.026 | R-Acc 0.972\n",
      "Batch 0 in 1.99s\n",
      "Train Idx 0 | F-Loss 0.027 | F-Acc 0.992| R-Loss 0.033 | R-Acc 0.989\n",
      "Train Idx 1 | F-Loss 0.017 | F-Acc 0.997| R-Loss 0.028 | R-Acc 0.992\n",
      "Train Idx 2 | F-Loss 0.016 | F-Acc 0.996| R-Loss 0.036 | R-Acc 0.990\n",
      "Train Idx 3 | F-Loss 0.021 | F-Acc 0.995| R-Loss 0.021 | R-Acc 0.993\n",
      "Train Idx 4 | F-Loss 0.025 | F-Acc 0.993| R-Loss 0.042 | R-Acc 0.986\n",
      "Train Idx 5 | F-Loss 0.021 | F-Acc 0.996| R-Loss 0.027 | R-Acc 0.992\n",
      "Batch 200 in 1.97s\n",
      "Train Idx 0 | F-Loss 0.020 | F-Acc 0.995| R-Loss 0.021 | R-Acc 0.993\n",
      "Train Idx 1 | F-Loss 0.014 | F-Acc 0.996| R-Loss 0.018 | R-Acc 0.993\n",
      "Train Idx 2 | F-Loss 0.012 | F-Acc 0.998| R-Loss 0.023 | R-Acc 0.991\n",
      "Train Idx 3 | F-Loss 0.020 | F-Acc 0.996| R-Loss 0.025 | R-Acc 0.992\n",
      "Train Idx 4 | F-Loss 0.021 | F-Acc 0.995| R-Loss 0.042 | R-Acc 0.988\n",
      "Train Idx 5 | F-Loss 0.020 | F-Acc 0.995| R-Loss 0.022 | R-Acc 0.994\n",
      "Batch 400 in 1.96s\n",
      "Train Idx 0 | F-Loss 0.013 | F-Acc 0.997| R-Loss 0.036 | R-Acc 0.990\n",
      "Train Idx 1 | F-Loss 0.009 | F-Acc 0.998| R-Loss 0.029 | R-Acc 0.990\n",
      "Train Idx 2 | F-Loss 0.013 | F-Acc 0.997| R-Loss 0.033 | R-Acc 0.991\n",
      "Train Idx 3 | F-Loss 0.016 | F-Acc 0.996| R-Loss 0.031 | R-Acc 0.991\n",
      "Train Idx 4 | F-Loss 0.018 | F-Acc 0.995| R-Loss 0.026 | R-Acc 0.993\n",
      "Train Idx 5 | F-Loss 0.021 | F-Acc 0.995| R-Loss 0.022 | R-Acc 0.994\n",
      "Epoch 7 finished in 905.3s\n",
      "Starting Epoch 8\n",
      "Test Idx 0 | F-Loss 0.017 | F-Acc 0.975 | R-Loss 0.042 | R-Acc 0.967\n",
      "Test Idx 1 | F-Loss 0.012 | F-Acc 0.977 | R-Loss 0.026 | R-Acc 0.972\n",
      "Test Idx 2 | F-Loss 0.013 | F-Acc 0.977 | R-Loss 0.028 | R-Acc 0.971\n",
      "Test Idx 3 | F-Loss 0.013 | F-Acc 0.977 | R-Loss 0.027 | R-Acc 0.972\n",
      "Test Idx 4 | F-Loss 0.015 | F-Acc 0.977 | R-Loss 0.037 | R-Acc 0.968\n",
      "Test Idx 5 | F-Loss 0.013 | F-Acc 0.977 | R-Loss 0.030 | R-Acc 0.970\n",
      "Batch 0 in 1.97s\n",
      "Train Idx 0 | F-Loss 0.020 | F-Acc 0.996| R-Loss 0.034 | R-Acc 0.990\n",
      "Train Idx 1 | F-Loss 0.013 | F-Acc 0.997| R-Loss 0.027 | R-Acc 0.992\n",
      "Train Idx 2 | F-Loss 0.009 | F-Acc 1.000| R-Loss 0.021 | R-Acc 0.992\n",
      "Train Idx 3 | F-Loss 0.012 | F-Acc 0.999| R-Loss 0.024 | R-Acc 0.994\n",
      "Train Idx 4 | F-Loss 0.017 | F-Acc 0.995| R-Loss 0.035 | R-Acc 0.986\n",
      "Train Idx 5 | F-Loss 0.017 | F-Acc 0.995| R-Loss 0.025 | R-Acc 0.993\n",
      "Batch 200 in 1.97s\n",
      "Train Idx 0 | F-Loss 0.030 | F-Acc 0.990| R-Loss 0.020 | R-Acc 0.995\n",
      "Train Idx 1 | F-Loss 0.021 | F-Acc 0.996| R-Loss 0.014 | R-Acc 0.995\n",
      "Train Idx 2 | F-Loss 0.022 | F-Acc 0.993| R-Loss 0.018 | R-Acc 0.994\n",
      "Train Idx 3 | F-Loss 0.019 | F-Acc 0.995| R-Loss 0.025 | R-Acc 0.992\n",
      "Train Idx 4 | F-Loss 0.025 | F-Acc 0.992| R-Loss 0.025 | R-Acc 0.994\n",
      "Train Idx 5 | F-Loss 0.018 | F-Acc 0.996| R-Loss 0.019 | R-Acc 0.995\n",
      "Batch 400 in 1.96s\n",
      "Train Idx 0 | F-Loss 0.010 | F-Acc 0.998| R-Loss 0.042 | R-Acc 0.987\n",
      "Train Idx 1 | F-Loss 0.007 | F-Acc 0.997| R-Loss 0.021 | R-Acc 0.994\n",
      "Train Idx 2 | F-Loss 0.010 | F-Acc 0.997| R-Loss 0.033 | R-Acc 0.989\n",
      "Train Idx 3 | F-Loss 0.013 | F-Acc 0.997| R-Loss 0.026 | R-Acc 0.992\n",
      "Train Idx 4 | F-Loss 0.009 | F-Acc 1.000| R-Loss 0.035 | R-Acc 0.989\n",
      "Train Idx 5 | F-Loss 0.009 | F-Acc 0.999| R-Loss 0.029 | R-Acc 0.990\n",
      "Epoch 8 finished in 903.0s\n",
      "Starting Epoch 9\n",
      "Test Idx 0 | F-Loss 0.020 | F-Acc 0.974 | R-Loss 0.033 | R-Acc 0.970\n",
      "Test Idx 1 | F-Loss 0.013 | F-Acc 0.976 | R-Loss 0.020 | R-Acc 0.974\n",
      "Test Idx 2 | F-Loss 0.014 | F-Acc 0.976 | R-Loss 0.022 | R-Acc 0.973\n",
      "Test Idx 3 | F-Loss 0.013 | F-Acc 0.976 | R-Loss 0.021 | R-Acc 0.973\n",
      "Test Idx 4 | F-Loss 0.018 | F-Acc 0.975 | R-Loss 0.031 | R-Acc 0.970\n",
      "Test Idx 5 | F-Loss 0.016 | F-Acc 0.976 | R-Loss 0.022 | R-Acc 0.973\n",
      "Batch 0 in 1.98s\n",
      "Train Idx 0 | F-Loss 0.018 | F-Acc 0.995| R-Loss 0.030 | R-Acc 0.988\n",
      "Train Idx 1 | F-Loss 0.007 | F-Acc 1.000| R-Loss 0.024 | R-Acc 0.991\n",
      "Train Idx 2 | F-Loss 0.011 | F-Acc 0.998| R-Loss 0.031 | R-Acc 0.992\n",
      "Train Idx 3 | F-Loss 0.014 | F-Acc 0.997| R-Loss 0.025 | R-Acc 0.994\n",
      "Train Idx 4 | F-Loss 0.013 | F-Acc 0.999| R-Loss 0.038 | R-Acc 0.989\n",
      "Train Idx 5 | F-Loss 0.018 | F-Acc 0.995| R-Loss 0.019 | R-Acc 0.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200 in 1.96s\n",
      "Train Idx 0 | F-Loss 0.017 | F-Acc 0.996| R-Loss 0.040 | R-Acc 0.987\n",
      "Train Idx 1 | F-Loss 0.008 | F-Acc 0.999| R-Loss 0.019 | R-Acc 0.994\n",
      "Train Idx 2 | F-Loss 0.011 | F-Acc 0.998| R-Loss 0.023 | R-Acc 0.993\n",
      "Train Idx 3 | F-Loss 0.010 | F-Acc 0.998| R-Loss 0.027 | R-Acc 0.993\n",
      "Train Idx 4 | F-Loss 0.015 | F-Acc 0.998| R-Loss 0.027 | R-Acc 0.989\n",
      "Train Idx 5 | F-Loss 0.013 | F-Acc 0.998| R-Loss 0.030 | R-Acc 0.990\n",
      "Batch 400 in 1.96s\n",
      "Train Idx 0 | F-Loss 0.020 | F-Acc 0.997| R-Loss 0.034 | R-Acc 0.990\n",
      "Train Idx 1 | F-Loss 0.013 | F-Acc 0.998| R-Loss 0.028 | R-Acc 0.993\n",
      "Train Idx 2 | F-Loss 0.010 | F-Acc 0.998| R-Loss 0.015 | R-Acc 0.996\n",
      "Train Idx 3 | F-Loss 0.012 | F-Acc 0.995| R-Loss 0.024 | R-Acc 0.994\n",
      "Train Idx 4 | F-Loss 0.020 | F-Acc 0.995| R-Loss 0.035 | R-Acc 0.991\n",
      "Train Idx 5 | F-Loss 0.013 | F-Acc 0.997| R-Loss 0.026 | R-Acc 0.991\n",
      "Epoch 9 finished in 902.9s\n",
      "Starting Epoch 10\n",
      "Test Idx 0 | F-Loss 0.017 | F-Acc 0.975 | R-Loss 0.032 | R-Acc 0.970\n",
      "Test Idx 1 | F-Loss 0.010 | F-Acc 0.977 | R-Loss 0.020 | R-Acc 0.974\n",
      "Test Idx 2 | F-Loss 0.012 | F-Acc 0.977 | R-Loss 0.021 | R-Acc 0.973\n",
      "Test Idx 3 | F-Loss 0.011 | F-Acc 0.977 | R-Loss 0.022 | R-Acc 0.973\n",
      "Test Idx 4 | F-Loss 0.015 | F-Acc 0.976 | R-Loss 0.031 | R-Acc 0.970\n",
      "Test Idx 5 | F-Loss 0.013 | F-Acc 0.977 | R-Loss 0.023 | R-Acc 0.972\n",
      "Batch 0 in 1.98s\n",
      "Train Idx 0 | F-Loss 0.017 | F-Acc 0.997| R-Loss 0.043 | R-Acc 0.988\n",
      "Train Idx 1 | F-Loss 0.012 | F-Acc 0.997| R-Loss 0.016 | R-Acc 0.995\n",
      "Train Idx 2 | F-Loss 0.010 | F-Acc 0.998| R-Loss 0.023 | R-Acc 0.993\n",
      "Train Idx 3 | F-Loss 0.015 | F-Acc 0.994| R-Loss 0.019 | R-Acc 0.994\n",
      "Train Idx 4 | F-Loss 0.013 | F-Acc 0.999| R-Loss 0.045 | R-Acc 0.986\n",
      "Train Idx 5 | F-Loss 0.012 | F-Acc 0.999| R-Loss 0.030 | R-Acc 0.991\n",
      "Batch 200 in 1.97s\n",
      "Train Idx 0 | F-Loss 0.012 | F-Acc 0.999| R-Loss 0.047 | R-Acc 0.988\n",
      "Train Idx 1 | F-Loss 0.005 | F-Acc 0.999| R-Loss 0.026 | R-Acc 0.991\n",
      "Train Idx 2 | F-Loss 0.012 | F-Acc 0.996| R-Loss 0.025 | R-Acc 0.993\n",
      "Train Idx 3 | F-Loss 0.011 | F-Acc 0.996| R-Loss 0.036 | R-Acc 0.992\n",
      "Train Idx 4 | F-Loss 0.015 | F-Acc 0.998| R-Loss 0.022 | R-Acc 0.993\n",
      "Train Idx 5 | F-Loss 0.017 | F-Acc 0.996| R-Loss 0.020 | R-Acc 0.993\n",
      "Batch 400 in 1.97s\n",
      "Train Idx 0 | F-Loss 0.012 | F-Acc 0.995| R-Loss 0.045 | R-Acc 0.988\n",
      "Train Idx 1 | F-Loss 0.006 | F-Acc 0.999| R-Loss 0.017 | R-Acc 0.994\n",
      "Train Idx 2 | F-Loss 0.006 | F-Acc 1.000| R-Loss 0.018 | R-Acc 0.996\n",
      "Train Idx 3 | F-Loss 0.011 | F-Acc 0.998| R-Loss 0.026 | R-Acc 0.992\n",
      "Train Idx 4 | F-Loss 0.011 | F-Acc 0.998| R-Loss 0.028 | R-Acc 0.990\n",
      "Train Idx 5 | F-Loss 0.015 | F-Acc 0.996| R-Loss 0.039 | R-Acc 0.992\n",
      "Epoch 10 finished in 904.2s\n",
      "Starting Epoch 11\n",
      "Test Idx 0 | F-Loss 0.011 | F-Acc 0.977 | R-Loss 0.032 | R-Acc 0.970\n",
      "Test Idx 1 | F-Loss 0.009 | F-Acc 0.977 | R-Loss 0.020 | R-Acc 0.974\n",
      "Test Idx 2 | F-Loss 0.008 | F-Acc 0.978 | R-Loss 0.024 | R-Acc 0.973\n",
      "Test Idx 3 | F-Loss 0.008 | F-Acc 0.978 | R-Loss 0.022 | R-Acc 0.973\n",
      "Test Idx 4 | F-Loss 0.011 | F-Acc 0.977 | R-Loss 0.031 | R-Acc 0.970\n",
      "Test Idx 5 | F-Loss 0.010 | F-Acc 0.978 | R-Loss 0.024 | R-Acc 0.972\n",
      "Batch 0 in 1.98s\n",
      "Train Idx 0 | F-Loss 0.014 | F-Acc 0.997| R-Loss 0.033 | R-Acc 0.987\n",
      "Train Idx 1 | F-Loss 0.009 | F-Acc 0.999| R-Loss 0.013 | R-Acc 0.995\n",
      "Train Idx 2 | F-Loss 0.007 | F-Acc 0.998| R-Loss 0.020 | R-Acc 0.996\n",
      "Train Idx 3 | F-Loss 0.012 | F-Acc 0.998| R-Loss 0.015 | R-Acc 0.995\n",
      "Train Idx 4 | F-Loss 0.012 | F-Acc 0.998| R-Loss 0.014 | R-Acc 0.995\n",
      "Train Idx 5 | F-Loss 0.010 | F-Acc 0.998| R-Loss 0.028 | R-Acc 0.991\n",
      "Batch 200 in 1.96s\n",
      "Train Idx 0 | F-Loss 0.015 | F-Acc 0.997| R-Loss 0.019 | R-Acc 0.993\n",
      "Train Idx 1 | F-Loss 0.014 | F-Acc 0.998| R-Loss 0.020 | R-Acc 0.994\n",
      "Train Idx 2 | F-Loss 0.011 | F-Acc 0.998| R-Loss 0.013 | R-Acc 0.996\n",
      "Train Idx 3 | F-Loss 0.012 | F-Acc 0.998| R-Loss 0.013 | R-Acc 0.996\n",
      "Train Idx 4 | F-Loss 0.015 | F-Acc 0.997| R-Loss 0.025 | R-Acc 0.992\n",
      "Train Idx 5 | F-Loss 0.014 | F-Acc 0.997| R-Loss 0.028 | R-Acc 0.992\n",
      "Batch 400 in 1.96s\n",
      "Train Idx 0 | F-Loss 0.013 | F-Acc 0.999| R-Loss 0.040 | R-Acc 0.989\n",
      "Train Idx 1 | F-Loss 0.012 | F-Acc 0.998| R-Loss 0.017 | R-Acc 0.995\n",
      "Train Idx 2 | F-Loss 0.015 | F-Acc 0.996| R-Loss 0.028 | R-Acc 0.992\n",
      "Train Idx 3 | F-Loss 0.007 | F-Acc 0.998| R-Loss 0.019 | R-Acc 0.995\n",
      "Train Idx 4 | F-Loss 0.013 | F-Acc 0.998| R-Loss 0.017 | R-Acc 0.993\n",
      "Train Idx 5 | F-Loss 0.008 | F-Acc 1.000| R-Loss 0.016 | R-Acc 0.996\n",
      "Epoch 11 finished in 905.1s\n",
      "Starting Epoch 12\n",
      "Test Idx 0 | F-Loss 0.017 | F-Acc 0.975 | R-Loss 0.022 | R-Acc 0.973\n",
      "Test Idx 1 | F-Loss 0.015 | F-Acc 0.976 | R-Loss 0.014 | R-Acc 0.976\n",
      "Test Idx 2 | F-Loss 0.015 | F-Acc 0.976 | R-Loss 0.015 | R-Acc 0.975\n",
      "Test Idx 3 | F-Loss 0.019 | F-Acc 0.975 | R-Loss 0.013 | R-Acc 0.976\n",
      "Test Idx 4 | F-Loss 0.019 | F-Acc 0.975 | R-Loss 0.021 | R-Acc 0.973\n",
      "Test Idx 5 | F-Loss 0.017 | F-Acc 0.976 | R-Loss 0.017 | R-Acc 0.975\n",
      "Batch 0 in 1.98s\n",
      "Train Idx 0 | F-Loss 0.020 | F-Acc 0.997| R-Loss 0.027 | R-Acc 0.992\n",
      "Train Idx 1 | F-Loss 0.010 | F-Acc 0.998| R-Loss 0.016 | R-Acc 0.996\n",
      "Train Idx 2 | F-Loss 0.012 | F-Acc 0.996| R-Loss 0.007 | R-Acc 0.997\n",
      "Train Idx 3 | F-Loss 0.016 | F-Acc 0.997| R-Loss 0.008 | R-Acc 0.998\n",
      "Train Idx 4 | F-Loss 0.024 | F-Acc 0.994| R-Loss 0.013 | R-Acc 0.997\n",
      "Train Idx 5 | F-Loss 0.018 | F-Acc 0.997| R-Loss 0.011 | R-Acc 0.995\n",
      "Batch 200 in 1.97s\n",
      "Train Idx 0 | F-Loss 0.030 | F-Acc 0.993| R-Loss 0.017 | R-Acc 0.994\n",
      "Train Idx 1 | F-Loss 0.021 | F-Acc 0.993| R-Loss 0.017 | R-Acc 0.995\n",
      "Train Idx 2 | F-Loss 0.025 | F-Acc 0.989| R-Loss 0.015 | R-Acc 0.996\n",
      "Train Idx 3 | F-Loss 0.022 | F-Acc 0.996| R-Loss 0.011 | R-Acc 0.996\n",
      "Train Idx 4 | F-Loss 0.031 | F-Acc 0.993| R-Loss 0.015 | R-Acc 0.995\n",
      "Train Idx 5 | F-Loss 0.030 | F-Acc 0.991| R-Loss 0.016 | R-Acc 0.995\n",
      "Batch 400 in 1.97s\n",
      "Train Idx 0 | F-Loss 0.014 | F-Acc 0.994| R-Loss 0.026 | R-Acc 0.994\n",
      "Train Idx 1 | F-Loss 0.004 | F-Acc 0.999| R-Loss 0.024 | R-Acc 0.994\n",
      "Train Idx 2 | F-Loss 0.006 | F-Acc 0.999| R-Loss 0.016 | R-Acc 0.995\n",
      "Train Idx 3 | F-Loss 0.007 | F-Acc 0.998| R-Loss 0.012 | R-Acc 0.994\n",
      "Train Idx 4 | F-Loss 0.008 | F-Acc 0.998| R-Loss 0.015 | R-Acc 0.995\n",
      "Train Idx 5 | F-Loss 0.009 | F-Acc 0.999| R-Loss 0.022 | R-Acc 0.993\n",
      "Epoch 12 finished in 904.9s\n",
      "Starting Epoch 13\n",
      "Test Idx 0 | F-Loss 0.012 | F-Acc 0.977 | R-Loss 0.026 | R-Acc 0.972\n",
      "Test Idx 1 | F-Loss 0.009 | F-Acc 0.978 | R-Loss 0.015 | R-Acc 0.975\n",
      "Test Idx 2 | F-Loss 0.010 | F-Acc 0.977 | R-Loss 0.018 | R-Acc 0.974\n",
      "Test Idx 3 | F-Loss 0.010 | F-Acc 0.977 | R-Loss 0.014 | R-Acc 0.975\n",
      "Test Idx 4 | F-Loss 0.011 | F-Acc 0.977 | R-Loss 0.023 | R-Acc 0.972\n",
      "Test Idx 5 | F-Loss 0.011 | F-Acc 0.977 | R-Loss 0.019 | R-Acc 0.974\n",
      "Batch 0 in 1.99s\n",
      "Train Idx 0 | F-Loss 0.013 | F-Acc 0.996| R-Loss 0.032 | R-Acc 0.990\n",
      "Train Idx 1 | F-Loss 0.008 | F-Acc 0.998| R-Loss 0.013 | R-Acc 0.994\n",
      "Train Idx 2 | F-Loss 0.010 | F-Acc 0.997| R-Loss 0.023 | R-Acc 0.993\n",
      "Train Idx 3 | F-Loss 0.008 | F-Acc 0.999| R-Loss 0.013 | R-Acc 0.997\n",
      "Train Idx 4 | F-Loss 0.011 | F-Acc 0.998| R-Loss 0.029 | R-Acc 0.992\n",
      "Train Idx 5 | F-Loss 0.008 | F-Acc 1.000| R-Loss 0.016 | R-Acc 0.994\n",
      "Batch 200 in 1.97s\n",
      "Train Idx 0 | F-Loss 0.009 | F-Acc 0.999| R-Loss 0.030 | R-Acc 0.991\n",
      "Train Idx 1 | F-Loss 0.007 | F-Acc 0.999| R-Loss 0.024 | R-Acc 0.993\n",
      "Train Idx 2 | F-Loss 0.008 | F-Acc 0.999| R-Loss 0.017 | R-Acc 0.996\n",
      "Train Idx 3 | F-Loss 0.007 | F-Acc 0.998| R-Loss 0.015 | R-Acc 0.996\n",
      "Train Idx 4 | F-Loss 0.008 | F-Acc 0.999| R-Loss 0.037 | R-Acc 0.991\n",
      "Train Idx 5 | F-Loss 0.010 | F-Acc 0.997| R-Loss 0.016 | R-Acc 0.993\n",
      "Batch 400 in 1.97s\n",
      "Train Idx 0 | F-Loss 0.016 | F-Acc 0.996| R-Loss 0.030 | R-Acc 0.991\n",
      "Train Idx 1 | F-Loss 0.007 | F-Acc 0.998| R-Loss 0.016 | R-Acc 0.996\n",
      "Train Idx 2 | F-Loss 0.016 | F-Acc 0.995| R-Loss 0.026 | R-Acc 0.995\n",
      "Train Idx 3 | F-Loss 0.007 | F-Acc 0.999| R-Loss 0.017 | R-Acc 0.994\n",
      "Train Idx 4 | F-Loss 0.013 | F-Acc 0.997| R-Loss 0.019 | R-Acc 0.995\n",
      "Train Idx 5 | F-Loss 0.017 | F-Acc 0.995| R-Loss 0.016 | R-Acc 0.995\n",
      "Epoch 13 finished in 902.9s\n",
      "Starting Epoch 14\n",
      "Test Idx 0 | F-Loss 0.011 | F-Acc 0.977 | R-Loss 0.025 | R-Acc 0.972\n",
      "Test Idx 1 | F-Loss 0.009 | F-Acc 0.977 | R-Loss 0.015 | R-Acc 0.975\n",
      "Test Idx 2 | F-Loss 0.009 | F-Acc 0.977 | R-Loss 0.016 | R-Acc 0.975\n",
      "Test Idx 3 | F-Loss 0.009 | F-Acc 0.977 | R-Loss 0.015 | R-Acc 0.975\n",
      "Test Idx 4 | F-Loss 0.012 | F-Acc 0.977 | R-Loss 0.022 | R-Acc 0.973\n",
      "Test Idx 5 | F-Loss 0.012 | F-Acc 0.977 | R-Loss 0.016 | R-Acc 0.975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 in 1.99s\n",
      "Train Idx 0 | F-Loss 0.009 | F-Acc 0.998| R-Loss 0.030 | R-Acc 0.993\n",
      "Train Idx 1 | F-Loss 0.014 | F-Acc 0.997| R-Loss 0.013 | R-Acc 0.996\n",
      "Train Idx 2 | F-Loss 0.011 | F-Acc 0.998| R-Loss 0.012 | R-Acc 0.998\n",
      "Train Idx 3 | F-Loss 0.007 | F-Acc 0.998| R-Loss 0.020 | R-Acc 0.996\n",
      "Train Idx 4 | F-Loss 0.013 | F-Acc 0.995| R-Loss 0.026 | R-Acc 0.991\n",
      "Train Idx 5 | F-Loss 0.013 | F-Acc 0.995| R-Loss 0.011 | R-Acc 0.997\n",
      "Batch 200 in 1.96s\n",
      "Train Idx 0 | F-Loss 0.012 | F-Acc 0.997| R-Loss 0.026 | R-Acc 0.994\n",
      "Train Idx 1 | F-Loss 0.006 | F-Acc 0.999| R-Loss 0.014 | R-Acc 0.994\n",
      "Train Idx 2 | F-Loss 0.017 | F-Acc 0.996| R-Loss 0.012 | R-Acc 0.995\n",
      "Train Idx 3 | F-Loss 0.012 | F-Acc 0.997| R-Loss 0.005 | R-Acc 0.999\n",
      "Train Idx 4 | F-Loss 0.011 | F-Acc 0.998| R-Loss 0.025 | R-Acc 0.991\n",
      "Train Idx 5 | F-Loss 0.016 | F-Acc 0.994| R-Loss 0.010 | R-Acc 0.997\n",
      "Batch 400 in 1.97s\n",
      "Train Idx 0 | F-Loss 0.013 | F-Acc 0.996| R-Loss 0.014 | R-Acc 0.995\n",
      "Train Idx 1 | F-Loss 0.008 | F-Acc 0.998| R-Loss 0.005 | R-Acc 0.998\n",
      "Train Idx 2 | F-Loss 0.010 | F-Acc 0.998| R-Loss 0.011 | R-Acc 0.997\n",
      "Train Idx 3 | F-Loss 0.012 | F-Acc 0.997| R-Loss 0.011 | R-Acc 0.996\n",
      "Train Idx 4 | F-Loss 0.012 | F-Acc 0.999| R-Loss 0.017 | R-Acc 0.996\n",
      "Train Idx 5 | F-Loss 0.011 | F-Acc 0.998| R-Loss 0.015 | R-Acc 0.996\n",
      "Epoch 14 finished in 903.6s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    \n",
    "    print('Starting Epoch {}'.format(epoch))\n",
    "    # Record epoch start time to calculate per epoch time\n",
    "    epoch_t0 = time.time()\n",
    "    \n",
    "    # Randomize the order of traininig examples\n",
    "    for morphIdx in trainingIdxs:\n",
    "        permutation = np.random.permutation(states_train[morphIdx].shape[0])\n",
    "\n",
    "        states_train[morphIdx] = states_train[morphIdx][permutation]\n",
    "        next_states_train[morphIdx] = next_states_train[morphIdx][permutation]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for morphIdx in trainingIdxs:\n",
    "        \n",
    "            testLosses[morphIdx].append(torch.zeros(4))\n",
    "            \n",
    "            for batch_ in range(0, numTestingBatches-1):\n",
    "                \n",
    "                # Get new graphs for each iteration\n",
    "                g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                g2 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                \n",
    "                current_states = states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "#                 forward_states = next_states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "                forward_states = current_states - next_states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "\n",
    "                forward_x = torch.cat((current_states, forward_states), -1).to(device)\n",
    "                predicted_sigmoids = gnn(g1, forward_x)\n",
    "                forwardLoss = binaryLoss(predicted_sigmoids, oneTensor)\n",
    "                \n",
    "                # Save Forward Loss and Accuracy\n",
    "                testLosses[morphIdx][-1][0] += forwardLoss.item()\n",
    "                testLosses[morphIdx][-1][1] += torch.eq(oneTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "                choices_range = np.arange(states_test[morphIdx].shape[0])\n",
    "                random_indeces = np.random.choice(choices_range, size=current_states.shape[0])\n",
    "                \n",
    "#                 random_states = states_test[morphIdx][random_indeces]\n",
    "                random_states = current_states - states_test[morphIdx][random_indeces]\n",
    "\n",
    "                random_x = torch.cat((current_states, random_states), -1).to(device)\n",
    "                \n",
    "                predicted_sigmoids = gnn(g2, random_x)\n",
    "\n",
    "                randomLoss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "\n",
    "                testLosses[morphIdx][-1][2] += randomLoss.item()\n",
    "                testLosses[morphIdx][-1][3] += torch.eq(zeroTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "            testLosses[morphIdx][-1] /= numTestingBatches\n",
    "    \n",
    "    for morphIdx in trainingIdxs:\n",
    "        print('Test Idx {} | F-Loss {:.3f} | F-Acc {:.3f} | R-Loss {:.3f} | R-Acc {:.3f}'.\n",
    "            format(morphIdx, testLosses[morphIdx][-1][0], testLosses[morphIdx][-1][1], testLosses[morphIdx][-1][2], testLosses[morphIdx][-1][3]))\n",
    "\n",
    "    \n",
    "    for batch in range(0, numTrainingBatches-1, numBatchesPerTrainingStep):\n",
    "                \n",
    "        batch_t0 = time.time()\n",
    "        \n",
    "        for morphIdx in trainingIdxs:\n",
    "            trainLosses[morphIdx].append(torch.zeros(4))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batchOffset in range(numBatchesPerTrainingStep):\n",
    "\n",
    "            if batch + batchOffset >= numTrainingBatches - 1:\n",
    "                break\n",
    "                \n",
    "            for morphIdx in trainingIdxs:\n",
    "                \n",
    "                # Get new graphs for each iteration\n",
    "                g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                g2 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                \n",
    "                current_states = states_train[morphIdx][(batch + batchOffset) * batch_size:(batch + batchOffset + 1)*batch_size]\n",
    "#                 forward_states = next_states_train[morphIdx][(batch + batchOffset) * batch_size:(batch + batchOffset + 1)*batch_size]\n",
    "                forward_states = current_states - next_states_train[morphIdx][(batch + batchOffset) * batch_size:(batch + batchOffset + 1)*batch_size]\n",
    "\n",
    "                forward_x = torch.cat((current_states, forward_states), -1).to(device)\n",
    "                \n",
    "                predicted_sigmoids = gnn(g1, forward_x)\n",
    "                forwardLoss = binaryLoss(predicted_sigmoids, oneTensor)\n",
    "                \n",
    "                # Save Forward Loss and Accuracy\n",
    "                trainLosses[morphIdx][-1][0] += forwardLoss.item()\n",
    "                trainLosses[morphIdx][-1][1] += torch.eq(oneTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "                forwardLoss.backward()\n",
    "\n",
    "\n",
    "                choices_range = np.arange(states_train[morphIdx].shape[0])\n",
    "                random_indeces = np.random.choice(choices_range, size=batch_size)\n",
    "\n",
    "#                 random_states = states_train[morphIdx][random_indeces]\n",
    "                random_states = current_states - states_train[morphIdx][random_indeces]\n",
    "                \n",
    "                random_x = torch.cat((current_states, random_states), -1).to(device)\n",
    "                \n",
    "                predicted_sigmoids = gnn(g2, random_x)\n",
    "\n",
    "                randomLoss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "                randomLoss.backward()\n",
    "\n",
    "                trainLosses[morphIdx][-1][2] += randomLoss.item()\n",
    "                trainLosses[morphIdx][-1][3] += torch.eq(zeroTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "                        \n",
    "        for morphIdx in trainingIdxs:\n",
    "            trainLosses[morphIdx][-1] /= numBatchesPerTrainingStep\n",
    "\n",
    "        optimizer.step()\n",
    "        batch_time = time.time() - batch_t0\n",
    "\n",
    "        if batch % 200 == 0:\n",
    "            print('Batch {} in {:.2f}s'.format(batch, batch_time))\n",
    "            \n",
    "            for morphIdx in trainingIdxs:\n",
    "                print('Train Idx {} | F-Loss {:.3f} | F-Acc {:.3f}| R-Loss {:.3f} | R-Acc {:.3f}'.\n",
    "                    format(morphIdx, trainLosses[morphIdx][-1][0], trainLosses[morphIdx][-1][1], trainLosses[morphIdx][-1][2], trainLosses[morphIdx][-1][3]))\n",
    "\n",
    "\n",
    "    print('Epoch {} finished in {:.1f}s'.format(epoch, time.time() - epoch_t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gnn.state_dict(), 'mixed-delta-validTransition.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn.load_state_dict(torch.load('mixed-delta-validTransition.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0.62110414, 0.82023278]), 1: array([0.03034668, 0.97377232]), 2: array([0.02481634, 0.97454959]), 3: array([0.04841778, 0.97093232]), 4: array([0.06627391, 0.96639828]), 5: array([0.06741389, 0.96696628])}\n"
     ]
    }
   ],
   "source": [
    "backwardLosses = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    \n",
    "    backwardLosses[morphIdx] = np.zeros(2)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        testLosses[morphIdx].append(torch.zeros(4))\n",
    "\n",
    "        for batch_ in range(0, numTestingBatches-1):\n",
    "\n",
    "            # Get new graphs for each iteration\n",
    "            g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "\n",
    "            current_states = states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "            forward_states = next_states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "            backward_x = torch.cat((forward_states, forward_states - current_states), -1).to(device)\n",
    "            predicted_sigmoids = gnn(g1, backward_x)\n",
    "            backward_loss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "\n",
    "            # Save Forward Loss and Accuracy\n",
    "            backwardLosses[morphIdx][0] += backward_loss.item()\n",
    "            backwardLosses[morphIdx][1] += torch.eq(zeroTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "        backwardLosses[morphIdx] /= numTestingBatches\n",
    "            \n",
    "print(backwardLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([2.23183527, 0.54564941]), 1: array([1.36457576, 0.71598633]), 2: array([0.65920722, 0.8647168 ]), 3: array([1.41406122, 0.72076172]), 4: array([0.71103541, 0.84983398]), 5: array([1.23965704, 0.70364258])}\n"
     ]
    }
   ],
   "source": [
    "velocityChangeLosses = {}\n",
    "num_samples = 2048\n",
    "num_states = 100\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    velocityChangeLosses[morphIdx] = np.zeros(2)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "                    \n",
    "        state_indeces = np.random.choice(np.arange(states_test[morphIdx].shape[0]), size=num_states)\n",
    "\n",
    "        for state_idx in state_indeces:\n",
    "\n",
    "            # Get new graphs for each iteration\n",
    "            g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "\n",
    "            current_states = states_test[morphIdx][state_idx].repeat(num_samples, 1)\n",
    "            velocities = torch.from_numpy(np.linspace(start=0, stop=2, num=num_samples))\n",
    "            forward_states = current_states\n",
    "            forward_states[:, 0] = velocities\n",
    "            velocities_changed_x = torch.cat((current_states, current_states - forward_states), -1).to(device)\n",
    "            predicted_sigmoids = gnn(g1, velocities_changed_x)\n",
    "            velocity_changed_loss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "            \n",
    "#             print(predicted_sigmoids)\n",
    "            \n",
    "            # Save Forward Loss and Accuracy\n",
    "            velocityChangeLosses[morphIdx][0] += velocity_changed_loss.item()\n",
    "            velocityChangeLosses[morphIdx][1] += torch.eq(zeroTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "        velocityChangeLosses[morphIdx] /= num_states\n",
    "            \n",
    "print(velocityChangeLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
