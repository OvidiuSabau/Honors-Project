{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.autograd\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "import dgl\n",
    "from graphenvs import HalfCheetahGraphEnv\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        hidden_sizes,\n",
    "        with_batch_norm=False,\n",
    "        activation=None\n",
    "    ):\n",
    "        super(Network, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(self.input_size, hidden_sizes[0]))\n",
    "        if with_batch_norm:\n",
    "            self.layers.append(nn.LayerNorm(normalized_shape=(hidden_sizes[0])))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            if with_batch_norm:\n",
    "                self.layers.append(nn.LayerNorm(normalized_shape=(hidden_sizes[i+1])))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_sizes[len(hidden_sizes) - 1], self.output_size))\n",
    "        \n",
    "        if activation is not None:\n",
    "            self.layers.append(activation())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputNetwork,\n",
    "        messageNetwork,\n",
    "        updateNetwork,\n",
    "        outputNetwork,\n",
    "        numMessagePassingIterations,\n",
    "        withInputNetwork = True\n",
    "    ):\n",
    "        \n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "                \n",
    "        self.inputNetwork = inputNetwork\n",
    "        self.messageNetwork = messageNetwork\n",
    "        self.updateNetwork = updateNetwork\n",
    "        self.outputNetwork = outputNetwork\n",
    "        \n",
    "        self.numMessagePassingIterations = numMessagePassingIterations\n",
    "        self.withInputNetwork = withInputNetwork\n",
    "        \n",
    "    def inputFunction(self, nodes):\n",
    "        return {'state' : self.inputNetwork(nodes.data['input'])}\n",
    "    \n",
    "    def messageFunction(self, edges):\n",
    "        \n",
    "        batchSize = edges.src['state'].shape[1]\n",
    "        edgeData = edges.data['feature'].repeat(batchSize, 1).T.unsqueeze(-1)\n",
    "        nodeInput = edges.src['input']\n",
    "        \n",
    "        return {'m' : self.messageNetwork(torch.cat((edges.src['state'], edgeData, nodeInput), -1))}\n",
    "    \n",
    "    def updateFunction(self, nodes):\n",
    "        return {'state': self.updateNetwork(torch.cat((nodes.data['m_hat'], nodes.data['state']), -1))}\n",
    "    \n",
    "    def outputFunction(self, nodes):\n",
    "        \n",
    "        return {'output': self.outputNetwork(nodes.data['state'])}\n",
    "\n",
    "\n",
    "    def forward(self, graph, state):\n",
    "        \n",
    "        self.update_states_in_graph(graph, state)\n",
    "        \n",
    "        if self.withInputNetwork:\n",
    "            graph.apply_nodes(self.inputFunction)\n",
    "        \n",
    "        for messagePassingIteration in range(self.numMessagePassingIterations):\n",
    "            graph.update_all(self.messageFunction, dgl.function.mean('m', 'm_hat'), self.updateFunction)\n",
    "        \n",
    "        graph.apply_nodes(self.outputFunction)\n",
    "        \n",
    "        output = graph.ndata['output']\n",
    "        output = output.squeeze(-1).mean(0)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    def update_states_in_graph(self, graph, state):\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        numGraphFeature = 6\n",
    "        numGlobalStateInformation = 5\n",
    "        numLocalStateInformation = 2\n",
    "        numStateVar = state.shape[1] // 2\n",
    "        globalInformation = torch.cat((state[:, 0:5], state[:, numStateVar:numStateVar+5]), -1)\n",
    "        \n",
    "        numNodes = (numStateVar - 5) // 2\n",
    "\n",
    "        nodeData = torch.empty((numNodes, state.shape[0], numGraphFeature + 2 * numGlobalStateInformation + 2 * numLocalStateInformation)).to(device)\n",
    "        for nodeIdx in range(numNodes):\n",
    "\n",
    "            # Assign global features from graph\n",
    "            nodeData[nodeIdx, :, :6] = graph.ndata['feature'][nodeIdx]\n",
    "            # Assign local state information\n",
    "            nodeData[nodeIdx, :, 16] = state[:, 5 + nodeIdx]\n",
    "            nodeData[nodeIdx, :, 17] = state[:, 5 + numNodes + nodeIdx]\n",
    "            nodeData[nodeIdx, :, 18] = state[:, numStateVar + 5 + nodeIdx]\n",
    "            nodeData[nodeIdx, :, 19] = state[:, numStateVar + 5 + numNodes + nodeIdx]\n",
    "\n",
    "        # Assdign global state information\n",
    "        nodeData[:, :, 6:16] = globalInformation\n",
    "        \n",
    "        if self.withInputNetwork:\n",
    "            graph.ndata['input'] = nodeData        \n",
    "        \n",
    "        else:\n",
    "            graph.ndata['state'] = nodeData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingIdxs = [0, 1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n",
      "NoneType: None\n",
      "NoneType: None\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n",
      "None\n",
      "*************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n",
      "NoneType: None\n"
     ]
    }
   ],
   "source": [
    "states = {}\n",
    "actions = {}\n",
    "rewards = {}\n",
    "next_states = {}\n",
    "dones = {}\n",
    "env = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "\n",
    "    prefix = '../datasets/{}/'.format(morphIdx)\n",
    "    \n",
    "    states[morphIdx] = np.load(prefix + 'states_array.npy')\n",
    "    actions[morphIdx] = np.load(prefix + 'actions_array.npy')\n",
    "    rewards[morphIdx] = np.load(prefix + 'rewards_array.npy')\n",
    "    next_states[morphIdx] = np.load(prefix + 'next_states_array.npy')\n",
    "    dones[morphIdx] = np.load(prefix + 'dones_array.npy')\n",
    "    \n",
    "    env[morphIdx] = HalfCheetahGraphEnv(None)\n",
    "    env[morphIdx].set_morphology(morphIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_train = {}\n",
    "states_test = {}\n",
    "next_states_train = {}\n",
    "next_states_test = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    permutation = np.random.permutation(states[morphIdx].shape[0])\n",
    "    \n",
    "    states[morphIdx] = states[morphIdx][permutation]\n",
    "    next_states[morphIdx] = next_states[morphIdx][permutation]\n",
    "    \n",
    "    states_train[morphIdx] = torch.from_numpy(states[morphIdx][100000:]).float()\n",
    "    states_test[morphIdx] = torch.from_numpy(states[morphIdx][:100000]).float()\n",
    "    \n",
    "    next_states_train[morphIdx] = torch.from_numpy(next_states[morphIdx][100000:]).float()\n",
    "    next_states_test[morphIdx] = torch.from_numpy(next_states[morphIdx][:100000]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [256, 256]\n",
    "\n",
    "inputSize = 20\n",
    "stateSize = 64\n",
    "messageSize = 64\n",
    "outputSize = 1\n",
    "numMessagePassingIterations = 6\n",
    "batch_size = 512\n",
    "with_batch_norm=True\n",
    "numBatchesPerTrainingStep = 1\n",
    "\n",
    "inputNetwork = Network(inputSize, stateSize, hidden_sizes, with_batch_norm)\n",
    "messageNetwork = Network(stateSize + inputSize + 1, messageSize, hidden_sizes, with_batch_norm, nn.Tanh)\n",
    "updateNetwork = Network(stateSize + messageSize, stateSize, hidden_sizes, with_batch_norm)\n",
    "outputNetwork = Network(stateSize, outputSize, hidden_sizes, with_batch_norm, nn.Sigmoid)\n",
    "\n",
    "gnn = GraphNeuralNetwork(inputNetwork, messageNetwork, updateNetwork, outputNetwork, numMessagePassingIterations).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = optim.Adam(itertools.chain(inputNetwork.parameters(), messageNetwork.parameters(), updateNetwork.parameters(), outputNetwork.parameters())\n",
    "                       , lr=lr, weight_decay=0)\n",
    "\n",
    "# lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0, verbose=True, min_lr=1e-5)\n",
    "binaryLoss = nn.BCELoss()\n",
    "\n",
    "zeroTensor = torch.zeros([batch_size]).to(device)\n",
    "oneTensor = torch.ones([batch_size]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrainingBatches = int(np.ceil(states_train[trainingIdxs[0]].shape[0] / batch_size))\n",
    "numTestingBatches = int(np.ceil(states_test[trainingIdxs[0]].shape[0] / batch_size))\n",
    "\n",
    "trainLosses = {}\n",
    "testLosses = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    trainLosses[morphIdx] = []\n",
    "    testLosses[morphIdx] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0\n",
      "Test Idx 0 | F-Loss 0.715 | F-Acc 0.234 | R-Loss 0.664 | R-Acc 0.735\n",
      "Test Idx 1 | F-Loss 0.740 | F-Acc 0.109 | R-Loss 0.649 | R-Acc 0.730\n",
      "Test Idx 2 | F-Loss 0.707 | F-Acc 0.385 | R-Loss 0.667 | R-Acc 0.605\n",
      "Test Idx 3 | F-Loss 0.722 | F-Acc 0.190 | R-Loss 0.659 | R-Acc 0.732\n",
      "Test Idx 4 | F-Loss 0.716 | F-Acc 0.422 | R-Loss 0.667 | R-Acc 0.564\n",
      "Test Idx 5 | F-Loss 0.715 | F-Acc 0.334 | R-Loss 0.663 | R-Acc 0.601\n",
      "Batch 0 in 0.58s\n",
      "Train Idx 0 | F-Loss 0.719 | F-Acc 0.236| R-Loss 0.670 | R-Acc 0.709\n",
      "Train Idx 1 | F-Loss 0.744 | F-Acc 0.100| R-Loss 0.652 | R-Acc 0.744\n",
      "Train Idx 2 | F-Loss 0.709 | F-Acc 0.377| R-Loss 0.671 | R-Acc 0.627\n",
      "Train Idx 3 | F-Loss 0.726 | F-Acc 0.178| R-Loss 0.662 | R-Acc 0.740\n",
      "Train Idx 4 | F-Loss 0.722 | F-Acc 0.408| R-Loss 0.672 | R-Acc 0.566\n",
      "Train Idx 5 | F-Loss 0.718 | F-Acc 0.332| R-Loss 0.668 | R-Acc 0.582\n",
      "Batch 200 in 0.59s\n",
      "Train Idx 0 | F-Loss 0.179 | F-Acc 0.945| R-Loss 0.096 | R-Acc 0.955\n",
      "Train Idx 1 | F-Loss 0.070 | F-Acc 0.988| R-Loss 0.135 | R-Acc 0.945\n",
      "Train Idx 2 | F-Loss 0.062 | F-Acc 0.998| R-Loss 0.125 | R-Acc 0.949\n",
      "Train Idx 3 | F-Loss 0.053 | F-Acc 0.994| R-Loss 0.279 | R-Acc 0.891\n",
      "Train Idx 4 | F-Loss 0.093 | F-Acc 0.984| R-Loss 0.182 | R-Acc 0.920\n",
      "Train Idx 5 | F-Loss 0.089 | F-Acc 0.984| R-Loss 0.158 | R-Acc 0.930\n",
      "Batch 400 in 0.63s\n",
      "Train Idx 0 | F-Loss 0.043 | F-Acc 0.988| R-Loss 0.120 | R-Acc 0.961\n",
      "Train Idx 1 | F-Loss 0.037 | F-Acc 0.992| R-Loss 0.064 | R-Acc 0.979\n",
      "Train Idx 2 | F-Loss 0.029 | F-Acc 0.992| R-Loss 0.061 | R-Acc 0.984\n",
      "Train Idx 3 | F-Loss 0.017 | F-Acc 0.998| R-Loss 0.072 | R-Acc 0.971\n",
      "Train Idx 4 | F-Loss 0.038 | F-Acc 0.994| R-Loss 0.115 | R-Acc 0.967\n",
      "Train Idx 5 | F-Loss 0.043 | F-Acc 0.994| R-Loss 0.083 | R-Acc 0.971\n",
      "Batch 600 in 0.66s\n",
      "Train Idx 0 | F-Loss 0.077 | F-Acc 0.967| R-Loss 0.108 | R-Acc 0.955\n",
      "Train Idx 1 | F-Loss 0.023 | F-Acc 0.994| R-Loss 0.022 | R-Acc 0.992\n",
      "Train Idx 2 | F-Loss 0.020 | F-Acc 0.998| R-Loss 0.035 | R-Acc 0.986\n",
      "Train Idx 3 | F-Loss 0.030 | F-Acc 0.992| R-Loss 0.046 | R-Acc 0.990\n",
      "Train Idx 4 | F-Loss 0.033 | F-Acc 0.992| R-Loss 0.076 | R-Acc 0.975\n",
      "Train Idx 5 | F-Loss 0.026 | F-Acc 1.000| R-Loss 0.089 | R-Acc 0.971\n",
      "Batch 800 in 0.93s\n",
      "Train Idx 0 | F-Loss 0.052 | F-Acc 0.992| R-Loss 0.051 | R-Acc 0.982\n",
      "Train Idx 1 | F-Loss 0.041 | F-Acc 0.990| R-Loss 0.036 | R-Acc 0.988\n",
      "Train Idx 2 | F-Loss 0.047 | F-Acc 0.986| R-Loss 0.044 | R-Acc 0.984\n",
      "Train Idx 3 | F-Loss 0.030 | F-Acc 0.992| R-Loss 0.013 | R-Acc 0.992\n",
      "Train Idx 4 | F-Loss 0.042 | F-Acc 0.988| R-Loss 0.056 | R-Acc 0.982\n",
      "Train Idx 5 | F-Loss 0.020 | F-Acc 1.000| R-Loss 0.023 | R-Acc 0.994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-39dfe45304b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mtrainLosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmorphIdx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moneTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_sigmoids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mforwardLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/honors-project/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/honors-project/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    \n",
    "    print('Starting Epoch {}'.format(epoch))\n",
    "    # Record epoch start time to calculate per epoch time\n",
    "    epoch_t0 = time.time()\n",
    "    \n",
    "    # Randomize the order of traininig examples\n",
    "    for morphIdx in trainingIdxs:\n",
    "        permutation = np.random.permutation(states_train[morphIdx].shape[0])\n",
    "\n",
    "        states_train[morphIdx] = states_train[morphIdx][permutation]\n",
    "        next_states_train[morphIdx] = next_states_train[morphIdx][permutation]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for morphIdx in trainingIdxs:\n",
    "        \n",
    "            testLosses[morphIdx].append(torch.zeros(4))\n",
    "            \n",
    "            for batch_ in range(0, numTestingBatches-1):\n",
    "                \n",
    "                # Get new graphs for each iteration\n",
    "                g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                g2 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                \n",
    "                current_states = states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "                forward_states = next_states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "                forward_x = torch.cat((current_states, forward_states), -1).to(device)\n",
    "                predicted_sigmoids = gnn(g1, forward_x)\n",
    "                forwardLoss = binaryLoss(predicted_sigmoids, oneTensor)\n",
    "                \n",
    "                # Save Forward Loss and Accuracy\n",
    "                testLosses[morphIdx][-1][0] += forwardLoss.item()\n",
    "                testLosses[morphIdx][-1][1] += torch.eq(oneTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "                choices_range = np.arange(states_test[morphIdx].shape[0])\n",
    "                random_indeces = np.random.choice(choices_range, size=current_states.shape[0])\n",
    "                \n",
    "                random_states = states_test[morphIdx][random_indeces]\n",
    "                random_x = torch.cat((current_states, random_states), -1).to(device)\n",
    "                \n",
    "                predicted_sigmoids = gnn(g2, random_x)\n",
    "\n",
    "                randomLoss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "\n",
    "                testLosses[morphIdx][-1][2] += randomLoss.item()\n",
    "                testLosses[morphIdx][-1][3] += torch.eq(zeroTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "            testLosses[morphIdx][-1] /= numTestingBatches\n",
    "    \n",
    "    for morphIdx in trainingIdxs:\n",
    "        print('Test Idx {} | F-Loss {:.3f} | F-Acc {:.3f} | R-Loss {:.3f} | R-Acc {:.3f}'.\n",
    "            format(morphIdx, testLosses[morphIdx][-1][0], testLosses[morphIdx][-1][1], testLosses[morphIdx][-1][2], testLosses[morphIdx][-1][3]))\n",
    "\n",
    "    \n",
    "    for batch in range(0, numTrainingBatches-1, numBatchesPerTrainingStep):\n",
    "                \n",
    "        batch_t0 = time.time()\n",
    "        \n",
    "        for morphIdx in trainingIdxs:\n",
    "            trainLosses[morphIdx].append(torch.zeros(4))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batchOffset in range(numBatchesPerTrainingStep):\n",
    "\n",
    "            if batch + batchOffset >= numTrainingBatches - 1:\n",
    "                break\n",
    "                \n",
    "            for morphIdx in trainingIdxs:\n",
    "                \n",
    "                # Get new graphs for each iteration\n",
    "                g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                g2 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "                \n",
    "                current_states = states_train[morphIdx][(batch + batchOffset) * batch_size:(batch + batchOffset + 1)*batch_size]\n",
    "                forward_states = next_states_train[morphIdx][(batch + batchOffset) * batch_size:(batch + batchOffset + 1)*batch_size]\n",
    "                forward_x = torch.cat((current_states, forward_states), -1).to(device)\n",
    "                \n",
    "                predicted_sigmoids = gnn(g1, forward_x)\n",
    "                forwardLoss = binaryLoss(predicted_sigmoids, oneTensor)\n",
    "                \n",
    "                # Save Forward Loss and Accuracy\n",
    "                trainLosses[morphIdx][-1][0] += forwardLoss.item()\n",
    "                trainLosses[morphIdx][-1][1] += torch.eq(oneTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "                forwardLoss.backward()\n",
    "\n",
    "\n",
    "                choices_range = np.arange(states_train[morphIdx].shape[0])\n",
    "                random_indeces = np.random.choice(choices_range, size=batch_size)\n",
    "\n",
    "                random_states = states_train[morphIdx][random_indeces]\n",
    "                random_x = torch.cat((current_states, random_states), -1).to(device)\n",
    "                \n",
    "                predicted_sigmoids = gnn(g2, random_x)\n",
    "\n",
    "                randomLoss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "                randomLoss.backward()\n",
    "\n",
    "                trainLosses[morphIdx][-1][2] += randomLoss.item()\n",
    "                trainLosses[morphIdx][-1][3] += torch.eq(zeroTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "                        \n",
    "        for morphIdx in trainingIdxs:\n",
    "            trainLosses[morphIdx][-1] /= numBatchesPerTrainingStep\n",
    "\n",
    "        optimizer.step()\n",
    "        batch_time = time.time() - batch_t0\n",
    "\n",
    "        if batch % 200 == 0:\n",
    "            print('Batch {} in {:.2f}s'.format(batch, batch_time))\n",
    "            \n",
    "            for morphIdx in trainingIdxs:\n",
    "                print('Train Idx {} | F-Loss {:.3f} | F-Acc {:.3f}| R-Loss {:.3f} | R-Acc {:.3f}'.\n",
    "                    format(morphIdx, trainLosses[morphIdx][-1][0], trainLosses[morphIdx][-1][1], trainLosses[morphIdx][-1][2], trainLosses[morphIdx][-1][3]))\n",
    "\n",
    "\n",
    "    print('Epoch {} finished in {:.1f}s'.format(epoch, time.time() - epoch_t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gnn.state_dict(), 'validTransition.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([1.15995437, 0.34645049]), 1: array([0.05894224, 0.01470823]), 2: array([0.06268149, 0.01556521]), 3: array([0.05439602, 0.01091159]), 4: array([0.07125656, 0.01589405]), 5: array([0.08082671, 0.01816606])}\n"
     ]
    }
   ],
   "source": [
    "backwardLosses = {}\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    \n",
    "    backwardLosses[morphIdx] = np.zeros(2)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        testLosses[morphIdx].append(torch.zeros(4))\n",
    "\n",
    "        for batch_ in range(0, numTestingBatches-1):\n",
    "\n",
    "            # Get new graphs for each iteration\n",
    "            g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "\n",
    "            current_states = states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "            forward_states = next_states_test[morphIdx][batch_ * batch_size:(batch_+1)*batch_size]\n",
    "            backward_x = torch.cat((forward_states, current_states), -1).to(device)\n",
    "            predicted_sigmoids = gnn(g1, backward_x)\n",
    "            backwardLoss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "\n",
    "            # Save Forward Loss and Accuracy\n",
    "            backwardLosses[morphIdx][0] += backwardLoss.item()\n",
    "            backwardLosses[morphIdx][1] += torch.eq(oneTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "        backwardLosses[morphIdx] /= numTestingBatches\n",
    "            \n",
    "print(backwardLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0.03045581, 0.73291016]), 1: array([0.03045581, 0.56162109]), 2: array([0.03045581, 0.47828125]), 3: array([0.03045581, 0.48162109]), 4: array([0.03045581, 0.46306641]), 5: array([0.03045581, 0.61314453])}\n"
     ]
    }
   ],
   "source": [
    "velocityChangeLosses = {}\n",
    "num_samples = 512\n",
    "num_states = 100\n",
    "\n",
    "for morphIdx in trainingIdxs:\n",
    "    velocityChangeLosses[morphIdx] = np.zeros(2)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        \n",
    "                    \n",
    "        state_indeces = np.random.choice(np.arange(states_test[morphIdx].shape[0]), size=num_states)\n",
    "\n",
    "        for state_idx in state_indeces:\n",
    "\n",
    "            # Get new graphs for each iteration\n",
    "            g1 = env[morphIdx].get_graph()._get_dgl_graph()\n",
    "\n",
    "            current_states = states_test[morphIdx][state_idx].repeat(num_samples, 1)\n",
    "            velocities = torch.from_numpy(np.linspace(start=-2, stop=3, num=num_samples))\n",
    "            forward_states = current_states\n",
    "            forward_states[:, 0] = velocities\n",
    "            velocities_changed_x = torch.cat((current_states, forward_states), -1).to(device)\n",
    "            predicted_sigmoids = gnn(g1, velocities_changed_x)\n",
    "            velocity_changed_loss = binaryLoss(predicted_sigmoids, zeroTensor)\n",
    "\n",
    "            # Save Forward Loss and Accuracy\n",
    "            velocityChangeLosses[morphIdx][0] += forwardLoss.item()\n",
    "            velocityChangeLosses[morphIdx][1] += torch.eq(oneTensor, torch.round(predicted_sigmoids)).sum().item() / float(batch_size)\n",
    "\n",
    "        velocityChangeLosses[morphIdx] /= num_states\n",
    "            \n",
    "print(velocityChangeLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
