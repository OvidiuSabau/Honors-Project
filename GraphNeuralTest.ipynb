{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yOEKHPQ6JsoQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GeX4P3qHJxtE"
   },
   "outputs": [],
   "source": [
    "class QvalueNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_sizes,\n",
    "        input_size_state,\n",
    "        input_size_action,\n",
    "        init_w=3e-3,\n",
    "        activation=torch.relu\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        input_size = input_size_action + input_size_state\n",
    "\n",
    "        self.input_size = input_size\n",
    "        output_size = 1\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        inp_size = input_size\n",
    "        self._layers = []\n",
    "        for i, out_size in enumerate(self.hidden_sizes):\n",
    "            layer = nn.Linear(inp_size, out_size)\n",
    "            self._layers.append(layer)\n",
    "            inp_size = out_size\n",
    "\n",
    "        self.last_layer = nn.Linear(inp_size, output_size)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        input = torch.cat((state, action), dim=-1)\n",
    "\n",
    "        for layer in self._layers:\n",
    "            output = layer(input)\n",
    "            output = self.activation(output)\n",
    "            input = output\n",
    "\n",
    "        output = self.last_layer(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_sizes,\n",
    "        output_size,\n",
    "        input_size,\n",
    "        init_w=3e-3,\n",
    "        activation=torch.relu\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        inp_size = input_size\n",
    "        self._layers = []\n",
    "        for i, out_size in enumerate(self.hidden_sizes):\n",
    "            layer = nn.Linear(inp_size, out_size)\n",
    "            self._layers.append(layer)\n",
    "            inp_size = out_size\n",
    "\n",
    "        self.last_layer = nn.Linear(inp_size, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        input = state\n",
    "\n",
    "        for layer in self._layers:\n",
    "            output = layer(input)\n",
    "            output = self.activation(output)\n",
    "            input = output\n",
    "\n",
    "        output = self.last_layer(input)\n",
    "        output = torch.tanh(output)\n",
    "        return output\n",
    "\n",
    "class MessageNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_sizes,\n",
    "        output_size,\n",
    "        input_size,\n",
    "        init_w=3e-3,\n",
    "        activation=torch.relu\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        inp_size = input_size\n",
    "        self._layers = []\n",
    "        for i, out_size in enumerate(self.hidden_sizes):\n",
    "            layer = nn.Linear(inp_size, out_size)\n",
    "            self._layers.append(layer)\n",
    "            inp_size = out_size\n",
    "\n",
    "        self.last_layer = nn.Linear(inp_size, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        input = state\n",
    "\n",
    "        for layer in self._layers:\n",
    "            output = layer(input)\n",
    "            output = self.activation(output)\n",
    "            input = output\n",
    "\n",
    "        output = self.last_layer(input)\n",
    "        output = torch.tanh(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0LUz3KUKd7i",
    "outputId": "36da702a-d1d7-425a-e5d7-d5d0092f1a18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0087],\n",
       "        [-0.0739],\n",
       "        [-0.0718],\n",
       "        [-0.0580],\n",
       "        [-0.0807]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = QvalueNetwork([200,200], 2, 2)\n",
    "test_state = a = torch.randn(5, 2, dtype=torch.float32)\n",
    "test(test_state, test_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tzMJe8dKqqg",
    "outputId": "0c73e9cc-94bf-4de1-d881-d378c838f810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0714,  0.0080],\n",
       "        [ 0.0627,  0.0290],\n",
       "        [ 0.0568,  0.0162],\n",
       "        [ 0.0258, -0.0194],\n",
       "        [ 0.0603,  0.0247]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_policy = PolicyNetwork([200,200], 2, 2)\n",
    "test_policy(test_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "320MweckPfyG"
   },
   "source": [
    "Test Pybullet Env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cDpdD8bNNxVR"
   },
   "outputs": [],
   "source": [
    "import pybullet\n",
    "import pybullet_envs\n",
    "import gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GCSeva8OWMt",
    "outputId": "5a077016-cabb-40f1-a016-bee1080bd7d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ovi/anaconda3/envs/honors-project/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('HalfCheetahBulletEnv-v0')\n",
    "state = env.reset()\n",
    "state = env.step(np.random.uniform(-1,1,size=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esXPP9DhOd3b",
    "outputId": "cdcc26f1-c6bf-4e97-be97-ee2e3cf6c876"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7.7637396e-04,  0.0000000e+00,  1.0000000e+00,  1.3323595e-02,\n",
       "         0.0000000e+00,  3.7451446e-02,  0.0000000e+00, -2.1979457e-03,\n",
       "        -2.8491670e-01,  1.2014151e-01, -4.8901629e-02, -1.0379118e-01,\n",
       "        -2.6211509e-01, -5.9825581e-01,  2.9288933e-01, -2.1395943e-01,\n",
       "         7.0723198e-02,  6.0370588e-01,  1.1347100e+00, -3.6393487e-01,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00], dtype=float32),\n",
       " 0.6853017254471632,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX5NNfMWPiMj"
   },
   "source": [
    "Set up a simple graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "smkL6FlVPjsI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "40Sg-ZyQPrem"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ovi/anaconda3/envs/honors-project/lib/python3.8/site-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "g = dgl.DGLGraph()\n",
    "g.add_nodes(7)\n",
    "edge_list = [(0,1), (1,2), (2,3), (0,4), (4,5), (5,6)]\n",
    "src, dst = tuple(zip(*edge_list))\n",
    "g.add_edges(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "f9F9f7aBQQC_",
    "outputId": "36656c9c-cb3a-4396-82de-553ecaa9b475"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhGklEQVR4nO3de3BU9f3/8Vd2sySEJAQCIeQCCayGcFdApCpVsVI7VRy1FBwVqhUvrYwgOl6YOl/xQgW04A3b0SIoFO8oOFqlFWpFFLlKEjBCJCEJC5o1CZdls7u/P2jyI2Q3BNjNObvn+ZjJmJz9ZHlHZvLi/Tmf8/nEBQKBgAAAsAib0QUAANCeCD4AgKUQfAAASyH4AACWQvABACyF4AMAWArBBwCwFIIPAGApBB8AwFIIPgCApRB8AABLIfgAAJZC8AEALIXgAwBYCsEHALAUgg8AYCkEHwDAUgg+AIClEHwAAEsh+AAAlkLwAQAsheADAFhKvNEFtJXH41F5ebnq6urk9XrlcDiUkpKi3NxcJSQkGF0eACBKxAUCgYDRRbTG7XartLRULpdLkuT3+5tes9mONawZGRlyOp1KS0szokQAQBQxdfCVlZWpuLhYPp/vpGPtdrsKCwuVl5cX+cIAAFHLtFOdZWVlKioqatbhtcbn86m4uFiSCD8AQEimDD63263i4uIWoTdv3jxt2bJFR44cUZcuXXTNNddo7NixTa83hl9aWhrTngCAoEw51blhwwZVV1e3uP79998rKytLDodD5eXlevDBB/Xwww/L6XQ2G5eZmanhw4e3V7kAgChiuscZPB5P00KWE/Xu3VsOh0OSFBcXp7i4OFVVVbUY53K55PF4IlonACA6mS74ysvLW339+eef17XXXqs77rhDXbp0CdnZVVRUNH3+ww8/6M0335QJm1sAQDsz3VTnpk2btHfv3lbH+Hw+lZSU6JtvvtG1116r+PiWtyqzsrL0448/av78+fr444/l8XhUV1en5OTkSJUOAIgCpuv4vF7vScfY7XYNGDBABw4c0AcffBB0zHvvvafLL79cK1eulMfjkd1u18qVK7VmzRqVlJTI7XbTAQKABZluVWfjPby28Pl8QRfBSNLZZ5+tpKQkeTweNTQ0qEOHDnr77bdVXV2t6upq7du3Tx6PRz169FBmZmbTR6ivO3XqFK4fEQBgINMFX0pKimw2W4tHGdxut7Zu3aoRI0aoQ4cO2rJli9auXasZM2a0eA+bzaaRI0dq7969mjFjhhYtWqT8/Hy9/vrrzcYdOnRI+/btawrCxlDcunWrPv7446avq6urFR8f3yIYg4Vkjx492ELtFLEdHYD2ZLp7fB6PR6tXr24RfD/99JOeeOIJlZWVye/3KyMjQ1deeWWz5/ga2Ww2jRkzpumX5ldffaXKykqNGzfutGoKBAKqq6tr1i0eH4rHf+1yuZScnNxq99j4effu3YPen7QKtqMDYATTBZ8U+jm+tjLyOT6/36+ampo2heSPP/6orl27njQkMzMz1aVLl6YwiAVsRwfAKKYMPrfbrXXr1rXpl+KJ7Ha7Ro0aFRUdQkNDgw4cOBAyGPft26eqqirt27dP9fX1ysjIaFNIpqSkKC4uzugfL6RT3Y5OIvwAhI8pg086tY6gUSz/cmx8sL+1kGz8vKGh4aSLdRo/T0pKatefI9g/arxer1544QVt3rxZ9fX16tmzp2688cYWXXs0/aMGgHmZNvgkpsNO18GDB1udYj3+8w4dOrQpJDMyMtShQ4czri3YNPaRI0f09ttva8yYMerevbs2bNiguXPn6plnnlGPHj2ajWU7OgBnytTBJ7EAIpICgYB++umnNoWky+VSampqm0KyW7dustvtLf68UAuXgrnrrrs0YcIEXXDBBc2un7hwCQBOlemXFKalpWn48OHyeDyqqKhQbW1t05L31NRU5eTk8EvwNMXFxTWdZFFQUNDqWL/frx9++CFoSG7btq1ZSNbU1Cg9Pb1FKA4YMEDdunU7aV01NTXau3evevXqFfT1iooK9e3bt+nro0eP6vDhw+rcufOp/Q8AYEmmD75GCQkJzX7ZoX3ZbDZ1795d3bt318CBA1sd6/V6tX///hYh2dDQcNJur6GhQfPmzdOll16q3NzcFq/7/X7V1tZKkjZv3qwXX3xRr776qkaPHq1Vq1ad/g8IwDKiJvgQPRwOh7KyspSVldXs+pdffhny5A3pWKg99dRTio+P1+233x5yXFlZmS655BIdOnRIPp9Pfr9f8fHxOnz4sDp27Bi2nwNAbCL40G5a244uEAhowYIFcrvdevjhh1t9sD85OVmJiYk6ePBgUwe5du1adenSRUlJScrOzlZ2draysrKaPj/+64yMjJh6JhLAqTH94hbEjtLSUu3cuTPodOdzzz2n3bt3a9asWa12bTabTQUFBcrLy9OCBQs0c+ZMHT16VM8//7x+//vf64cfflBlZaX27t3b4qPxutvtVmZmZtBwPP4aJ3kAsYngQ7sJtarT5XLplltukcPhaLYa9A9/+IMuvvjiZmNPXNW5Z88e3XPPPZo5c6aGDBnS5jqqqqqahWGwgHQ4HCcNxx49elh627m2YC9WmA3Bh3YVLdvRBQIBud3uoIF4/McPP/ygjIyMkwZkamqqqXfTiQQeRYJZEXxoV7G2HZ3X61V1dfVJA1JSyHuPjdd69ux5SsdymRmbT8DMCD60O6ttR9d4ukew+47HB6XL5VJ6enqrAZmdna20tDRTd4/sxQqzI/hgCDqClhoaGrRv376TLs7xer0hV6w2fvTs2dOQ+2fBOvqVK1dq9erVKisr0+jRozVt2rSg32vGjh6xieCDYbgHdHrq6+tbnVatrKxUdXW1OnfuHPKeY+NHenp6WLvHYPdwP//8c8XFxWnTpk3yeDwhg09iL1a0D5ajwTBsR3d6kpOTVVBQ0Oo2cz6fT/v3728RiOvWrWt27fDhw02bDbQWkomJiSetq/EEkRP97Gc/k3TscRaPx9Pqe7hcLnk8Hv7eEVEEHwzHdnThZ7fbm/ZKHTZsWMhxhw4datE1VlRUaP369U1fV1VVKTk5udX7jllZWaqrqwtL7SfuxQqEG8EHWFhSUpKcTqecTmfIMY0blJ84pfrVV19pxYoVTd3k4sWLT2lBS6g/q3EvVunYdPiOHTs0cuTIM3pf4HgEH4BWHb9B+dChQ0OOO9lerG115MgRLV++XH/729+0Zs0aJScnq6am5ozfF2hE8AEIi3A9g/jGG29o/vz5Td1jUlKSNm/erD59+ig1NTUsfwasjeADEBYpKSmy2Wwtpjt9Pl/TKRp+v19Hjx6V3W4PelixzWZTfn6+HA6Hjh49qkAgoIaGBt10003atWuXEhIS1KdPH+Xn57f4b69evdShQ4f2+nERxXicAUBYhNqLdenSpVq2bFmzaxMnTtT111/f4j0a92ItLy/X+PHjtWXLFt1zzz168sknFQgEdODAAe3atUu7d+9u8d+9e/cqMzMzaCj26dNHPXr0MPWD/2g/BB+AsAnnXqw+n08LFy7UyJEj2/RsX0NDg8rLy0MGY319vfLz84MGY35+vlJSUk67bkQXgg9A2Jh5L9b6+vqggdj4kZSUFLJbzM3NjZl9VEHwAQizaNyLNRAIyOVyhewWq6qqlJWV1aJLbPw8IyODadQoQvABCLtY24v16NGjrU6jHj58OGS3mJ+fr06dOhn9I4RdNJ+zSPABiAgr7cVaW1sbNBB37dqlsrIypaamhry3mJubG1WHGcfC3yvBByCirL4Xq9/vV3V1ddO9xBPDcd++fcrOzg7ZLXbr1s0006ix0skTfABgII/Hoz179oScRvV6vSG7xfz8fCUlJbVLnbF0ziLBBwAm5na7Q3aLZWVlSktLC9kt5uTkBN0o4HRqCLZat66uTgsWLNCmTZuUmpqqm266SRdffHGzMWY8Z5HgA4Ao5ff7VVVVFbJb3L9/v3Jzc4N2i3369FHXrl3bNI0a6vnMOXPmyO/3a+rUqdq1a5ceeeQRPfnkk+rdu3ezcWY7Z5HgA4AYdeTIEX3//fchF974/f6Q3WJeXp46duwYckeeI0eOaOLEiXr22WeVnZ0tSZo3b57S09M1efLkZmMbd+Qxyz3d6FlKBAA4JYmJia0eWlxTU9MsCLdv3673339fu3fv1p49e9S1a1fNmTNHXbp0afG9e/fulc1mawo9ScrPz9c333wT9M8y0zmLBB8AWFSXLl00bNiwoIcV+3w+VVZWqqqqKuhxU0eOHGmxsKZTp046fPhwi7EnnrNoNJvRBQAAzMdutys3Nzfk64mJiTp06FCza4cOHVLHjh2Djvd6vWGt70wQfACAkELtUZqdnS2/36/Kysqma7t371avXr1O6X2MQPABAEJqPGfxRImJiRo1apRee+01HTlyREVFRVq/fr0uueSSFmNtNpupDhFmVScAIKRQqzqlY8/xzZ8/X5s3b1ZKSoomTZrU4jk+yXyrOgk+AECrwnnOohkw1QkAaJXT6TztHWDsdrucTmeYKzozBB8AoFVpaWkqLCw85fBr3KvTTNuVSQQfAKAN8vLyVFhY2OYDhs26QbVE8AEA2ig+Pl6PPPKIunTpIpvN1mK1Z+O1zMxMjRo1ypShJ7G4BQDQRlOnTlVCQoLmzJkT1ecsEnwAgJOqrq5W//79VVRUpMzMTKPLOSNMdQIATmru3Lm64YYboj70JDo+AMBJ7N+/XwUFBdq6datycnKMLueM0fEBAFr1l7/8RePHj4+J0JPo+AAAraipqZHT6dSGDRuUn59vdDlhQccHAAhpwYIFuuqqq2Im9CQ6PgBACLW1terbt68+//xznXXWWUaXEzZ0fACAoJ577jldfvnlMRV6Eh0fACCIgwcPqk+fPvr3v/+t/v37G11OWNHxAQBaePHFF3XRRRfFXOhJdHwAgBMcPnxYffv21QcffKChQ4caXU7Y0fEBAJp56aWXNHz48JgMPYmODwBwHI/HI6fTqbfeekvnnXee0eVEBB0fAKDJ4sWL1b9//5gNPYmODwDwP16vVwUFBVq8eLEuvPBCo8uJGDo+AIAkaenSperdu3dMh55ExwcAkOTz+dS/f3+98MILuvTSS40uJ6Lo+AAAev3115Wenq5LLrnE6FIijo4PACzO7/dr8ODBmjNnjq644gqjy4k4Oj4AsLh3331XHTt21C9/+UujS2kXBB8AWFggENCjjz6qmTNnKi4uzuhy2gXBBwAWtmrVKjU0NOjKK680upR2Q/ABgEUFAgHNmjVLM2fOlM1mnTiwzk8KAGjmk08+UW1tra699lqjS2lXBB8AWNSsWbP00EMPyW63G11KuyL4AMCC1qxZo8rKSk2YMMHoUtodwQcAFjRr1iw98MADio+PN7qUdkfwAYDFrFu3Tt9++61uvPFGo0sxBMEHABbz6KOP6v7771eHDh2MLsUQbFkGABby9ddfa9y4cSotLVViYqLR5RiCjg8ALOTRRx/Vvffea9nQk+j4AMAytm7dqssvv1y7du1SUlKS0eUYho4PACziscce0/Tp0y0dehIdHwBYQklJiUaPHq3vvvtOKSkpRpdjKDo+ALCAxx9/XFOnTrV86El0fAAQ87777juNHDlS3333nTp37mx0OYaj4wOAGPfEE0/ozjvvJPT+h44PAGLYnj17NHToUH377bdKT083uhxToOMDgBj25z//Wbfeeiuhdxw6PgCIUZWVlRo4cKBKSkqUkZFhdDmmQfABQIyaNm2aJOnpp582uBJzIfgAIAa5XC7169dP33zzjbKysowux1S4xwcAMeipp57SxIkTCb0g6PgAIEbs2LFDK1as0HXXXacRI0Zo06ZN6tWrl9FlmQ4dHwDEiC+++EIPPfSQ+vXrp5ycHCUnJxtdkikRfAAQI2w2mxISEuT1elVcXKycnBzt27fP6LJMh+ADgBhht9vl9XolSfHx8fq///s/HmMIIt7oAgAA4XH06FEdPXpUnTt31qpVq3TBBRcYXZIpEXwAEGU8Ho/Ky8tVV1cnr9crh8OhlJQUORwO9enTR+vWraPTawWrOgEgSrjdbpWWlsrlckmS/H5/02s227E7VxkZGXI6nUpLSzOixKhA8AFAFCgrK1NxcbF8Pt9Jx9rtdhUWFiovLy/yhUUhpjoBwOTKyspUVFTUrMNrjc/nU3FxsSQRfkGwqhMATMztdqu4uDhk6FVWVuqaa67RvHnzml1vDD+3290OVUYXgg8ATKy0tLTV6c0XXnhBZ511VtDXfD6fSktLI1Va1CL4AMCkPB5P00KWYNauXatOnTppyJAhIce4XC55PJ5IlBe1CD4AMKny8vKQrx06dEivvfaabrnllpO+T0VFRTjLinoEHwCYVF1dXch7e6+++qp+8YtfqHv37q2+h9/vV21tbSTKi1oEHwCYVOP2YyfatWuXNm/erHHjxp3R+1gVjzMAgEk5HI6g17dt2yaXy6Wbb75ZknTkyBH5/X7t2bNH8+fPb/P7WBXBBwAmlZKSIpvN1mK6c+zYsRo9enTT12+//bZcLpfuvPPOFu9hs9mUmpoa8VqjCcEHACaVm5urnTt3triemJioxMTEpq87duyoDh06qHPnzkHfJycnJ2I1RiO2LAMAE9uwYYOqq6tP+/szMzM1fPjwMFYU/VjcAgAm5nQ6ZbfbT+t77Xa7nE5nmCuKfgQfAJhYWlqaCgsLTzn8Gjeq5pSGlgg+ADC5vLw8FRQUqKGhoU3jOZ2hdQQfAESBTz75RMuWLVOPHj1ks9mazt9r1HgtMzNTo0aNIvRaweIWADC52tpaFRQUaNWqVTr33HPl8XhUUVGh2traphPYU1NTlZOTo4SEBKPLNT2CDwBM7oEHHlBVVZUWLVpkdCkxgeADABMrKyvTsGHDtHXrVmVnZxtdTkzgHh8AmNj999+vqVOnEnphRMcHACa1bt06jR8/XiUlJerUqZPR5cQMOj4AMKFAIKBp06bpscceI/TCjOADABP6xz/+oYaGBt1www1GlxJzmOoEAJM5fPiw+vXrpyVLljQ7hQHhQccHACbz9NNPa/jw4YRehNDxAYCJVFdXa+DAgVq/fr369u1rdDkxieADABOZMmWKUlNTNXfuXKNLiVkcRAsAJrF161atWLFCO3bsMLqUmMY9PgAwgUAgoOnTp+tPf/oTRwlFGMEHACawatUq7d27V1OmTDG6lJjHPT4AMJjX69WgQYP01FNP6Ve/+pXR5cQ8Oj4AMNjChQvVq1cvXXHFFUaXYgl0fABgoJqaGhUUFGj16tUaNGiQ0eVYAsEHAAa65557VF9frxdffNHoUiyD4AMAg3z77bcaNWqUtm/frh49ehhdjmUQfABgkGuuuUbnnXee7r//fqNLsRQeYAcAA3z66afauHGjli5danQplsOqTgBoZ36/X9OnT9fs2bOVmJhodDmWQ/ABQDtbsmSJEhIS9Nvf/tboUiyJe3wA0I4OHjyogoICvfnmmzr//PONLseS6PgAoB3NmTNHF110EaFnIDo+AGgnFRUVGjJkiDZu3KjevXsbXY5lEXwA0E4mTZqk7OxsPf7440aXYmk8zgAA7WDDhg365z//qZ07dxpdiuVxjw8AIqzxrL1HHnlEKSkpRpdjeQQfAETYO++8I7fbrZtvvtnoUiDu8QFARHk8Hg0YMEALFy7UZZddZnQ5EB0fAETUs88+q379+hF6JkLHBwARcuDAARUWFuo///mP+vXrZ3Q5+B+CDwAi5I9//KPi4uL0zDPPGF0KjkPwAUAEFBcXa/To0SopKVF6errR5eA43OMDgAi499579cADDxB6JsQD7AAQZh9//LFKSkr01ltvGV0KgqDjA4Aw8vl8mj59uubMmaOEhASjy0EQBB8AhNFLL72k9PR0XX311UaXghBY3AIAYVJbW6uCggKtWrVK5557rtHlIAQ6PgAIk9mzZ2vs2LGEnsnR8QFAGJSVlWnYsGHaunWrsrOzjS4HrSD4ACAMJk6cqH79+unhhx82uhScBMEHAGdo3bp1+s1vfqMdO3aoU6dORpeDk+AeHwCchrq6Oo0dO1affPKJpk2bpscee4zQixJ0fABwGkpKSjRkyBDFxcXJ4XDoyy+/VGFhodFloQ3o+ADgNNTX1ysxMVEej0cHDx7UwIEDtX79eqPLQhuwZRkAnIa6ujp5PB5JUmJiou666y6dc845BleFtiD4AOA0lJeXy+PxKCsrS++++65GjBhhdEloI4IPAELweDwqLy9XXV2dvF6vHA6HUlJSlJubq9zcXF199dVatmyZEhMTjS4Vp4DFLQBwArfbrdLSUrlcLkmS3+9ves1mO7Y0IiMjQ06nU2lpaUaUiDNA8AHAccrKylRcXCyfz3fSsXa7XYWFhcrLy4t8YQgbpjoB4H/KyspUVFTUrMNrjc/nU3FxsSQRflGE4AMAHZveLC4ubhF6DzzwgHbs2CG73S5JSk9P18KFC5tebwy/tLQ0pj2jBMEHAJJKS0tDTm/edtttGjt2bMjv9fl8Ki0t1fDhwyNVHsKIB9gBWJ7H42layHK6XC5X03N9MDeCD4DllZeXt/r64sWLdf311+u+++7Ttm3bQo6rqKgId2mIAKY6AVheXV1dyAUtkydPVm5urhwOh9auXatZs2Zp/vz56tmzZ7Nxfr9ftbW17VEuzhAdHwDL83q9IV8rKChQUlKSHA6HxowZo8LCQm3YsOGU3wfmQfABsDyHw3FK40M9/nyq7wNjEHwALC8lJaVpR5bj1dfXa+PGjTp69Kh8Pp8+/fRTbd++Xeeee26LsTabTampqe1RLs4Q9/gAWF5ubq527tzZ4rrP59OSJUu0d+9e2Ww25eTk6KGHHlJOTk7Q9wl1HebClmUAIGnDhg2qrq4+7e/PzMzkOb4owVQnAEhyOp1Nu7OcKrvdLqfTGeaKECkEHwBISktLU2FhYdB7fa1p3Kia7cqiB8EHAP+Tl5enoqIiNTQ0tGk8pzNEJ+7xAcD/rFmzRhMnTtS6deu0f/9+zuOLUQQfAOjYowuDBw/W/PnzdeWVV0o6todnRUWFamtrm05gT01NVU5OjhISEgyuGKeL4AMASXfccYc8Ho9efvllo0tBhPEcHwDL++ijj7Rq1apWN6BG7KDjA2BpbrdbgwYN0t///ndddtllRpeDdkDwAbC0SZMmKTk5Wc8995zRpaCdMNUJwLJWrFih//73v9q8ebPRpaAd0fEBsKQDBw5o8ODBev3113XhhRcaXQ7aEcEHwHICgYDGjx+v3r17a+7cuUaXg3bGVCcAy1m+fLm2b9+uJUuWGF0KDEDHB8BSqqqqNHToUK1cuVIjRowwuhwYgOADYBmBQEBXXXWVhg4dqlmzZhldDgzCVCcAy1i0aJHKy8v11ltvGV0KDETHB8ASvv/+ew0fPlyrV6/W4MGDjS4HBuJYIgAxz+/365ZbbtH06dMJPRB8AGLfwoULVVdXp3vvvdfoUmACTHUCiGmlpaU6//zz9dlnn6lfv35GlwMToOMDELN8Pp8mT56smTNnEnpoQvABiFlPP/204uPjNXXqVKNLgYkw1QkgJhUVFennP/+5vvzyS+Xn5xtdDkyEjg9AzPF6vZo0aZIeffRRQg8tEHwAYs7s2bPVtWtXTZkyxehSYEJMdQKIKZs2bdLYsWO1ceNG5eTkGF0OTIiOD0DM8Hg8mjRpkubNm0foISQ6PgAx48EHH1RRUZHeeecdxcXFGV0OTIpNqgHEhC+++EIvv/yytmzZQuihVUx1Aoh6hw4d0qRJk/Tss8+qR48eRpcDk2OqE0DUu/vuu+VyubR06VKjS0EUYKoTQFT79NNP9cYbb2jbtm1Gl4IowVQngKhVV1en3/3ud/rrX/+qrl27Gl0OogRTnQCi1m233aaGhga99NJLRpeCKMJUJ4Co9OGHH+qjjz7S1q1bjS4FUYaOD0DUqamp0aBBg/TKK69ozJgxRpeDKEPwAYg6N954o9LS0vTMM88YXQqiEFOdAKLKO++8oy+++EKbN282uhREKTo+AFFj//79Gjx4sN58801dcMEFRpeDKEXwAYgKgUBA1113nfr27asnn3zS6HIQxZjqBBAVli1bppKSEr322mtGl4IoR8cHwPQqKyt1zjnn6IMPPtCwYcOMLgdRjp1bAJhaIBDQrbfeqttvv53QQ1gQfABM7eWXX1ZVVZUeeugho0tBjGCqE4BplZWVacSIEfrXv/6lQYMGGV0OYgQdHwBT8vv9uvnmmzVjxgxCD2FF8AEwDb/fr3379kmSnn/+eR0+fFgzZswwuCrEGh5nAGAaH374ocaNG6e77rpLr7zyitatWye73W50WYgxdHwATGPnzp2Ki4vT/PnzlZiYqLi4OKNLQgwi+ACYRnFxsbxer/x+v6qrq3XBBReI9XcIN4IPgGl89dVXkqSOHTvq0ksv1WeffUbXh7DjcQYA7cbj8ai8vFx1dXXyer1yOBxKSUlRbm6uEhISlJycrE6dOmn58uW6+OKLjS4XMYrgAxBxbrdbpaWlcrlcko6t3mxksx2beMrIyJAknXPOOSxoQUQRfAAiqqysTMXFxfL5fCcda7fbVVhYqLy8vMgXBsvicQYAEVNWVqaioqJmHV5rfD6fiouLJYnwQ8QQfAAiwu12q7i4OGjorV27VsuWLdP+/fvVpUsX3X333RowYICk/x9+aWlpSktLa+eqYQUEH4CIKC0tDTq9uWnTJi1atEj33Xefzj77bNXU1LQY4/P5VFpaquHDh7dHqbAYgg9A2Hk8nqaFLCdaunSpJkyYoH79+kmS0tPTg45zuVzyeDxKSEiIWJ2wJp7jAxB25eXlQa83dnI//fSTpkyZosmTJ2vhwoXyeDxBx1dUVESyTFgUwQcg7Orq6oLe23O73WpoaNDnn3+u2bNna/78+dq1a5eWL1/eYqzf71dtbW17lAuLIfgAhJ3X6w16vXHa8te//rW6du2qzp07a9y4cfr6669P6X2AM0HwAQg7h8MR9HpycrK6devW5m3IQr0PcCYIPgBhl5KS0rQjy4nGjBmj999/X263W/X19Xrvvfc0YsSIFuNsNptSU1MjXSosiFWdAMIuNzdXO3fuDPrahAkTVFtbq9tvv10Oh0MXXnihxo8fH3RsTk5OJMuERbFlGYCI2LBhg6qrq0/7+zMzM3mODxHBVCeAiHA6nae92bTdbpfT6QxzRcAxBB+AiEhLS1NhYeEph1/jRtVsV4ZIIfgARExeXt4phR+nM6A9cI8PQMS19Tw+p9NJp4eII/gAtBuPx6OKigrV1tY2ncCempqqnJwc9uREuyH4AACWwj0+AIClEHwAAEsh+AAAlkLwAQAsheADAFgKwQcAsBSCDwBgKQQfAMBSCD4AgKUQfAAASyH4AACWQvABACyF4AMAWArBBwCwFIIPAGApBB8AwFIIPgCApRB8AABLIfgAAJZC8AEALIXgAwBYyv8DbNQTe6p87AsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "nx_G = g.to_networkx()\n",
    "# Kamada-Kawaii layout usually looks pretty for arbitrary graphs\n",
    "pos = nx.kamada_kawai_layout(nx_G)\n",
    "nx.draw(nx_G, pos, with_labels=True, node_color=[[.7, .7, .7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "zVEqPOEwQQyH"
   },
   "outputs": [],
   "source": [
    "def update_states_in_graph(state, graph):\n",
    "    state_size = state.shape\n",
    "    if len(state_size) == 1:\n",
    "        state = np.expand_dims(state, 0)\n",
    "        state_size = state.shape\n",
    "    batch_size = state_size[0]\n",
    "    root_state = np.expand_dims(np.array(state[:, 0:8]), 0)\n",
    "    joint_states = np.array(state[:, 8:(8+2*6)]).reshape((6, batch_size, 2))\n",
    "    graph.nodes[0].data['root_state'] = torch.from_numpy(root_state)\n",
    "    graph.nodes[list(range(1,7))].data['state'] = torch.from_numpy(joint_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "cz2o_W5ATuTD",
    "outputId": "f8e55200-90b7-4107-9c85-46bea6cb9e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.7637396e-04  0.0000000e+00  1.0000000e+00  1.3323595e-02\n",
      "   0.0000000e+00  3.7451446e-02  0.0000000e+00 -2.1979457e-03\n",
      "  -2.8491670e-01  1.2014151e-01 -4.8901629e-02 -1.0379118e-01\n",
      "  -2.6211509e-01 -5.9825581e-01  2.9288933e-01 -2.1395943e-01\n",
      "   7.0723198e-02  6.0370588e-01  1.1347100e+00 -3.6393487e-01\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n",
      "***\n",
      "[[[ 7.7637396e-04  0.0000000e+00  1.0000000e+00  1.3323595e-02\n",
      "    0.0000000e+00  3.7451446e-02  0.0000000e+00 -2.1979457e-03]]]\n",
      "***\n",
      "[[[-0.2849167   0.12014151]]\n",
      "\n",
      " [[-0.04890163 -0.10379118]]\n",
      "\n",
      " [[-0.2621151  -0.5982558 ]]\n",
      "\n",
      " [[ 0.29288933 -0.21395943]]\n",
      "\n",
      " [[ 0.0707232   0.6037059 ]]\n",
      "\n",
      " [[ 1.13471    -0.36393487]]]\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "a = state[0][np.newaxis, ...]\n",
    "print(a)\n",
    "print('***')\n",
    "root_state = np.array(a[:, 0:8])[np.newaxis, ...]\n",
    "print(root_state)\n",
    "print('***')\n",
    "joint_states = np.array(a[:, 8:(8+2*6)]).reshape((6, 1, 2))\n",
    "print(joint_states)\n",
    "print('***')\n",
    "g.nodes[0].data['root_state'] = torch.from_numpy(root_state)\n",
    "g.nodes[list(range(1,7))].data['state'] = torch.from_numpy(joint_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "3p5q1BcUT0Ak"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root_state': tensor([[[ 7.7637e-04,  0.0000e+00,  1.0000e+00,  1.3324e-02,  0.0000e+00,\n",
      "           3.7451e-02,  0.0000e+00, -2.1979e-03]]]), 'state': tensor([[[0., 0.]]])}\n"
     ]
    }
   ],
   "source": [
    "print(g.nodes[2].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd1CPrSrVcQk"
   },
   "outputs": [],
   "source": [
    "def gcn_message(edges):\n",
    "    # The argument is a batch of edges.\n",
    "    # This computes a (batch of) message called 'msg' using the source node's feature 'h'.\n",
    "    return {'msg' : edges.src['h_out']}\n",
    "\n",
    "def gcn_reduce(nodes):\n",
    "    # The argument is a batch of nodes.\n",
    "    # This computes the new 'h' features by summing received 'msg' in each node's mailbox.\n",
    "    return {'h_in' : torch.sum(nodes.mailbox['msg'], dim=1)}\n",
    "\n",
    "def gcn_message_trainer(edges):\n",
    "    # The argument is a batch of edges.\n",
    "    # This computes a (batch of) message called 'msg' using the source node's feature 'h'.\n",
    "    return {'msg' : edges.src['h_out'], 'msg_t': edges.src['h_out_t']}\n",
    "\n",
    "def gcn_reduce_trainer(nodes):\n",
    "    # The argument is a batch of nodes.\n",
    "    # This computes the new 'h' features by summing received 'msg' in each node's mailbox.\n",
    "    return {'h_in' : torch.sum(nodes.mailbox['msg'], dim=1), 'h_in_t' : torch.sum(nodes.mailbox['msg_t'], dim=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6sRNSyaYWyhs"
   },
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, policy, value, message_root, message):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self._policy = policy\n",
    "        self._value =  value\n",
    "        self._message_root = message_root\n",
    "        self._message = message\n",
    "\n",
    "    def apply_policy(self, nodes):\n",
    "        return {'action': self._policy(torch.cat((nodes.data['state'], nodes.data['h_in']), -1))}\n",
    "    \n",
    "    def apply_value(self, nodes):\n",
    "        return {'value': self._value(torch.cat((nodes.data['state'], nodes.data['h_in']), -1), nodes.data['action'])}\n",
    "\n",
    "    def forward(self, graph, state):\n",
    "        self.update_states_in_graph(state, graph)\n",
    "        graph.nodes[0].data['h_out'] = self._message_root(graph.nodes[0].data['root_state'])\n",
    "        graph.nodes[list(range(1,7))].data['h_out'] = self._message(torch.cat((graph.nodes[list(range(1,7))].data['state'], graph.nodes[list(range(1,7))].data['h_in']), -1))\n",
    "        # trigger message passing on all edges\n",
    "        graph.send(graph.edges(), gcn_message)\n",
    "        # trigger aggregation at all nodes\n",
    "        graph.recv(graph.nodes(), gcn_reduce)\n",
    "\n",
    "        graph.apply_nodes(func=self.apply_policy)\n",
    "        graph.apply_nodes(func=self.apply_value)\n",
    "\n",
    "        # get the result node features\n",
    "        action = graph.nodes[list(range(1,7))].data['action']\n",
    "        value = graph.nodes[list(range(1,7))].data['value']\n",
    "        # perform linear transformation\n",
    "        return action, value\n",
    "    \n",
    "    def reset(self, graph, batch_size):\n",
    "        graph.ndata['h_in'] = torch.zeros(size=(7, batch_size, 16))\n",
    "        graph.ndata['h_out'] = torch.zeros(size=(7, batch_size, 16))\n",
    "\n",
    "    def update_states_in_graph(self, state, graph):\n",
    "        state_size = state.shape\n",
    "        if len(state_size) == 1:\n",
    "            state = np.expand_dims(state, 0)\n",
    "            state_size = state.shape\n",
    "        batch_size = state_size[0]\n",
    "        root_state = np.expand_dims(np.array(state[:, 0:8]), 0)\n",
    "        # joint_states = np.array(state_t[:, 8:(8+2*6)]).reshape((6, batch_size, 2))\n",
    "        state_new = []\n",
    "        for i in range(6):\n",
    "            start_idx = 8 + 2* i\n",
    "            end_idx = 8 + 2*i + 2\n",
    "            state_new.append(state[:, start_idx:end_idx])\n",
    "        joint_states = np.array(state_new)\n",
    "        graph.nodes[0].data['root_state'] = torch.from_numpy(root_state)\n",
    "        graph.nodes[list(range(1,7))].data['state'] = torch.from_numpy(joint_states)\n",
    "\n",
    "class GCNLayerTrainer(nn.Module):\n",
    "    def __init__(self, policy, value, message_root, message, policy_target, value_target):\n",
    "        super(GCNLayerTrainer, self).__init__()\n",
    "        self._policy = policy\n",
    "        self._value =  value\n",
    "        self._message_root = message_root\n",
    "        self._message = message\n",
    "        self._policy_target = policy_target\n",
    "        self._value_target = value_target\n",
    "\n",
    "    def apply_policy(self, nodes):\n",
    "        return {'action': self._policy(torch.cat((nodes.data['state'], nodes.data['h_in']), -1)), 'action_t': self._policy_target(torch.cat((nodes.data['state_t'], nodes.data['h_in_t']), -1))}\n",
    "    \n",
    "    def apply_value(self, nodes):\n",
    "        return {'value': self._value(torch.cat((nodes.data['state'], nodes.data['h_in']), -1), nodes.data['action']), # this is Q(s,mu(s))\n",
    "                'value_t': self._value_target(torch.cat((nodes.data['state_t'], nodes.data['h_in_t'].detach()), -1), nodes.data['action_t'].detach())} # this is Q(s_t, mu(s_t))\n",
    "\n",
    "    def apply_action_value(self, nodes):\n",
    "        return {'value_trans_taken': self._value(torch.cat((nodes.data['state'], nodes.data['h_in']), -1), nodes.data['action_batch'])} # this is Q(s_tm, batch_action)\n",
    "\n",
    "    def forward(self, graph, state_tm, action, state_t):\n",
    "        self._update_states_in_graph(state_tm, action, state_t, graph)\n",
    "        graph.nodes[0].data['h_out'] = self._message_root(graph.nodes[0].data['root_state'])\n",
    "        graph.nodes[list(range(1,7))].data['h_out'] = self._message(torch.cat((graph.nodes[list(range(1,7))].data['state'], graph.nodes[list(range(1,7))].data['h_in']), -1))\n",
    "        # trigger message passing on all edges\n",
    "        graph.send(graph.edges(), gcn_message_trainer)\n",
    "        # trigger aggregation at all nodes\n",
    "        graph.recv(graph.nodes(), gcn_reduce_trainer)\n",
    "\n",
    "        graph.apply_nodes(func=self.apply_policy)\n",
    "        graph.apply_nodes(func=self.apply_value)\n",
    "        graph.apply_nodes(func=self.apply_action_value)\n",
    "\n",
    "        # get the result node features\n",
    "        value_a_batch = graph.nodes[list(range(1,7))].data['value_trans_taken']\n",
    "        action_value = graph.nodes[list(range(1,7))].data['value']\n",
    "        future_value = graph.nodes[list(range(1,7))].data['value_t']\n",
    "        # perform linear transformation\n",
    "        return value_a_batch, action_value, future_value # Q(s_tm, batch_action), Q(s,mu(s)), Q(s_t, mu(s_t))\n",
    "    \n",
    "    def reset(self, graph, batch_size):\n",
    "        graph.ndata['h_in'] = torch.zeros(size=(7, batch_size, 16), dtype=torch.float32)\n",
    "        graph.ndata['h_out'] = torch.zeros(size=(7, batch_size, 16), dtype=torch.float32)\n",
    "        graph.ndata['h_in_t'] = torch.zeros(size=(7, batch_size, 16), dtype=torch.float32)\n",
    "        graph.ndata['h_out_t'] = torch.zeros(size=(7, batch_size, 16), dtype=torch.float32)\n",
    "\n",
    "    # def _update_states_in_graph(self, state_tm1, action, state_t, graph):\n",
    "    #   state_size = state_tm1.shape\n",
    "    #   if len(state_size) == 1:\n",
    "    #     state_tm1 = np.expand_dims(state_tm1, 0)\n",
    "    #     state_size = state_tm1.shape\n",
    "    #   batch_size = state_size[0]\n",
    "    #   root_state = np.expand_dims(np.array(state_tm1[:, 0:8]), 0)\n",
    "    #   joint_states = np.array(state_tm1[:, 8:(8+2*6)]).reshape((6, batch_size, 2))\n",
    "    #   graph.nodes[0].data['root_state'] = torch.from_numpy(root_state)\n",
    "    #   graph.nodes[list(range(1,7))].data['state'] = torch.from_numpy(joint_states)\n",
    "    #   ##############\n",
    "    #   state_size = state_t.shape\n",
    "    #   if len(state_size) == 1:\n",
    "    #     state_t = np.expand_dims(state_t, 0)\n",
    "    #     state_size = state_t.shape\n",
    "    #   batch_size = state_size[0]\n",
    "    #   root_state = np.expand_dims(np.array(state_t[:, 0:8]), 0)\n",
    "    #   joint_states = np.array(state_t[:, 8:(8+2*6)]).reshape((6, batch_size, 2))\n",
    "    #   graph.nodes[0].data['root_state_t'] = torch.from_numpy(root_state)\n",
    "    #   graph.nodes[list(range(1,7))].data['state_t'] = torch.from_numpy(joint_states)\n",
    "    #   #############\n",
    "    #   if len(state_size) == 1:\n",
    "    #     action = np.expand_dims(action,0)\n",
    "    #   #action = action.reshape(6, batch_size, )\n",
    "    #   action = action.transpose()\n",
    "    #   action = np.expand_dims(action, -1)\n",
    "    #   graph.nodes[list(range(1,7))].data['action_batch'] = torch.from_numpy(action)\n",
    "    def _update_states_in_graph(self, state_tm1, action, state_t, graph):\n",
    "        state_size = state_tm1.shape\n",
    "        if len(state_size) == 1:\n",
    "            state_tm1 = np.expand_dims(state_tm1, 0)\n",
    "            state_size = state_tm1.shape\n",
    "        batch_size = state_size[0]\n",
    "        root_state = np.expand_dims(np.array(state_tm1[:, 0:8]), 0)\n",
    "        # joint_states = np.array(state_tm1[:, 8:(8+2*6)]).reshape((6, batch_size, 2))\n",
    "        state_new = []\n",
    "        for i in range(6):\n",
    "            start_idx = 8 + 2* i\n",
    "            end_idx = 8 + 2*i + 2\n",
    "            state_new.append(state_tm1[:, start_idx:end_idx])\n",
    "        joint_states = np.array(state_new)\n",
    "        graph.nodes[0].data['root_state'] = torch.from_numpy(root_state)\n",
    "        graph.nodes[list(range(1,7))].data['state'] = torch.from_numpy(joint_states)\n",
    "        ##############\n",
    "        state_size = state_t.shape\n",
    "        if len(state_size) == 1:\n",
    "            state_t = np.expand_dims(state_t, 0)\n",
    "            state_size = state_t.shape\n",
    "        batch_size = state_size[0]\n",
    "        root_state = np.expand_dims(np.array(state_t[:, 0:8]), 0)\n",
    "        # joint_states = np.array(state_t[:, 8:(8+2*6)]).reshape((6, batch_size, 2))\n",
    "        state_new = []\n",
    "        for i in range(6):\n",
    "            start_idx = 8 + 2* i\n",
    "            end_idx = 8 + 2*i + 2\n",
    "            state_new.append(state_t[:, start_idx:end_idx])\n",
    "        joint_states = np.array(state_new)\n",
    "        graph.nodes[0].data['root_state_t'] = torch.from_numpy(root_state)\n",
    "        graph.nodes[list(range(1,7))].data['state_t'] = torch.from_numpy(joint_states)\n",
    "        #############\n",
    "        if len(state_size) == 1:\n",
    "            action = np.expand_dims(action,0)\n",
    "        #action = action.reshape(6, batch_size, )\n",
    "        action = action.transpose()\n",
    "        action = np.expand_dims(action, -1)\n",
    "        graph.nodes[list(range(1,7))].data['action_batch'] = torch.from_numpy(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "-fHA8X-FcJQc"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 4 required positional arguments: 'policy', 'value', 'message_root', and 'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b920ebc3dd29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGCNLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 4 required positional arguments: 'policy', 'value', 'message_root', and 'message'"
     ]
    }
   ],
   "source": [
    "test = GCNLayer()\n",
    "test.reset(g, 1)\n",
    "a = time.time()\n",
    "for i in range(5):\n",
    "    action, value = test(g, state)\n",
    "print(a - time.time())\n",
    "print(action.shape)\n",
    "print(value.shape)\n",
    "a = np.squeeze(action.data.numpy())\n",
    "env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQ4MMT3ucPki"
   },
   "outputs": [],
   "source": [
    "states_tmp = np.expand_dims(state, 0)\n",
    "states_2 = np.concatenate((states_tmp, states_tmp), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4NPXTzgt1_P"
   },
   "outputs": [],
   "source": [
    "test2 = GCNLayer()\n",
    "g2 = dgl.DGLGraph()\n",
    "g2.add_nodes(7)\n",
    "edge_list = [(0,1), (1,2), (2,3), (0,4), (4,5), (5,6)]\n",
    "src, dst = tuple(zip(*edge_list))\n",
    "g2.add_edges(src, dst)\n",
    "test2.reset(g2, 2)\n",
    "print(states_2.shape)\n",
    "for i in range(5):\n",
    "  action, value = test2(g2, states_2)\n",
    "print(action.shape)\n",
    "print(value.shape)\n",
    "a = np.squeeze(action.data.numpy())\n",
    "a = a.transpose()\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PR-Q-RTCt206"
   },
   "outputs": [],
   "source": [
    "policy_network = PolicyNetwork(hidden_sizes=[200,200], output_size=1, input_size=(2+16))\n",
    "q_network = QvalueNetwork(hidden_sizes=[200,200], input_size_state=2+16, input_size_action=1)\n",
    "policy_network_target = PolicyNetwork(hidden_sizes=[200,200], output_size=1, input_size=(2+16))\n",
    "q_network_target = QvalueNetwork(hidden_sizes=[200,200], input_size_state=2+16, input_size_action=1)\n",
    "message_root_network = MessageNetwork(hidden_sizes=[200, 200], output_size=16, input_size=8)\n",
    "message_network = MessageNetwork(hidden_sizes=[200, 200], output_size=16, input_size=(2+16))\n",
    "\n",
    "def soft_update_from_to(source, target, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - tau) + param.data * tau\n",
    "        )\n",
    "soft_update_from_to(policy_network, policy_network_target, 1.0)\n",
    "soft_update_from_to(q_network, q_network_target, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWUTgwKWAtJm"
   },
   "outputs": [],
   "source": [
    "test_training = GCNLayerTrainer(policy=policy_network, value=q_network, message_root=message_root_network, message=message_network, policy_target=policy_network_target, value_target=q_network_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mU_9eDNzBF7E"
   },
   "outputs": [],
   "source": [
    "states_tmp = np.expand_dims(state, 0)\n",
    "states_2 = np.concatenate([states_tmp] * 32, 0)\n",
    "\n",
    "g_train = dgl.DGLGraph()\n",
    "g_train.add_nodes(7)\n",
    "edge_list = [(0,1), (1,2), (2,3), (0,4), (4,5), (5,6)]\n",
    "src, dst = tuple(zip(*edge_list))\n",
    "g_train.add_edges(src, dst)\n",
    "test_training.reset(g_train, 32)\n",
    "action = np.random.uniform(-1, 1, size=(32,6)).astype(dtype=np.float32)\n",
    "a = time.time()\n",
    "for i in range(5):\n",
    "  value_a_batch, action_value, future_value = test_training(g_train, states_2, action, states_2)\n",
    "print(time.time() - a)\n",
    "print(value_a_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YXfxnbMBp5k"
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "\n",
    "class ReplayBuffer(object, metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    A class used to save and replay data.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def add_sample(self, observation, action, reward, next_observation,\n",
    "                   terminal, **kwargs):\n",
    "        \"\"\"\n",
    "        Add a transition tuple.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def terminate_episode(self):\n",
    "        \"\"\"\n",
    "        Let the replay buffer know that the episode has terminated in case some\n",
    "        special book-keeping has to happen.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def num_steps_can_sample(self, **kwargs):\n",
    "        \"\"\"\n",
    "        :return: # of unique items that can be sampled.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def add_path(self, path):\n",
    "        \"\"\"\n",
    "        Add a path to the replay buffer.\n",
    "        This default implementation naively goes through every step, but you\n",
    "        may want to optimize this.\n",
    "        NOTE: You should NOT call \"terminate_episode\" after calling add_path.\n",
    "        It's assumed that this function handles the episode termination.\n",
    "        :param path: Dict like one outputted by rlkit.samplers.util.rollout\n",
    "        \"\"\"\n",
    "        for i, (\n",
    "                obs,\n",
    "                action,\n",
    "                reward,\n",
    "                next_obs,\n",
    "                terminal,\n",
    "                agent_info,\n",
    "                env_info\n",
    "        ) in enumerate(zip(\n",
    "            path[\"observations\"],\n",
    "            path[\"actions\"],\n",
    "            path[\"rewards\"],\n",
    "            path[\"next_observations\"],\n",
    "            path[\"terminals\"],\n",
    "            path[\"agent_infos\"],\n",
    "            path[\"env_infos\"],\n",
    "        )):\n",
    "            self.add_sample(\n",
    "                observation=obs,\n",
    "                action=action,\n",
    "                reward=reward,\n",
    "                next_observation=next_obs,\n",
    "                terminal=terminal,\n",
    "                agent_info=agent_info,\n",
    "                env_info=env_info,\n",
    "            )\n",
    "        self.terminate_episode()\n",
    "\n",
    "    def add_paths(self, paths):\n",
    "        for path in paths:\n",
    "            self.add_path(path)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def random_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Return a batch of size `batch_size`.\n",
    "        :param batch_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_diagnostics(self):\n",
    "        return {}\n",
    "\n",
    "    def get_snapshot(self):\n",
    "        return {}\n",
    "\n",
    "    def end_epoch(self, epoch):\n",
    "        return\n",
    "  \n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SimpleReplayBuffer(ReplayBuffer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_replay_buffer_size,\n",
    "        observation_dim,\n",
    "        action_dim,\n",
    "        env_info_sizes,\n",
    "    ):\n",
    "        self._observation_dim = observation_dim\n",
    "        self._action_dim = action_dim\n",
    "        self._max_replay_buffer_size = max_replay_buffer_size\n",
    "        self._observations = np.zeros((max_replay_buffer_size, observation_dim))\n",
    "        # It's a bit memory inefficient to save the observations twice,\n",
    "        # but it makes the code *much* easier since you no longer have to\n",
    "        # worry about termination conditions.\n",
    "        self._next_obs = np.zeros((max_replay_buffer_size, observation_dim))\n",
    "        self._actions = np.zeros((max_replay_buffer_size, action_dim))\n",
    "        # Make everything a 2D np array to make it easier for other code to\n",
    "        # reason about the shape of the data\n",
    "        self._rewards = np.zeros((max_replay_buffer_size, 1))\n",
    "        # self._terminals[i] = a terminal was received at time i\n",
    "        self._terminals = np.zeros((max_replay_buffer_size, 1), dtype='uint8')\n",
    "        # Define self._env_infos[key][i] to be the return value of env_info[key]\n",
    "        # at time i\n",
    "        self._env_infos = {}\n",
    "        for key, size in env_info_sizes.items():\n",
    "            self._env_infos[key] = np.zeros((max_replay_buffer_size, size))\n",
    "        self._env_info_keys = env_info_sizes.keys()\n",
    "\n",
    "        self._top = 0\n",
    "        self._size = 0\n",
    "\n",
    "    def add_sample(self, observation, action, reward, next_observation,\n",
    "                   terminal, env_info, **kwargs):\n",
    "        self._observations[self._top] = observation\n",
    "        self._actions[self._top] = action\n",
    "        self._rewards[self._top] = reward\n",
    "        self._terminals[self._top] = terminal\n",
    "        self._next_obs[self._top] = next_observation\n",
    "\n",
    "        for key in self._env_info_keys:\n",
    "            self._env_infos[key][self._top] = env_info[key]\n",
    "        self._advance()\n",
    "\n",
    "    def terminate_episode(self):\n",
    "        pass\n",
    "\n",
    "    def _advance(self):\n",
    "        self._top = (self._top + 1) % self._max_replay_buffer_size\n",
    "        if self._size < self._max_replay_buffer_size:\n",
    "            self._size += 1\n",
    "\n",
    "    def random_batch(self, batch_size):\n",
    "        indices = np.random.randint(0, self._size, batch_size)\n",
    "        batch = dict(\n",
    "            observations=self._observations[indices],\n",
    "            actions=self._actions[indices],\n",
    "            rewards=self._rewards[indices],\n",
    "            terminals=self._terminals[indices],\n",
    "            next_observations=self._next_obs[indices],\n",
    "        )\n",
    "        for key in self._env_info_keys:\n",
    "            assert key not in batch.keys()\n",
    "            batch[key] = self._env_infos[key][indices]\n",
    "        return batch\n",
    "\n",
    "    def rebuild_env_info_dict(self, idx):\n",
    "        return {\n",
    "            key: self._env_infos[key][idx]\n",
    "            for key in self._env_info_keys\n",
    "        }\n",
    "\n",
    "    def batch_env_info_dict(self, indices):\n",
    "        return {\n",
    "            key: self._env_infos[key][indices]\n",
    "            for key in self._env_info_keys\n",
    "        }\n",
    "\n",
    "    def num_steps_can_sample(self):\n",
    "        return self._size\n",
    "\n",
    "    def get_diagnostics(self):\n",
    "        return OrderedDict([\n",
    "            ('size', self._size)\n",
    "        ])\n",
    "\n",
    "\n",
    "from gym.spaces import Discrete\n",
    "from gym.spaces import Box, Discrete, Tuple\n",
    "\n",
    "def get_dim(space):\n",
    "    if isinstance(space, Box):\n",
    "        return space.low.size\n",
    "    elif isinstance(space, Discrete):\n",
    "        return space.n\n",
    "    elif isinstance(space, Tuple):\n",
    "        return sum(get_dim(subspace) for subspace in space.spaces)\n",
    "    elif hasattr(space, 'flat_dim'):\n",
    "        return space.flat_dim\n",
    "    else:\n",
    "        raise TypeError(\"Unknown space: {}\".format(space))\n",
    "\n",
    "class EnvReplayBuffer(SimpleReplayBuffer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_replay_buffer_size,\n",
    "            env,\n",
    "            env_info_sizes=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param max_replay_buffer_size:\n",
    "        :param env:\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self._ob_space = env.observation_space\n",
    "        self._action_space = env.action_space\n",
    "\n",
    "        if env_info_sizes is None:\n",
    "            if hasattr(env, 'info_sizes'):\n",
    "                env_info_sizes = env.info_sizes\n",
    "            else:\n",
    "                env_info_sizes = dict()\n",
    "\n",
    "        super().__init__(\n",
    "            max_replay_buffer_size=max_replay_buffer_size,\n",
    "            observation_dim=get_dim(self._ob_space),\n",
    "            action_dim=get_dim(self._action_space),\n",
    "            env_info_sizes=env_info_sizes\n",
    "        )\n",
    "\n",
    "    def add_sample(self, observation, action, reward, terminal,\n",
    "                   next_observation, **kwargs):\n",
    "        if isinstance(self._action_space, Discrete):\n",
    "            new_action = np.zeros(self._action_dim)\n",
    "            new_action[action] = 1\n",
    "        else:\n",
    "            new_action = action\n",
    "        return super().add_sample(\n",
    "            observation=observation,\n",
    "            action=new_action,\n",
    "            reward=reward,\n",
    "            next_observation=next_observation,\n",
    "            terminal=terminal,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1dV9NrSUaN-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_optimizer(policy, value, message, message_root):\n",
    "  policy_optimizer = torch.optim.Adam( list(policy.parameters()) + list(message.parameters()) + list(message_root.parameters()),lr=1e-3)\n",
    "  value_optimizer = torch.optim.Adam(list(value.parameters()), lr=1e-3) # + list(message.parameters()) + list(message_root.parameters()\n",
    "  \n",
    "  return policy_optimizer, value_optimizer\n",
    "\n",
    "def training(policy, value, message, graph, graph_trainer, replay, policy_optimizer, value_optimizer, policy_target, value_target):\n",
    "        batch = replay.random_batch(32)\n",
    "        rewards = batch['rewards'].astype(dtype=np.float32)\n",
    "        rewards = torch.from_numpy(rewards)\n",
    "        terminals = batch['rewards'].astype(dtype=np.float32)\n",
    "        terminals = torch.from_numpy(terminals)\n",
    "        states = batch['observations'].astype(dtype=np.float32)\n",
    "        actions = batch['actions'].astype(dtype=np.float32)\n",
    "        states_t = batch['next_observations'].astype(dtype=np.float32)\n",
    "\n",
    "        graph_trainer.reset(graph, 32)\n",
    "\n",
    "        for i in range(5):\n",
    "          value_a_batch, action_value, future_value = graph_trainer(graph, states, actions, states_t)\n",
    "        \n",
    "        loss_policy = -action_value.mean()\n",
    "\n",
    "        future_value = future_value\n",
    "        q_target = rewards + (1.-terminals) * 0.95 * future_value\n",
    "        q_target = q_target.detach()\n",
    "        loss_q = torch.sqrt(((value_a_batch - q_target)**2)).mean()\n",
    "\n",
    "\n",
    "        policy_optimizer.zero_grad()\n",
    "        loss_policy.backward(retain_graph=True)\n",
    "        #loss_policy.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        value_optimizer.zero_grad()\n",
    "        loss_q.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        soft_update_from_to(policy_network, policy_target, 0.05)\n",
    "        soft_update_from_to(value, value_target, 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "GbOFiUUWZvyY",
    "outputId": "65e11496-2d31-45b6-d01a-866049414f14"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-94b2b5e3aaa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Collect some random data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HalfCheetahBulletEnv-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvReplayBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_replay_buffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# Collect some random data\n",
    "env = gym.make('HalfCheetahBulletEnv-v0')\n",
    "state = env.reset()\n",
    "\n",
    "replay = EnvReplayBuffer(max_replay_buffer_size=100000, env=env)\n",
    "\n",
    "for i in range(2):\n",
    "  current_state = env.reset()\n",
    "  terminal = 0\n",
    "  while not terminal:\n",
    "    action = np.random.uniform(-1, 1, size=6).astype(dtype=np.float32)\n",
    "    state, reward, terminal, info = env.step(action)\n",
    "    state = state.astype(dtype=np.float32)\n",
    "    reward = float(reward)\n",
    "    terminal = float(terminal)\n",
    "    replay.add_sample(observation=current_state, action=action, reward=reward, terminal=0,\n",
    "                   next_observation=state, env_info = info)\n",
    "    current_state = state\n",
    "\n",
    "policy_network = PolicyNetwork(hidden_sizes=[100,100], output_size=1, input_size=(2+16))\n",
    "q_network = QvalueNetwork(hidden_sizes=[100,100], input_size_state=2+16, input_size_action=1)\n",
    "policy_network_target = PolicyNetwork(hidden_sizes=[100,100], output_size=1, input_size=(2+16))\n",
    "q_network_target = QvalueNetwork(hidden_sizes=[100,100], input_size_state=2+16, input_size_action=1)\n",
    "message_root_network = MessageNetwork(hidden_sizes=[100, 100], output_size=16, input_size=8)\n",
    "message_network = MessageNetwork(hidden_sizes=[100, 100], output_size=16, input_size=(2+16))\n",
    "\n",
    "soft_update_from_to(policy_network, policy_network_target, 1.0)\n",
    "soft_update_from_to(q_network, q_network_target, 1.0)\n",
    "\n",
    "test_training = GCNLayerTrainer(policy=policy_network, value=q_network, message_root=message_root_network, message=message_network, policy_target=policy_network_target, value_target=q_network_target)\n",
    "\n",
    "policy_optimizer, value_optimizer = get_optimizer(value=q_network, message=message_network, message_root=message_root_network, policy=policy_network)\n",
    "\n",
    "g_train = dgl.DGLGraph()\n",
    "g_train.add_nodes(7)\n",
    "edge_list = [(0,1), (1,2), (2,3), (0,4), (4,5), (5,6)]\n",
    "src, dst = tuple(zip(*edge_list))\n",
    "g_train.add_edges(src, dst)\n",
    "\n",
    "\n",
    "\n",
    "execution_network = GCNLayer(policy=policy_network, value=q_network, message_root=message_root_network, message=message_network)\n",
    "execution_graph = dgl.DGLGraph()\n",
    "execution_graph.add_nodes(7)\n",
    "edge_list = [(0,1), (1,2), (2,3), (0,4), (4,5), (5,6)]\n",
    "src, dst = tuple(zip(*edge_list))\n",
    "execution_graph.add_edges(src, dst)\n",
    "execution_network.reset(execution_graph, 1)\n",
    "print(states_2.shape)\n",
    "for i in range(5):\n",
    "  action, value = execution_network(execution_graph, state)\n",
    "print(action.shape)\n",
    "print(value.shape)\n",
    "a = np.squeeze(action.data.numpy())\n",
    "a = a.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf9CyZK8aO8N"
   },
   "outputs": [],
   "source": [
    "training(policy=policy_network, value=q_network, message=message_network, graph=g_train, graph_trainer=test_training, replay=replay, policy_optimizer=policy_optimizer, value_optimizer=value_optimizer, policy_target=policy_network_target, value_target=q_network_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwRBx8QSc25K"
   },
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "  current_state = env.reset()\n",
    "  terminal = 0\n",
    "  reward_all = 0\n",
    "  steps = 0\n",
    "  while not terminal:\n",
    "    execution_network.reset(execution_graph, 1)\n",
    "    for i in range(5):\n",
    "      action, value = execution_network(execution_graph, current_state)\n",
    "    a = np.squeeze(action.data.detach().numpy())\n",
    "    a = a.transpose()\n",
    "    a += np.random.normal(scale=0.2, size=6)\n",
    "    a = np.clip(a, -1,1)\n",
    "    # action = np.random.uniform(-1, 1, size=6).astype(dtype=np.float32)\n",
    "    state, reward, terminal, info = env.step(a)\n",
    "    state = state.astype(dtype=np.float32)\n",
    "    reward = float(reward)\n",
    "    reward_all += reward\n",
    "    steps += 1\n",
    "    terminal = float(terminal)\n",
    "    replay.add_sample(observation=current_state, action=a, reward=reward, terminal=0,\n",
    "                   next_observation=state, env_info = info)\n",
    "    current_state = state\n",
    "  print(reward_all/steps)\n",
    "  for i in range(100):\n",
    "    training(policy=policy_network, value=q_network, message=message_network, graph=g_train, graph_trainer=test_training, replay=replay, policy_optimizer=policy_optimizer, value_optimizer=value_optimizer, policy_target=policy_network_target, value_target=q_network_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIttGsanhuuD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GraphNeuralTest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
